{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from pororo import Pororo\n",
    "from jamo import h2j, j2hcj\n",
    "\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "from koparadigm import Paradigm, prettify\n",
    "\n",
    "from pkg.util import read_problemsheet, write_problemsheet\n",
    "from pkg.parse import *\n",
    "from pkg.words import *\n",
    "from pkg.dataset import ProblemDataset\n",
    "\n",
    "from problems import *\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "vocab_size = 51\n",
    "\n",
    "dir_token = 'tokenization'\n",
    "path_text = os.path.join(dir_token, 'text4token.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spm_train(corpus, vocab_size, prefix=dir_token + '/prob', lowercase=False, \n",
    "              add_space_token=False, user_defined_symbols='<cls>'):\n",
    "    prefix += '_' + str(vocab_size)\n",
    "    unicode_case_folding = 'nmt_nfkc' \n",
    "    if lowercase:\n",
    "        unicode_case_folding += '_cf'\n",
    "        prefix += '_lower'\n",
    "    if add_space_token:\n",
    "        prefix += '_'\n",
    "        user_defined_symbols += ',▁'\n",
    "    \n",
    "    spm.SentencePieceTrainer.train(f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size}\" + \n",
    "        \" --model_type=unigram\" +\n",
    "        \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "        \" --pad_id=0 --pad_piece=<pad>\" + # pad (0)\n",
    "        \" --bos_id=1 --bos_piece=<bos>\" + # begin of sequence (2)\n",
    "        \" --eos_id=2 --eos_piece=<eos>\" + # end of sequence (3)\n",
    "        \" --unk_id=3 --unk_piece=<unk>\" + # unknown (1)\n",
    "        \" --train_extremely_large_corpus=true\"         \n",
    "        f\" --normalization_rule_name={unicode_case_folding}\"\n",
    "        f\" --user_defined_symbols={user_defined_symbols}\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws_predifined 162\n",
      "<cls>,남준,석진,윤기,호석,지민,태형,정국,민영,유정,은지,유나,(가),(나),(다),(라),(마),(바),(사),(아),(자),(차),(카),(타),(파),(하),A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,영,한,두,세,네,다섯,여섯,일곱,여덟,아홉,열,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 512\n",
    "\n",
    "_ws_user_defined = []\n",
    "_ws_user_defined += ws_person + ws_gname + ws_variable #+ ws_vehicle + ws_location + ws_stationary + ws_food + ws_flower\n",
    "#_ws_user_defined += ws_animal + ws_unit + ws_subject + ws_color # + ws_ball + ws_container + ws_sport + ws_color\n",
    "_ws_user_defined += pos_mdn\n",
    "_ws_user_defined += [str(i) for i in range(100)]\n",
    "print('ws_predifined', len(_ws_user_defined))\n",
    "\n",
    "user_defined_symbols = '<cls>'\n",
    "user_defined_symbols += ',' + ','.join(_ws_user_defined)\n",
    "\n",
    "print(user_defined_symbols)\n",
    "\n",
    "for add_space_token in (True, False):\n",
    "    spm_train(corpus, vocab_size, lowercase=False, add_space_token=add_space_token, \n",
    "              user_defined_symbols=user_defined_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws_predifined 188\n",
      "<cls>,남준,석진,윤기,호석,지민,태형,정국,민영,유정,은지,유나,(가),(나),(다),(라),(마),(바),(사),(아),(자),(차),(카),(타),(파),(하),A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,km,m,cm,mm,kg,g,ton,ml,liter,마리,송이,대,그루,권,자루,쪽,살,켤레,줄,벌,타,조각,리터,점,점수,가지,영,한,두,세,네,다섯,여섯,일곱,여덟,아홉,열,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 768\n",
    "\n",
    "_ws_user_defined = []\n",
    "_ws_user_defined += ws_person + ws_gname + ws_variable\n",
    "_ws_user_defined += ws_unit\n",
    "_ws_user_defined += pos_mdn\n",
    "_ws_user_defined += [str(i) for i in range(100)]\n",
    "print('ws_predifined', len(_ws_user_defined))\n",
    "\n",
    "user_defined_symbols = '<cls>'\n",
    "user_defined_symbols += ',' + ','.join(_ws_user_defined)\n",
    "\n",
    "print(user_defined_symbols)\n",
    "\n",
    "for add_space_token in (True, False):\n",
    "    spm_train(corpus, vocab_size, lowercase=False, add_space_token=add_space_token, \n",
    "              user_defined_symbols=user_defined_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intentional error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text Data for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pororo Factory Base, task_config TaskConfig(task='gec', lang='ko', n_model='charbert.base.ko.spacing')\n",
      "Init PororoGecFactory\n",
      "gec\n",
      "ko\n",
      "None\n",
      "As of now, this beta model tries to correct spacing errors in Korean text.\n"
     ]
    }
   ],
   "source": [
    "from pororo import Pororo\n",
    "gec, gec_factory = Pororo(task=\"gec\", lang=\"kr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [P1_1_1, P1_1_2, P1_1_3, P1_1_4, P1_1_5, P1_1_6, P1_1_7, P1_1_8, P1_1_9, P1_1_10, P1_1_11, P1_1_12, \n",
    "            P1_2_1, P1_2_2, P1_3_1, P1_4_1, \n",
    "            P2_1_1, P2_2_2, P2_3_1, \n",
    "            P3_1_1, P3_2_1, P3_2_2, P3_3_1, \n",
    "            P4_1_1, P4_2_1, P4_2_2, P4_3_1, \n",
    "            P5_1_1, P5_2_1, P5_3_1,\n",
    "            P6_1_1, P6_3_1, P6_4_1,\n",
    "            P7_1_1, P7_1_2, P7_3_1,\n",
    "            P8_1_1, P8_2_1, P8_3_1, \n",
    "            P9_1_1, P9_2_1, P9_2_2, P9_3_1, P9_3_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ProblemDataset(problems, batch_size=64, gec=gec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4490.60777592659\n",
      "num of lines 1408000\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "start = time.time()\n",
    "count = 0\n",
    "for i in range(500):\n",
    "    for j in range(len(ds)):\n",
    "        batch = ds[j]\n",
    "        texts.extend(batch['text'])\n",
    "        count += len(batch['text'])\n",
    "print(time.time() - start)\n",
    "print('num of lines', count)\n",
    "with open(path_text, 'w') as f:\n",
    "    f.write('\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
