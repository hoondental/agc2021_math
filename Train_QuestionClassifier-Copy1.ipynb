{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./log/question_classifier/20210617_235128\n"
     ]
    }
   ],
   "source": [
    "device_id = 1\n",
    "device = 'cuda:' + str(device_id)\n",
    "#device = 'cpu'\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "log_dir = os.path.join('./log/question_classifier', time.strftime('%Y%m%d_%H%M%S', time.localtime(time.time())))\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.parse import *\n",
    "from pkg.words import *\n",
    "from problems import *\n",
    "\n",
    "problems = [P1_1_1, P1_1_2, P1_1_3, P1_1_4, P1_1_5, P1_1_6, P1_1_7, P1_1_8, P1_1_9, P1_1_10, P1_1_11, P1_1_12, \n",
    "            P1_2_1, P1_2_2, P1_3_1, P1_4_1, \n",
    "            P2_1_1, P2_2_2, P2_3_1, \n",
    "            P3_1_1, P3_2_1, P3_2_2, P3_3_1, \n",
    "            P4_1_1, P4_2_1, P4_2_2, P4_3_1, \n",
    "            P5_1_1, P5_2_1, P5_3_1,\n",
    "            P6_1_1, P6_3_1, P6_4_1,\n",
    "            P7_1_1, P7_1_2, P7_3_1,\n",
    "            P8_1_1, P8_2_1, P8_3_1, \n",
    "            P9_1_1, P9_2_1, P9_2_2, P9_3_1, P9_3_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dhlee/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from pkg.trainer_question_classifier import Hyper as hp\n",
    "\n",
    "# dataset\n",
    "hp.add_bos = False\n",
    "hp.add_eos = False\n",
    "\n",
    "hp.batch_size = None\n",
    "hp.batch_type = 'normal'\n",
    "hp.ds_batch_size = 256\n",
    "\n",
    "# train\n",
    "hp.num_workers = 2\n",
    "\n",
    "hp.steps_log = 10\n",
    "hp.steps_eval = 50\n",
    "hp.steps_save = 10000\n",
    "\n",
    "hp.weight_decay = 0.000001\n",
    "hp.initial_lr = 0.0001\n",
    "hp.final_lr = 0.00001\n",
    "hp.lr_decay_factor = 0.99\n",
    "hp.lr_patience = 300\n",
    "hp.ema = 0.99\n",
    "hp.grad_norm_max = 10.0\n",
    "\n",
    "hp.adam_alpha = 2e-4\n",
    "hp.adam_betas = (0.5, 0.9)\n",
    "hp.adam_eps = 1e-6\n",
    "\n",
    "hp.vocab_size = 512\n",
    "hp.add_space_token = False\n",
    "\n",
    "hp.num_problems = len(problems)\n",
    "hp.problems = problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816000 2252800 563200\n"
     ]
    }
   ],
   "source": [
    "from pkg.dataset import ProblemDataset, QuestionDataset, read_questions, write_questions\n",
    "\n",
    "dir_question = 'data/question'\n",
    "_prefix = 'question_'\n",
    "_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "path_questions = [os.path.join(dir_question, _prefix + str(i) + '.txt') for i in _ids]\n",
    "metas = [read_questions(path) for path in path_questions]\n",
    "meta = []\n",
    "for m in metas:\n",
    "    meta += m\n",
    "len_train = int(len(meta) * 0.8)\n",
    "meta_train = meta[:len_train]\n",
    "meta_val = meta[len_train:]\n",
    "print(len(meta), len(meta_train), len(meta_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization/prob_512.model\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "from pkg.vocab import Vocab, CharVocab, SPVocab\n",
    "dir_token = 'tokenization'\n",
    "filename = 'prob'\n",
    "filename += '_' + str(hp.vocab_size)\n",
    "if hp.add_space_token:\n",
    "    filename += '_'\n",
    "filename += '.model'\n",
    "path_model = os.path.join(dir_token, filename)\n",
    "print(path_model)\n",
    "vocab = SPVocab(path_model)\n",
    "print(vocab.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653.4771099090576\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dsTrain = QuestionDataset(meta_train, vocab, batch_size=hp.ds_batch_size)\n",
    "dsVal = QuestionDataset(meta_val, vocab, batch_size=hp.ds_batch_size)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.models.model import QuestionClassifier\n",
    "from pkg.models.config import Config\n",
    "from pkg.models.extractor import AverageExtractor, RNNExtractor\n",
    "from pkg.models.encoders_conv import ConvEncoder, HighwayEncoder\n",
    "from pkg.models.embedding import Embed, Regressor\n",
    "\n",
    "cfg = QuestionClassifier.default_config()\n",
    "cfg.text_embed.num_symbols = hp.vocab_size\n",
    "cfg.regressor.num_symbols = hp.num_problems\n",
    "cfg.extractor = RNNExtractor.default_config()\n",
    "model = cfg.create_object().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- hp -------------------\n",
      "\n",
      "__module__ pkg.trainer\n",
      "steps_log 10\n",
      "steps_eval 50\n",
      "steps_save 10000\n",
      "save_model_only True\n",
      "max_lr 0.001\n",
      "base_lr 0.001\n",
      "lr_decay_factor 0.99\n",
      "lr_patience 300\n",
      "scheduler_mode triangular2\n",
      "step_size_up 10\n",
      "ema 0.99\n",
      "max_grad_norm 10.0\n",
      "adam_alpha 0.0002\n",
      "adam_betas (0.5, 0.9)\n",
      "adam_eps 1e-06\n",
      "weight_decay 1e-06\n",
      "__dict__ <attribute '__dict__' of 'Hyper' objects>\n",
      "__weakref__ <attribute '__weakref__' of 'Hyper' objects>\n",
      "__doc__ None\n",
      "add_bos False\n",
      "add_eos False\n",
      "batch_size None\n",
      "batch_type normal\n",
      "ds_batch_size 256\n",
      "num_workers 4\n",
      "initial_lr 0.0001\n",
      "final_lr 1e-05\n",
      "grad_norm_max 10.0\n",
      "vocab_size 512\n",
      "add_space_token False\n",
      "num_problems 44\n",
      "problems [<class 'problems.P1_1_1'>, <class 'problems.P1_1_2'>, <class 'problems.P1_1_3'>, <class 'problems.P1_1_4'>, <class 'problems.P1_1_5'>, <class 'problems.P1_1_6'>, <class 'problems.P1_1_7'>, <class 'problems.P1_1_8'>, <class 'problems.P1_1_9'>, <class 'problems.P1_1_10'>, <class 'problems.P1_1_11'>, <class 'problems.P1_1_12'>, <class 'problems.P1_2_1'>, <class 'problems.P1_2_2'>, <class 'problems.P1_3_1'>, <class 'problems.P1_4_1'>, <class 'problems.P2_1_1'>, <class 'problems.P2_2_2'>, <class 'problems.P2_3_1'>, <class 'problems.P3_1_1'>, <class 'problems.P3_2_1'>, <class 'problems.P3_2_2'>, <class 'problems.P3_3_1'>, <class 'problems.P4_1_1'>, <class 'problems.P4_2_1'>, <class 'problems.P4_2_2'>, <class 'problems.P4_3_1'>, <class 'problems.P5_1_1'>, <class 'problems.P5_2_1'>, <class 'problems.P5_3_1'>, <class 'problems.P6_1_1'>, <class 'problems.P6_3_1'>, <class 'problems.P6_4_1'>, <class 'problems.P7_1_1'>, <class 'problems.P7_1_2'>, <class 'problems.P7_3_1'>, <class 'problems.P8_1_1'>, <class 'problems.P8_2_1'>, <class 'problems.P8_3_1'>, <class 'problems.P9_1_1'>, <class 'problems.P9_2_1'>, <class 'problems.P9_2_2'>, <class 'problems.P9_3_1'>, <class 'problems.P9_3_2'>]\n",
      "--------------------- model cfg -------------------\n",
      "\n",
      "************** model 0 ****************\n",
      "\n",
      "cls : <class 'pkg.models.model.QuestionClassifier'>\n",
      "text_embed : \n",
      "    cls : <class 'pkg.models.embedding.Embed'>\n",
      "    num_symbols : 512\n",
      "    embedding_dim : 128\n",
      "    num_upsample : 1\n",
      "    padding_idx : None\n",
      "encoders : \n",
      "    cls : <class 'pkg.models.config.ConfigList'>\n",
      "    0 : \n",
      "        cls : <class 'pkg.models.encoders_conv.HighwayEncoder'>\n",
      "        in_dim : 128\n",
      "        out_dim : 128\n",
      "        kernel_size : 3\n",
      "        stride : 1\n",
      "        num_blocks : 1\n",
      "        num_layers : 5\n",
      "        dilation_base : 1\n",
      "        dilation_power : 1\n",
      "        dropout_rate : 0.0\n",
      "        padding : same\n",
      "        groups : 1\n",
      "        bias : True\n",
      "        normalization : batch\n",
      "extractor : \n",
      "    cls : <class 'pkg.models.extractor.RNNExtractor'>\n",
      "    in_dim : 128\n",
      "    out_dim : 1024\n",
      "    num_layers : 1\n",
      "    rnn : gru\n",
      "    bidirectional : True\n",
      "regressor : \n",
      "    cls : <class 'pkg.models.embedding.Regressor'>\n",
      "    num_symbols : 44\n",
      "    embedding_dim : 1024\n",
      "    hidden_dims : \n",
      "        cls : <class 'pkg.models.config.ConfigList'>\n",
      "    external_embed : None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pkg.trainer_question_classifier import Trainer_QuestionClassifier\n",
    "\n",
    "trainer = Trainer_QuestionClassifier(model, dsTrain, dsVal, hp=hp, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10 train: 0.08227705955505371 elapsed, loss: 3.6839209\n",
      "step: 20 train: 0.08821368217468262 elapsed, loss: 3.4062886\n",
      "step: 30 train: 0.07718873023986816 elapsed, loss: 2.997042\n",
      "step: 40 train: 0.07712173461914062 elapsed, loss: 2.467427\n",
      "step: 50 train: 0.07581782341003418 elapsed, loss: 2.2078242\n",
      "step: 60 train: 0.07978153228759766 elapsed, loss: 1.7570738\n",
      "step: 70 train: 0.07337403297424316 elapsed, loss: 1.6777754\n",
      "step: 80 train: 0.08415603637695312 elapsed, loss: 1.2404146\n",
      "step: 90 train: 0.08092045783996582 elapsed, loss: 1.2091846\n",
      "step: 100 train: 0.08152079582214355 elapsed, loss: 1.0216315\n",
      "step: 110 train: 0.08567500114440918 elapsed, loss: 0.89112204\n",
      "step: 120 train: 0.0851280689239502 elapsed, loss: 0.8294809\n",
      "step: 130 train: 0.08115625381469727 elapsed, loss: 0.7105387\n",
      "step: 140 train: 0.07694387435913086 elapsed, loss: 0.6105763\n",
      "step: 150 train: 0.07736968994140625 elapsed, loss: 0.5314468\n",
      "step: 160 train: 0.08264875411987305 elapsed, loss: 0.42523825\n",
      "step: 170 train: 0.07970857620239258 elapsed, loss: 0.36261094\n",
      "step: 180 train: 0.07573294639587402 elapsed, loss: 0.22978282\n",
      "step: 190 train: 0.08226585388183594 elapsed, loss: 0.19755556\n",
      "step: 200 train: 0.07731366157531738 elapsed, loss: 0.15911847\n",
      "step: 210 train: 0.08171606063842773 elapsed, loss: 0.14275426\n",
      "step: 220 train: 0.0790262222290039 elapsed, loss: 0.11974129\n",
      "step: 230 train: 0.0791018009185791 elapsed, loss: 0.10009211\n",
      "step: 240 train: 0.07615375518798828 elapsed, loss: 0.100080565\n",
      "step: 250 train: 0.08396077156066895 elapsed, loss: 0.06487994\n",
      "step: 260 train: 0.07584190368652344 elapsed, loss: 0.054820113\n",
      "step: 270 train: 0.07746458053588867 elapsed, loss: 0.09141937\n",
      "step: 280 train: 0.07859063148498535 elapsed, loss: 0.06177437\n",
      "step: 290 train: 0.0718381404876709 elapsed, loss: 0.057211824\n",
      "step: 300 train: 0.07572126388549805 elapsed, loss: 0.060741365\n",
      "step: 310 train: 0.0776815414428711 elapsed, loss: 0.047900032\n",
      "step: 320 train: 0.07691431045532227 elapsed, loss: 0.048284132\n",
      "step: 330 train: 0.07918024063110352 elapsed, loss: 0.03265014\n",
      "step: 340 train: 0.07544660568237305 elapsed, loss: 0.033064775\n",
      "step: 350 train: 0.08202505111694336 elapsed, loss: 0.0229097\n",
      "step: 360 train: 0.08256268501281738 elapsed, loss: 0.028292362\n",
      "step: 370 train: 0.07927680015563965 elapsed, loss: 0.00859547\n",
      "step: 380 train: 0.07930660247802734 elapsed, loss: 0.010965238\n",
      "step: 390 train: 0.07915973663330078 elapsed, loss: 0.013749894\n",
      "step: 400 train: 0.0808110237121582 elapsed, loss: 0.0064972546\n",
      "step: 410 train: 0.07648849487304688 elapsed, loss: 0.009297336\n",
      "step: 420 train: 0.08164238929748535 elapsed, loss: 0.0039468044\n",
      "step: 430 train: 0.08121204376220703 elapsed, loss: 0.0031997275\n",
      "step: 440 train: 0.07684707641601562 elapsed, loss: 0.0035805702\n",
      "step: 450 train: 0.07532453536987305 elapsed, loss: 0.0019789615\n",
      "step: 460 train: 0.07696819305419922 elapsed, loss: 0.0017419143\n",
      "step: 470 train: 0.07823872566223145 elapsed, loss: 0.0015532821\n",
      "step: 480 train: 0.08083057403564453 elapsed, loss: 0.0012185641\n",
      "step: 490 train: 0.07909703254699707 elapsed, loss: 0.0018331251\n",
      "step: 500 train: 0.07762813568115234 elapsed, loss: 0.0011824761\n",
      "step: 510 train: 0.07851243019104004 elapsed, loss: 0.0017176555\n",
      "step: 520 train: 0.08183431625366211 elapsed, loss: 0.000876869\n",
      "step: 530 train: 0.08395528793334961 elapsed, loss: 0.0006982962\n",
      "step: 540 train: 0.07353544235229492 elapsed, loss: 0.0020679145\n",
      "step: 550 train: 0.0787954330444336 elapsed, loss: 0.0005511858\n",
      "step: 560 train: 0.07622456550598145 elapsed, loss: 0.0006391849\n",
      "step: 570 train: 0.07372403144836426 elapsed, loss: 0.00054878066\n",
      "step: 580 train: 0.08254742622375488 elapsed, loss: 0.00038848442\n",
      "step: 590 train: 0.07537841796875 elapsed, loss: 0.00033971894\n",
      "step: 600 train: 0.09184551239013672 elapsed, loss: 0.0002898734\n",
      "step: 610 train: 0.08078217506408691 elapsed, loss: 0.00023340469\n",
      "step: 620 train: 0.07563900947570801 elapsed, loss: 0.0002239356\n",
      "step: 630 train: 0.07895255088806152 elapsed, loss: 0.00014860649\n",
      "step: 640 train: 0.08364582061767578 elapsed, loss: 0.000121028694\n",
      "step: 650 train: 0.08353114128112793 elapsed, loss: 0.00011818133\n",
      "step: 660 train: 0.07594633102416992 elapsed, loss: 0.00013921454\n",
      "step: 670 train: 0.0797872543334961 elapsed, loss: 6.63923e-05\n",
      "step: 680 train: 0.07671427726745605 elapsed, loss: 0.0028095446\n",
      "step: 690 train: 0.07731413841247559 elapsed, loss: 0.00012313474\n",
      "step: 700 train: 0.07425498962402344 elapsed, loss: 0.000104143444\n",
      "step: 710 train: 0.0820615291595459 elapsed, loss: 8.049893e-05\n",
      "step: 720 train: 0.08260560035705566 elapsed, loss: 0.00033909074\n",
      "step: 730 train: 0.07498002052307129 elapsed, loss: 0.000111609916\n",
      "step: 740 train: 0.07364678382873535 elapsed, loss: 0.00012198082\n",
      "step: 750 train: 0.08142781257629395 elapsed, loss: 0.00013300803\n",
      "step: 760 train: 0.07534003257751465 elapsed, loss: 9.086283e-05\n",
      "step: 770 train: 0.08078479766845703 elapsed, loss: 6.306188e-05\n",
      "step: 780 train: 0.07613039016723633 elapsed, loss: 5.7160956e-05\n",
      "step: 790 train: 0.08108377456665039 elapsed, loss: 4.9189184e-05\n",
      "step: 800 train: 0.07842755317687988 elapsed, loss: 5.3059608e-05\n",
      "step: 810 train: 0.07983970642089844 elapsed, loss: 9.95433e-05\n",
      "step: 820 train: 0.07734322547912598 elapsed, loss: 5.1494753e-05\n",
      "step: 830 train: 0.08494448661804199 elapsed, loss: 4.6741232e-05\n",
      "step: 840 train: 0.0780341625213623 elapsed, loss: 5.899242e-05\n",
      "step: 850 train: 0.08520221710205078 elapsed, loss: 3.4535995e-05\n",
      "step: 860 train: 0.08391475677490234 elapsed, loss: 4.215957e-05\n",
      "step: 870 train: 0.07572412490844727 elapsed, loss: 2.9167903e-05\n",
      "step: 880 train: 0.07727742195129395 elapsed, loss: 3.1182753e-05\n",
      "step: 890 train: 0.07788443565368652 elapsed, loss: 2.0937121e-05\n",
      "step: 900 train: 0.08455133438110352 elapsed, loss: 2.4003228e-05\n",
      "step: 910 train: 0.07670998573303223 elapsed, loss: 2.4434015e-05\n",
      "step: 920 train: 0.08562183380126953 elapsed, loss: 0.0003078479\n",
      "step: 930 train: 0.08600521087646484 elapsed, loss: 3.965971e-05\n",
      "step: 940 train: 0.08344221115112305 elapsed, loss: 3.474523e-05\n",
      "step: 950 train: 0.09342694282531738 elapsed, loss: 4.384423e-05\n",
      "step: 960 train: 0.08004045486450195 elapsed, loss: 2.2521835e-05\n",
      "step: 970 train: 0.08517789840698242 elapsed, loss: 1.6583304e-05\n",
      "step: 980 train: 0.08088302612304688 elapsed, loss: 3.3650773e-05\n",
      "step: 990 train: 0.0758512020111084 elapsed, loss: 1.6196711e-05\n",
      "step: 1000 train: 0.07417774200439453 elapsed, loss: 1.4282975e-05\n",
      "step: 1010 train: 0.09139013290405273 elapsed, loss: 1.2591313e-05\n",
      "step: 1020 train: 0.07692170143127441 elapsed, loss: 1.4440816e-05\n",
      "step: 1030 train: 0.08100271224975586 elapsed, loss: 2.1971711e-05\n",
      "step: 1040 train: 0.08168530464172363 elapsed, loss: 1.1227885e-05\n",
      "step: 1050 train: 0.0725245475769043 elapsed, loss: 1.5530943e-05\n",
      "step: 1060 train: 0.08405756950378418 elapsed, loss: 1.4511792e-05\n",
      "step: 1070 train: 0.08356571197509766 elapsed, loss: 1.3557407e-05\n",
      "step: 1080 train: 0.0774698257446289 elapsed, loss: 1.1250204e-05\n",
      "step: 1090 train: 0.08004164695739746 elapsed, loss: 8.191817e-06\n",
      "step: 1100 train: 0.07482671737670898 elapsed, loss: 1.0351399e-05\n",
      "step: 1110 train: 0.07572579383850098 elapsed, loss: 7.758333e-06\n",
      "step: 1120 train: 0.07811141014099121 elapsed, loss: 8.916382e-06\n",
      "step: 1130 train: 0.0766456127166748 elapsed, loss: 8.160097e-06\n",
      "step: 1140 train: 0.07992911338806152 elapsed, loss: 7.874933e-06\n",
      "step: 1150 train: 0.07892727851867676 elapsed, loss: 6.9047737e-06\n",
      "step: 1160 train: 0.0821378231048584 elapsed, loss: 6.5513027e-06\n",
      "step: 1170 train: 0.07653141021728516 elapsed, loss: 0.00014883271\n",
      "step: 1180 train: 0.08115696907043457 elapsed, loss: 1.0974123e-05\n",
      "step: 1190 train: 0.07808971405029297 elapsed, loss: 8.8469415e-06\n",
      "step: 1200 train: 0.08036661148071289 elapsed, loss: 8.396093e-06\n",
      "step: 1210 train: 0.0791938304901123 elapsed, loss: 6.9820735e-06\n",
      "step: 1220 train: 0.0844719409942627 elapsed, loss: 1.31945135e-05\n",
      "step: 1230 train: 0.08577990531921387 elapsed, loss: 7.3317087e-06\n",
      "step: 1240 train: 0.08581924438476562 elapsed, loss: 6.968528e-06\n",
      "step: 1250 train: 0.08981990814208984 elapsed, loss: 5.7010466e-06\n",
      "step: 1260 train: 0.08726310729980469 elapsed, loss: 8.022957e-06\n",
      "step: 1270 train: 0.08167147636413574 elapsed, loss: 6.042389e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1280 train: 0.08230900764465332 elapsed, loss: 0.011225957\n",
      "step: 1290 train: 0.07554841041564941 elapsed, loss: 1.5694965e-05\n",
      "step: 1300 train: 0.07854652404785156 elapsed, loss: 1.8316168e-05\n",
      "step: 1310 train: 0.08622312545776367 elapsed, loss: 2.4123434e-05\n",
      "step: 1320 train: 0.08711004257202148 elapsed, loss: 1.5294994e-05\n",
      "step: 1330 train: 0.08301091194152832 elapsed, loss: 5.2346622e-05\n",
      "step: 1340 train: 0.08238673210144043 elapsed, loss: 1.0064088e-05\n",
      "step: 1350 train: 0.07632875442504883 elapsed, loss: 7.904957e-06\n",
      "step: 1360 train: 0.08085417747497559 elapsed, loss: 8.316585e-06\n",
      "step: 1370 train: 0.08417344093322754 elapsed, loss: 7.806716e-06\n",
      "step: 1380 train: 0.077728271484375 elapsed, loss: 0.0013406198\n",
      "step: 1390 train: 0.08191394805908203 elapsed, loss: 1.9180377e-05\n",
      "step: 1400 train: 0.08301186561584473 elapsed, loss: 0.0004538429\n",
      "step: 1410 train: 0.07830953598022461 elapsed, loss: 0.0019284594\n",
      "step: 1420 train: 0.0765688419342041 elapsed, loss: 1.1348265e-05\n",
      "step: 1430 train: 0.08179259300231934 elapsed, loss: 1.2859757e-05\n",
      "step: 1440 train: 0.07938265800476074 elapsed, loss: 1.232254e-05\n",
      "step: 1450 train: 0.07869577407836914 elapsed, loss: 1.1060013e-05\n",
      "step: 1460 train: 0.08420228958129883 elapsed, loss: 1.3156335e-05\n",
      "step: 1470 train: 0.08266258239746094 elapsed, loss: 0.0035614297\n",
      "step: 1480 train: 0.08022403717041016 elapsed, loss: 0.00012044922\n",
      "step: 1490 train: 0.08114480972290039 elapsed, loss: 1.5921582e-05\n",
      "step: 1500 train: 0.07995223999023438 elapsed, loss: 1.6628517e-05\n",
      "step: 1510 train: 0.07355570793151855 elapsed, loss: 0.0001480562\n",
      "step: 1520 train: 0.08530545234680176 elapsed, loss: 9.87792e-06\n",
      "step: 1530 train: 0.08249878883361816 elapsed, loss: 1.3413408e-05\n",
      "step: 1540 train: 0.08170533180236816 elapsed, loss: 9.677686e-05\n",
      "step: 1550 train: 0.07929778099060059 elapsed, loss: 8.747675e-06\n",
      "step: 1560 train: 0.07407331466674805 elapsed, loss: 9.9891195e-06\n",
      "step: 1570 train: 0.07958149909973145 elapsed, loss: 9.6105205e-06\n",
      "step: 1580 train: 0.07697606086730957 elapsed, loss: 6.6588887e-06\n",
      "step: 1590 train: 0.0857546329498291 elapsed, loss: 6.5689774e-06\n",
      "step: 1600 train: 0.07864975929260254 elapsed, loss: 7.0644437e-06\n",
      "step: 1610 train: 0.07537722587585449 elapsed, loss: 5.0691656e-06\n",
      "step: 1620 train: 0.07723355293273926 elapsed, loss: 6.2894674e-06\n",
      "step: 1630 train: 0.08269000053405762 elapsed, loss: 4.0740483e-06\n",
      "step: 1640 train: 0.08127450942993164 elapsed, loss: 4.6086197e-06\n",
      "step: 1650 train: 0.07447934150695801 elapsed, loss: 4.758547e-06\n",
      "step: 1660 train: 0.08411526679992676 elapsed, loss: 3.450994e-06\n",
      "step: 1670 train: 0.07818031311035156 elapsed, loss: 4.488009e-06\n",
      "step: 1680 train: 0.08296418190002441 elapsed, loss: 4.6789073e-06\n",
      "step: 1690 train: 0.07998776435852051 elapsed, loss: 4.2398283e-06\n",
      "step: 1700 train: 0.08066844940185547 elapsed, loss: 4.0093255e-06\n",
      "step: 1710 train: 0.08401918411254883 elapsed, loss: 6.478547e-05\n",
      "step: 1720 train: 0.07893228530883789 elapsed, loss: 5.609296e-06\n",
      "step: 1730 train: 0.07893013954162598 elapsed, loss: 4.733873e-06\n",
      "step: 1740 train: 0.08046150207519531 elapsed, loss: 3.9483252e-06\n",
      "step: 1750 train: 0.08136415481567383 elapsed, loss: 3.904556e-06\n",
      "step: 1760 train: 0.07828998565673828 elapsed, loss: 3.7294512e-06\n",
      "step: 1770 train: 0.08703327178955078 elapsed, loss: 3.0025747e-06\n",
      "step: 1780 train: 0.07979488372802734 elapsed, loss: 4.388802e-06\n",
      "step: 1790 train: 0.07937979698181152 elapsed, loss: 3.5907008e-06\n",
      "step: 1800 train: 0.0732269287109375 elapsed, loss: 4.5071088e-06\n",
      "step: 1810 train: 0.07567048072814941 elapsed, loss: 3.9394827e-06\n",
      "step: 1820 train: 0.08688640594482422 elapsed, loss: 3.512007e-06\n",
      "step: 1830 train: 0.08472299575805664 elapsed, loss: 2.847978e-06\n",
      "step: 1840 train: 0.0783851146697998 elapsed, loss: 2.899664e-06\n",
      "step: 1850 train: 0.07735252380371094 elapsed, loss: 3.5301175e-06\n",
      "step: 1860 train: 0.07602667808532715 elapsed, loss: 3.2870944e-06\n",
      "step: 1870 train: 0.08420944213867188 elapsed, loss: 3.0072313e-06\n",
      "step: 1880 train: 0.07647418975830078 elapsed, loss: 3.107814e-06\n",
      "step: 1890 train: 0.08873844146728516 elapsed, loss: 2.995592e-06\n",
      "step: 1900 train: 0.07974529266357422 elapsed, loss: 1.476503e-05\n",
      "step: 1910 train: 0.08170008659362793 elapsed, loss: 5.76716e-06\n",
      "step: 1920 train: 0.0817863941192627 elapsed, loss: 4.2579804e-06\n",
      "step: 1930 train: 0.07914066314697266 elapsed, loss: 4.2076795e-06\n",
      "step: 1940 train: 0.0825800895690918 elapsed, loss: 3.5543728e-06\n",
      "step: 1950 train: 0.08511185646057129 elapsed, loss: 3.0058309e-06\n",
      "step: 1960 train: 0.07915830612182617 elapsed, loss: 3.1753355e-06\n",
      "step: 1970 train: 0.0818178653717041 elapsed, loss: 4.9086866e-06\n",
      "step: 1980 train: 0.07726812362670898 elapsed, loss: 2.9159644e-06\n",
      "step: 1990 train: 0.0749661922454834 elapsed, loss: 3.2419248e-06\n",
      "step: 2000 train: 0.08511114120483398 elapsed, loss: 0.03176223\n",
      "step: 2010 train: 0.08174943923950195 elapsed, loss: 1.4277693e-05\n",
      "step: 2020 train: 0.08368229866027832 elapsed, loss: 7.036897e-06\n",
      "step: 2030 train: 0.08116912841796875 elapsed, loss: 5.805289e-06\n",
      "step: 2040 train: 0.07805991172790527 elapsed, loss: 6.969349e-06\n",
      "step: 2050 train: 0.08011531829833984 elapsed, loss: 6.7714973e-06\n",
      "step: 2060 train: 0.08038020133972168 elapsed, loss: 6.721665e-06\n",
      "step: 2070 train: 0.08096790313720703 elapsed, loss: 4.909423e-06\n",
      "step: 2080 train: 0.07579255104064941 elapsed, loss: 4.915494e-06\n",
      "step: 2090 train: 0.07928156852722168 elapsed, loss: 5.885401e-06\n",
      "step: 2100 train: 0.07613325119018555 elapsed, loss: 4.9094406e-06\n",
      "step: 2110 train: 0.07826805114746094 elapsed, loss: 4.9010605e-06\n",
      "step: 2120 train: 0.07502484321594238 elapsed, loss: 4.7091244e-06\n",
      "step: 2130 train: 0.08397912979125977 elapsed, loss: 3.4281834e-06\n",
      "step: 2140 train: 0.07649540901184082 elapsed, loss: 3.2531002e-06\n",
      "step: 2150 train: 0.08136844635009766 elapsed, loss: 3.840758e-06\n",
      "step: 2160 train: 0.07883143424987793 elapsed, loss: 4.203038e-06\n",
      "step: 2170 train: 0.07590746879577637 elapsed, loss: 4.458201e-06\n",
      "step: 2180 train: 0.07801651954650879 elapsed, loss: 3.64751e-06\n",
      "step: 2190 train: 0.07448101043701172 elapsed, loss: 3.057989e-06\n",
      "step: 2200 train: 0.07734894752502441 elapsed, loss: 3.1701986e-06\n",
      "step: 2210 train: 0.08484888076782227 elapsed, loss: 2.953679e-06\n",
      "step: 2220 train: 0.08213424682617188 elapsed, loss: 3.1445811e-06\n",
      "step: 2230 train: 0.0866708755493164 elapsed, loss: 3.118524e-06\n",
      "step: 2240 train: 0.07756948471069336 elapsed, loss: 5.1552483e-06\n",
      "step: 2250 train: 0.08050775527954102 elapsed, loss: 3.0398282e-06\n",
      "step: 2260 train: 0.0807490348815918 elapsed, loss: 3.1054876e-06\n",
      "step: 2270 train: 0.07765769958496094 elapsed, loss: 2.9876755e-06\n",
      "step: 2280 train: 0.0787506103515625 elapsed, loss: 2.9369164e-06\n",
      "step: 2290 train: 0.07503199577331543 elapsed, loss: 3.3713775e-06\n",
      "step: 2300 train: 0.07700014114379883 elapsed, loss: 3.679644e-06\n",
      "step: 2310 train: 0.08603739738464355 elapsed, loss: 2.9951252e-06\n",
      "step: 2320 train: 0.08379292488098145 elapsed, loss: 2.635637e-06\n",
      "step: 2330 train: 0.08029651641845703 elapsed, loss: 3.2163118e-06\n",
      "step: 2340 train: 0.08197712898254395 elapsed, loss: 2.5941938e-06\n",
      "step: 2350 train: 0.0767662525177002 elapsed, loss: 3.471961e-06\n",
      "step: 2360 train: 0.0811910629272461 elapsed, loss: 2.859153e-06\n",
      "step: 2370 train: 0.07902812957763672 elapsed, loss: 2.9876728e-06\n",
      "step: 2380 train: 0.08394646644592285 elapsed, loss: 3.3290016e-06\n",
      "step: 2390 train: 0.08060932159423828 elapsed, loss: 2.52993e-06\n",
      "step: 2400 train: 0.07521891593933105 elapsed, loss: 3.0370331e-06\n",
      "step: 2410 train: 0.07888221740722656 elapsed, loss: 2.3650896e-06\n",
      "step: 2420 train: 0.07850265502929688 elapsed, loss: 2.6840655e-06\n",
      "step: 2430 train: 0.07423758506774902 elapsed, loss: 3.483602e-06\n",
      "step: 2440 train: 0.07554197311401367 elapsed, loss: 3.3345923e-06\n",
      "step: 2450 train: 0.08043122291564941 elapsed, loss: 3.512432e-06\n",
      "step: 2460 train: 0.08711576461791992 elapsed, loss: 2.8773086e-06\n",
      "step: 2470 train: 0.08036160469055176 elapsed, loss: 3.6740466e-06\n",
      "step: 2480 train: 0.07595086097717285 elapsed, loss: 3.5045528e-06\n",
      "step: 2490 train: 0.08068609237670898 elapsed, loss: 3.8295766e-06\n",
      "step: 2500 train: 0.07927894592285156 elapsed, loss: 4.7680364e-06\n",
      "step: 2510 train: 0.08600068092346191 elapsed, loss: 2.5057188e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2520 train: 0.08189201354980469 elapsed, loss: 2.6761504e-06\n",
      "step: 2530 train: 0.0789797306060791 elapsed, loss: 3.5678845e-06\n",
      "step: 2540 train: 0.07893228530883789 elapsed, loss: 2.5802224e-06\n",
      "step: 2550 train: 0.07836484909057617 elapsed, loss: 2.9536805e-06\n",
      "step: 2560 train: 0.09025120735168457 elapsed, loss: 2.7040774e-06\n",
      "step: 2570 train: 0.07968711853027344 elapsed, loss: 3.418875e-06\n",
      "step: 2580 train: 0.07780647277832031 elapsed, loss: 8.3577515e-06\n",
      "step: 2590 train: 0.07709217071533203 elapsed, loss: 3.3462316e-06\n",
      "step: 2600 train: 0.08466768264770508 elapsed, loss: 2.6058356e-06\n",
      "step: 2610 train: 0.08156895637512207 elapsed, loss: 2.6458813e-06\n",
      "step: 2620 train: 0.07698488235473633 elapsed, loss: 3.9008205e-06\n",
      "step: 2630 train: 0.08096957206726074 elapsed, loss: 3.6232984e-06\n",
      "step: 2640 train: 0.07982635498046875 elapsed, loss: 3.1450643e-06\n",
      "step: 2650 train: 0.07782697677612305 elapsed, loss: 3.0621788e-06\n",
      "step: 2660 train: 0.08789181709289551 elapsed, loss: 2.8326112e-06\n",
      "step: 2670 train: 0.08252954483032227 elapsed, loss: 2.972774e-06\n",
      "step: 2680 train: 0.08029723167419434 elapsed, loss: 3.5934959e-06\n",
      "step: 2690 train: 0.07959485054016113 elapsed, loss: 5.129233e-06\n",
      "step: 2700 train: 0.08047699928283691 elapsed, loss: 3.4705633e-06\n",
      "step: 2710 train: 0.07979798316955566 elapsed, loss: 3.332208e-06\n",
      "step: 2720 train: 0.07797455787658691 elapsed, loss: 3.1995455e-06\n",
      "step: 2730 train: 0.07507967948913574 elapsed, loss: 3.1287711e-06\n",
      "step: 2740 train: 0.07861566543579102 elapsed, loss: 2.9024584e-06\n",
      "step: 2750 train: 0.07963013648986816 elapsed, loss: 3.0728831e-06\n",
      "step: 2760 train: 0.08019256591796875 elapsed, loss: 1.1016343e-05\n",
      "step: 2770 train: 0.08414077758789062 elapsed, loss: 3.0053693e-06\n",
      "step: 2780 train: 0.07788634300231934 elapsed, loss: 3.0621786e-06\n",
      "step: 2790 train: 0.08707165718078613 elapsed, loss: 2.8498398e-06\n",
      "step: 2800 train: 0.07696795463562012 elapsed, loss: 2.8232987e-06\n",
      "step: 2810 train: 0.07599711418151855 elapsed, loss: 3.3448318e-06\n",
      "step: 2820 train: 0.07572245597839355 elapsed, loss: 3.5618305e-06\n",
      "step: 2830 train: 0.08188462257385254 elapsed, loss: 2.5560098e-06\n",
      "step: 2840 train: 0.08342242240905762 elapsed, loss: 2.887551e-06\n",
      "step: 2850 train: 0.07906627655029297 elapsed, loss: 3.6344752e-06\n",
      "step: 2860 train: 0.08771920204162598 elapsed, loss: 2.5583379e-06\n",
      "step: 2870 train: 0.07698321342468262 elapsed, loss: 3.4333104e-06\n",
      "step: 2880 train: 0.07991838455200195 elapsed, loss: 2.6170108e-06\n",
      "step: 2890 train: 0.08922719955444336 elapsed, loss: 2.9932644e-06\n",
      "step: 2900 train: 0.08347821235656738 elapsed, loss: 4.03168e-06\n",
      "step: 2910 train: 0.08404779434204102 elapsed, loss: 2.893612e-06\n",
      "step: 2920 train: 0.07479357719421387 elapsed, loss: 5.3335525e-06\n",
      "step: 2930 train: 0.07232999801635742 elapsed, loss: 4.011649e-06\n",
      "step: 2940 train: 0.08039975166320801 elapsed, loss: 3.155307e-06\n",
      "step: 2950 train: 0.07630562782287598 elapsed, loss: 3.6232805e-06\n",
      "step: 2960 train: 0.07876729965209961 elapsed, loss: 2.7222504e-06\n",
      "step: 2970 train: 0.08013343811035156 elapsed, loss: 3.4663694e-06\n",
      "step: 2980 train: 0.07790541648864746 elapsed, loss: 2.8219006e-06\n",
      "step: 2990 train: 0.07604217529296875 elapsed, loss: 3.6107094e-06\n",
      "step: 3000 train: 0.08291363716125488 elapsed, loss: 2.894078e-06\n",
      "step: 3010 train: 0.08492732048034668 elapsed, loss: 3.1459995e-06\n",
      "step: 3020 train: 0.0810692310333252 elapsed, loss: 4.534582e-06\n",
      "step: 3030 train: 0.07703495025634766 elapsed, loss: 4.1406465e-06\n",
      "step: 3040 train: 0.08020162582397461 elapsed, loss: 4.354847e-06\n",
      "step: 3050 train: 0.08375906944274902 elapsed, loss: 3.1632287e-06\n",
      "step: 3060 train: 0.0790712833404541 elapsed, loss: 4.905712e-06\n",
      "step: 3070 train: 0.07580447196960449 elapsed, loss: 3.850075e-06\n",
      "step: 3080 train: 0.07865500450134277 elapsed, loss: 3.4505306e-06\n",
      "step: 3090 train: 0.0829312801361084 elapsed, loss: 3.9436654e-06\n",
      "step: 3100 train: 0.08090734481811523 elapsed, loss: 5.134345e-06\n",
      "step: 3110 train: 0.07354998588562012 elapsed, loss: 6.230031e-06\n",
      "step: 3120 train: 0.07733821868896484 elapsed, loss: 3.993498e-06\n",
      "step: 3130 train: 0.08172345161437988 elapsed, loss: 3.1264403e-06\n",
      "step: 3140 train: 0.08270692825317383 elapsed, loss: 3.9459746e-06\n",
      "step: 3150 train: 0.07995915412902832 elapsed, loss: 2.7720753e-06\n",
      "step: 3160 train: 0.08160543441772461 elapsed, loss: 4.205837e-06\n",
      "step: 3170 train: 0.07751822471618652 elapsed, loss: 4.515033e-06\n",
      "step: 3180 train: 0.08475136756896973 elapsed, loss: 3.0998985e-06\n",
      "step: 3190 train: 0.08261799812316895 elapsed, loss: 3.422133e-06\n",
      "step: 3200 train: 0.07364392280578613 elapsed, loss: 5.165088e-06\n",
      "step: 3210 train: 0.08917522430419922 elapsed, loss: 3.6293404e-06\n",
      "step: 3220 train: 0.07929778099060059 elapsed, loss: 3.092445e-06\n",
      "step: 3230 train: 0.07845020294189453 elapsed, loss: 3.2265557e-06\n",
      "step: 3240 train: 0.07994890213012695 elapsed, loss: 3.6046717e-06\n",
      "step: 3250 train: 0.07992148399353027 elapsed, loss: 3.0840677e-06\n",
      "step: 3260 train: 0.08538365364074707 elapsed, loss: 2.9010628e-06\n",
      "step: 3270 train: 0.08894681930541992 elapsed, loss: 2.8903514e-06\n",
      "step: 3280 train: 0.08033466339111328 elapsed, loss: 4.142506e-06\n",
      "step: 3290 train: 0.07853388786315918 elapsed, loss: 3.1781265e-06\n",
      "step: 3300 train: 0.07168793678283691 elapsed, loss: 4.3557825e-06\n",
      "step: 3310 train: 0.08230280876159668 elapsed, loss: 4.2421025e-06\n",
      "step: 3320 train: 0.07567048072814941 elapsed, loss: 3.7085147e-06\n",
      "step: 3330 train: 0.08172845840454102 elapsed, loss: 2.7185106e-06\n",
      "step: 3340 train: 0.07647013664245605 elapsed, loss: 4.514102e-06\n",
      "step: 3350 train: 0.07943987846374512 elapsed, loss: 2.6100238e-06\n",
      "step: 3360 train: 0.07976174354553223 elapsed, loss: 4.4344674e-06\n",
      "step: 3370 train: 0.08032107353210449 elapsed, loss: 2.8908125e-06\n",
      "step: 3380 train: 0.0776209831237793 elapsed, loss: 3.3066476e-06\n",
      "step: 3390 train: 0.08024168014526367 elapsed, loss: 5.712108e-06\n",
      "step: 3400 train: 0.07884335517883301 elapsed, loss: 3.5120088e-06\n",
      "step: 3410 train: 0.0829010009765625 elapsed, loss: 2.744602e-06\n",
      "step: 3420 train: 0.08186078071594238 elapsed, loss: 3.151586e-06\n",
      "step: 3430 train: 0.07977008819580078 elapsed, loss: 2.6044381e-06\n",
      "step: 3440 train: 0.0780935287475586 elapsed, loss: 3.475221e-06\n",
      "step: 3450 train: 0.08228421211242676 elapsed, loss: 2.7306316e-06\n",
      "step: 3460 train: 0.0813298225402832 elapsed, loss: 3.2666007e-06\n",
      "step: 3470 train: 0.08727884292602539 elapsed, loss: 0.0005649899\n",
      "step: 3480 train: 0.08134055137634277 elapsed, loss: 0.00013383004\n",
      "step: 3490 train: 0.08052754402160645 elapsed, loss: 9.448126e-06\n",
      "step: 3500 train: 0.07582592964172363 elapsed, loss: 1.2079967e-05\n",
      "step: 3510 train: 0.07651042938232422 elapsed, loss: 3.187683e-05\n",
      "step: 3520 train: 0.07824158668518066 elapsed, loss: 2.742259e-05\n",
      "step: 3530 train: 0.07525491714477539 elapsed, loss: 1.5535836e-05\n",
      "step: 3540 train: 0.08170700073242188 elapsed, loss: 1.004297e-05\n",
      "step: 3550 train: 0.07760763168334961 elapsed, loss: 9.649735e-06\n",
      "step: 3560 train: 0.07815933227539062 elapsed, loss: 8.870154e-06\n",
      "step: 3570 train: 0.08570337295532227 elapsed, loss: 6.147616e-06\n",
      "step: 3580 train: 0.08157157897949219 elapsed, loss: 6.8022655e-06\n",
      "step: 3590 train: 0.07523703575134277 elapsed, loss: 6.76863e-06\n",
      "step: 3600 train: 0.08336234092712402 elapsed, loss: 8.390509e-06\n",
      "step: 3610 train: 0.0809321403503418 elapsed, loss: 9.167816e-06\n",
      "step: 3620 train: 0.07584595680236816 elapsed, loss: 9.207384e-06\n",
      "step: 3630 train: 0.08379673957824707 elapsed, loss: 6.5289737e-06\n",
      "step: 3640 train: 0.08631348609924316 elapsed, loss: 6.6575094e-06\n",
      "step: 3650 train: 0.08215546607971191 elapsed, loss: 5.029111e-06\n",
      "step: 3660 train: 0.07422423362731934 elapsed, loss: 6.1168826e-06\n",
      "step: 3670 train: 0.08419442176818848 elapsed, loss: 3.537617e-06\n",
      "step: 3680 train: 0.08310985565185547 elapsed, loss: 3.368118e-06\n",
      "step: 3690 train: 0.08182048797607422 elapsed, loss: 3.8090936e-06\n",
      "step: 3700 train: 0.08222126960754395 elapsed, loss: 3.4863892e-06\n",
      "step: 3710 train: 0.08644843101501465 elapsed, loss: 3.349488e-06\n",
      "step: 3720 train: 0.08476114273071289 elapsed, loss: 2.709211e-06\n",
      "step: 3730 train: 0.08005166053771973 elapsed, loss: 4.8959405e-06\n",
      "step: 3740 train: 0.08472704887390137 elapsed, loss: 2.8614809e-06\n",
      "step: 3750 train: 0.08459663391113281 elapsed, loss: 3.555312e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3760 train: 0.08091545104980469 elapsed, loss: 3.6088609e-06\n",
      "step: 3770 train: 0.07899808883666992 elapsed, loss: 4.730079e-06\n",
      "step: 3780 train: 0.07857394218444824 elapsed, loss: 2.4670671e-06\n",
      "step: 3790 train: 0.08114933967590332 elapsed, loss: 3.6833494e-06\n",
      "step: 3800 train: 0.08415699005126953 elapsed, loss: 2.7981496e-06\n",
      "step: 3810 train: 0.08741354942321777 elapsed, loss: 2.4773126e-06\n",
      "step: 3820 train: 0.07451033592224121 elapsed, loss: 3.7350592e-06\n",
      "step: 3830 train: 0.07959151268005371 elapsed, loss: 2.7664878e-06\n",
      "step: 3840 train: 0.08043527603149414 elapsed, loss: 3.1646105e-06\n",
      "step: 3850 train: 0.08030414581298828 elapsed, loss: 3.2521707e-06\n",
      "step: 3860 train: 0.07507491111755371 elapsed, loss: 1.3646011e-05\n",
      "step: 3870 train: 0.0764167308807373 elapsed, loss: 3.7182908e-06\n",
      "step: 3880 train: 0.08387541770935059 elapsed, loss: 2.9825533e-06\n",
      "step: 3890 train: 0.07720565795898438 elapsed, loss: 3.3788283e-06\n",
      "step: 3900 train: 0.0798637866973877 elapsed, loss: 2.8335414e-06\n",
      "step: 3910 train: 0.0816812515258789 elapsed, loss: 2.991867e-06\n",
      "step: 3920 train: 0.07986021041870117 elapsed, loss: 2.5290017e-06\n",
      "step: 3930 train: 0.07602500915527344 elapsed, loss: 3.5064147e-06\n",
      "step: 3940 train: 0.08756446838378906 elapsed, loss: 3.1986194e-06\n",
      "step: 3950 train: 0.08096432685852051 elapsed, loss: 2.9667112e-06\n",
      "step: 3960 train: 0.07766604423522949 elapsed, loss: 3.5930327e-06\n",
      "step: 3970 train: 0.0799417495727539 elapsed, loss: 2.7753354e-06\n",
      "step: 3980 train: 0.08447504043579102 elapsed, loss: 2.8568188e-06\n",
      "step: 3990 train: 0.08400845527648926 elapsed, loss: 4.8348784e-06\n",
      "step: 4000 train: 0.08383941650390625 elapsed, loss: 3.1585687e-06\n",
      "step: 4010 train: 0.08249258995056152 elapsed, loss: 3.489652e-06\n",
      "step: 4020 train: 0.07927370071411133 elapsed, loss: 3.0225917e-06\n",
      "step: 4030 train: 0.08208465576171875 elapsed, loss: 2.8377317e-06\n",
      "step: 4040 train: 0.08072280883789062 elapsed, loss: 2.7255091e-06\n",
      "step: 4050 train: 0.07529282569885254 elapsed, loss: 3.3317974e-06\n",
      "step: 4060 train: 0.07770586013793945 elapsed, loss: 3.1492598e-06\n",
      "step: 4070 train: 0.07851219177246094 elapsed, loss: 3.2875605e-06\n",
      "step: 4080 train: 0.08546686172485352 elapsed, loss: 6.2517283e-06\n",
      "step: 4090 train: 0.08083057403564453 elapsed, loss: 2.939711e-06\n",
      "step: 4100 train: 0.07907462120056152 elapsed, loss: 3.2172463e-06\n",
      "step: 4110 train: 0.07923269271850586 elapsed, loss: 3.090122e-06\n",
      "step: 4120 train: 0.07891154289245605 elapsed, loss: 3.867768e-06\n",
      "step: 4130 train: 0.08355021476745605 elapsed, loss: 2.9061819e-06\n",
      "step: 4140 train: 0.07993650436401367 elapsed, loss: 2.6621803e-06\n",
      "step: 4150 train: 0.0802927017211914 elapsed, loss: 2.6919818e-06\n",
      "step: 4160 train: 0.0755314826965332 elapsed, loss: 2.7320298e-06\n",
      "step: 4170 train: 0.08147740364074707 elapsed, loss: 2.664508e-06\n",
      "step: 4180 train: 0.08337211608886719 elapsed, loss: 3.3713754e-06\n",
      "step: 4190 train: 0.07802176475524902 elapsed, loss: 3.4496088e-06\n",
      "step: 4200 train: 0.0863335132598877 elapsed, loss: 2.8307472e-06\n",
      "step: 4210 train: 0.07858943939208984 elapsed, loss: 3.894313e-06\n",
      "step: 4220 train: 0.07527661323547363 elapsed, loss: 4.4330654e-06\n",
      "step: 4230 train: 0.08339095115661621 elapsed, loss: 2.663574e-06\n",
      "step: 4240 train: 0.07674193382263184 elapsed, loss: 4.43071e-06\n",
      "step: 4250 train: 0.07942700386047363 elapsed, loss: 9.49282e-06\n",
      "step: 4260 train: 0.0830073356628418 elapsed, loss: 9.114678e-06\n",
      "step: 4270 train: 0.08147168159484863 elapsed, loss: 9.466132e-06\n",
      "step: 4280 train: 0.07293510437011719 elapsed, loss: 1.9732364e-05\n",
      "step: 4290 train: 0.0822758674621582 elapsed, loss: 9.044855e-06\n",
      "step: 4300 train: 0.07785677909851074 elapsed, loss: 1.1915563e-05\n",
      "step: 4310 train: 0.08068490028381348 elapsed, loss: 7.1259024e-06\n",
      "step: 4320 train: 0.08409857749938965 elapsed, loss: 5.85421e-06\n",
      "step: 4330 train: 0.07555341720581055 elapsed, loss: 7.4123236e-06\n",
      "step: 4340 train: 0.08095097541809082 elapsed, loss: 1.2955312e-05\n",
      "step: 4350 train: 0.08083558082580566 elapsed, loss: 9.903337e-06\n",
      "step: 4360 train: 0.0784459114074707 elapsed, loss: 2.8587183e-05\n",
      "step: 4370 train: 0.07771515846252441 elapsed, loss: 2.0223952e-05\n",
      "step: 4380 train: 0.08122897148132324 elapsed, loss: 1.3452861e-05\n",
      "step: 4390 train: 0.0794675350189209 elapsed, loss: 9.2353e-06\n",
      "step: 4400 train: 0.08002662658691406 elapsed, loss: 6.554077e-06\n",
      "step: 4410 train: 0.07994318008422852 elapsed, loss: 7.5308017e-06\n",
      "step: 4420 train: 0.08074140548706055 elapsed, loss: 8.5780575e-06\n",
      "step: 4430 train: 0.07657504081726074 elapsed, loss: 7.795505e-06\n",
      "step: 4440 train: 0.07989621162414551 elapsed, loss: 5.1925585e-06\n",
      "step: 4450 train: 0.08108162879943848 elapsed, loss: 5.001168e-06\n",
      "step: 4460 train: 0.07384657859802246 elapsed, loss: 5.308504e-06\n",
      "step: 4470 train: 0.07359051704406738 elapsed, loss: 6.0381853e-06\n",
      "step: 4480 train: 0.07698655128479004 elapsed, loss: 4.826042e-06\n",
      "step: 4490 train: 0.07557010650634766 elapsed, loss: 1.5151383e-05\n",
      "step: 4500 train: 0.08394670486450195 elapsed, loss: 3.0673023e-06\n",
      "step: 4510 train: 0.07729411125183105 elapsed, loss: 3.2740536e-06\n",
      "step: 4520 train: 0.0774075984954834 elapsed, loss: 3.1380844e-06\n",
      "step: 4530 train: 0.07869458198547363 elapsed, loss: 3.2209691e-06\n",
      "step: 4540 train: 0.07582640647888184 elapsed, loss: 3.3243457e-06\n",
      "step: 4550 train: 0.07743620872497559 elapsed, loss: 2.7124697e-06\n",
      "step: 4560 train: 0.08253002166748047 elapsed, loss: 2.6961734e-06\n",
      "step: 4570 train: 0.07694196701049805 elapsed, loss: 2.991867e-06\n",
      "step: 4580 train: 0.07779884338378906 elapsed, loss: 2.9699809e-06\n",
      "step: 4590 train: 0.07795071601867676 elapsed, loss: 3.5413423e-06\n",
      "step: 4600 train: 0.07621192932128906 elapsed, loss: 3.7555474e-06\n",
      "step: 4610 train: 0.07634782791137695 elapsed, loss: 4.4191e-06\n",
      "step: 4620 train: 0.08423161506652832 elapsed, loss: 3.5413443e-06\n",
      "step: 4630 train: 0.08427882194519043 elapsed, loss: 2.6295836e-06\n",
      "step: 4640 train: 0.08406662940979004 elapsed, loss: 2.8209677e-06\n",
      "step: 4650 train: 0.08297252655029297 elapsed, loss: 2.5276045e-06\n",
      "step: 4660 train: 0.08140897750854492 elapsed, loss: 2.8838267e-06\n",
      "step: 4670 train: 0.0780024528503418 elapsed, loss: 3.4440209e-06\n",
      "step: 4680 train: 0.08014321327209473 elapsed, loss: 2.8503052e-06\n",
      "step: 4690 train: 0.08290982246398926 elapsed, loss: 2.4563583e-06\n",
      "step: 4700 train: 0.0826869010925293 elapsed, loss: 3.1338525e-06\n",
      "step: 4710 train: 0.0855252742767334 elapsed, loss: 2.5709112e-06\n",
      "step: 4720 train: 0.08672833442687988 elapsed, loss: 3.070563e-06\n",
      "step: 4730 train: 0.07638239860534668 elapsed, loss: 3.6400634e-06\n",
      "step: 4740 train: 0.08026361465454102 elapsed, loss: 4.0209725e-06\n",
      "step: 4750 train: 0.0763397216796875 elapsed, loss: 3.2889218e-06\n",
      "step: 4760 train: 0.0829613208770752 elapsed, loss: 3.7001314e-06\n",
      "step: 4770 train: 0.07851648330688477 elapsed, loss: 3.5101452e-06\n",
      "step: 4780 train: 0.08450174331665039 elapsed, loss: 2.2542617e-06\n",
      "step: 4790 train: 0.08043074607849121 elapsed, loss: 2.888955e-06\n",
      "step: 4800 train: 0.07985734939575195 elapsed, loss: 2.8200302e-06\n",
      "step: 4810 train: 0.07174992561340332 elapsed, loss: 5.37411e-06\n",
      "step: 4820 train: 0.07602572441101074 elapsed, loss: 3.9730094e-06\n",
      "step: 4830 train: 0.07985353469848633 elapsed, loss: 4.21929e-06\n",
      "step: 4840 train: 0.0789337158203125 elapsed, loss: 3.0416925e-06\n",
      "step: 4850 train: 0.07461977005004883 elapsed, loss: 3.548795e-06\n",
      "step: 4860 train: 0.07740545272827148 elapsed, loss: 3.5976782e-06\n",
      "step: 4870 train: 0.07962274551391602 elapsed, loss: 3.285694e-06\n",
      "step: 4880 train: 0.07885360717773438 elapsed, loss: 3.4584573e-06\n",
      "step: 4890 train: 0.07978057861328125 elapsed, loss: 5.7935868e-06\n",
      "step: 4900 train: 0.08121585845947266 elapsed, loss: 3.0379676e-06\n",
      "step: 4910 train: 0.07822012901306152 elapsed, loss: 3.861247e-06\n",
      "step: 4920 train: 0.08245515823364258 elapsed, loss: 4.4563303e-06\n",
      "step: 4930 train: 0.07966303825378418 elapsed, loss: 2.9071166e-06\n",
      "step: 4940 train: 0.07875609397888184 elapsed, loss: 3.3890744e-06\n",
      "step: 4950 train: 0.08256411552429199 elapsed, loss: 2.7995466e-06\n",
      "step: 4960 train: 0.0807805061340332 elapsed, loss: 4.262636e-06\n",
      "step: 4970 train: 0.08102703094482422 elapsed, loss: 4.3971972e-06\n",
      "step: 4980 train: 0.0774223804473877 elapsed, loss: 3.7676523e-06\n",
      "step: 4990 train: 0.0784902572631836 elapsed, loss: 2.8274856e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5000 train: 0.07777833938598633 elapsed, loss: 3.4845343e-06\n",
      "step: 5010 train: 0.07867622375488281 elapsed, loss: 3.1022278e-06\n",
      "step: 5020 train: 0.08187723159790039 elapsed, loss: 2.5196878e-06\n",
      "step: 5030 train: 0.08135986328125 elapsed, loss: 3.2349344e-06\n",
      "step: 5040 train: 0.08207440376281738 elapsed, loss: 2.5508866e-06\n",
      "step: 5050 train: 0.07750535011291504 elapsed, loss: 2.7250444e-06\n",
      "step: 5060 train: 0.07976889610290527 elapsed, loss: 2.8326112e-06\n",
      "step: 5070 train: 0.07912993431091309 elapsed, loss: 3.5087478e-06\n",
      "step: 5080 train: 0.07447052001953125 elapsed, loss: 3.8943103e-06\n",
      "step: 5090 train: 0.08267092704772949 elapsed, loss: 3.1534514e-06\n",
      "step: 5100 train: 0.07971453666687012 elapsed, loss: 2.4107223e-06\n",
      "step: 5110 train: 0.0777275562286377 elapsed, loss: 3.887785e-06\n",
      "step: 5120 train: 0.07452988624572754 elapsed, loss: 4.933622e-06\n",
      "step: 5130 train: 0.08531045913696289 elapsed, loss: 2.8745171e-06\n",
      "step: 5140 train: 0.08206486701965332 elapsed, loss: 2.8400623e-06\n",
      "step: 5150 train: 0.0767662525177002 elapsed, loss: 3.6768483e-06\n",
      "step: 5160 train: 0.08009004592895508 elapsed, loss: 4.8651814e-06\n",
      "step: 5170 train: 0.0830538272857666 elapsed, loss: 2.7026917e-06\n",
      "step: 5180 train: 0.07696914672851562 elapsed, loss: 4.636106e-06\n",
      "step: 5190 train: 0.08248591423034668 elapsed, loss: 3.117124e-06\n",
      "step: 5200 train: 0.07701396942138672 elapsed, loss: 4.47359e-06\n",
      "step: 5210 train: 0.07649469375610352 elapsed, loss: 3.9976894e-06\n",
      "step: 5220 train: 0.0760953426361084 elapsed, loss: 3.4761492e-06\n",
      "step: 5230 train: 0.08002090454101562 elapsed, loss: 7.713458e-06\n",
      "step: 5240 train: 0.07703423500061035 elapsed, loss: 3.8924513e-06\n",
      "step: 5250 train: 0.08467364311218262 elapsed, loss: 3.9162e-06\n",
      "step: 5260 train: 0.08056116104125977 elapsed, loss: 3.6293518e-06\n",
      "step: 5270 train: 0.07690954208374023 elapsed, loss: 8.0746095e-06\n",
      "step: 5280 train: 0.08570671081542969 elapsed, loss: 4.7511244e-06\n",
      "step: 5290 train: 0.07993650436401367 elapsed, loss: 3.431442e-06\n",
      "step: 5300 train: 0.0767207145690918 elapsed, loss: 3.3671868e-06\n",
      "step: 5310 train: 0.08163905143737793 elapsed, loss: 3.419803e-06\n",
      "step: 5320 train: 0.07715582847595215 elapsed, loss: 5.370448e-06\n",
      "step: 5330 train: 0.07751917839050293 elapsed, loss: 4.3199257e-06\n",
      "step: 5340 train: 0.07844853401184082 elapsed, loss: 3.3271413e-06\n",
      "step: 5350 train: 0.07791709899902344 elapsed, loss: 3.3345918e-06\n",
      "step: 5360 train: 0.08397531509399414 elapsed, loss: 8.008065e-06\n",
      "step: 5370 train: 0.07653522491455078 elapsed, loss: 4.3222476e-06\n",
      "step: 5380 train: 0.0762782096862793 elapsed, loss: 4.4004846e-06\n",
      "step: 5390 train: 0.08213329315185547 elapsed, loss: 3.5161966e-06\n",
      "step: 5400 train: 0.08108663558959961 elapsed, loss: 2.6659056e-06\n",
      "step: 5410 train: 0.08130168914794922 elapsed, loss: 2.7082806e-06\n",
      "step: 5420 train: 0.07694482803344727 elapsed, loss: 3.3061806e-06\n",
      "step: 5430 train: 0.08234548568725586 elapsed, loss: 2.726441e-06\n",
      "step: 5440 train: 0.07941055297851562 elapsed, loss: 2.7227147e-06\n",
      "step: 5450 train: 0.0790715217590332 elapsed, loss: 3.069167e-06\n",
      "step: 5460 train: 0.085968017578125 elapsed, loss: 2.6733564e-06\n",
      "step: 5470 train: 0.07819771766662598 elapsed, loss: 3.069632e-06\n",
      "step: 5480 train: 0.08539819717407227 elapsed, loss: 3.0300505e-06\n",
      "step: 5490 train: 0.08456277847290039 elapsed, loss: 3.0035071e-06\n",
      "step: 5500 train: 0.08387279510498047 elapsed, loss: 2.470328e-06\n",
      "step: 5510 train: 0.08117556571960449 elapsed, loss: 3.4319128e-06\n",
      "step: 5520 train: 0.07684159278869629 elapsed, loss: 3.3359831e-06\n",
      "step: 5530 train: 0.07823681831359863 elapsed, loss: 3.5720725e-06\n",
      "step: 5540 train: 0.08080935478210449 elapsed, loss: 2.748792e-06\n",
      "step: 5550 train: 0.08024430274963379 elapsed, loss: 4.1322655e-06\n",
      "step: 5560 train: 0.08521008491516113 elapsed, loss: 2.9550795e-06\n",
      "step: 5570 train: 0.07863259315490723 elapsed, loss: 2.8177103e-06\n",
      "step: 5580 train: 0.07559800148010254 elapsed, loss: 4.459159e-06\n",
      "step: 5590 train: 0.07848191261291504 elapsed, loss: 4.806039e-06\n",
      "step: 5600 train: 0.07523775100708008 elapsed, loss: 4.9243476e-06\n",
      "step: 5610 train: 0.08130240440368652 elapsed, loss: 2.7348221e-06\n",
      "step: 5620 train: 0.08380889892578125 elapsed, loss: 2.7590372e-06\n",
      "step: 5630 train: 0.0764627456665039 elapsed, loss: 3.466374e-06\n",
      "step: 5640 train: 0.0874185562133789 elapsed, loss: 3.287546e-06\n",
      "step: 5650 train: 0.07800626754760742 elapsed, loss: 4.1234152e-06\n",
      "step: 5660 train: 0.08088254928588867 elapsed, loss: 2.751587e-06\n",
      "step: 5670 train: 0.08212876319885254 elapsed, loss: 5.7150337e-06\n",
      "step: 5680 train: 0.0763540267944336 elapsed, loss: 4.6444884e-06\n",
      "step: 5690 train: 0.08320760726928711 elapsed, loss: 2.9830198e-06\n",
      "step: 5700 train: 0.0794215202331543 elapsed, loss: 3.688957e-06\n",
      "step: 5710 train: 0.07906889915466309 elapsed, loss: 3.1599675e-06\n",
      "step: 5720 train: 0.07905244827270508 elapsed, loss: 3.7057212e-06\n",
      "step: 5730 train: 0.08662748336791992 elapsed, loss: 3.1678528e-06\n",
      "step: 5740 train: 0.07884550094604492 elapsed, loss: 5.2610794e-06\n",
      "step: 5750 train: 0.08114242553710938 elapsed, loss: 6.7966066e-06\n",
      "step: 5760 train: 0.0825812816619873 elapsed, loss: 1.6397049e-05\n",
      "step: 5770 train: 0.09062385559082031 elapsed, loss: 8.625648e-06\n",
      "step: 5780 train: 0.08778500556945801 elapsed, loss: 0.002799489\n",
      "step: 5790 train: 0.08368110656738281 elapsed, loss: 2.4101691e-05\n",
      "step: 5800 train: 0.08469271659851074 elapsed, loss: 1.5740043e-05\n",
      "step: 5810 train: 0.07888674736022949 elapsed, loss: 2.7691392e-05\n",
      "step: 5820 train: 0.07549762725830078 elapsed, loss: 1.8547726e-05\n",
      "step: 5830 train: 0.0769193172454834 elapsed, loss: 2.0716168e-05\n",
      "step: 5840 train: 0.08735060691833496 elapsed, loss: 1.17424e-05\n",
      "step: 5850 train: 0.07862472534179688 elapsed, loss: 1.1839894e-05\n",
      "step: 5860 train: 0.07669281959533691 elapsed, loss: 1.1817168e-05\n",
      "step: 5870 train: 0.0820016860961914 elapsed, loss: 4.57603e-06\n",
      "step: 5880 train: 0.07885003089904785 elapsed, loss: 8.433817e-06\n",
      "step: 5890 train: 0.07541084289550781 elapsed, loss: 4.476781e-06\n",
      "step: 5900 train: 0.08592891693115234 elapsed, loss: 3.6572906e-06\n",
      "step: 5910 train: 0.07979464530944824 elapsed, loss: 5.5045452e-06\n",
      "step: 5920 train: 0.07938981056213379 elapsed, loss: 3.8016458e-06\n",
      "step: 5930 train: 0.07703948020935059 elapsed, loss: 3.821664e-06\n",
      "step: 5940 train: 0.07970046997070312 elapsed, loss: 3.3941956e-06\n",
      "step: 5950 train: 0.08033466339111328 elapsed, loss: 3.3150334e-06\n",
      "step: 5960 train: 0.08164238929748535 elapsed, loss: 2.8964062e-06\n",
      "step: 5970 train: 0.0840158462524414 elapsed, loss: 2.0167759e-06\n",
      "step: 5980 train: 0.0834202766418457 elapsed, loss: 2.2081617e-06\n",
      "step: 5990 train: 0.0803074836730957 elapsed, loss: 2.8144482e-06\n",
      "step: 6000 train: 0.08488988876342773 elapsed, loss: 4.3567074e-06\n",
      "step: 6010 train: 0.07193899154663086 elapsed, loss: 4.248679e-06\n",
      "step: 6020 train: 0.07862234115600586 elapsed, loss: 3.3723095e-06\n",
      "step: 6030 train: 0.08220100402832031 elapsed, loss: 2.9061862e-06\n",
      "step: 6040 train: 0.07323765754699707 elapsed, loss: 4.5904726e-06\n",
      "step: 6050 train: 0.08133530616760254 elapsed, loss: 2.647743e-06\n",
      "step: 6060 train: 0.07917380332946777 elapsed, loss: 2.5685824e-06\n",
      "step: 6070 train: 0.08402037620544434 elapsed, loss: 2.2607815e-06\n",
      "step: 6080 train: 0.07716131210327148 elapsed, loss: 2.7245771e-06\n",
      "step: 6090 train: 0.0791933536529541 elapsed, loss: 3.4700993e-06\n",
      "step: 6100 train: 0.07991862297058105 elapsed, loss: 4.861421e-06\n",
      "step: 6110 train: 0.08081936836242676 elapsed, loss: 3.8901208e-06\n",
      "step: 6120 train: 0.08236908912658691 elapsed, loss: 3.5469302e-06\n",
      "step: 6130 train: 0.0770421028137207 elapsed, loss: 3.2875594e-06\n",
      "step: 6140 train: 0.08065652847290039 elapsed, loss: 3.8947796e-06\n",
      "step: 6150 train: 0.07663536071777344 elapsed, loss: 4.466576e-06\n",
      "step: 6160 train: 0.08433723449707031 elapsed, loss: 2.6132857e-06\n",
      "step: 6170 train: 0.07995438575744629 elapsed, loss: 3.4277246e-06\n",
      "step: 6180 train: 0.07945823669433594 elapsed, loss: 3.0579831e-06\n",
      "step: 6190 train: 0.07953476905822754 elapsed, loss: 3.160901e-06\n",
      "step: 6200 train: 0.07822823524475098 elapsed, loss: 2.6873217e-06\n",
      "step: 6210 train: 0.07977437973022461 elapsed, loss: 3.6856977e-06\n",
      "step: 6220 train: 0.0804281234741211 elapsed, loss: 3.3294682e-06\n",
      "step: 6230 train: 0.08393430709838867 elapsed, loss: 2.4172405e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6240 train: 0.07871150970458984 elapsed, loss: 2.90246e-06\n",
      "step: 6250 train: 0.08362364768981934 elapsed, loss: 2.8638094e-06\n",
      "step: 6260 train: 0.0832219123840332 elapsed, loss: 2.399082e-06\n",
      "step: 6270 train: 0.08065915107727051 elapsed, loss: 2.886162e-06\n",
      "step: 6280 train: 0.07542991638183594 elapsed, loss: 3.1767336e-06\n",
      "step: 6290 train: 0.07639169692993164 elapsed, loss: 3.9045563e-06\n",
      "step: 6300 train: 0.08219599723815918 elapsed, loss: 3.5501907e-06\n",
      "step: 6310 train: 0.08198857307434082 elapsed, loss: 2.599316e-06\n",
      "step: 6320 train: 0.08044195175170898 elapsed, loss: 2.5979168e-06\n",
      "step: 6330 train: 0.07884407043457031 elapsed, loss: 5.105914e-06\n",
      "step: 6340 train: 0.07641863822937012 elapsed, loss: 3.288956e-06\n",
      "step: 6350 train: 0.08210301399230957 elapsed, loss: 4.1899293e-06\n",
      "step: 6360 train: 0.08142590522766113 elapsed, loss: 3.0328442e-06\n",
      "step: 6370 train: 0.07509064674377441 elapsed, loss: 3.1185268e-06\n",
      "step: 6380 train: 0.08126425743103027 elapsed, loss: 3.989773e-06\n",
      "step: 6390 train: 0.08122682571411133 elapsed, loss: 2.5494908e-06\n",
      "step: 6400 train: 0.08117103576660156 elapsed, loss: 4.4985222e-06\n",
      "step: 6410 train: 0.0788888931274414 elapsed, loss: 5.1913253e-06\n",
      "step: 6420 train: 0.08421087265014648 elapsed, loss: 3.1175946e-06\n",
      "step: 6430 train: 0.08395171165466309 elapsed, loss: 3.6326023e-06\n",
      "step: 6440 train: 0.0771477222442627 elapsed, loss: 3.307583e-06\n",
      "step: 6450 train: 0.07789492607116699 elapsed, loss: 3.0915144e-06\n",
      "step: 6460 train: 0.08133196830749512 elapsed, loss: 2.4163123e-06\n",
      "step: 6470 train: 0.07964348793029785 elapsed, loss: 2.9434366e-06\n",
      "step: 6480 train: 0.08041763305664062 elapsed, loss: 2.669165e-06\n",
      "step: 6490 train: 0.07593131065368652 elapsed, loss: 3.9921033e-06\n",
      "step: 6500 train: 0.08661985397338867 elapsed, loss: 2.8260883e-06\n",
      "step: 6510 train: 0.08337211608886719 elapsed, loss: 2.489885e-06\n",
      "step: 6520 train: 0.0878298282623291 elapsed, loss: 2.9522846e-06\n",
      "step: 6530 train: 0.0823519229888916 elapsed, loss: 2.3958232e-06\n",
      "step: 6540 train: 0.08526802062988281 elapsed, loss: 2.7837145e-06\n",
      "step: 6550 train: 0.08086490631103516 elapsed, loss: 3.509681e-06\n",
      "step: 6560 train: 0.0767829418182373 elapsed, loss: 3.3355223e-06\n",
      "step: 6570 train: 0.07986831665039062 elapsed, loss: 4.538768e-06\n",
      "step: 6580 train: 0.08757877349853516 elapsed, loss: 2.6435528e-06\n",
      "step: 6590 train: 0.08703422546386719 elapsed, loss: 3.2074636e-06\n",
      "step: 6600 train: 0.07697606086730957 elapsed, loss: 4.5829884e-06\n",
      "step: 6610 train: 0.08123373985290527 elapsed, loss: 3.0980332e-06\n",
      "step: 6620 train: 0.08367347717285156 elapsed, loss: 2.3422726e-06\n",
      "step: 6630 train: 0.07933568954467773 elapsed, loss: 3.7276e-06\n",
      "step: 6640 train: 0.08377552032470703 elapsed, loss: 2.611423e-06\n",
      "step: 6650 train: 0.0859067440032959 elapsed, loss: 3.312235e-06\n",
      "step: 6660 train: 0.07770776748657227 elapsed, loss: 3.01515e-06\n",
      "step: 6670 train: 0.0781710147857666 elapsed, loss: 3.058456e-06\n",
      "step: 6680 train: 0.07845616340637207 elapsed, loss: 3.632615e-06\n",
      "step: 6690 train: 0.08335161209106445 elapsed, loss: 2.471259e-06\n",
      "step: 6700 train: 0.08056950569152832 elapsed, loss: 2.5224826e-06\n",
      "step: 6710 train: 0.0774989128112793 elapsed, loss: 4.1630005e-06\n",
      "step: 6720 train: 0.08063793182373047 elapsed, loss: 3.0449526e-06\n",
      "step: 6730 train: 0.07971763610839844 elapsed, loss: 5.0449535e-06\n",
      "step: 6740 train: 0.08113813400268555 elapsed, loss: 3.857527e-06\n",
      "step: 6750 train: 0.08006787300109863 elapsed, loss: 2.562994e-06\n",
      "step: 6760 train: 0.08376908302307129 elapsed, loss: 2.7757997e-06\n",
      "step: 6770 train: 0.08438420295715332 elapsed, loss: 3.5571352e-06\n",
      "step: 6780 train: 0.08297371864318848 elapsed, loss: 2.3320267e-06\n",
      "step: 6790 train: 0.08688831329345703 elapsed, loss: 4.437735e-06\n",
      "step: 6800 train: 0.08229446411132812 elapsed, loss: 4.2146858e-06\n",
      "step: 6810 train: 0.08061003684997559 elapsed, loss: 3.0053707e-06\n",
      "step: 6820 train: 0.07998418807983398 elapsed, loss: 4.05523e-06\n",
      "step: 6830 train: 0.0819697380065918 elapsed, loss: 4.2989523e-06\n",
      "step: 6840 train: 0.08386635780334473 elapsed, loss: 2.9313271e-06\n",
      "step: 6850 train: 0.07903289794921875 elapsed, loss: 3.3774313e-06\n",
      "step: 6860 train: 0.0845804214477539 elapsed, loss: 2.4419237e-06\n",
      "step: 6870 train: 0.08183956146240234 elapsed, loss: 2.8172453e-06\n",
      "step: 6880 train: 0.08066225051879883 elapsed, loss: 3.5683524e-06\n",
      "step: 6890 train: 0.07762980461120605 elapsed, loss: 3.042625e-06\n",
      "step: 6900 train: 0.07978391647338867 elapsed, loss: 3.8221247e-06\n",
      "step: 6910 train: 0.08185935020446777 elapsed, loss: 3.0956985e-06\n",
      "step: 6920 train: 0.07986927032470703 elapsed, loss: 2.6696302e-06\n",
      "step: 6930 train: 0.07989144325256348 elapsed, loss: 4.1364574e-06\n",
      "step: 6940 train: 0.08501100540161133 elapsed, loss: 2.571377e-06\n",
      "step: 6950 train: 0.08160901069641113 elapsed, loss: 2.7599685e-06\n",
      "step: 6960 train: 0.07707357406616211 elapsed, loss: 3.3979206e-06\n",
      "step: 6970 train: 0.07755446434020996 elapsed, loss: 4.171821e-06\n",
      "step: 6980 train: 0.07909679412841797 elapsed, loss: 3.1944294e-06\n",
      "step: 6990 train: 0.07846879959106445 elapsed, loss: 4.576968e-06\n",
      "step: 7000 train: 0.07989835739135742 elapsed, loss: 4.365562e-06\n",
      "step: 7010 train: 0.08094358444213867 elapsed, loss: 4.019097e-06\n",
      "step: 7020 train: 0.08275508880615234 elapsed, loss: 3.986979e-06\n",
      "step: 7030 train: 0.0805673599243164 elapsed, loss: 3.5110743e-06\n",
      "step: 7040 train: 0.07868790626525879 elapsed, loss: 5.945528e-06\n",
      "step: 7050 train: 0.07996416091918945 elapsed, loss: 2.9262055e-06\n",
      "step: 7060 train: 0.07973408699035645 elapsed, loss: 3.345769e-06\n",
      "step: 7070 train: 0.08099102973937988 elapsed, loss: 4.601517e-06\n",
      "step: 7080 train: 0.07681727409362793 elapsed, loss: 8.400204e-06\n",
      "step: 7090 train: 0.08177900314331055 elapsed, loss: 5.4267775e-06\n",
      "step: 7100 train: 0.08131957054138184 elapsed, loss: 3.825863e-06\n",
      "step: 7110 train: 0.07631683349609375 elapsed, loss: 3.2759162e-06\n",
      "step: 7120 train: 0.08460211753845215 elapsed, loss: 2.6193388e-06\n",
      "step: 7130 train: 0.07529067993164062 elapsed, loss: 3.882673e-06\n",
      "step: 7140 train: 0.07811927795410156 elapsed, loss: 3.920856e-06\n",
      "step: 7150 train: 0.0837559700012207 elapsed, loss: 3.0775486e-06\n",
      "step: 7160 train: 0.08489465713500977 elapsed, loss: 2.490352e-06\n",
      "step: 7170 train: 0.07851719856262207 elapsed, loss: 3.800244e-06\n",
      "step: 7180 train: 0.08428072929382324 elapsed, loss: 2.95042e-06\n",
      "step: 7190 train: 0.07705497741699219 elapsed, loss: 3.6409947e-06\n",
      "step: 7200 train: 0.08099532127380371 elapsed, loss: 2.7394792e-06\n",
      "step: 7210 train: 0.07862591743469238 elapsed, loss: 4.056361e-06\n",
      "step: 7220 train: 0.07259869575500488 elapsed, loss: 5.4277198e-06\n",
      "step: 7230 train: 0.07624220848083496 elapsed, loss: 4.718992e-06\n",
      "step: 7240 train: 0.07953596115112305 elapsed, loss: 3.0151496e-06\n",
      "step: 7250 train: 0.07860946655273438 elapsed, loss: 3.6605506e-06\n",
      "step: 7260 train: 0.08083343505859375 elapsed, loss: 2.7171277e-06\n",
      "step: 7270 train: 0.08053708076477051 elapsed, loss: 3.4002385e-06\n",
      "step: 7280 train: 0.08794593811035156 elapsed, loss: 2.73948e-06\n",
      "step: 7290 train: 0.07541155815124512 elapsed, loss: 4.3902373e-06\n",
      "step: 7300 train: 0.07819032669067383 elapsed, loss: 3.98139e-06\n",
      "step: 7310 train: 0.0807342529296875 elapsed, loss: 3.7215225e-06\n",
      "step: 7320 train: 0.07681941986083984 elapsed, loss: 4.2910506e-06\n",
      "step: 7330 train: 0.07314252853393555 elapsed, loss: 4.057758e-06\n",
      "step: 7340 train: 0.0786287784576416 elapsed, loss: 3.1106101e-06\n",
      "step: 7350 train: 0.08235287666320801 elapsed, loss: 3.7662548e-06\n",
      "step: 7360 train: 0.08380961418151855 elapsed, loss: 2.9089792e-06\n",
      "step: 7370 train: 0.08040165901184082 elapsed, loss: 4.434478e-06\n",
      "step: 7380 train: 0.08053302764892578 elapsed, loss: 2.3855787e-06\n",
      "step: 7390 train: 0.08561015129089355 elapsed, loss: 2.770205e-06\n",
      "step: 7400 train: 0.08463501930236816 elapsed, loss: 2.9341236e-06\n",
      "step: 7410 train: 0.07778286933898926 elapsed, loss: 3.5175892e-06\n",
      "step: 7420 train: 0.07860612869262695 elapsed, loss: 3.698271e-06\n",
      "step: 7430 train: 0.08284997940063477 elapsed, loss: 3.635874e-06\n",
      "step: 7440 train: 0.0808556079864502 elapsed, loss: 3.5822545e-06\n",
      "step: 7450 train: 0.08164620399475098 elapsed, loss: 0.01646992\n",
      "step: 7460 train: 0.08136367797851562 elapsed, loss: 3.3383454e-05\n",
      "step: 7470 train: 0.08040785789489746 elapsed, loss: 1.5525595e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7480 train: 0.07448482513427734 elapsed, loss: 2.5397818e-05\n",
      "step: 7490 train: 0.09070229530334473 elapsed, loss: 8.222463e-06\n",
      "step: 7500 train: 0.07779765129089355 elapsed, loss: 6.752959e-06\n",
      "step: 7510 train: 0.08819365501403809 elapsed, loss: 2.2299033e-05\n",
      "step: 7520 train: 0.08488774299621582 elapsed, loss: 5.7313264e-06\n",
      "step: 7530 train: 0.07711601257324219 elapsed, loss: 6.589489e-06\n",
      "step: 7540 train: 0.0798807144165039 elapsed, loss: 5.5902246e-06\n",
      "step: 7550 train: 0.0853433609008789 elapsed, loss: 4.6356317e-06\n",
      "step: 7560 train: 0.07506275177001953 elapsed, loss: 5.1073494e-06\n",
      "step: 7570 train: 0.08550882339477539 elapsed, loss: 5.8128126e-06\n",
      "step: 7580 train: 0.08656167984008789 elapsed, loss: 3.5934963e-06\n",
      "step: 7590 train: 0.08239102363586426 elapsed, loss: 4.03026e-06\n",
      "step: 7600 train: 0.07987594604492188 elapsed, loss: 5.086292e-06\n",
      "step: 7610 train: 0.0832366943359375 elapsed, loss: 3.7504262e-06\n",
      "step: 7620 train: 0.08243703842163086 elapsed, loss: 3.5394642e-06\n",
      "step: 7630 train: 0.08442568778991699 elapsed, loss: 3.0337715e-06\n",
      "step: 7640 train: 0.08162546157836914 elapsed, loss: 2.5522786e-06\n",
      "step: 7650 train: 0.08204197883605957 elapsed, loss: 3.2130536e-06\n",
      "step: 7660 train: 0.085174560546875 elapsed, loss: 2.5764978e-06\n",
      "step: 7670 train: 0.08231139183044434 elapsed, loss: 3.2843002e-06\n",
      "step: 7680 train: 0.0894012451171875 elapsed, loss: 2.3483258e-06\n",
      "step: 7690 train: 0.08510661125183105 elapsed, loss: 1.30640565e-05\n",
      "step: 7700 train: 0.07827973365783691 elapsed, loss: 4.1480935e-06\n",
      "step: 7710 train: 0.08309006690979004 elapsed, loss: 5.0453755e-06\n",
      "step: 7720 train: 0.08490490913391113 elapsed, loss: 3.4118884e-06\n",
      "step: 7730 train: 0.08158564567565918 elapsed, loss: 3.4086297e-06\n",
      "step: 7740 train: 0.07954120635986328 elapsed, loss: 2.9411092e-06\n",
      "step: 7750 train: 0.07948708534240723 elapsed, loss: 3.331329e-06\n",
      "step: 7760 train: 0.08157038688659668 elapsed, loss: 2.664508e-06\n",
      "step: 7770 train: 0.08449745178222656 elapsed, loss: 2.5131685e-06\n",
      "step: 7780 train: 0.07856869697570801 elapsed, loss: 2.6556604e-06\n",
      "step: 7790 train: 0.08233332633972168 elapsed, loss: 2.7147998e-06\n",
      "step: 7800 train: 0.08196163177490234 elapsed, loss: 2.332958e-06\n",
      "step: 7810 train: 0.08738160133361816 elapsed, loss: 2.2719569e-06\n",
      "step: 7820 train: 0.07904171943664551 elapsed, loss: 2.7418087e-06\n",
      "step: 7830 train: 0.08298897743225098 elapsed, loss: 3.3681192e-06\n",
      "step: 7840 train: 0.07344627380371094 elapsed, loss: 4.0982695e-06\n",
      "step: 7850 train: 0.0783698558807373 elapsed, loss: 2.6631087e-06\n",
      "step: 7860 train: 0.07846856117248535 elapsed, loss: 3.3914023e-06\n",
      "step: 7870 train: 0.07238316535949707 elapsed, loss: 5.1646302e-06\n",
      "step: 7880 train: 0.07842206954956055 elapsed, loss: 2.9671837e-06\n",
      "step: 7890 train: 0.08038449287414551 elapsed, loss: 3.1818463e-06\n",
      "step: 7900 train: 0.07989883422851562 elapsed, loss: 2.8889567e-06\n",
      "step: 7910 train: 0.08169198036193848 elapsed, loss: 2.3450652e-06\n",
      "step: 7920 train: 0.07481598854064941 elapsed, loss: 4.925279e-06\n",
      "step: 7930 train: 0.07919645309448242 elapsed, loss: 3.6731224e-06\n",
      "step: 7940 train: 0.08060789108276367 elapsed, loss: 2.7678846e-06\n",
      "step: 7950 train: 0.08428478240966797 elapsed, loss: 2.479174e-06\n",
      "step: 7960 train: 0.08254408836364746 elapsed, loss: 3.2000146e-06\n",
      "step: 7970 train: 0.07912540435791016 elapsed, loss: 3.4705627e-06\n",
      "step: 7980 train: 0.08191251754760742 elapsed, loss: 2.9848763e-06\n",
      "step: 7990 train: 0.0798182487487793 elapsed, loss: 2.4263729e-05\n",
      "step: 8000 train: 0.08618950843811035 elapsed, loss: 0.00022886667\n",
      "step: 8010 train: 0.08339619636535645 elapsed, loss: 3.6182762e-05\n",
      "step: 8020 train: 0.08493590354919434 elapsed, loss: 1.6787395e-05\n",
      "step: 8030 train: 0.0855875015258789 elapsed, loss: 9.190162e-06\n",
      "step: 8040 train: 0.08077192306518555 elapsed, loss: 1.0654042e-05\n",
      "step: 8050 train: 0.08029747009277344 elapsed, loss: 7.923142e-06\n",
      "step: 8060 train: 0.08172321319580078 elapsed, loss: 6.676973e-06\n",
      "step: 8070 train: 0.08301925659179688 elapsed, loss: 4.909907e-06\n",
      "step: 8080 train: 0.07927560806274414 elapsed, loss: 8.967601e-06\n",
      "step: 8090 train: 0.08332276344299316 elapsed, loss: 8.760713e-06\n",
      "step: 8100 train: 0.08203005790710449 elapsed, loss: 4.044716e-06\n",
      "step: 8110 train: 0.07536959648132324 elapsed, loss: 5.655422e-06\n",
      "step: 8120 train: 0.08648920059204102 elapsed, loss: 3.037959e-06\n",
      "step: 8130 train: 0.08141827583312988 elapsed, loss: 3.2996663e-06\n",
      "step: 8140 train: 0.0827021598815918 elapsed, loss: 3.164604e-06\n",
      "step: 8150 train: 0.0806119441986084 elapsed, loss: 3.5390144e-06\n",
      "step: 8160 train: 0.0784463882446289 elapsed, loss: 5.1944144e-06\n",
      "step: 8170 train: 0.08123087882995605 elapsed, loss: 5.6869208e-06\n",
      "step: 8180 train: 0.08546781539916992 elapsed, loss: 0.0043459437\n",
      "step: 8190 train: 0.08372068405151367 elapsed, loss: 1.1857306e-05\n",
      "step: 8200 train: 0.07777953147888184 elapsed, loss: 1.103153e-05\n",
      "step: 8210 train: 0.08143377304077148 elapsed, loss: 8.860021e-06\n",
      "step: 8220 train: 0.07630133628845215 elapsed, loss: 1.2628893e-05\n",
      "step: 8230 train: 0.08312582969665527 elapsed, loss: 8.2437155e-06\n",
      "step: 8240 train: 0.08302855491638184 elapsed, loss: 8.0445925e-06\n",
      "step: 8250 train: 0.07635879516601562 elapsed, loss: 9.215306e-06\n",
      "step: 8260 train: 0.08082246780395508 elapsed, loss: 1.852216e-05\n",
      "step: 8270 train: 0.07558369636535645 elapsed, loss: 6.5587956e-06\n",
      "step: 8280 train: 0.07782983779907227 elapsed, loss: 5.1571706e-06\n",
      "step: 8290 train: 0.07808113098144531 elapsed, loss: 8.139663e-06\n",
      "step: 8300 train: 0.0842142105102539 elapsed, loss: 3.4989625e-06\n",
      "step: 8310 train: 0.08168387413024902 elapsed, loss: 4.6412224e-06\n",
      "step: 8320 train: 0.08408355712890625 elapsed, loss: 3.3164308e-06\n",
      "step: 8330 train: 0.08404254913330078 elapsed, loss: 2.760432e-06\n",
      "step: 8340 train: 0.08277225494384766 elapsed, loss: 2.4763794e-06\n",
      "step: 8350 train: 0.08904361724853516 elapsed, loss: 2.708743e-06\n",
      "step: 8360 train: 0.08111071586608887 elapsed, loss: 3.1706786e-06\n",
      "step: 8370 train: 0.08078765869140625 elapsed, loss: 2.8977947e-06\n",
      "step: 8380 train: 0.07992005348205566 elapsed, loss: 2.697105e-06\n",
      "step: 8390 train: 0.08210372924804688 elapsed, loss: 2.9513542e-06\n",
      "step: 8400 train: 0.0802011489868164 elapsed, loss: 1.9706754e-06\n",
      "step: 8410 train: 0.07712483406066895 elapsed, loss: 2.7120054e-06\n",
      "step: 8420 train: 0.0833134651184082 elapsed, loss: 2.191398e-06\n",
      "step: 8430 train: 0.08417320251464844 elapsed, loss: 2.331559e-06\n",
      "step: 8440 train: 0.07982587814331055 elapsed, loss: 1.984179e-06\n",
      "step: 8450 train: 0.07825803756713867 elapsed, loss: 3.874285e-06\n",
      "step: 8460 train: 0.08131027221679688 elapsed, loss: 3.4379593e-06\n",
      "step: 8470 train: 0.07955670356750488 elapsed, loss: 2.152748e-06\n",
      "step: 8480 train: 0.0755605697631836 elapsed, loss: 3.0635779e-06\n",
      "step: 8490 train: 0.08056116104125977 elapsed, loss: 2.750637e-06\n",
      "step: 8500 train: 0.08070254325866699 elapsed, loss: 3.1962793e-06\n",
      "step: 8510 train: 0.08387589454650879 elapsed, loss: 3.5813807e-06\n",
      "step: 8520 train: 0.08491754531860352 elapsed, loss: 2.1131646e-06\n",
      "step: 8530 train: 0.08160662651062012 elapsed, loss: 2.3385455e-06\n",
      "step: 8540 train: 0.08288788795471191 elapsed, loss: 2.7031165e-06\n",
      "step: 8550 train: 0.08297109603881836 elapsed, loss: 2.2258555e-06\n",
      "step: 8560 train: 0.08044719696044922 elapsed, loss: 2.1299302e-06\n",
      "step: 8570 train: 0.07616949081420898 elapsed, loss: 3.1222517e-06\n",
      "step: 8580 train: 0.07710695266723633 elapsed, loss: 3.6493766e-06\n",
      "step: 8590 train: 0.08210444450378418 elapsed, loss: 2.9294683e-06\n",
      "step: 8600 train: 0.08009648323059082 elapsed, loss: 3.3643921e-06\n",
      "step: 8610 train: 0.0821845531463623 elapsed, loss: 2.6151486e-06\n",
      "step: 8620 train: 0.08117151260375977 elapsed, loss: 2.8791778e-06\n",
      "step: 8630 train: 0.08347249031066895 elapsed, loss: 2.2188724e-06\n",
      "step: 8640 train: 0.08258652687072754 elapsed, loss: 2.498266e-06\n",
      "step: 8650 train: 0.08616232872009277 elapsed, loss: 2.941576e-06\n",
      "step: 8660 train: 0.08044743537902832 elapsed, loss: 2.2351692e-06\n",
      "step: 8670 train: 0.07878351211547852 elapsed, loss: 2.4600836e-06\n",
      "step: 8680 train: 0.07772636413574219 elapsed, loss: 2.5192212e-06\n",
      "step: 8690 train: 0.08073639869689941 elapsed, loss: 2.6849743e-06\n",
      "step: 8700 train: 0.08600664138793945 elapsed, loss: 2.289187e-06\n",
      "step: 8710 train: 0.07629942893981934 elapsed, loss: 3.936221e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8720 train: 0.07963442802429199 elapsed, loss: 2.1662524e-06\n",
      "step: 8730 train: 0.07840228080749512 elapsed, loss: 2.72411e-06\n",
      "step: 8740 train: 0.07697081565856934 elapsed, loss: 3.2274884e-06\n",
      "step: 8750 train: 0.07800889015197754 elapsed, loss: 2.8088575e-06\n",
      "step: 8760 train: 0.07887721061706543 elapsed, loss: 2.420968e-06\n",
      "step: 8770 train: 0.07777619361877441 elapsed, loss: 4.099671e-06\n",
      "step: 8780 train: 0.08385229110717773 elapsed, loss: 2.9583398e-06\n",
      "step: 8790 train: 0.07877230644226074 elapsed, loss: 2.7795252e-06\n",
      "step: 8800 train: 0.08664155006408691 elapsed, loss: 2.082899e-06\n",
      "step: 8810 train: 0.08107757568359375 elapsed, loss: 2.7944225e-06\n",
      "step: 8820 train: 0.07652091979980469 elapsed, loss: 5.9823005e-06\n",
      "step: 8830 train: 0.08053183555603027 elapsed, loss: 3.1017598e-06\n",
      "step: 8840 train: 0.08283233642578125 elapsed, loss: 2.458683e-06\n",
      "step: 8850 train: 0.0764460563659668 elapsed, loss: 4.0414625e-06\n",
      "step: 8860 train: 0.08331060409545898 elapsed, loss: 3.596758e-06\n",
      "step: 8870 train: 0.08324670791625977 elapsed, loss: 4.3869695e-06\n",
      "step: 8880 train: 0.07883596420288086 elapsed, loss: 5.624227e-06\n",
      "step: 8890 train: 0.08354043960571289 elapsed, loss: 2.2705601e-06\n",
      "step: 8900 train: 0.08414721488952637 elapsed, loss: 4.0246996e-06\n",
      "step: 8910 train: 0.0845332145690918 elapsed, loss: 4.445894e-06\n",
      "step: 8920 train: 0.0802621841430664 elapsed, loss: 2.8521683e-06\n",
      "step: 8930 train: 0.0851907730102539 elapsed, loss: 1.1494973e-05\n",
      "step: 8940 train: 0.07923698425292969 elapsed, loss: 7.2605185e-06\n",
      "step: 8950 train: 0.07625985145568848 elapsed, loss: 1.1472682e-05\n",
      "step: 8960 train: 0.07947182655334473 elapsed, loss: 7.362425e-06\n",
      "step: 8970 train: 0.08265376091003418 elapsed, loss: 4.007835e-06\n",
      "step: 8980 train: 0.0738992691040039 elapsed, loss: 7.685543e-06\n",
      "step: 8990 train: 0.07924509048461914 elapsed, loss: 5.1400466e-06\n",
      "step: 9000 train: 0.08106851577758789 elapsed, loss: 2.81026e-06\n",
      "step: 9010 train: 0.08014082908630371 elapsed, loss: 3.6898896e-06\n",
      "step: 9020 train: 0.07937359809875488 elapsed, loss: 3.3848828e-06\n",
      "step: 9030 train: 0.08429884910583496 elapsed, loss: 2.3129344e-06\n",
      "step: 9040 train: 0.07744884490966797 elapsed, loss: 4.4070025e-06\n",
      "step: 9050 train: 0.07858729362487793 elapsed, loss: 3.5841847e-06\n",
      "step: 9060 train: 0.07678675651550293 elapsed, loss: 3.4216705e-06\n",
      "step: 9070 train: 0.07756400108337402 elapsed, loss: 2.379525e-06\n",
      "step: 9080 train: 0.07839226722717285 elapsed, loss: 3.2056055e-06\n",
      "step: 9090 train: 0.08022832870483398 elapsed, loss: 4.5480606e-06\n",
      "step: 9100 train: 0.07312965393066406 elapsed, loss: 0.013445957\n",
      "step: 9110 train: 0.0825505256652832 elapsed, loss: 0.0001116034\n",
      "step: 9120 train: 0.07812809944152832 elapsed, loss: 6.726307e-05\n",
      "step: 9130 train: 0.08657574653625488 elapsed, loss: 2.7975113e-05\n",
      "step: 9140 train: 0.08105945587158203 elapsed, loss: 2.7781021e-05\n",
      "step: 9150 train: 0.07761931419372559 elapsed, loss: 1.4738469e-05\n",
      "step: 9160 train: 0.08325338363647461 elapsed, loss: 1.6078327e-05\n",
      "step: 9170 train: 0.08506131172180176 elapsed, loss: 9.5248615e-06\n",
      "step: 9180 train: 0.08548855781555176 elapsed, loss: 7.395951e-06\n",
      "step: 9190 train: 0.07956290245056152 elapsed, loss: 0.00058819144\n",
      "step: 9200 train: 0.08112502098083496 elapsed, loss: 2.8640656e-05\n",
      "step: 9210 train: 0.08772468566894531 elapsed, loss: 2.6155416e-05\n",
      "step: 9220 train: 0.08524155616760254 elapsed, loss: 1.7842987e-05\n",
      "step: 9230 train: 0.08357739448547363 elapsed, loss: 1.5593854e-05\n",
      "step: 9240 train: 0.08342385292053223 elapsed, loss: 1.4021946e-05\n",
      "step: 9250 train: 0.0821380615234375 elapsed, loss: 1.13921715e-05\n",
      "step: 9260 train: 0.0754539966583252 elapsed, loss: 8.5741085e-06\n",
      "step: 9270 train: 0.07979369163513184 elapsed, loss: 7.4978884e-06\n",
      "step: 9280 train: 0.07776546478271484 elapsed, loss: 6.392065e-06\n",
      "step: 9290 train: 0.08308053016662598 elapsed, loss: 4.216538e-06\n",
      "step: 9300 train: 0.0797417163848877 elapsed, loss: 4.8581887e-06\n",
      "step: 9310 train: 0.07596802711486816 elapsed, loss: 4.2458773e-06\n",
      "step: 9320 train: 0.0844888687133789 elapsed, loss: 3.3783606e-06\n",
      "step: 9330 train: 0.07207870483398438 elapsed, loss: 4.8931443e-06\n",
      "step: 9340 train: 0.07971310615539551 elapsed, loss: 3.7993184e-06\n",
      "step: 9350 train: 0.08176159858703613 elapsed, loss: 2.4130518e-06\n",
      "step: 9360 train: 0.07876467704772949 elapsed, loss: 4.342741e-06\n",
      "step: 9370 train: 0.08346414566040039 elapsed, loss: 2.7525161e-06\n",
      "step: 9380 train: 0.07282757759094238 elapsed, loss: 3.283835e-06\n",
      "step: 9390 train: 0.08005571365356445 elapsed, loss: 2.8591535e-06\n",
      "step: 9400 train: 0.08559346199035645 elapsed, loss: 2.207696e-06\n",
      "step: 9410 train: 0.07823514938354492 elapsed, loss: 2.6551947e-06\n",
      "step: 9420 train: 0.0758509635925293 elapsed, loss: 3.3029269e-06\n",
      "step: 9430 train: 0.08244609832763672 elapsed, loss: 2.5671848e-06\n",
      "step: 9440 train: 0.07629561424255371 elapsed, loss: 2.844253e-06\n",
      "step: 9450 train: 0.07631564140319824 elapsed, loss: 3.345302e-06\n",
      "step: 9460 train: 0.08486294746398926 elapsed, loss: 2.980684e-06\n",
      "step: 9470 train: 0.08044981956481934 elapsed, loss: 2.1415722e-06\n",
      "step: 9480 train: 0.07795405387878418 elapsed, loss: 3.2079329e-06\n",
      "step: 9490 train: 0.08336091041564941 elapsed, loss: 1.9613622e-06\n",
      "step: 9500 train: 0.08055663108825684 elapsed, loss: 2.9909133e-06\n",
      "step: 9510 train: 0.08185219764709473 elapsed, loss: 3.5767137e-06\n",
      "step: 9520 train: 0.08231282234191895 elapsed, loss: 1.7681136e-06\n",
      "step: 9530 train: 0.08459949493408203 elapsed, loss: 6.522857e-05\n",
      "step: 9540 train: 0.08295941352844238 elapsed, loss: 7.27748e-06\n",
      "step: 9550 train: 0.08029508590698242 elapsed, loss: 4.3995406e-06\n",
      "step: 9560 train: 0.08408284187316895 elapsed, loss: 3.4915156e-06\n",
      "step: 9570 train: 0.08291912078857422 elapsed, loss: 3.1664863e-06\n",
      "step: 9580 train: 0.08175897598266602 elapsed, loss: 2.696638e-06\n",
      "step: 9590 train: 0.0825345516204834 elapsed, loss: 2.5466961e-06\n",
      "step: 9600 train: 0.07431387901306152 elapsed, loss: 4.3781274e-06\n",
      "step: 9610 train: 0.08522224426269531 elapsed, loss: 1.9106055e-06\n",
      "step: 9620 train: 0.08187294006347656 elapsed, loss: 2.3939597e-06\n",
      "step: 9630 train: 0.07992386817932129 elapsed, loss: 2.5024588e-06\n",
      "step: 9640 train: 0.07839226722717285 elapsed, loss: 2.7171268e-06\n",
      "step: 9650 train: 0.0781409740447998 elapsed, loss: 4.217479e-06\n",
      "step: 9660 train: 0.079925537109375 elapsed, loss: 2.228183e-06\n",
      "step: 9670 train: 0.07615017890930176 elapsed, loss: 2.6482085e-06\n",
      "step: 9680 train: 0.08794903755187988 elapsed, loss: 2.18255e-06\n",
      "step: 9690 train: 0.08163857460021973 elapsed, loss: 2.129931e-06\n",
      "step: 9700 train: 0.08297348022460938 elapsed, loss: 2.2212005e-06\n",
      "step: 9710 train: 0.08133316040039062 elapsed, loss: 1.839825e-06\n",
      "step: 9720 train: 0.0795891284942627 elapsed, loss: 2.4679994e-06\n",
      "step: 9730 train: 0.0732583999633789 elapsed, loss: 4.6472774e-06\n",
      "step: 9740 train: 0.09244346618652344 elapsed, loss: 1.971141e-06\n",
      "step: 9750 train: 0.07852768898010254 elapsed, loss: 2.1113024e-06\n",
      "step: 9760 train: 0.08563399314880371 elapsed, loss: 2.0666002e-06\n",
      "step: 9770 train: 0.08433055877685547 elapsed, loss: 3.6096299e-06\n",
      "step: 9780 train: 0.07829475402832031 elapsed, loss: 2.315264e-06\n",
      "step: 9790 train: 0.0864708423614502 elapsed, loss: 2.0693956e-06\n",
      "step: 9800 train: 0.07664918899536133 elapsed, loss: 2.4312133e-06\n",
      "step: 9810 train: 0.07847452163696289 elapsed, loss: 2.2426204e-06\n",
      "step: 9820 train: 0.0838310718536377 elapsed, loss: 0.00011438846\n",
      "step: 9830 train: 0.08565855026245117 elapsed, loss: 4.79913e-05\n",
      "step: 9840 train: 0.08153629302978516 elapsed, loss: 1.5878295e-05\n",
      "step: 9850 train: 0.07885169982910156 elapsed, loss: 0.00013843554\n",
      "step: 9860 train: 0.07586836814880371 elapsed, loss: 1.1269574e-05\n",
      "step: 9870 train: 0.08301877975463867 elapsed, loss: 2.0215604e-05\n",
      "step: 9880 train: 0.08668708801269531 elapsed, loss: 8.07751e-06\n",
      "step: 9890 train: 0.08505630493164062 elapsed, loss: 0.0002208508\n",
      "step: 9900 train: 0.08044981956481934 elapsed, loss: 0.04234421\n",
      "step: 9910 train: 0.08077430725097656 elapsed, loss: 2.876834e-05\n",
      "step: 9920 train: 0.0838325023651123 elapsed, loss: 2.9357465e-05\n",
      "step: 9930 train: 0.07988190650939941 elapsed, loss: 3.380228e-05\n",
      "step: 9940 train: 0.07981681823730469 elapsed, loss: 1.830942e-05\n",
      "step: 9950 train: 0.08227849006652832 elapsed, loss: 1.7787452e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9960 train: 0.08116507530212402 elapsed, loss: 1.241273e-05\n",
      "step: 9970 train: 0.08107113838195801 elapsed, loss: 2.5176902e-05\n",
      "step: 9980 train: 0.07289814949035645 elapsed, loss: 1.0224163e-05\n",
      "step: 9990 train: 0.07734823226928711 elapsed, loss: 1.131465e-05\n",
      "step: 10000 train: 0.07537674903869629 elapsed, loss: 1.511809e-05\n",
      "step: 10010 train: 0.0753943920135498 elapsed, loss: 5.571541e-06\n",
      "step: 10020 train: 0.08078312873840332 elapsed, loss: 6.419097e-06\n",
      "step: 10030 train: 0.0808877944946289 elapsed, loss: 3.5832447e-06\n",
      "step: 10040 train: 0.07987546920776367 elapsed, loss: 3.2894204e-06\n",
      "step: 10050 train: 0.08098411560058594 elapsed, loss: 4.3278333e-06\n",
      "step: 10060 train: 0.08371186256408691 elapsed, loss: 2.1248077e-06\n",
      "step: 10070 train: 0.08044981956481934 elapsed, loss: 3.542274e-06\n",
      "step: 10080 train: 0.08153295516967773 elapsed, loss: 2.3874377e-06\n",
      "step: 10090 train: 0.08214879035949707 elapsed, loss: 2.1560018e-06\n",
      "step: 10100 train: 0.08614897727966309 elapsed, loss: 1.7224789e-06\n",
      "step: 10110 train: 0.08832716941833496 elapsed, loss: 2.3944258e-06\n",
      "step: 10120 train: 0.08275938034057617 elapsed, loss: 2.8735885e-06\n",
      "step: 10130 train: 0.07872605323791504 elapsed, loss: 2.337149e-06\n",
      "step: 10140 train: 0.08290886878967285 elapsed, loss: 1.757868e-06\n",
      "step: 10150 train: 0.07573366165161133 elapsed, loss: 3.7788298e-06\n",
      "step: 10160 train: 0.07901978492736816 elapsed, loss: 1.9543772e-06\n",
      "step: 10170 train: 0.08070230484008789 elapsed, loss: 2.0042023e-06\n",
      "step: 10180 train: 0.0800943374633789 elapsed, loss: 2.0847615e-06\n",
      "step: 10190 train: 0.07993483543395996 elapsed, loss: 2.0666012e-06\n",
      "step: 10200 train: 0.08275246620178223 elapsed, loss: 1.978592e-06\n",
      "step: 10210 train: 0.08836603164672852 elapsed, loss: 2.4721899e-06\n",
      "step: 10220 train: 0.08372831344604492 elapsed, loss: 2.7115398e-06\n",
      "step: 10230 train: 0.07930850982666016 elapsed, loss: 2.0740522e-06\n",
      "step: 10240 train: 0.08392167091369629 elapsed, loss: 2.5061363e-06\n",
      "step: 10250 train: 0.08091878890991211 elapsed, loss: 2.8856666e-06\n",
      "step: 10260 train: 0.08333683013916016 elapsed, loss: 2.320386e-06\n",
      "step: 10270 train: 0.08877396583557129 elapsed, loss: 2.2207282e-06\n",
      "step: 10280 train: 0.08698511123657227 elapsed, loss: 1.9124682e-06\n",
      "step: 10290 train: 0.07590126991271973 elapsed, loss: 2.9839503e-06\n",
      "step: 10300 train: 0.08493518829345703 elapsed, loss: 1.7797541e-06\n",
      "step: 10310 train: 0.08089113235473633 elapsed, loss: 2.3720736e-06\n",
      "step: 10320 train: 0.07678890228271484 elapsed, loss: 3.036569e-06\n",
      "step: 10330 train: 0.08045768737792969 elapsed, loss: 2.5485579e-06\n",
      "step: 10340 train: 0.08004450798034668 elapsed, loss: 3.810906e-06\n",
      "step: 10350 train: 0.07990097999572754 elapsed, loss: 2.8079317e-06\n",
      "step: 10360 train: 0.08222246170043945 elapsed, loss: 3.3997808e-06\n",
      "step: 10370 train: 0.0834660530090332 elapsed, loss: 2.057286e-06\n",
      "step: 10380 train: 0.08481669425964355 elapsed, loss: 2.1452975e-06\n",
      "step: 10390 train: 0.07660984992980957 elapsed, loss: 3.7052462e-06\n",
      "step: 10400 train: 0.08097314834594727 elapsed, loss: 2.3376156e-06\n",
      "step: 10410 train: 0.08186864852905273 elapsed, loss: 3.3257415e-06\n",
      "step: 10420 train: 0.08076786994934082 elapsed, loss: 2.0321422e-06\n",
      "step: 10430 train: 0.08477640151977539 elapsed, loss: 2.0009438e-06\n",
      "step: 10440 train: 0.08394718170166016 elapsed, loss: 2.6612493e-06\n",
      "step: 10450 train: 0.08455467224121094 elapsed, loss: 1.7709074e-06\n",
      "step: 10460 train: 0.08283519744873047 elapsed, loss: 1.8663677e-06\n",
      "step: 10470 train: 0.07878327369689941 elapsed, loss: 4.661717e-06\n",
      "step: 10480 train: 0.08184146881103516 elapsed, loss: 1.855192e-06\n",
      "step: 10490 train: 0.0794363021850586 elapsed, loss: 2.9280677e-06\n",
      "step: 10500 train: 0.08152151107788086 elapsed, loss: 2.3799898e-06\n",
      "step: 10510 train: 0.08043670654296875 elapsed, loss: 1.8626424e-06\n",
      "step: 10520 train: 0.08123302459716797 elapsed, loss: 3.7755703e-06\n",
      "step: 10530 train: 0.07871150970458984 elapsed, loss: 2.2100248e-06\n",
      "step: 10540 train: 0.07783150672912598 elapsed, loss: 2.5667202e-06\n",
      "step: 10550 train: 0.07975912094116211 elapsed, loss: 3.1571765e-06\n",
      "step: 10560 train: 0.08660340309143066 elapsed, loss: 2.078708e-06\n",
      "step: 10570 train: 0.07940411567687988 elapsed, loss: 2.3213179e-06\n",
      "step: 10580 train: 0.0839841365814209 elapsed, loss: 2.3064156e-06\n",
      "step: 10590 train: 0.07885861396789551 elapsed, loss: 2.1322599e-06\n",
      "step: 10600 train: 0.07869696617126465 elapsed, loss: 2.4652065e-06\n",
      "step: 10610 train: 0.0819251537322998 elapsed, loss: 3.203273e-06\n",
      "step: 10620 train: 0.0821065902709961 elapsed, loss: 3.0672884e-06\n",
      "step: 10630 train: 0.07617855072021484 elapsed, loss: 2.5671861e-06\n",
      "step: 10640 train: 0.08295297622680664 elapsed, loss: 2.3208518e-06\n",
      "step: 10650 train: 0.07584500312805176 elapsed, loss: 3.182788e-06\n",
      "step: 10660 train: 0.08143877983093262 elapsed, loss: 6.9092634e-06\n",
      "step: 10670 train: 0.07623744010925293 elapsed, loss: 3.824465e-06\n",
      "step: 10680 train: 0.07884669303894043 elapsed, loss: 3.9082806e-06\n",
      "step: 10690 train: 0.07368254661560059 elapsed, loss: 2.9867451e-06\n",
      "step: 10700 train: 0.07811379432678223 elapsed, loss: 3.1180616e-06\n",
      "step: 10710 train: 0.07713055610656738 elapsed, loss: 3.2675373e-06\n",
      "step: 10720 train: 0.0837545394897461 elapsed, loss: 2.191398e-06\n",
      "step: 10730 train: 0.08339357376098633 elapsed, loss: 2.257988e-06\n",
      "step: 10740 train: 0.08585071563720703 elapsed, loss: 3.025392e-06\n",
      "step: 10750 train: 0.08188533782958984 elapsed, loss: 1.1332086e-05\n",
      "step: 10760 train: 0.08221149444580078 elapsed, loss: 0.000100592355\n",
      "step: 10770 train: 0.08293700218200684 elapsed, loss: 0.00020954401\n",
      "step: 10780 train: 0.08326220512390137 elapsed, loss: 6.049065e-05\n",
      "step: 10790 train: 0.07753705978393555 elapsed, loss: 1.3301287e-05\n",
      "step: 10800 train: 0.08123111724853516 elapsed, loss: 0.0016330564\n",
      "step: 10810 train: 0.08633017539978027 elapsed, loss: 2.3770466e-05\n",
      "step: 10820 train: 0.08156418800354004 elapsed, loss: 1.3135083e-05\n",
      "step: 10830 train: 0.08301496505737305 elapsed, loss: 3.52539e-05\n",
      "step: 10840 train: 0.0780341625213623 elapsed, loss: 1.0024132e-05\n",
      "step: 10850 train: 0.07813262939453125 elapsed, loss: 1.058196e-05\n",
      "step: 10860 train: 0.07944607734680176 elapsed, loss: 0.00048090843\n",
      "step: 10870 train: 0.07805371284484863 elapsed, loss: 4.082587e-05\n",
      "step: 10880 train: 0.07640576362609863 elapsed, loss: 0.023156572\n",
      "step: 10890 train: 0.08598709106445312 elapsed, loss: 5.3038733e-05\n",
      "step: 10900 train: 0.07639312744140625 elapsed, loss: 5.397382e-05\n",
      "step: 10910 train: 0.07651138305664062 elapsed, loss: 2.2212751e-05\n",
      "step: 10920 train: 0.09231042861938477 elapsed, loss: 1.265758e-05\n",
      "step: 10930 train: 0.07529330253601074 elapsed, loss: 1.963226e-05\n",
      "step: 10940 train: 0.07663893699645996 elapsed, loss: 1.0214119e-05\n",
      "step: 10950 train: 0.07357168197631836 elapsed, loss: 1.1004287e-05\n",
      "step: 10960 train: 0.07888650894165039 elapsed, loss: 7.806743e-06\n",
      "step: 10970 train: 0.08354330062866211 elapsed, loss: 6.1336336e-06\n",
      "step: 10980 train: 0.07935404777526855 elapsed, loss: 5.0672866e-06\n",
      "step: 10990 train: 0.08134818077087402 elapsed, loss: 5.3047784e-06\n",
      "step: 11000 train: 0.08428096771240234 elapsed, loss: 3.146928e-06\n",
      "step: 11010 train: 0.08172750473022461 elapsed, loss: 2.8363352e-06\n",
      "step: 11020 train: 0.08011245727539062 elapsed, loss: 3.7876744e-06\n",
      "step: 11030 train: 0.07574224472045898 elapsed, loss: 4.114569e-06\n",
      "step: 11040 train: 0.09023165702819824 elapsed, loss: 2.1951228e-06\n",
      "step: 11050 train: 0.07583069801330566 elapsed, loss: 3.391393e-06\n",
      "step: 11060 train: 0.08195018768310547 elapsed, loss: 4.131277e-06\n",
      "step: 11070 train: 0.07804417610168457 elapsed, loss: 2.6742869e-06\n",
      "step: 11080 train: 0.08329534530639648 elapsed, loss: 3.571081e-06\n",
      "step: 11090 train: 0.08076071739196777 elapsed, loss: 2.7241103e-06\n",
      "step: 11100 train: 0.08018612861633301 elapsed, loss: 2.2342242e-06\n",
      "step: 11110 train: 0.07526731491088867 elapsed, loss: 3.5105961e-06\n",
      "step: 11120 train: 0.08187627792358398 elapsed, loss: 1.8733526e-06\n",
      "step: 11130 train: 0.07982373237609863 elapsed, loss: 1.5705538e-05\n",
      "step: 11140 train: 0.08455562591552734 elapsed, loss: 2.9871999e-06\n",
      "step: 11150 train: 0.0803530216217041 elapsed, loss: 2.2752165e-06\n",
      "step: 11160 train: 0.0774221420288086 elapsed, loss: 2.4451806e-06\n",
      "step: 11170 train: 0.07978701591491699 elapsed, loss: 2.4530991e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11180 train: 0.07973313331604004 elapsed, loss: 2.0242264e-06\n",
      "step: 11190 train: 0.08001279830932617 elapsed, loss: 2.1075778e-06\n",
      "step: 11200 train: 0.07667970657348633 elapsed, loss: 1.8826662e-06\n",
      "step: 11210 train: 0.08689665794372559 elapsed, loss: 2.3068767e-06\n",
      "step: 11220 train: 0.08933806419372559 elapsed, loss: 1.7601969e-06\n",
      "step: 11230 train: 0.07971358299255371 elapsed, loss: 2.70409e-06\n",
      "step: 11240 train: 0.07875967025756836 elapsed, loss: 1.923644e-06\n",
      "step: 11250 train: 0.07936525344848633 elapsed, loss: 2.0763775e-06\n",
      "step: 11260 train: 0.0834052562713623 elapsed, loss: 1.8617104e-06\n",
      "step: 11270 train: 0.0821523666381836 elapsed, loss: 2.1257313e-06\n",
      "step: 11280 train: 0.0807955265045166 elapsed, loss: 1.7848774e-06\n",
      "step: 11290 train: 0.08286452293395996 elapsed, loss: 3.0202693e-06\n",
      "step: 11300 train: 0.08153676986694336 elapsed, loss: 2.7711383e-06\n",
      "step: 11310 train: 0.07706356048583984 elapsed, loss: 3.0081608e-06\n",
      "step: 11320 train: 0.07999944686889648 elapsed, loss: 1.8579856e-06\n",
      "step: 11330 train: 0.07752108573913574 elapsed, loss: 1.9352856e-06\n",
      "step: 11340 train: 0.07795953750610352 elapsed, loss: 2.7390147e-06\n",
      "step: 11350 train: 0.07544374465942383 elapsed, loss: 3.3918682e-06\n",
      "step: 11360 train: 0.08035945892333984 elapsed, loss: 3.0137528e-06\n",
      "step: 11370 train: 0.08141660690307617 elapsed, loss: 1.7988473e-06\n",
      "step: 11380 train: 0.07945489883422852 elapsed, loss: 2.4796386e-06\n",
      "step: 11390 train: 0.08060550689697266 elapsed, loss: 1.7252729e-06\n",
      "step: 11400 train: 0.08261251449584961 elapsed, loss: 2.585784e-06\n",
      "step: 11410 train: 0.08165645599365234 elapsed, loss: 2.0917475e-06\n",
      "step: 11420 train: 0.08024954795837402 elapsed, loss: 2.1699761e-06\n",
      "step: 11430 train: 0.0809626579284668 elapsed, loss: 2.4111896e-06\n",
      "step: 11440 train: 0.08370542526245117 elapsed, loss: 2.0922034e-06\n",
      "step: 11450 train: 0.08167648315429688 elapsed, loss: 1.9562399e-06\n",
      "step: 11460 train: 0.07750129699707031 elapsed, loss: 2.6845323e-06\n",
      "step: 11470 train: 0.08156919479370117 elapsed, loss: 2.5071126e-06\n",
      "step: 11480 train: 0.07946372032165527 elapsed, loss: 1.9422705e-06\n",
      "step: 11490 train: 0.07939624786376953 elapsed, loss: 4.0391337e-06\n",
      "step: 11500 train: 0.07900667190551758 elapsed, loss: 2.3036223e-06\n",
      "step: 11510 train: 0.07772445678710938 elapsed, loss: 3.8714943e-06\n",
      "step: 11520 train: 0.08248138427734375 elapsed, loss: 3.017944e-06\n",
      "step: 11530 train: 0.08707261085510254 elapsed, loss: 2.2379643e-06\n",
      "step: 11540 train: 0.0774693489074707 elapsed, loss: 2.7907024e-06\n",
      "step: 11550 train: 0.08113789558410645 elapsed, loss: 2.2174734e-06\n",
      "step: 11560 train: 0.08053708076477051 elapsed, loss: 3.9124725e-06\n",
      "step: 11570 train: 0.0795128345489502 elapsed, loss: 0.00010281038\n",
      "step: 11580 train: 0.08014249801635742 elapsed, loss: 5.276845e-06\n",
      "step: 11590 train: 0.0866081714630127 elapsed, loss: 3.7951168e-06\n",
      "step: 11600 train: 0.07930278778076172 elapsed, loss: 2.9764994e-06\n",
      "step: 11610 train: 0.07981443405151367 elapsed, loss: 3.0542647e-06\n",
      "step: 11620 train: 0.07507205009460449 elapsed, loss: 3.5818562e-06\n",
      "step: 11630 train: 0.0813438892364502 elapsed, loss: 4.8688835e-06\n",
      "step: 11640 train: 0.07477164268493652 elapsed, loss: 3.5157327e-06\n",
      "step: 11650 train: 0.07854986190795898 elapsed, loss: 4.069867e-06\n",
      "step: 11660 train: 0.08032608032226562 elapsed, loss: 2.2924467e-06\n",
      "step: 11670 train: 0.07820510864257812 elapsed, loss: 2.408394e-06\n",
      "step: 11680 train: 0.07838177680969238 elapsed, loss: 2.4293508e-06\n",
      "step: 11690 train: 0.08352303504943848 elapsed, loss: 0.0026803478\n",
      "step: 11700 train: 0.07695722579956055 elapsed, loss: 0.0033905907\n",
      "step: 11710 train: 0.07682156562805176 elapsed, loss: 3.8879392e-05\n",
      "step: 11720 train: 0.08316183090209961 elapsed, loss: 9.160237e-05\n",
      "step: 11730 train: 0.07620525360107422 elapsed, loss: 1.7416405e-05\n",
      "step: 11740 train: 0.07652783393859863 elapsed, loss: 1.4980207e-05\n",
      "step: 11750 train: 0.08733701705932617 elapsed, loss: 4.6793408e-05\n",
      "step: 11760 train: 0.07634782791137695 elapsed, loss: 9.024821e-06\n",
      "step: 11770 train: 0.08195042610168457 elapsed, loss: 4.8603553e-05\n",
      "step: 11780 train: 0.07758212089538574 elapsed, loss: 0.0014665178\n",
      "step: 11790 train: 0.08323907852172852 elapsed, loss: 0.00022984912\n",
      "step: 11800 train: 0.08053231239318848 elapsed, loss: 3.092086e-05\n",
      "step: 11810 train: 0.08008813858032227 elapsed, loss: 1.3951787e-05\n",
      "step: 11820 train: 0.0761268138885498 elapsed, loss: 2.172871e-05\n",
      "step: 11830 train: 0.0839540958404541 elapsed, loss: 2.192176e-05\n",
      "step: 11840 train: 0.07823896408081055 elapsed, loss: 1.9751638e-05\n",
      "step: 11850 train: 0.0743248462677002 elapsed, loss: 8.581597e-06\n",
      "step: 11860 train: 0.08266639709472656 elapsed, loss: 6.514012e-06\n",
      "step: 11870 train: 0.07985568046569824 elapsed, loss: 5.286617e-06\n",
      "step: 11880 train: 0.08360719680786133 elapsed, loss: 3.959495e-06\n",
      "step: 11890 train: 0.07667303085327148 elapsed, loss: 4.995586e-06\n",
      "step: 11900 train: 0.07875180244445801 elapsed, loss: 3.6945348e-06\n",
      "step: 11910 train: 0.07733511924743652 elapsed, loss: 3.0114227e-06\n",
      "step: 11920 train: 0.08388376235961914 elapsed, loss: 2.6086286e-06\n",
      "step: 11930 train: 0.08034753799438477 elapsed, loss: 3.4156076e-06\n",
      "step: 11940 train: 0.07790303230285645 elapsed, loss: 2.6193384e-06\n",
      "step: 11950 train: 0.08319520950317383 elapsed, loss: 2.420029e-06\n",
      "step: 11960 train: 0.08367466926574707 elapsed, loss: 3.0453743e-06\n",
      "step: 11970 train: 0.08764362335205078 elapsed, loss: 1.9879044e-06\n",
      "step: 11980 train: 0.07833695411682129 elapsed, loss: 2.3050181e-06\n",
      "step: 11990 train: 0.07921910285949707 elapsed, loss: 3.0067677e-06\n",
      "step: 12000 train: 0.08362150192260742 elapsed, loss: 2.1951241e-06\n",
      "step: 12010 train: 0.0821843147277832 elapsed, loss: 2.3352868e-06\n",
      "step: 12020 train: 0.07847094535827637 elapsed, loss: 2.8242289e-06\n",
      "step: 12030 train: 0.07703232765197754 elapsed, loss: 2.0177076e-06\n",
      "step: 12040 train: 0.07456588745117188 elapsed, loss: 2.9667217e-06\n",
      "step: 12050 train: 0.08285236358642578 elapsed, loss: 1.9697434e-06\n",
      "step: 12060 train: 0.07933664321899414 elapsed, loss: 3.9357565e-06\n",
      "step: 12070 train: 0.08138513565063477 elapsed, loss: 2.3641521e-06\n",
      "step: 12080 train: 0.07935762405395508 elapsed, loss: 4.7505735e-05\n",
      "step: 12090 train: 0.08303332328796387 elapsed, loss: 1.2645521e-05\n",
      "step: 12100 train: 0.08231854438781738 elapsed, loss: 7.511904e-06\n",
      "step: 12110 train: 0.08132767677307129 elapsed, loss: 7.99621e-06\n",
      "step: 12120 train: 0.07775712013244629 elapsed, loss: 8.692582e-06\n",
      "step: 12130 train: 0.07480692863464355 elapsed, loss: 6.4279e-06\n",
      "step: 12140 train: 0.0795745849609375 elapsed, loss: 5.9729678e-06\n",
      "step: 12150 train: 0.08065581321716309 elapsed, loss: 6.128043e-06\n",
      "step: 12160 train: 0.08399224281311035 elapsed, loss: 3.1823083e-06\n",
      "step: 12170 train: 0.0800628662109375 elapsed, loss: 4.138772e-06\n",
      "step: 12180 train: 0.08146214485168457 elapsed, loss: 4.0279538e-06\n",
      "step: 12190 train: 0.07822346687316895 elapsed, loss: 3.732262e-06\n",
      "step: 12200 train: 0.08387517929077148 elapsed, loss: 2.1238775e-06\n",
      "step: 12210 train: 0.08452677726745605 elapsed, loss: 2.2589184e-06\n",
      "step: 12220 train: 0.07852482795715332 elapsed, loss: 3.2125877e-06\n",
      "step: 12230 train: 0.08165287971496582 elapsed, loss: 2.3408722e-06\n",
      "step: 12240 train: 0.07680249214172363 elapsed, loss: 3.5292173e-06\n",
      "step: 12250 train: 0.0761251449584961 elapsed, loss: 3.967423e-06\n",
      "step: 12260 train: 0.08268046379089355 elapsed, loss: 1.6638057e-06\n",
      "step: 12270 train: 0.08112406730651855 elapsed, loss: 3.1236484e-06\n",
      "step: 12280 train: 0.07782936096191406 elapsed, loss: 2.0652035e-06\n",
      "step: 12290 train: 0.07601261138916016 elapsed, loss: 2.6770804e-06\n",
      "step: 12300 train: 0.08567667007446289 elapsed, loss: 2.0754364e-06\n",
      "step: 12310 train: 0.08349442481994629 elapsed, loss: 2.3436687e-06\n",
      "step: 12320 train: 0.07793927192687988 elapsed, loss: 2.2291144e-06\n",
      "step: 12330 train: 0.0811758041381836 elapsed, loss: 1.7951218e-06\n",
      "step: 12340 train: 0.07543659210205078 elapsed, loss: 2.7883734e-06\n",
      "step: 12350 train: 0.07506179809570312 elapsed, loss: 2.6049047e-06\n",
      "step: 12360 train: 0.07972359657287598 elapsed, loss: 2.1331907e-06\n",
      "step: 12370 train: 0.083099365234375 elapsed, loss: 1.7727691e-06\n",
      "step: 12380 train: 0.08341598510742188 elapsed, loss: 2.2486745e-06\n",
      "step: 12390 train: 0.08301711082458496 elapsed, loss: 3.5926894e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 12400 train: 0.08337998390197754 elapsed, loss: 1.6568208e-06\n",
      "step: 12410 train: 0.08347225189208984 elapsed, loss: 1.8067632e-06\n",
      "step: 12420 train: 0.08340001106262207 elapsed, loss: 3.029119e-06\n",
      "step: 12430 train: 0.07808041572570801 elapsed, loss: 2.5075801e-06\n",
      "step: 12440 train: 0.07808661460876465 elapsed, loss: 0.0032902798\n",
      "step: 12450 train: 0.07872891426086426 elapsed, loss: 3.76653e-05\n",
      "step: 12460 train: 0.08299636840820312 elapsed, loss: 2.1390471e-05\n",
      "step: 12470 train: 0.07679867744445801 elapsed, loss: 1.1304004e-05\n",
      "step: 12480 train: 0.08331131935119629 elapsed, loss: 8.624309e-06\n",
      "step: 12490 train: 0.0826728343963623 elapsed, loss: 8.783141e-06\n",
      "step: 12500 train: 0.08507037162780762 elapsed, loss: 6.1718147e-06\n",
      "step: 12510 train: 0.07787942886352539 elapsed, loss: 8.273773e-06\n",
      "step: 12520 train: 0.08138799667358398 elapsed, loss: 3.7643913e-06\n",
      "step: 12530 train: 0.07740283012390137 elapsed, loss: 7.4691307e-06\n",
      "step: 12540 train: 0.07879376411437988 elapsed, loss: 4.0591494e-06\n",
      "step: 12550 train: 0.08097481727600098 elapsed, loss: 2.9802227e-06\n",
      "step: 12560 train: 0.07978296279907227 elapsed, loss: 3.7252778e-06\n",
      "step: 12570 train: 0.07674717903137207 elapsed, loss: 3.3369138e-06\n",
      "step: 12580 train: 0.07696199417114258 elapsed, loss: 2.9141015e-06\n",
      "step: 12590 train: 0.08137845993041992 elapsed, loss: 1.7946563e-06\n",
      "step: 12600 train: 0.08295059204101562 elapsed, loss: 1.955309e-06\n",
      "step: 12610 train: 0.07675862312316895 elapsed, loss: 5.7103744e-06\n",
      "step: 12620 train: 0.07615423202514648 elapsed, loss: 1.06901125e-05\n",
      "step: 12630 train: 0.07986569404602051 elapsed, loss: 2.9052537e-06\n",
      "step: 12640 train: 0.07940387725830078 elapsed, loss: 2.7799906e-06\n",
      "step: 12650 train: 0.08042192459106445 elapsed, loss: 2.527604e-06\n",
      "step: 12660 train: 0.07628536224365234 elapsed, loss: 3.5306323e-06\n",
      "step: 12670 train: 0.0846564769744873 elapsed, loss: 2.602109e-06\n",
      "step: 12680 train: 0.0795140266418457 elapsed, loss: 1.9180552e-06\n",
      "step: 12690 train: 0.07427310943603516 elapsed, loss: 2.7087456e-06\n",
      "step: 12700 train: 0.07536697387695312 elapsed, loss: 2.6728899e-06\n",
      "step: 12710 train: 0.08078718185424805 elapsed, loss: 2.8181762e-06\n",
      "step: 12720 train: 0.0816338062286377 elapsed, loss: 1.6246904e-06\n",
      "step: 12730 train: 0.08480525016784668 elapsed, loss: 2.6430882e-06\n",
      "step: 12740 train: 0.08305621147155762 elapsed, loss: 2.0503037e-06\n",
      "step: 12750 train: 0.08037877082824707 elapsed, loss: 1.9883685e-06\n",
      "step: 12760 train: 0.07727909088134766 elapsed, loss: 2.6230655e-06\n",
      "step: 12770 train: 0.07965564727783203 elapsed, loss: 1.6605453e-06\n",
      "step: 12780 train: 0.07436251640319824 elapsed, loss: 3.678248e-06\n",
      "step: 12790 train: 0.08183169364929199 elapsed, loss: 1.8211982e-06\n",
      "step: 12800 train: 0.07805514335632324 elapsed, loss: 1.9008261e-06\n",
      "step: 12810 train: 0.08208847045898438 elapsed, loss: 2.6845323e-06\n",
      "step: 12820 train: 0.08505773544311523 elapsed, loss: 1.4747477e-06\n",
      "step: 12830 train: 0.08915138244628906 elapsed, loss: 1.9120025e-06\n",
      "step: 12840 train: 0.07936215400695801 elapsed, loss: 2.641226e-06\n",
      "step: 12850 train: 0.07408356666564941 elapsed, loss: 4.201644e-06\n",
      "step: 12860 train: 0.08075284957885742 elapsed, loss: 2.0693956e-06\n",
      "step: 12870 train: 0.08797097206115723 elapsed, loss: 2.2421543e-06\n",
      "step: 12880 train: 0.07996964454650879 elapsed, loss: 4.967283e-06\n",
      "step: 12890 train: 0.08221840858459473 elapsed, loss: 3.3392491e-06\n",
      "step: 12900 train: 0.07772684097290039 elapsed, loss: 1.15748135e-05\n",
      "step: 12910 train: 0.07540202140808105 elapsed, loss: 4.2672864e-06\n",
      "step: 12920 train: 0.07691192626953125 elapsed, loss: 0.00055250444\n",
      "step: 12930 train: 0.08122014999389648 elapsed, loss: 0.017471742\n",
      "step: 12940 train: 0.08061933517456055 elapsed, loss: 9.293723e-05\n",
      "step: 12950 train: 0.08821630477905273 elapsed, loss: 3.0326322e-05\n",
      "step: 12960 train: 0.08655929565429688 elapsed, loss: 1.7387563e-05\n",
      "step: 12970 train: 0.08320760726928711 elapsed, loss: 2.4929652e-05\n",
      "step: 12980 train: 0.07774591445922852 elapsed, loss: 2.8256085e-05\n",
      "step: 12990 train: 0.08272385597229004 elapsed, loss: 8.048406e-06\n",
      "step: 13000 train: 0.08156967163085938 elapsed, loss: 8.289628e-06\n",
      "step: 13010 train: 0.07560443878173828 elapsed, loss: 1.166959e-05\n",
      "step: 13020 train: 0.07936787605285645 elapsed, loss: 8.246763e-06\n",
      "step: 13030 train: 0.0754690170288086 elapsed, loss: 5.4016405e-06\n",
      "step: 13040 train: 0.07300853729248047 elapsed, loss: 1.1407136e-05\n",
      "step: 13050 train: 0.08073306083679199 elapsed, loss: 4.657057e-06\n",
      "step: 13060 train: 0.07833266258239746 elapsed, loss: 3.926906e-06\n",
      "step: 13070 train: 0.08352422714233398 elapsed, loss: 2.3040861e-06\n",
      "step: 13080 train: 0.07648205757141113 elapsed, loss: 3.410029e-06\n",
      "step: 13090 train: 0.08043146133422852 elapsed, loss: 2.3040875e-06\n",
      "step: 13100 train: 0.07405519485473633 elapsed, loss: 5.7350517e-06\n",
      "step: 13110 train: 0.07532668113708496 elapsed, loss: 4.0246905e-06\n",
      "step: 13120 train: 0.08273863792419434 elapsed, loss: 2.4181745e-06\n",
      "step: 13130 train: 0.08716082572937012 elapsed, loss: 1.8696271e-06\n",
      "step: 13140 train: 0.0757913589477539 elapsed, loss: 3.370908e-06\n",
      "step: 13150 train: 0.08459830284118652 elapsed, loss: 1.7816169e-06\n",
      "step: 13160 train: 0.08018755912780762 elapsed, loss: 1.5934907e-06\n",
      "step: 13170 train: 0.08350610733032227 elapsed, loss: 2.0870857e-06\n",
      "step: 13180 train: 0.07893013954162598 elapsed, loss: 2.4554256e-06\n",
      "step: 13190 train: 0.07282781600952148 elapsed, loss: 4.4824415e-06\n",
      "step: 13200 train: 0.08323192596435547 elapsed, loss: 2.8046657e-06\n",
      "step: 13210 train: 0.08028483390808105 elapsed, loss: 2.2021086e-06\n",
      "step: 13220 train: 0.07563352584838867 elapsed, loss: 3.0775486e-06\n",
      "step: 13230 train: 0.08846569061279297 elapsed, loss: 1.9795223e-06\n",
      "step: 13240 train: 0.08152031898498535 elapsed, loss: 2.8959412e-06\n",
      "step: 13250 train: 0.08454060554504395 elapsed, loss: 2.0158438e-06\n",
      "step: 13260 train: 0.08453869819641113 elapsed, loss: 1.6307438e-06\n",
      "step: 13270 train: 0.08614540100097656 elapsed, loss: 2.2831323e-06\n",
      "step: 13280 train: 0.08103609085083008 elapsed, loss: 1.7466931e-06\n",
      "step: 13290 train: 0.07625508308410645 elapsed, loss: 3.5101434e-06\n",
      "step: 13300 train: 0.08233642578125 elapsed, loss: 4.8558886e-06\n",
      "step: 13310 train: 0.07916927337646484 elapsed, loss: 3.175803e-06\n",
      "step: 13320 train: 0.08555340766906738 elapsed, loss: 1.9841796e-06\n",
      "step: 13330 train: 0.07712650299072266 elapsed, loss: 2.7283038e-06\n",
      "step: 13340 train: 0.08124876022338867 elapsed, loss: 3.6628821e-06\n",
      "step: 13350 train: 0.08042764663696289 elapsed, loss: 3.6833635e-06\n",
      "step: 13360 train: 0.07755088806152344 elapsed, loss: 2.2184065e-06\n",
      "step: 13370 train: 0.07954668998718262 elapsed, loss: 2.9206217e-06\n",
      "step: 13380 train: 0.07996320724487305 elapsed, loss: 2.3124683e-06\n",
      "step: 13390 train: 0.08207511901855469 elapsed, loss: 2.2705594e-06\n",
      "step: 13400 train: 0.0778656005859375 elapsed, loss: 3.1962916e-06\n",
      "step: 13410 train: 0.08360552787780762 elapsed, loss: 2.0712587e-06\n",
      "step: 13420 train: 0.08493828773498535 elapsed, loss: 5.870094e-06\n",
      "step: 13430 train: 0.07829570770263672 elapsed, loss: 3.2223688e-06\n",
      "step: 13440 train: 0.08306074142456055 elapsed, loss: 1.8393563e-06\n",
      "step: 13450 train: 0.0800926685333252 elapsed, loss: 2.1266712e-06\n",
      "step: 13460 train: 0.07537007331848145 elapsed, loss: 3.1529858e-06\n",
      "step: 13470 train: 0.07622647285461426 elapsed, loss: 3.0714948e-06\n",
      "step: 13480 train: 0.0758979320526123 elapsed, loss: 6.1527526e-06\n",
      "step: 13490 train: 0.08403372764587402 elapsed, loss: 3.0789417e-06\n",
      "step: 13500 train: 0.07666945457458496 elapsed, loss: 3.0165445e-06\n",
      "step: 13510 train: 0.08159303665161133 elapsed, loss: 8.425104e-06\n",
      "step: 13520 train: 0.07648921012878418 elapsed, loss: 2.8791758e-06\n",
      "step: 13530 train: 0.07290983200073242 elapsed, loss: 3.9525203e-06\n",
      "step: 13540 train: 0.07771611213684082 elapsed, loss: 2.3483258e-06\n",
      "step: 13550 train: 0.07895135879516602 elapsed, loss: 2.4530987e-06\n",
      "step: 13560 train: 0.08024096488952637 elapsed, loss: 2.0950065e-06\n",
      "step: 13570 train: 0.0771641731262207 elapsed, loss: 3.2316816e-06\n",
      "step: 13580 train: 0.07917928695678711 elapsed, loss: 3.872862e-06\n",
      "step: 13590 train: 0.07741260528564453 elapsed, loss: 2.5508878e-06\n",
      "step: 13600 train: 0.07681059837341309 elapsed, loss: 2.2337736e-06\n",
      "step: 13610 train: 0.07615399360656738 elapsed, loss: 2.1406408e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13620 train: 0.07807326316833496 elapsed, loss: 2.8777774e-06\n",
      "step: 13630 train: 0.07679915428161621 elapsed, loss: 3.4742902e-06\n",
      "step: 13640 train: 0.0800318717956543 elapsed, loss: 2.6617085e-06\n",
      "step: 13650 train: 0.08701562881469727 elapsed, loss: 2.0163102e-06\n",
      "step: 13660 train: 0.07919621467590332 elapsed, loss: 2.953679e-06\n",
      "step: 13670 train: 0.07467007637023926 elapsed, loss: 4.5029165e-06\n",
      "step: 13680 train: 0.07638812065124512 elapsed, loss: 3.910145e-06\n",
      "step: 13690 train: 0.07802987098693848 elapsed, loss: 2.6407602e-06\n",
      "step: 13700 train: 0.07908916473388672 elapsed, loss: 2.1648552e-06\n",
      "step: 13710 train: 0.08769512176513672 elapsed, loss: 2.1853439e-06\n",
      "step: 13720 train: 0.08381485939025879 elapsed, loss: 1.901758e-06\n",
      "step: 13730 train: 0.08142781257629395 elapsed, loss: 2.7217825e-06\n",
      "step: 13740 train: 0.07866334915161133 elapsed, loss: 2.9094422e-06\n",
      "step: 13750 train: 0.07659077644348145 elapsed, loss: 4.6994355e-06\n",
      "step: 13760 train: 0.08229470252990723 elapsed, loss: 2.583018e-06\n",
      "step: 13770 train: 0.0810396671295166 elapsed, loss: 1.9245754e-06\n",
      "step: 13780 train: 0.08202195167541504 elapsed, loss: 2.4326105e-06\n",
      "step: 13790 train: 0.08292818069458008 elapsed, loss: 3.250308e-06\n",
      "step: 13800 train: 0.07633662223815918 elapsed, loss: 3.3625327e-06\n",
      "step: 13810 train: 0.07958340644836426 elapsed, loss: 3.3704473e-06\n",
      "step: 13820 train: 0.08060598373413086 elapsed, loss: 2.5588047e-06\n",
      "step: 13830 train: 0.08327651023864746 elapsed, loss: 3.4007135e-06\n",
      "step: 13840 train: 0.08455038070678711 elapsed, loss: 1.4428651e-05\n",
      "step: 13850 train: 0.08418130874633789 elapsed, loss: 0.036561534\n",
      "step: 13860 train: 0.0842740535736084 elapsed, loss: 0.0020358209\n",
      "step: 13870 train: 0.08855557441711426 elapsed, loss: 3.8540125e-05\n",
      "step: 13880 train: 0.08358573913574219 elapsed, loss: 6.221364e-05\n",
      "step: 13890 train: 0.08000373840332031 elapsed, loss: 3.1893498e-05\n",
      "step: 13900 train: 0.07740306854248047 elapsed, loss: 2.258732e-05\n",
      "step: 13910 train: 0.07936525344848633 elapsed, loss: 2.3173488e-05\n",
      "step: 13920 train: 0.0841522216796875 elapsed, loss: 9.798202e-06\n",
      "step: 13930 train: 0.0827796459197998 elapsed, loss: 1.0526046e-05\n",
      "step: 13940 train: 0.08485674858093262 elapsed, loss: 8.019875e-06\n",
      "step: 13950 train: 0.08087801933288574 elapsed, loss: 6.3510724e-06\n",
      "step: 13960 train: 0.08430886268615723 elapsed, loss: 5.8742744e-06\n",
      "step: 13970 train: 0.07663846015930176 elapsed, loss: 8.176908e-06\n",
      "step: 13980 train: 0.08168983459472656 elapsed, loss: 3.8128114e-06\n",
      "step: 13990 train: 0.08804774284362793 elapsed, loss: 3.2456492e-06\n",
      "step: 14000 train: 0.0792086124420166 elapsed, loss: 3.4808068e-06\n",
      "step: 14010 train: 0.07443976402282715 elapsed, loss: 3.718291e-06\n",
      "step: 14020 train: 0.0765082836151123 elapsed, loss: 3.3830195e-06\n",
      "step: 14030 train: 0.08129024505615234 elapsed, loss: 4.52293e-06\n",
      "step: 14040 train: 0.08688545227050781 elapsed, loss: 3.0985022e-06\n",
      "step: 14050 train: 0.08305811882019043 elapsed, loss: 2.8456468e-06\n",
      "step: 14060 train: 0.08034944534301758 elapsed, loss: 2.6612367e-06\n",
      "step: 14070 train: 0.08426547050476074 elapsed, loss: 2.578822e-06\n",
      "step: 14080 train: 0.07646298408508301 elapsed, loss: 3.6926765e-06\n",
      "step: 14090 train: 0.08498358726501465 elapsed, loss: 1.967881e-06\n",
      "step: 14100 train: 0.07899355888366699 elapsed, loss: 0.00011487055\n",
      "step: 14110 train: 0.0805959701538086 elapsed, loss: 4.463435e-05\n",
      "step: 14120 train: 0.08502984046936035 elapsed, loss: 9.410876e-06\n",
      "step: 14130 train: 0.08135724067687988 elapsed, loss: 0.00092498877\n",
      "step: 14140 train: 0.0785667896270752 elapsed, loss: 0.00021683975\n",
      "step: 14150 train: 0.08548235893249512 elapsed, loss: 1.4510655e-05\n",
      "step: 14160 train: 0.08176612854003906 elapsed, loss: 1.630971e-05\n",
      "step: 14170 train: 0.08355951309204102 elapsed, loss: 1.4278722e-05\n",
      "step: 14180 train: 0.07950186729431152 elapsed, loss: 9.691177e-06\n",
      "step: 14190 train: 0.07398104667663574 elapsed, loss: 1.4889461e-05\n",
      "step: 14200 train: 0.07909083366394043 elapsed, loss: 5.8058026e-06\n",
      "step: 14210 train: 0.08395886421203613 elapsed, loss: 6.420376e-06\n",
      "step: 14220 train: 0.07745242118835449 elapsed, loss: 9.51987e-06\n",
      "step: 14230 train: 0.08296847343444824 elapsed, loss: 4.2165348e-06\n",
      "step: 14240 train: 0.07842731475830078 elapsed, loss: 3.5865087e-06\n",
      "step: 14250 train: 0.08303499221801758 elapsed, loss: 4.666785e-06\n",
      "step: 14260 train: 0.08169031143188477 elapsed, loss: 2.6915131e-06\n",
      "step: 14270 train: 0.07999539375305176 elapsed, loss: 2.5522843e-06\n",
      "step: 14280 train: 0.08528304100036621 elapsed, loss: 2.0093253e-06\n",
      "step: 14290 train: 0.07962536811828613 elapsed, loss: 2.3092102e-06\n",
      "step: 14300 train: 0.08246660232543945 elapsed, loss: 3.8481994e-06\n",
      "step: 14310 train: 0.08810567855834961 elapsed, loss: 1.721082e-06\n",
      "step: 14320 train: 0.07539844512939453 elapsed, loss: 2.6589207e-06\n",
      "step: 14330 train: 0.07496809959411621 elapsed, loss: 3.4472812e-06\n",
      "step: 14340 train: 0.08700156211853027 elapsed, loss: 2.3673356e-06\n",
      "step: 14350 train: 0.08674216270446777 elapsed, loss: 1.6125821e-06\n",
      "step: 14360 train: 0.08572959899902344 elapsed, loss: 1.872887e-06\n",
      "step: 14370 train: 0.07883429527282715 elapsed, loss: 3.9436645e-06\n",
      "step: 14380 train: 0.07951855659484863 elapsed, loss: 1.8910479e-06\n",
      "step: 14390 train: 0.07712268829345703 elapsed, loss: 2.7529832e-06\n",
      "step: 14400 train: 0.08300280570983887 elapsed, loss: 1.8905808e-06\n",
      "step: 14410 train: 0.07960176467895508 elapsed, loss: 2.7692804e-06\n",
      "step: 14420 train: 0.07722306251525879 elapsed, loss: 2.1951232e-06\n",
      "step: 14430 train: 0.08003544807434082 elapsed, loss: 2.2868585e-06\n",
      "step: 14440 train: 0.07645750045776367 elapsed, loss: 2.1834815e-06\n",
      "step: 14450 train: 0.08043599128723145 elapsed, loss: 3.385344e-06\n",
      "step: 14460 train: 0.07698988914489746 elapsed, loss: 2.918758e-06\n",
      "step: 14470 train: 0.0798501968383789 elapsed, loss: 2.9355208e-06\n",
      "step: 14480 train: 0.08519697189331055 elapsed, loss: 2.2058089e-06\n",
      "step: 14490 train: 0.08045101165771484 elapsed, loss: 1.6083923e-06\n",
      "step: 14500 train: 0.0918278694152832 elapsed, loss: 2.3841321e-06\n",
      "step: 14510 train: 0.08036255836486816 elapsed, loss: 2.8009458e-06\n",
      "step: 14520 train: 0.08358883857727051 elapsed, loss: 2.1895348e-06\n",
      "step: 14530 train: 0.08409857749938965 elapsed, loss: 1.6852259e-06\n",
      "step: 14540 train: 0.08284139633178711 elapsed, loss: 1.7839461e-06\n",
      "step: 14550 train: 0.08613944053649902 elapsed, loss: 1.9576373e-06\n",
      "step: 14560 train: 0.08411860466003418 elapsed, loss: 1.6600804e-06\n",
      "step: 14570 train: 0.0830678939819336 elapsed, loss: 2.0335376e-06\n",
      "step: 14580 train: 0.08496642112731934 elapsed, loss: 2.2156128e-06\n",
      "step: 14590 train: 0.08278322219848633 elapsed, loss: 1.956706e-06\n",
      "step: 14600 train: 0.08099484443664551 elapsed, loss: 3.5590385e-06\n",
      "step: 14610 train: 0.08553361892700195 elapsed, loss: 1.895704e-06\n",
      "step: 14620 train: 0.07615447044372559 elapsed, loss: 3.1581048e-06\n",
      "step: 14630 train: 0.07800889015197754 elapsed, loss: 2.0521666e-06\n",
      "step: 14640 train: 0.08098530769348145 elapsed, loss: 2.8167792e-06\n",
      "step: 14650 train: 0.07744455337524414 elapsed, loss: 2.9578737e-06\n",
      "step: 14660 train: 0.07874107360839844 elapsed, loss: 1.9762635e-06\n",
      "step: 14670 train: 0.08881449699401855 elapsed, loss: 2.063341e-06\n",
      "step: 14680 train: 0.07859206199645996 elapsed, loss: 2.123878e-06\n",
      "step: 14690 train: 0.08265137672424316 elapsed, loss: 1.5790554e-06\n",
      "step: 14700 train: 0.08095741271972656 elapsed, loss: 2.6500716e-06\n",
      "step: 14710 train: 0.08098149299621582 elapsed, loss: 1.7634571e-06\n",
      "step: 14720 train: 0.08046627044677734 elapsed, loss: 1.8649711e-06\n",
      "step: 14730 train: 0.08028078079223633 elapsed, loss: 2.1066485e-06\n",
      "step: 14740 train: 0.0770561695098877 elapsed, loss: 2.250537e-06\n",
      "step: 14750 train: 0.07909703254699707 elapsed, loss: 2.0149132e-06\n",
      "step: 14760 train: 0.08505964279174805 elapsed, loss: 2.257515e-06\n",
      "step: 14770 train: 0.08278679847717285 elapsed, loss: 1.8859256e-06\n",
      "step: 14780 train: 0.08301949501037598 elapsed, loss: 2.1988246e-06\n",
      "step: 14790 train: 0.07821083068847656 elapsed, loss: 2.0391276e-06\n",
      "step: 14800 train: 0.07817935943603516 elapsed, loss: 2.752984e-06\n",
      "step: 14810 train: 0.07751607894897461 elapsed, loss: 5.1031498e-06\n",
      "step: 14820 train: 0.0797719955444336 elapsed, loss: 2.5173604e-06\n",
      "step: 14830 train: 0.07554507255554199 elapsed, loss: 3.2628805e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14840 train: 0.08326005935668945 elapsed, loss: 2.5210861e-06\n",
      "step: 14850 train: 0.07801270484924316 elapsed, loss: 3.3308665e-06\n",
      "step: 14860 train: 0.0846562385559082 elapsed, loss: 2.2989655e-06\n",
      "step: 14870 train: 0.07965397834777832 elapsed, loss: 2.169978e-06\n",
      "step: 14880 train: 0.07381629943847656 elapsed, loss: 4.6081636e-06\n",
      "step: 14890 train: 0.0807642936706543 elapsed, loss: 2.1867359e-06\n",
      "step: 14900 train: 0.08182740211486816 elapsed, loss: 1.9669499e-06\n",
      "step: 14910 train: 0.08116412162780762 elapsed, loss: 2.3916323e-06\n",
      "step: 14920 train: 0.08497810363769531 elapsed, loss: 4.6072337e-06\n",
      "step: 14930 train: 0.07951998710632324 elapsed, loss: 7.431874e-06\n",
      "step: 14940 train: 0.07944989204406738 elapsed, loss: 2.6561256e-06\n",
      "step: 14950 train: 0.08460879325866699 elapsed, loss: 2.1201527e-06\n",
      "step: 14960 train: 0.08870863914489746 elapsed, loss: 2.6854632e-06\n",
      "step: 14970 train: 0.08426475524902344 elapsed, loss: 3.6060596e-06\n",
      "step: 14980 train: 0.08514070510864258 elapsed, loss: 2.2891854e-06\n",
      "step: 14990 train: 0.0813601016998291 elapsed, loss: 2.7785954e-06\n",
      "step: 15000 train: 0.08228230476379395 elapsed, loss: 2.4973372e-06\n",
      "step: 15010 train: 0.09274673461914062 elapsed, loss: 2.2230633e-06\n",
      "step: 15020 train: 0.07642555236816406 elapsed, loss: 3.2083994e-06\n",
      "step: 15030 train: 0.08472537994384766 elapsed, loss: 5.53288e-06\n",
      "step: 15040 train: 0.08077430725097656 elapsed, loss: 2.5331783e-06\n",
      "step: 15050 train: 0.07697463035583496 elapsed, loss: 3.4049067e-06\n",
      "step: 15060 train: 0.08601188659667969 elapsed, loss: 2.0642735e-06\n",
      "step: 15070 train: 0.08196210861206055 elapsed, loss: 2.1620604e-06\n",
      "step: 15080 train: 0.07961249351501465 elapsed, loss: 2.7315637e-06\n",
      "step: 15090 train: 0.08338117599487305 elapsed, loss: 2.0293492e-06\n",
      "step: 15100 train: 0.07932686805725098 elapsed, loss: 2.625859e-06\n",
      "step: 15110 train: 0.07967305183410645 elapsed, loss: 2.7343578e-06\n",
      "step: 15120 train: 0.08085393905639648 elapsed, loss: 3.5716132e-06\n",
      "step: 15130 train: 0.08456850051879883 elapsed, loss: 3.2381972e-06\n",
      "step: 15140 train: 0.08699727058410645 elapsed, loss: 3.41841e-06\n",
      "step: 15150 train: 0.09455728530883789 elapsed, loss: 2.0754496e-06\n",
      "step: 15160 train: 0.08025383949279785 elapsed, loss: 2.4810388e-06\n",
      "step: 15170 train: 0.08019137382507324 elapsed, loss: 2.2128188e-06\n",
      "step: 15180 train: 0.07873868942260742 elapsed, loss: 3.1525187e-06\n",
      "step: 15190 train: 0.08024787902832031 elapsed, loss: 2.5625245e-06\n",
      "step: 15200 train: 0.07563209533691406 elapsed, loss: 3.596751e-06\n",
      "step: 15210 train: 0.0834341049194336 elapsed, loss: 2.3050197e-06\n",
      "step: 15220 train: 0.08002591133117676 elapsed, loss: 4.429356e-06\n",
      "step: 15230 train: 0.0749664306640625 elapsed, loss: 4.4996673e-06\n",
      "step: 15240 train: 0.07712507247924805 elapsed, loss: 2.7976876e-06\n",
      "step: 15250 train: 0.08213257789611816 elapsed, loss: 2.4829017e-06\n",
      "step: 15260 train: 0.0830986499786377 elapsed, loss: 4.7115172e-06\n",
      "step: 15270 train: 0.08218598365783691 elapsed, loss: 2.4931455e-06\n",
      "step: 15280 train: 0.08039617538452148 elapsed, loss: 2.0624102e-06\n",
      "step: 15290 train: 0.0765848159790039 elapsed, loss: 2.6719572e-06\n",
      "step: 15300 train: 0.0838785171508789 elapsed, loss: 2.3641558e-06\n",
      "step: 15310 train: 0.08275938034057617 elapsed, loss: 2.7255098e-06\n",
      "step: 15320 train: 0.08757400512695312 elapsed, loss: 2.1830165e-06\n",
      "step: 15330 train: 0.07674789428710938 elapsed, loss: 3.373243e-06\n",
      "step: 15340 train: 0.07882404327392578 elapsed, loss: 2.8228335e-06\n",
      "step: 15350 train: 0.08591866493225098 elapsed, loss: 8.401701e-06\n",
      "step: 15360 train: 0.08507490158081055 elapsed, loss: 0.0004901144\n",
      "step: 15370 train: 0.08189535140991211 elapsed, loss: 0.0009898923\n",
      "step: 15380 train: 0.07858943939208984 elapsed, loss: 5.689432e-05\n",
      "step: 15390 train: 0.08644390106201172 elapsed, loss: 1.7904113e-05\n",
      "step: 15400 train: 0.08556628227233887 elapsed, loss: 1.55114e-05\n",
      "step: 15410 train: 0.08505105972290039 elapsed, loss: 1.6048194e-05\n",
      "step: 15420 train: 0.08172082901000977 elapsed, loss: 9.9689205e-06\n",
      "step: 15430 train: 0.08551311492919922 elapsed, loss: 8.174031e-06\n",
      "step: 15440 train: 0.08316206932067871 elapsed, loss: 8.064253e-06\n",
      "step: 15450 train: 0.08335256576538086 elapsed, loss: 7.773637e-06\n",
      "step: 15460 train: 0.07909870147705078 elapsed, loss: 6.4251517e-06\n",
      "step: 15470 train: 0.08388829231262207 elapsed, loss: 4.246348e-06\n",
      "step: 15480 train: 0.07945799827575684 elapsed, loss: 4.529462e-06\n",
      "step: 15490 train: 0.07646751403808594 elapsed, loss: 4.1709013e-06\n",
      "step: 15500 train: 0.08705544471740723 elapsed, loss: 1.0801059e-05\n",
      "step: 15510 train: 0.08108162879943848 elapsed, loss: 4.064266e-06\n",
      "step: 15520 train: 0.08768701553344727 elapsed, loss: 2.736675e-06\n",
      "step: 15530 train: 0.07544636726379395 elapsed, loss: 4.2882602e-06\n",
      "step: 15540 train: 0.07771611213684082 elapsed, loss: 2.4116548e-06\n",
      "step: 15550 train: 0.0793600082397461 elapsed, loss: 2.843782e-06\n",
      "step: 15560 train: 0.08062553405761719 elapsed, loss: 2.8065347e-06\n",
      "step: 15570 train: 0.08347010612487793 elapsed, loss: 2.7143315e-06\n",
      "step: 15580 train: 0.08025693893432617 elapsed, loss: 2.188139e-06\n",
      "step: 15590 train: 0.08279156684875488 elapsed, loss: 2.1252747e-06\n",
      "step: 15600 train: 0.07700729370117188 elapsed, loss: 2.3925636e-06\n",
      "step: 15610 train: 0.07835912704467773 elapsed, loss: 2.9378466e-06\n",
      "step: 15620 train: 0.08454298973083496 elapsed, loss: 2.1001292e-06\n",
      "step: 15630 train: 0.08499407768249512 elapsed, loss: 1.9022169e-06\n",
      "step: 15640 train: 0.08393073081970215 elapsed, loss: 2.0153789e-06\n",
      "step: 15650 train: 0.08570265769958496 elapsed, loss: 4.0645267e-05\n",
      "step: 15660 train: 0.08438420295715332 elapsed, loss: 0.0002595356\n",
      "step: 15670 train: 0.07984304428100586 elapsed, loss: 6.634189e-05\n",
      "step: 15680 train: 0.08417487144470215 elapsed, loss: 3.1692733e-05\n",
      "step: 15690 train: 0.08682107925415039 elapsed, loss: 3.415521e-05\n",
      "step: 15700 train: 0.08102583885192871 elapsed, loss: 4.0247025e-05\n",
      "step: 15710 train: 0.08308029174804688 elapsed, loss: 1.36507115e-05\n",
      "step: 15720 train: 0.08732080459594727 elapsed, loss: 7.964995e-06\n",
      "step: 15730 train: 0.08267593383789062 elapsed, loss: 7.1222203e-06\n",
      "step: 15740 train: 0.08142900466918945 elapsed, loss: 5.8751966e-06\n",
      "step: 15750 train: 0.08197283744812012 elapsed, loss: 7.5612734e-06\n",
      "step: 15760 train: 0.07946062088012695 elapsed, loss: 5.2433015e-06\n",
      "step: 15770 train: 0.07752799987792969 elapsed, loss: 4.8978004e-06\n",
      "step: 15780 train: 0.08520960807800293 elapsed, loss: 3.2065166e-06\n",
      "step: 15790 train: 0.0854942798614502 elapsed, loss: 2.4014093e-06\n",
      "step: 15800 train: 0.0763392448425293 elapsed, loss: 5.391599e-06\n",
      "step: 15810 train: 0.08162093162536621 elapsed, loss: 2.916892e-06\n",
      "step: 15820 train: 0.08293008804321289 elapsed, loss: 2.0572847e-06\n",
      "step: 15830 train: 0.0792386531829834 elapsed, loss: 1.8211981e-06\n",
      "step: 15840 train: 0.07737445831298828 elapsed, loss: 2.2086278e-06\n",
      "step: 15850 train: 0.08024811744689941 elapsed, loss: 1.7820829e-06\n",
      "step: 15860 train: 0.07951021194458008 elapsed, loss: 1.8882537e-06\n",
      "step: 15870 train: 0.07911992073059082 elapsed, loss: 2.0135162e-06\n",
      "step: 15880 train: 0.07883715629577637 elapsed, loss: 1.8109533e-06\n",
      "step: 15890 train: 0.0806264877319336 elapsed, loss: 1.8211988e-06\n",
      "step: 15900 train: 0.07951021194458008 elapsed, loss: 1.9036186e-06\n",
      "step: 15910 train: 0.07969045639038086 elapsed, loss: 1.6954691e-06\n",
      "step: 15920 train: 0.0827178955078125 elapsed, loss: 1.8784749e-06\n",
      "step: 15930 train: 0.08187198638916016 elapsed, loss: 1.7224778e-06\n",
      "step: 15940 train: 0.0798954963684082 elapsed, loss: 2.3390126e-06\n",
      "step: 15950 train: 0.08421087265014648 elapsed, loss: 1.6572858e-06\n",
      "step: 15960 train: 0.0818178653717041 elapsed, loss: 2.1145609e-06\n",
      "step: 15970 train: 0.07821297645568848 elapsed, loss: 3.1082823e-06\n",
      "step: 15980 train: 0.08158016204833984 elapsed, loss: 3.0314472e-06\n",
      "step: 15990 train: 0.07664179801940918 elapsed, loss: 2.3874409e-06\n",
      "step: 16000 train: 0.0836489200592041 elapsed, loss: 2.83913e-06\n",
      "step: 16010 train: 0.08733367919921875 elapsed, loss: 1.7583347e-06\n",
      "step: 16020 train: 0.09016704559326172 elapsed, loss: 1.6638053e-06\n",
      "step: 16030 train: 0.09211015701293945 elapsed, loss: 1.7643874e-06\n",
      "step: 16040 train: 0.07447481155395508 elapsed, loss: 3.329915e-06\n",
      "step: 16050 train: 0.08218050003051758 elapsed, loss: 2.5951235e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16060 train: 0.07763075828552246 elapsed, loss: 2.110374e-06\n",
      "step: 16070 train: 0.08209919929504395 elapsed, loss: 2.1201513e-06\n",
      "step: 16080 train: 0.07878279685974121 elapsed, loss: 2.9117725e-06\n",
      "step: 16090 train: 0.07608175277709961 elapsed, loss: 2.7189906e-06\n",
      "step: 16100 train: 0.07665419578552246 elapsed, loss: 2.2398253e-06\n",
      "step: 16110 train: 0.07852339744567871 elapsed, loss: 1.6628744e-06\n",
      "step: 16120 train: 0.08594584465026855 elapsed, loss: 1.7518148e-06\n",
      "step: 16130 train: 0.08078289031982422 elapsed, loss: 3.2950106e-06\n",
      "step: 16140 train: 0.08250737190246582 elapsed, loss: 2.3599675e-06\n",
      "step: 16150 train: 0.08645105361938477 elapsed, loss: 1.8295804e-06\n",
      "step: 16160 train: 0.08013200759887695 elapsed, loss: 3.650303e-06\n",
      "step: 16170 train: 0.0801844596862793 elapsed, loss: 2.1667183e-06\n",
      "step: 16180 train: 0.08413529396057129 elapsed, loss: 1.7844117e-06\n",
      "step: 16190 train: 0.07898449897766113 elapsed, loss: 3.0593883e-06\n",
      "step: 16200 train: 0.07798433303833008 elapsed, loss: 2.2253907e-06\n",
      "step: 16210 train: 0.08263325691223145 elapsed, loss: 2.0200353e-06\n",
      "step: 16220 train: 0.07989740371704102 elapsed, loss: 0.002429966\n",
      "step: 16230 train: 0.08236861228942871 elapsed, loss: 0.00068964344\n",
      "step: 16240 train: 0.08231782913208008 elapsed, loss: 6.616381e-05\n",
      "step: 16250 train: 0.08099508285522461 elapsed, loss: 0.004728701\n",
      "step: 16260 train: 0.0867013931274414 elapsed, loss: 0.00019028618\n",
      "step: 16270 train: 0.07976317405700684 elapsed, loss: 5.6125482e-05\n",
      "step: 16280 train: 0.07902956008911133 elapsed, loss: 4.562652e-05\n",
      "step: 16290 train: 0.07943391799926758 elapsed, loss: 2.4195502e-05\n",
      "step: 16300 train: 0.08537030220031738 elapsed, loss: 2.0278218e-05\n",
      "step: 16310 train: 0.08642864227294922 elapsed, loss: 2.2268738e-05\n",
      "step: 16320 train: 0.08790278434753418 elapsed, loss: 1.1587659e-05\n",
      "step: 16330 train: 0.08566141128540039 elapsed, loss: 8.5596985e-06\n",
      "step: 16340 train: 0.07689404487609863 elapsed, loss: 1.3886117e-05\n",
      "step: 16350 train: 0.08042621612548828 elapsed, loss: 7.6213946e-06\n",
      "step: 16360 train: 0.08022546768188477 elapsed, loss: 5.8724063e-06\n",
      "step: 16370 train: 0.08158159255981445 elapsed, loss: 3.9255083e-06\n",
      "step: 16380 train: 0.07737040519714355 elapsed, loss: 6.25937e-06\n",
      "step: 16390 train: 0.07640790939331055 elapsed, loss: 6.846093e-06\n",
      "step: 16400 train: 0.08325409889221191 elapsed, loss: 2.2826653e-06\n",
      "step: 16410 train: 0.07930111885070801 elapsed, loss: 2.2118866e-06\n",
      "step: 16420 train: 0.08418059349060059 elapsed, loss: 2.5425056e-06\n",
      "step: 16430 train: 0.0871574878692627 elapsed, loss: 1.7001269e-06\n",
      "step: 16440 train: 0.07962894439697266 elapsed, loss: 3.1599698e-06\n",
      "step: 16450 train: 0.0803074836730957 elapsed, loss: 1.6284155e-06\n",
      "step: 16460 train: 0.07808852195739746 elapsed, loss: 2.2416898e-06\n",
      "step: 16470 train: 0.08949089050292969 elapsed, loss: 1.4659001e-06\n",
      "step: 16480 train: 0.08629012107849121 elapsed, loss: 1.5581008e-06\n",
      "step: 16490 train: 0.07690691947937012 elapsed, loss: 3.5217868e-06\n",
      "step: 16500 train: 0.07616782188415527 elapsed, loss: 1.895238e-06\n",
      "step: 16510 train: 0.0776209831237793 elapsed, loss: 2.3068822e-06\n",
      "step: 16520 train: 0.0808413028717041 elapsed, loss: 2.4279539e-06\n",
      "step: 16530 train: 0.07752442359924316 elapsed, loss: 2.1476262e-06\n",
      "step: 16540 train: 0.08243012428283691 elapsed, loss: 1.4016389e-06\n",
      "step: 16550 train: 0.08037400245666504 elapsed, loss: 2.4158467e-06\n",
      "step: 16560 train: 0.08561372756958008 elapsed, loss: 3.046349e-06\n",
      "step: 16570 train: 0.08000373840332031 elapsed, loss: 1.8570541e-06\n",
      "step: 16580 train: 0.0787656307220459 elapsed, loss: 4.3152595e-06\n",
      "step: 16590 train: 0.08416342735290527 elapsed, loss: 1.9250404e-06\n",
      "step: 16600 train: 0.09052371978759766 elapsed, loss: 2.2551935e-06\n",
      "step: 16610 train: 0.08375310897827148 elapsed, loss: 2.770677e-06\n",
      "step: 16620 train: 0.08496689796447754 elapsed, loss: 1.784412e-06\n",
      "step: 16630 train: 0.08334040641784668 elapsed, loss: 1.8170081e-06\n",
      "step: 16640 train: 0.07767057418823242 elapsed, loss: 3.3038582e-06\n",
      "step: 16650 train: 0.0815436840057373 elapsed, loss: 1.8742761e-06\n",
      "step: 16660 train: 0.08123087882995605 elapsed, loss: 2.7879087e-06\n",
      "step: 16670 train: 0.08198142051696777 elapsed, loss: 2.011654e-06\n",
      "step: 16680 train: 0.07852697372436523 elapsed, loss: 2.3040882e-06\n",
      "step: 16690 train: 0.0849759578704834 elapsed, loss: 2.1303974e-06\n",
      "step: 16700 train: 0.07834053039550781 elapsed, loss: 3.1525174e-06\n",
      "step: 16710 train: 0.08417630195617676 elapsed, loss: 2.0791697e-06\n",
      "step: 16720 train: 0.08185434341430664 elapsed, loss: 1.8537953e-06\n",
      "step: 16730 train: 0.08385062217712402 elapsed, loss: 1.6600798e-06\n",
      "step: 16740 train: 0.08661222457885742 elapsed, loss: 1.6316749e-06\n",
      "step: 16750 train: 0.08976936340332031 elapsed, loss: 2.275672e-06\n",
      "step: 16760 train: 0.0830087661743164 elapsed, loss: 1.8826659e-06\n",
      "step: 16770 train: 0.08075952529907227 elapsed, loss: 1.9320255e-06\n",
      "step: 16780 train: 0.07994318008422852 elapsed, loss: 3.7615996e-06\n",
      "step: 16790 train: 0.09559202194213867 elapsed, loss: 1.7737015e-06\n",
      "step: 16800 train: 0.08240652084350586 elapsed, loss: 3.0174783e-06\n",
      "step: 16810 train: 0.08070111274719238 elapsed, loss: 3.223765e-06\n",
      "step: 16820 train: 0.08690547943115234 elapsed, loss: 1.8617109e-06\n",
      "step: 16830 train: 0.08604621887207031 elapsed, loss: 1.7569374e-06\n",
      "step: 16840 train: 0.08141160011291504 elapsed, loss: 1.88965e-06\n",
      "step: 16850 train: 0.0774984359741211 elapsed, loss: 4.3525224e-06\n",
      "step: 16860 train: 0.08150553703308105 elapsed, loss: 3.0067688e-06\n",
      "step: 16870 train: 0.08504819869995117 elapsed, loss: 2.7888393e-06\n",
      "step: 16880 train: 0.07857894897460938 elapsed, loss: 3.3080496e-06\n",
      "step: 16890 train: 0.0845034122467041 elapsed, loss: 2.1429692e-06\n",
      "step: 16900 train: 0.08376836776733398 elapsed, loss: 2.1504202e-06\n",
      "step: 16910 train: 0.07387089729309082 elapsed, loss: 3.1641607e-06\n",
      "step: 16920 train: 0.08084344863891602 elapsed, loss: 2.0712582e-06\n",
      "step: 16930 train: 0.08139467239379883 elapsed, loss: 2.1913945e-06\n",
      "step: 16940 train: 0.0821835994720459 elapsed, loss: 2.2705583e-06\n",
      "step: 16950 train: 0.08047080039978027 elapsed, loss: 2.3324897e-06\n",
      "step: 16960 train: 0.09021520614624023 elapsed, loss: 2.144367e-06\n",
      "step: 16970 train: 0.08323073387145996 elapsed, loss: 2.2547254e-06\n",
      "step: 16980 train: 0.08504438400268555 elapsed, loss: 2.197918e-06\n",
      "step: 16990 train: 0.07945871353149414 elapsed, loss: 2.8288848e-06\n",
      "step: 17000 train: 0.08203268051147461 elapsed, loss: 1.9841798e-06\n",
      "step: 17010 train: 0.08133625984191895 elapsed, loss: 2.5215468e-06\n",
      "step: 17020 train: 0.08161139488220215 elapsed, loss: 2.2533313e-06\n",
      "step: 17030 train: 0.08496379852294922 elapsed, loss: 2.184414e-06\n",
      "step: 17040 train: 0.08715462684631348 elapsed, loss: 2.228186e-06\n",
      "step: 17050 train: 0.08328604698181152 elapsed, loss: 2.7432006e-06\n",
      "step: 17060 train: 0.07956981658935547 elapsed, loss: 4.327841e-06\n",
      "step: 17070 train: 0.07492637634277344 elapsed, loss: 6.6808093e-06\n",
      "step: 17080 train: 0.07793521881103516 elapsed, loss: 2.427953e-06\n",
      "step: 17090 train: 0.07742595672607422 elapsed, loss: 2.4670692e-06\n",
      "step: 17100 train: 0.0840609073638916 elapsed, loss: 2.3958232e-06\n",
      "step: 17110 train: 0.08313441276550293 elapsed, loss: 3.9823162e-06\n",
      "step: 17120 train: 0.0781707763671875 elapsed, loss: 4.7455324e-06\n",
      "step: 17130 train: 0.08848404884338379 elapsed, loss: 2.4619465e-06\n",
      "step: 17140 train: 0.08233189582824707 elapsed, loss: 3.9771776e-06\n",
      "step: 17150 train: 0.0770421028137207 elapsed, loss: 3.2200403e-06\n",
      "step: 17160 train: 0.08337903022766113 elapsed, loss: 2.7539156e-06\n",
      "step: 17170 train: 0.08051538467407227 elapsed, loss: 2.382785e-06\n",
      "step: 17180 train: 0.07674407958984375 elapsed, loss: 3.6107276e-06\n",
      "step: 17190 train: 0.0854191780090332 elapsed, loss: 2.992792e-06\n",
      "step: 17200 train: 0.08482885360717773 elapsed, loss: 2.582548e-06\n",
      "step: 17210 train: 0.08089637756347656 elapsed, loss: 3.834244e-06\n",
      "step: 17220 train: 0.07672762870788574 elapsed, loss: 4.2249294e-06\n",
      "step: 17230 train: 0.08301043510437012 elapsed, loss: 2.4116553e-06\n",
      "step: 17240 train: 0.08032369613647461 elapsed, loss: 0.0001591874\n",
      "step: 17250 train: 0.07821083068847656 elapsed, loss: 7.975694e-05\n",
      "step: 17260 train: 0.0783841609954834 elapsed, loss: 0.0009212374\n",
      "step: 17270 train: 0.08366894721984863 elapsed, loss: 2.814044e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17280 train: 0.08051276206970215 elapsed, loss: 4.3575696e-05\n",
      "step: 17290 train: 0.08220839500427246 elapsed, loss: 1.7693717e-05\n",
      "step: 17300 train: 0.08278298377990723 elapsed, loss: 1.6058637e-05\n",
      "step: 17310 train: 0.0790567398071289 elapsed, loss: 1.146668e-05\n",
      "step: 17320 train: 0.08122014999389648 elapsed, loss: 7.690796e-06\n",
      "step: 17330 train: 0.07985091209411621 elapsed, loss: 8.772039e-06\n",
      "step: 17340 train: 0.08799266815185547 elapsed, loss: 5.857056e-06\n",
      "step: 17350 train: 0.08840131759643555 elapsed, loss: 4.856348e-06\n",
      "step: 17360 train: 0.08324027061462402 elapsed, loss: 3.356942e-06\n",
      "step: 17370 train: 0.08080601692199707 elapsed, loss: 5.691266e-06\n",
      "step: 17380 train: 0.08166265487670898 elapsed, loss: 3.403034e-06\n",
      "step: 17390 train: 0.08310604095458984 elapsed, loss: 3.639598e-06\n",
      "step: 17400 train: 0.07814145088195801 elapsed, loss: 2.6677685e-06\n",
      "step: 17410 train: 0.0822134017944336 elapsed, loss: 1.9487898e-06\n",
      "step: 17420 train: 0.0790400505065918 elapsed, loss: 4.4855424e-06\n",
      "step: 17430 train: 0.07847332954406738 elapsed, loss: 3.2433213e-06\n",
      "step: 17440 train: 0.07960033416748047 elapsed, loss: 2.1080443e-06\n",
      "step: 17450 train: 0.07675695419311523 elapsed, loss: 2.160199e-06\n",
      "step: 17460 train: 0.07962179183959961 elapsed, loss: 4.633778e-06\n",
      "step: 17470 train: 0.07786202430725098 elapsed, loss: 0.00022135722\n",
      "step: 17480 train: 0.08301353454589844 elapsed, loss: 0.011934217\n",
      "step: 17490 train: 0.0784919261932373 elapsed, loss: 0.00014482299\n",
      "step: 17500 train: 0.08141398429870605 elapsed, loss: 9.098917e-05\n",
      "step: 17510 train: 0.08593988418579102 elapsed, loss: 1.7330764e-05\n",
      "step: 17520 train: 0.08445453643798828 elapsed, loss: 2.6893149e-05\n",
      "step: 17530 train: 0.08407878875732422 elapsed, loss: 1.1764785e-05\n",
      "step: 17540 train: 0.08001303672790527 elapsed, loss: 1.4973433e-05\n",
      "step: 17550 train: 0.08374452590942383 elapsed, loss: 4.263953e-05\n",
      "step: 17560 train: 0.0817255973815918 elapsed, loss: 6.6570556e-06\n",
      "step: 17570 train: 0.08083534240722656 elapsed, loss: 7.581137e-06\n",
      "step: 17580 train: 0.08305978775024414 elapsed, loss: 4.955538e-06\n",
      "step: 17590 train: 0.07556724548339844 elapsed, loss: 4.6854643e-06\n",
      "step: 17600 train: 0.08208847045898438 elapsed, loss: 3.4281866e-06\n",
      "step: 17610 train: 0.07857131958007812 elapsed, loss: 3.2442538e-06\n",
      "step: 17620 train: 0.08329129219055176 elapsed, loss: 2.1196852e-06\n",
      "step: 17630 train: 0.08385944366455078 elapsed, loss: 2.148556e-06\n",
      "step: 17640 train: 0.08546853065490723 elapsed, loss: 2.488489e-06\n",
      "step: 17650 train: 0.08416342735290527 elapsed, loss: 2.1280684e-06\n",
      "step: 17660 train: 0.08913922309875488 elapsed, loss: 2.2458798e-06\n",
      "step: 17670 train: 0.08110404014587402 elapsed, loss: 2.0782409e-06\n",
      "step: 17680 train: 0.08449840545654297 elapsed, loss: 1.6144458e-06\n",
      "step: 17690 train: 0.08264875411987305 elapsed, loss: 1.911536e-06\n",
      "step: 17700 train: 0.07790184020996094 elapsed, loss: 3.4602476e-06\n",
      "step: 17710 train: 0.08016300201416016 elapsed, loss: 2.56206e-06\n",
      "step: 17720 train: 0.08040618896484375 elapsed, loss: 1.938079e-06\n",
      "step: 17730 train: 0.08036446571350098 elapsed, loss: 1.9385454e-06\n",
      "step: 17740 train: 0.07895326614379883 elapsed, loss: 1.7113023e-06\n",
      "step: 17750 train: 0.0870363712310791 elapsed, loss: 1.8980317e-06\n",
      "step: 17760 train: 0.07464289665222168 elapsed, loss: 2.5895365e-06\n",
      "step: 17770 train: 0.0770711898803711 elapsed, loss: 1.8943074e-06\n",
      "step: 17780 train: 0.07809281349182129 elapsed, loss: 1.9129338e-06\n",
      "step: 17790 train: 0.0765848159790039 elapsed, loss: 5.986941e-06\n",
      "step: 17800 train: 0.08129334449768066 elapsed, loss: 4.548083e-06\n",
      "step: 17810 train: 0.07938408851623535 elapsed, loss: 2.0121188e-06\n",
      "step: 17820 train: 0.08126664161682129 elapsed, loss: 3.9166116e-06\n",
      "step: 17830 train: 0.08262944221496582 elapsed, loss: 1.7434322e-06\n",
      "step: 17840 train: 0.08002543449401855 elapsed, loss: 1.6237592e-06\n",
      "step: 17850 train: 0.08114385604858398 elapsed, loss: 2.481038e-06\n",
      "step: 17860 train: 0.07582449913024902 elapsed, loss: 4.228189e-06\n",
      "step: 17870 train: 0.07983207702636719 elapsed, loss: 2.0279517e-06\n",
      "step: 17880 train: 0.07846426963806152 elapsed, loss: 2.1643903e-06\n",
      "step: 17890 train: 0.07979631423950195 elapsed, loss: 2.1206185e-06\n",
      "step: 17900 train: 0.08153796195983887 elapsed, loss: 2.5355207e-06\n",
      "step: 17910 train: 0.08405232429504395 elapsed, loss: 2.1965202e-06\n",
      "step: 17920 train: 0.07840585708618164 elapsed, loss: 2.2230342e-06\n",
      "step: 17930 train: 0.07755804061889648 elapsed, loss: 2.7520532e-06\n",
      "step: 17940 train: 0.08067011833190918 elapsed, loss: 1.6968677e-06\n",
      "step: 17950 train: 0.08068585395812988 elapsed, loss: 2.35111e-06\n",
      "step: 17960 train: 0.08000659942626953 elapsed, loss: 2.6556615e-06\n",
      "step: 17970 train: 0.07944512367248535 elapsed, loss: 2.8945396e-06\n",
      "step: 17980 train: 0.08143854141235352 elapsed, loss: 1.827718e-06\n",
      "step: 17990 train: 0.0820474624633789 elapsed, loss: 2.2677639e-06\n",
      "step: 18000 train: 0.07896161079406738 elapsed, loss: 2.3706612e-06\n",
      "step: 18010 train: 0.0756838321685791 elapsed, loss: 2.8656714e-06\n",
      "step: 18020 train: 0.08450508117675781 elapsed, loss: 2.0707926e-06\n",
      "step: 18030 train: 0.07582545280456543 elapsed, loss: 3.9768718e-05\n",
      "step: 18040 train: 0.08153080940246582 elapsed, loss: 1.496662e-05\n",
      "step: 18050 train: 0.08562302589416504 elapsed, loss: 8.994505e-06\n",
      "step: 18060 train: 0.07563567161560059 elapsed, loss: 6.9536736e-06\n",
      "step: 18070 train: 0.07974362373352051 elapsed, loss: 4.2856835e-05\n",
      "step: 18080 train: 0.08344054222106934 elapsed, loss: 0.00029246183\n",
      "step: 18090 train: 0.07872438430786133 elapsed, loss: 4.7592897e-05\n",
      "step: 18100 train: 0.07948827743530273 elapsed, loss: 2.7524056e-05\n",
      "step: 18110 train: 0.08039975166320801 elapsed, loss: 3.5184534e-05\n",
      "step: 18120 train: 0.08149099349975586 elapsed, loss: 3.4763114e-05\n",
      "step: 18130 train: 0.08317399024963379 elapsed, loss: 1.1030281e-05\n",
      "step: 18140 train: 0.08555412292480469 elapsed, loss: 0.0007379255\n",
      "step: 18150 train: 0.07737994194030762 elapsed, loss: 0.00037901252\n",
      "step: 18160 train: 0.08352088928222656 elapsed, loss: 0.00077969575\n",
      "step: 18170 train: 0.08647537231445312 elapsed, loss: 4.4974826e-05\n",
      "step: 18180 train: 0.08043074607849121 elapsed, loss: 3.1831987e-05\n",
      "step: 18190 train: 0.08110570907592773 elapsed, loss: 3.2847136e-05\n",
      "step: 18200 train: 0.07444238662719727 elapsed, loss: 1.5253781e-05\n",
      "step: 18210 train: 0.08473634719848633 elapsed, loss: 1.4401115e-05\n",
      "step: 18220 train: 0.08070492744445801 elapsed, loss: 8.473435e-06\n",
      "step: 18230 train: 0.0818333625793457 elapsed, loss: 6.185774e-06\n",
      "step: 18240 train: 0.08238554000854492 elapsed, loss: 7.8975045e-06\n",
      "step: 18250 train: 0.08397293090820312 elapsed, loss: 4.0223676e-06\n",
      "step: 18260 train: 0.08647298812866211 elapsed, loss: 3.6363306e-06\n",
      "step: 18270 train: 0.07958269119262695 elapsed, loss: 3.5972216e-06\n",
      "step: 18280 train: 0.07754206657409668 elapsed, loss: 3.366721e-06\n",
      "step: 18290 train: 0.08876681327819824 elapsed, loss: 2.254727e-06\n",
      "step: 18300 train: 0.07889223098754883 elapsed, loss: 1.918986e-06\n",
      "step: 18310 train: 0.07808327674865723 elapsed, loss: 2.2570557e-06\n",
      "step: 18320 train: 0.08424782752990723 elapsed, loss: 2.149023e-06\n",
      "step: 18330 train: 0.08014202117919922 elapsed, loss: 1.9203844e-06\n",
      "step: 18340 train: 0.0835263729095459 elapsed, loss: 1.9669503e-06\n",
      "step: 18350 train: 0.07734274864196777 elapsed, loss: 2.2547276e-06\n",
      "step: 18360 train: 0.07503890991210938 elapsed, loss: 2.3101416e-06\n",
      "step: 18370 train: 0.07965087890625 elapsed, loss: 1.4458763e-06\n",
      "step: 18380 train: 0.07686114311218262 elapsed, loss: 5.28738e-06\n",
      "step: 18390 train: 0.0825047492980957 elapsed, loss: 0.00024183851\n",
      "step: 18400 train: 0.07828378677368164 elapsed, loss: 0.029360835\n",
      "step: 18410 train: 0.08652472496032715 elapsed, loss: 5.7369707e-05\n",
      "step: 18420 train: 0.08241891860961914 elapsed, loss: 7.517511e-05\n",
      "step: 18430 train: 0.07938742637634277 elapsed, loss: 4.738781e-05\n",
      "step: 18440 train: 0.07511115074157715 elapsed, loss: 2.7133305e-05\n",
      "step: 18450 train: 0.07974553108215332 elapsed, loss: 1.2593093e-05\n",
      "step: 18460 train: 0.08152198791503906 elapsed, loss: 1.4621403e-05\n",
      "step: 18470 train: 0.0792229175567627 elapsed, loss: 1.6301916e-05\n",
      "step: 18480 train: 0.08538675308227539 elapsed, loss: 4.5373845e-06\n",
      "step: 18490 train: 0.09088873863220215 elapsed, loss: 4.5112783e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18500 train: 0.0745859146118164 elapsed, loss: 6.602087e-06\n",
      "step: 18510 train: 0.07595610618591309 elapsed, loss: 6.2593017e-06\n",
      "step: 18520 train: 0.07665348052978516 elapsed, loss: 8.319936e-06\n",
      "step: 18530 train: 0.08001422882080078 elapsed, loss: 2.404202e-06\n",
      "step: 18540 train: 0.08083367347717285 elapsed, loss: 5.14134e-06\n",
      "step: 18550 train: 0.08126378059387207 elapsed, loss: 1.8309752e-06\n",
      "step: 18560 train: 0.08210349082946777 elapsed, loss: 1.595819e-06\n",
      "step: 18570 train: 0.08026647567749023 elapsed, loss: 1.8221297e-06\n",
      "step: 18580 train: 0.08389139175415039 elapsed, loss: 2.0489056e-06\n",
      "step: 18590 train: 0.07751178741455078 elapsed, loss: 1.9343527e-06\n",
      "step: 18600 train: 0.08563923835754395 elapsed, loss: 1.7899994e-06\n",
      "step: 18610 train: 0.07441949844360352 elapsed, loss: 2.6174766e-06\n",
      "step: 18620 train: 0.07707929611206055 elapsed, loss: 1.7769612e-06\n",
      "step: 18630 train: 0.08219242095947266 elapsed, loss: 3.633993e-05\n",
      "step: 18640 train: 0.08395910263061523 elapsed, loss: 8.731866e-06\n",
      "step: 18650 train: 0.08022499084472656 elapsed, loss: 2.1916274e-05\n",
      "step: 18660 train: 0.0860588550567627 elapsed, loss: 9.647779e-06\n",
      "step: 18670 train: 0.0799710750579834 elapsed, loss: 0.00154074\n",
      "step: 18680 train: 0.08035540580749512 elapsed, loss: 1.079886e-05\n",
      "step: 18690 train: 0.07955431938171387 elapsed, loss: 1.3398384e-05\n",
      "step: 18700 train: 0.07856321334838867 elapsed, loss: 1.42435e-05\n",
      "step: 18710 train: 0.08279156684875488 elapsed, loss: 7.789839e-06\n",
      "step: 18720 train: 0.07745742797851562 elapsed, loss: 1.38825435e-05\n",
      "step: 18730 train: 0.0782327651977539 elapsed, loss: 7.959769e-06\n",
      "step: 18740 train: 0.08219265937805176 elapsed, loss: 4.9839437e-06\n",
      "step: 18750 train: 0.08074378967285156 elapsed, loss: 5.1620805e-06\n",
      "step: 18760 train: 0.08307766914367676 elapsed, loss: 4.2094366e-06\n",
      "step: 18770 train: 0.08188843727111816 elapsed, loss: 2.9611308e-06\n",
      "step: 18780 train: 0.08378720283508301 elapsed, loss: 2.6714902e-06\n",
      "step: 18790 train: 0.08738493919372559 elapsed, loss: 2.0572813e-06\n",
      "step: 18800 train: 0.08390569686889648 elapsed, loss: 1.8794037e-06\n",
      "step: 18810 train: 0.08141231536865234 elapsed, loss: 3.1366844e-06\n",
      "step: 18820 train: 0.08068561553955078 elapsed, loss: 1.9557742e-06\n",
      "step: 18830 train: 0.09214663505554199 elapsed, loss: 1.4742776e-06\n",
      "step: 18840 train: 0.07835578918457031 elapsed, loss: 2.3575608e-06\n",
      "step: 18850 train: 0.07577753067016602 elapsed, loss: 2.2142153e-06\n",
      "step: 18860 train: 0.0851285457611084 elapsed, loss: 1.2563527e-06\n",
      "step: 18870 train: 0.08914494514465332 elapsed, loss: 1.1022194e-06\n",
      "step: 18880 train: 0.07686209678649902 elapsed, loss: 1.7844111e-06\n",
      "step: 18890 train: 0.07773494720458984 elapsed, loss: 1.8784732e-06\n",
      "step: 18900 train: 0.07921051979064941 elapsed, loss: 2.3730054e-06\n",
      "step: 18910 train: 0.08104252815246582 elapsed, loss: 1.6759122e-06\n",
      "step: 18920 train: 0.07677984237670898 elapsed, loss: 2.2090935e-06\n",
      "step: 18930 train: 0.08393692970275879 elapsed, loss: 1.4426171e-06\n",
      "step: 18940 train: 0.0826878547668457 elapsed, loss: 1.279636e-06\n",
      "step: 18950 train: 0.07352733612060547 elapsed, loss: 1.7452963e-06\n",
      "step: 18960 train: 0.08046722412109375 elapsed, loss: 1.5427341e-06\n",
      "step: 18970 train: 0.0766298770904541 elapsed, loss: 1.9213155e-06\n",
      "step: 18980 train: 0.08053016662597656 elapsed, loss: 1.39046e-06\n",
      "step: 18990 train: 0.08132362365722656 elapsed, loss: 1.3192132e-06\n",
      "step: 19000 train: 0.07735800743103027 elapsed, loss: 1.674516e-06\n",
      "step: 19010 train: 0.08105039596557617 elapsed, loss: 1.6628742e-06\n",
      "step: 19020 train: 0.09006810188293457 elapsed, loss: 0.00012849111\n",
      "step: 19030 train: 0.08489155769348145 elapsed, loss: 0.0014179642\n",
      "step: 19040 train: 0.08305144309997559 elapsed, loss: 0.00092059275\n",
      "step: 19050 train: 0.08121013641357422 elapsed, loss: 3.478535e-05\n",
      "step: 19060 train: 0.08171629905700684 elapsed, loss: 0.00012666106\n",
      "step: 19070 train: 0.08181142807006836 elapsed, loss: 2.156688e-05\n",
      "step: 19080 train: 0.08076715469360352 elapsed, loss: 2.3955297e-05\n",
      "step: 19090 train: 0.0828557014465332 elapsed, loss: 1.649483e-05\n",
      "step: 19100 train: 0.08489227294921875 elapsed, loss: 3.0472856e-05\n",
      "step: 19110 train: 0.07996416091918945 elapsed, loss: 2.103837e-05\n",
      "step: 19120 train: 0.08086800575256348 elapsed, loss: 8.131171e-06\n",
      "step: 19130 train: 0.0826575756072998 elapsed, loss: 1.1748605e-05\n",
      "step: 19140 train: 0.07393765449523926 elapsed, loss: 9.888181e-06\n",
      "step: 19150 train: 0.08097982406616211 elapsed, loss: 4.733849e-06\n",
      "step: 19160 train: 0.08049154281616211 elapsed, loss: 4.8911297e-06\n",
      "step: 19170 train: 0.0864863395690918 elapsed, loss: 2.297567e-06\n",
      "step: 19180 train: 0.08229565620422363 elapsed, loss: 3.7709121e-06\n",
      "step: 19190 train: 0.07896852493286133 elapsed, loss: 2.5420386e-06\n",
      "step: 19200 train: 0.07858753204345703 elapsed, loss: 2.7925648e-06\n",
      "step: 19210 train: 0.08044242858886719 elapsed, loss: 1.6367972e-06\n",
      "step: 19220 train: 0.07739114761352539 elapsed, loss: 1.5445967e-06\n",
      "step: 19230 train: 0.08162093162536621 elapsed, loss: 1.3499506e-06\n",
      "step: 19240 train: 0.08137369155883789 elapsed, loss: 1.6465764e-06\n",
      "step: 19250 train: 0.08101415634155273 elapsed, loss: 4.443235e-06\n",
      "step: 19260 train: 0.07533645629882812 elapsed, loss: 2.865206e-06\n",
      "step: 19270 train: 0.07928729057312012 elapsed, loss: 2.0940743e-06\n",
      "step: 19280 train: 0.08068490028381348 elapsed, loss: 1.3280644e-06\n",
      "step: 19290 train: 0.0890798568725586 elapsed, loss: 1.0887152e-06\n",
      "step: 19300 train: 0.07634234428405762 elapsed, loss: 1.8812689e-06\n",
      "step: 19310 train: 0.08085393905639648 elapsed, loss: 1.2782389e-06\n",
      "step: 19320 train: 0.08384442329406738 elapsed, loss: 1.0570502e-06\n",
      "step: 19330 train: 0.07841038703918457 elapsed, loss: 1.3676458e-06\n",
      "step: 19340 train: 0.077239990234375 elapsed, loss: 1.4444796e-06\n",
      "step: 19350 train: 0.07923364639282227 elapsed, loss: 1.3122321e-06\n",
      "step: 19360 train: 0.08170604705810547 elapsed, loss: 1.2540246e-06\n",
      "step: 19370 train: 0.08641433715820312 elapsed, loss: 1.5189839e-06\n",
      "step: 19380 train: 0.07992076873779297 elapsed, loss: 2.6095463e-06\n",
      "step: 19390 train: 0.07834124565124512 elapsed, loss: 1.4682286e-06\n",
      "step: 19400 train: 0.08370161056518555 elapsed, loss: 2.0237576e-06\n",
      "step: 19410 train: 0.08672571182250977 elapsed, loss: 1.3960495e-06\n",
      "step: 19420 train: 0.08422565460205078 elapsed, loss: 1.3816152e-06\n",
      "step: 19430 train: 0.08078455924987793 elapsed, loss: 1.708975e-06\n",
      "step: 19440 train: 0.08387517929077148 elapsed, loss: 1.427715e-06\n",
      "step: 19450 train: 0.07894372940063477 elapsed, loss: 1.9459505e-06\n",
      "step: 19460 train: 0.08732938766479492 elapsed, loss: 1.769044e-06\n",
      "step: 19470 train: 0.07652163505554199 elapsed, loss: 2.166253e-06\n",
      "step: 19480 train: 0.07955765724182129 elapsed, loss: 1.621896e-06\n",
      "step: 19490 train: 0.08325719833374023 elapsed, loss: 1.8067628e-06\n",
      "step: 19500 train: 0.07874941825866699 elapsed, loss: 2.3986145e-06\n",
      "step: 19510 train: 0.08599209785461426 elapsed, loss: 1.2586813e-06\n",
      "step: 19520 train: 0.07984781265258789 elapsed, loss: 1.8132824e-06\n",
      "step: 19530 train: 0.07741618156433105 elapsed, loss: 2.3944253e-06\n",
      "step: 19540 train: 0.08015060424804688 elapsed, loss: 2.2086278e-06\n",
      "step: 19550 train: 0.08337926864624023 elapsed, loss: 1.6731187e-06\n",
      "step: 19560 train: 0.08220434188842773 elapsed, loss: 1.3941883e-06\n",
      "step: 19570 train: 0.08125758171081543 elapsed, loss: 0.00010369519\n",
      "step: 19580 train: 0.08512234687805176 elapsed, loss: 9.758618e-05\n",
      "step: 19590 train: 0.07755064964294434 elapsed, loss: 1.8326844e-05\n",
      "step: 19600 train: 0.0876457691192627 elapsed, loss: 3.7044407e-05\n",
      "step: 19610 train: 0.08434438705444336 elapsed, loss: 3.5326684e-05\n",
      "step: 19620 train: 0.08243751525878906 elapsed, loss: 9.239597e-06\n",
      "step: 19630 train: 0.0789635181427002 elapsed, loss: 6.5917284e-06\n",
      "step: 19640 train: 0.08055567741394043 elapsed, loss: 4.428416e-06\n",
      "step: 19650 train: 0.07774853706359863 elapsed, loss: 4.198385e-06\n",
      "step: 19660 train: 0.07670307159423828 elapsed, loss: 3.8235303e-06\n",
      "step: 19670 train: 0.08820986747741699 elapsed, loss: 2.55368e-06\n",
      "step: 19680 train: 0.08099794387817383 elapsed, loss: 2.4074632e-06\n",
      "step: 19690 train: 0.0771176815032959 elapsed, loss: 2.7697474e-06\n",
      "step: 19700 train: 0.08413338661193848 elapsed, loss: 1.4072268e-06\n",
      "step: 19710 train: 0.08251261711120605 elapsed, loss: 2.4363267e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19720 train: 0.07905268669128418 elapsed, loss: 1.9865083e-06\n",
      "step: 19730 train: 0.08281421661376953 elapsed, loss: 1.8272522e-06\n",
      "step: 19740 train: 0.08107995986938477 elapsed, loss: 2.1476208e-06\n",
      "step: 19750 train: 0.0852820873260498 elapsed, loss: 1.3448273e-06\n",
      "step: 19760 train: 0.07490801811218262 elapsed, loss: 2.233308e-06\n",
      "step: 19770 train: 0.08145570755004883 elapsed, loss: 1.4649688e-06\n",
      "step: 19780 train: 0.08306241035461426 elapsed, loss: 2.9494897e-06\n",
      "step: 19790 train: 0.08692383766174316 elapsed, loss: 1.5571691e-06\n",
      "step: 19800 train: 0.07801175117492676 elapsed, loss: 2.139709e-06\n",
      "step: 19810 train: 0.08754324913024902 elapsed, loss: 1.3285305e-06\n",
      "step: 19820 train: 0.08386826515197754 elapsed, loss: 1.7704406e-06\n",
      "step: 19830 train: 0.08027482032775879 elapsed, loss: 2.051228e-06\n",
      "step: 19840 train: 0.07838964462280273 elapsed, loss: 1.718288e-06\n",
      "step: 19850 train: 0.0829765796661377 elapsed, loss: 2.6975708e-06\n",
      "step: 19860 train: 0.0835568904876709 elapsed, loss: 2.1196865e-06\n",
      "step: 19870 train: 0.08015942573547363 elapsed, loss: 1.4705566e-06\n",
      "step: 19880 train: 0.08170509338378906 elapsed, loss: 1.8556575e-06\n",
      "step: 19890 train: 0.08113694190979004 elapsed, loss: 2.8824372e-06\n",
      "step: 19900 train: 0.07457780838012695 elapsed, loss: 3.390006e-06\n",
      "step: 19910 train: 0.08106637001037598 elapsed, loss: 1.556702e-06\n",
      "step: 19920 train: 0.08287739753723145 elapsed, loss: 1.601873e-06\n",
      "step: 19930 train: 0.07705926895141602 elapsed, loss: 1.9585689e-06\n",
      "step: 19940 train: 0.08164024353027344 elapsed, loss: 2.8703303e-06\n",
      "step: 19950 train: 0.0729215145111084 elapsed, loss: 0.00039311562\n",
      "step: 19960 train: 0.08533143997192383 elapsed, loss: 4.4567503e-05\n",
      "step: 19970 train: 0.0861358642578125 elapsed, loss: 3.2124026e-05\n",
      "step: 19980 train: 0.07797956466674805 elapsed, loss: 1.2313497e-05\n",
      "step: 19990 train: 0.07743048667907715 elapsed, loss: 1.0848323e-05\n",
      "step: 20000 train: 0.08129596710205078 elapsed, loss: 9.1621805e-06\n",
      "step: 20010 train: 0.08127784729003906 elapsed, loss: 7.456549e-06\n",
      "step: 20020 train: 0.0783839225769043 elapsed, loss: 9.6944605e-06\n",
      "step: 20030 train: 0.07812643051147461 elapsed, loss: 5.9585645e-06\n",
      "step: 20040 train: 0.08905315399169922 elapsed, loss: 1.0007157e-05\n",
      "step: 20050 train: 0.08347392082214355 elapsed, loss: 3.5455332e-06\n",
      "step: 20060 train: 0.08203268051147461 elapsed, loss: 4.996948e-06\n",
      "step: 20070 train: 0.08656167984008789 elapsed, loss: 2.6221323e-06\n",
      "step: 20080 train: 0.08181452751159668 elapsed, loss: 2.6621792e-06\n",
      "step: 20090 train: 0.08287739753723145 elapsed, loss: 2.099663e-06\n",
      "step: 20100 train: 0.08155417442321777 elapsed, loss: 2.0354012e-06\n",
      "step: 20110 train: 0.077545166015625 elapsed, loss: 3.0072326e-06\n",
      "step: 20120 train: 0.07780337333679199 elapsed, loss: 2.9220155e-06\n",
      "step: 20130 train: 0.0784139633178711 elapsed, loss: 2.36183e-06\n",
      "step: 20140 train: 0.08414888381958008 elapsed, loss: 1.6964007e-06\n",
      "step: 20150 train: 0.08291959762573242 elapsed, loss: 1.3676458e-06\n",
      "step: 20160 train: 0.08587193489074707 elapsed, loss: 1.2987279e-06\n",
      "step: 20170 train: 0.08065485954284668 elapsed, loss: 2.5364507e-06\n",
      "step: 20180 train: 0.08277010917663574 elapsed, loss: 1.558101e-06\n",
      "step: 20190 train: 0.08006501197814941 elapsed, loss: 2.2915142e-06\n",
      "step: 20200 train: 0.08015251159667969 elapsed, loss: 2.0507684e-06\n",
      "step: 20210 train: 0.08327937126159668 elapsed, loss: 1.9967515e-06\n",
      "step: 20220 train: 0.08278775215148926 elapsed, loss: 1.6731169e-06\n",
      "step: 20230 train: 0.08265495300292969 elapsed, loss: 0.0076780436\n",
      "step: 20240 train: 0.0838937759399414 elapsed, loss: 2.9922383e-05\n",
      "step: 20250 train: 0.08354520797729492 elapsed, loss: 0.00018887818\n",
      "step: 20260 train: 0.0774540901184082 elapsed, loss: 1.2584943e-05\n",
      "step: 20270 train: 0.07917356491088867 elapsed, loss: 8.220657e-06\n",
      "step: 20280 train: 0.07979798316955566 elapsed, loss: 5.660523e-06\n",
      "step: 20290 train: 0.0794515609741211 elapsed, loss: 4.045648e-06\n",
      "step: 20300 train: 0.0754694938659668 elapsed, loss: 5.668916e-06\n",
      "step: 20310 train: 0.0790412425994873 elapsed, loss: 4.919146e-06\n",
      "step: 20320 train: 0.07733607292175293 elapsed, loss: 3.2964035e-06\n",
      "step: 20330 train: 0.08064556121826172 elapsed, loss: 3.5487733e-06\n",
      "step: 20340 train: 0.07770943641662598 elapsed, loss: 2.4717253e-06\n",
      "step: 20350 train: 0.08163285255432129 elapsed, loss: 6.926152e-06\n",
      "step: 20360 train: 0.0905005931854248 elapsed, loss: 2.0968675e-06\n",
      "step: 20370 train: 0.0784904956817627 elapsed, loss: 1.9972165e-06\n",
      "step: 20380 train: 0.07718062400817871 elapsed, loss: 2.4125864e-06\n",
      "step: 20390 train: 0.08775520324707031 elapsed, loss: 1.518985e-06\n",
      "step: 20400 train: 0.07644510269165039 elapsed, loss: 1.9045516e-06\n",
      "step: 20410 train: 0.08210945129394531 elapsed, loss: 1.3401717e-06\n",
      "step: 20420 train: 0.08214282989501953 elapsed, loss: 1.4412201e-06\n",
      "step: 20430 train: 0.08724093437194824 elapsed, loss: 1.8272522e-06\n",
      "step: 20440 train: 0.07590842247009277 elapsed, loss: 5.89831e-05\n",
      "step: 20450 train: 0.08303546905517578 elapsed, loss: 0.0004584334\n",
      "step: 20460 train: 0.07844758033752441 elapsed, loss: 0.00043529342\n",
      "step: 20470 train: 0.08094215393066406 elapsed, loss: 2.4196126e-05\n",
      "step: 20480 train: 0.07554221153259277 elapsed, loss: 1.713577e-05\n",
      "step: 20490 train: 0.0758357048034668 elapsed, loss: 2.5609823e-05\n",
      "step: 20500 train: 0.08472156524658203 elapsed, loss: 8.0851805e-06\n",
      "step: 20510 train: 0.08134126663208008 elapsed, loss: 8.753552e-06\n",
      "step: 20520 train: 0.0805366039276123 elapsed, loss: 6.080085e-06\n",
      "step: 20530 train: 0.07853293418884277 elapsed, loss: 5.905479e-06\n",
      "step: 20540 train: 0.08026242256164551 elapsed, loss: 6.197102e-06\n",
      "step: 20550 train: 0.0782315731048584 elapsed, loss: 3.0542606e-06\n",
      "step: 20560 train: 0.08218145370483398 elapsed, loss: 3.9165734e-06\n",
      "step: 20570 train: 0.08902263641357422 elapsed, loss: 2.3678826e-06\n",
      "step: 20580 train: 0.08105325698852539 elapsed, loss: 1.828183e-06\n",
      "step: 20590 train: 0.08105230331420898 elapsed, loss: 1.8654339e-06\n",
      "step: 20600 train: 0.07403373718261719 elapsed, loss: 3.9789942e-05\n",
      "step: 20610 train: 0.07817196846008301 elapsed, loss: 6.699543e-05\n",
      "step: 20620 train: 0.0833125114440918 elapsed, loss: 1.795042e-05\n",
      "step: 20630 train: 0.07866764068603516 elapsed, loss: 1.8214861e-05\n",
      "step: 20640 train: 0.08239054679870605 elapsed, loss: 2.9617568e-05\n",
      "step: 20650 train: 0.07939910888671875 elapsed, loss: 2.5689433e-05\n",
      "step: 20660 train: 0.09364008903503418 elapsed, loss: 6.9992266e-06\n",
      "step: 20670 train: 0.07976794242858887 elapsed, loss: 8.620129e-06\n",
      "step: 20680 train: 0.07648444175720215 elapsed, loss: 6.9696675e-06\n",
      "step: 20690 train: 0.07978248596191406 elapsed, loss: 5.130628e-06\n",
      "step: 20700 train: 0.07471442222595215 elapsed, loss: 5.7424854e-06\n",
      "step: 20710 train: 0.0833730697631836 elapsed, loss: 2.7716083e-06\n",
      "step: 20720 train: 0.0813758373260498 elapsed, loss: 2.8628783e-06\n",
      "step: 20730 train: 0.0812845230102539 elapsed, loss: 2.8405254e-06\n",
      "step: 20740 train: 0.08492898941040039 elapsed, loss: 1.7899993e-06\n",
      "step: 20750 train: 0.07216143608093262 elapsed, loss: 2.4512367e-06\n",
      "step: 20760 train: 0.07790732383728027 elapsed, loss: 2.791166e-06\n",
      "step: 20770 train: 0.08791804313659668 elapsed, loss: 1.5106032e-06\n",
      "step: 20780 train: 0.07956671714782715 elapsed, loss: 1.5948879e-06\n",
      "step: 20790 train: 0.08033227920532227 elapsed, loss: 2.2328138e-06\n",
      "step: 20800 train: 0.07814311981201172 elapsed, loss: 1.3532099e-06\n",
      "step: 20810 train: 0.07929372787475586 elapsed, loss: 1.309904e-06\n",
      "step: 20820 train: 0.0803976058959961 elapsed, loss: 2.063341e-06\n",
      "step: 20830 train: 0.0823981761932373 elapsed, loss: 1.2083898e-06\n",
      "step: 20840 train: 0.07653260231018066 elapsed, loss: 1.5012902e-06\n",
      "step: 20850 train: 0.07942652702331543 elapsed, loss: 1.2544904e-06\n",
      "step: 20860 train: 0.07245063781738281 elapsed, loss: 2.284065e-06\n",
      "step: 20870 train: 0.08230304718017578 elapsed, loss: 1.5329554e-06\n",
      "step: 20880 train: 0.08116936683654785 elapsed, loss: 1.4989616e-06\n",
      "step: 20890 train: 0.07390642166137695 elapsed, loss: 3.126443e-06\n",
      "step: 20900 train: 0.07605791091918945 elapsed, loss: 2.9443684e-06\n",
      "step: 20910 train: 0.07829666137695312 elapsed, loss: 1.4337681e-06\n",
      "step: 20920 train: 0.07547163963317871 elapsed, loss: 2.5611325e-06\n",
      "step: 20930 train: 0.07877326011657715 elapsed, loss: 1.4873206e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20940 train: 0.07944631576538086 elapsed, loss: 2.1210838e-06\n",
      "step: 20950 train: 0.07740116119384766 elapsed, loss: 1.9213157e-06\n",
      "step: 20960 train: 0.08292531967163086 elapsed, loss: 1.7848772e-06\n",
      "step: 20970 train: 0.0783696174621582 elapsed, loss: 2.245415e-06\n",
      "step: 20980 train: 0.0791921615600586 elapsed, loss: 1.8160761e-06\n",
      "step: 20990 train: 0.08691048622131348 elapsed, loss: 1.5888338e-06\n",
      "step: 21000 train: 0.08127522468566895 elapsed, loss: 1.966484e-06\n",
      "step: 21010 train: 0.08349943161010742 elapsed, loss: 1.6642712e-06\n",
      "step: 21020 train: 0.0791630744934082 elapsed, loss: 3.7397072e-06\n",
      "step: 21030 train: 0.07687592506408691 elapsed, loss: 3.0682334e-06\n",
      "step: 21040 train: 0.07764506340026855 elapsed, loss: 2.2118877e-06\n",
      "step: 21050 train: 0.08174014091491699 elapsed, loss: 1.8607799e-06\n",
      "step: 21060 train: 0.07749581336975098 elapsed, loss: 2.0428524e-06\n",
      "step: 21070 train: 0.07776141166687012 elapsed, loss: 1.7802207e-06\n",
      "step: 21080 train: 0.0807037353515625 elapsed, loss: 1.586972e-06\n",
      "step: 21090 train: 0.07996821403503418 elapsed, loss: 2.1974515e-06\n",
      "step: 21100 train: 0.08396792411804199 elapsed, loss: 1.3820793e-06\n",
      "step: 21110 train: 0.08281087875366211 elapsed, loss: 1.5962851e-06\n",
      "step: 21120 train: 0.07712674140930176 elapsed, loss: 2.7958245e-06\n",
      "step: 21130 train: 0.08107423782348633 elapsed, loss: 1.989767e-06\n",
      "step: 21140 train: 0.07849740982055664 elapsed, loss: 2.6584512e-06\n",
      "step: 21150 train: 0.07199764251708984 elapsed, loss: 3.2796452e-06\n",
      "step: 21160 train: 0.07918381690979004 elapsed, loss: 1.9189874e-06\n",
      "step: 21170 train: 0.08154487609863281 elapsed, loss: 1.6698596e-06\n",
      "step: 21180 train: 0.07950901985168457 elapsed, loss: 2.0279492e-06\n",
      "step: 21190 train: 0.07439708709716797 elapsed, loss: 3.99117e-06\n",
      "step: 21200 train: 0.0815436840057373 elapsed, loss: 1.5385432e-06\n",
      "step: 21210 train: 0.0788123607635498 elapsed, loss: 2.7529845e-06\n",
      "step: 21220 train: 0.07559347152709961 elapsed, loss: 5.4072243e-06\n",
      "step: 21230 train: 0.08073806762695312 elapsed, loss: 1.7778927e-06\n",
      "step: 21240 train: 0.08335161209106445 elapsed, loss: 1.4947711e-06\n",
      "step: 21250 train: 0.08374834060668945 elapsed, loss: 1.6125825e-06\n",
      "step: 21260 train: 0.0839073657989502 elapsed, loss: 3.487355e-05\n",
      "step: 21270 train: 0.07498860359191895 elapsed, loss: 2.7175363e-05\n",
      "step: 21280 train: 0.07949399948120117 elapsed, loss: 1.937203e-05\n",
      "step: 21290 train: 0.08693408966064453 elapsed, loss: 8.295613e-06\n",
      "step: 21300 train: 0.08351850509643555 elapsed, loss: 7.0485185e-06\n",
      "step: 21310 train: 0.08314847946166992 elapsed, loss: 5.0179387e-06\n",
      "step: 21320 train: 0.07889604568481445 elapsed, loss: 7.1477207e-06\n",
      "step: 21330 train: 0.07943201065063477 elapsed, loss: 4.9965183e-06\n",
      "step: 21340 train: 0.0775141716003418 elapsed, loss: 3.6773156e-06\n",
      "step: 21350 train: 0.08648562431335449 elapsed, loss: 2.8335412e-06\n",
      "step: 21360 train: 0.07715392112731934 elapsed, loss: 4.0484456e-06\n",
      "step: 21370 train: 0.08154773712158203 elapsed, loss: 2.9983858e-06\n",
      "step: 21380 train: 0.09084367752075195 elapsed, loss: 1.387669e-06\n",
      "step: 21390 train: 0.0812075138092041 elapsed, loss: 1.7727698e-06\n",
      "step: 21400 train: 0.0800623893737793 elapsed, loss: 1.5976814e-06\n",
      "step: 21410 train: 0.08153033256530762 elapsed, loss: 2.0307457e-06\n",
      "step: 21420 train: 0.08104825019836426 elapsed, loss: 1.9138633e-06\n",
      "step: 21430 train: 0.07807683944702148 elapsed, loss: 2.22213e-06\n",
      "step: 21440 train: 0.0797429084777832 elapsed, loss: 1.6796381e-06\n",
      "step: 21450 train: 0.08023357391357422 elapsed, loss: 2.0996617e-06\n",
      "step: 21460 train: 0.0774226188659668 elapsed, loss: 1.8184039e-06\n",
      "step: 21470 train: 0.07676815986633301 elapsed, loss: 1.8281837e-06\n",
      "step: 21480 train: 0.08459043502807617 elapsed, loss: 1.6582177e-06\n",
      "step: 21490 train: 0.08064627647399902 elapsed, loss: 2.0256239e-06\n",
      "step: 21500 train: 0.08275341987609863 elapsed, loss: 1.9622921e-06\n",
      "step: 21510 train: 0.08136248588562012 elapsed, loss: 2.0181724e-06\n",
      "step: 21520 train: 0.08491396903991699 elapsed, loss: 1.8896501e-06\n",
      "step: 21530 train: 0.08197712898254395 elapsed, loss: 6.9429384e-06\n",
      "step: 21540 train: 0.08263254165649414 elapsed, loss: 3.9478646e-06\n",
      "step: 21550 train: 0.07991385459899902 elapsed, loss: 2.2519307e-06\n",
      "step: 21560 train: 0.08491373062133789 elapsed, loss: 2.0204998e-06\n",
      "step: 21570 train: 0.0778965950012207 elapsed, loss: 2.5941326e-06\n",
      "step: 21580 train: 0.07885527610778809 elapsed, loss: 2.5648578e-06\n",
      "step: 21590 train: 0.07432913780212402 elapsed, loss: 2.8489083e-06\n",
      "step: 21600 train: 0.08305931091308594 elapsed, loss: 1.6689282e-06\n",
      "step: 21610 train: 0.08655929565429688 elapsed, loss: 1.622826e-06\n",
      "step: 21620 train: 0.08449530601501465 elapsed, loss: 1.4086229e-06\n",
      "step: 21630 train: 0.0808725357055664 elapsed, loss: 2.8358709e-06\n",
      "step: 21640 train: 0.07934927940368652 elapsed, loss: 2.1182898e-06\n",
      "step: 21650 train: 0.07966303825378418 elapsed, loss: 3.5455057e-06\n",
      "step: 21660 train: 0.07916402816772461 elapsed, loss: 2.3567043e-06\n",
      "step: 21670 train: 0.08345651626586914 elapsed, loss: 1.7909305e-06\n",
      "step: 21680 train: 0.08373451232910156 elapsed, loss: 1.5106034e-06\n",
      "step: 21690 train: 0.0838320255279541 elapsed, loss: 1.740174e-06\n",
      "step: 21700 train: 0.07901310920715332 elapsed, loss: 3.5091884e-06\n",
      "step: 21710 train: 0.0796363353729248 elapsed, loss: 2.5122372e-06\n",
      "step: 21720 train: 0.0830087661743164 elapsed, loss: 1.8742842e-06\n",
      "step: 21730 train: 0.0850529670715332 elapsed, loss: 2.4964056e-06\n",
      "step: 21740 train: 0.07642817497253418 elapsed, loss: 2.6319126e-06\n",
      "step: 21750 train: 0.08704710006713867 elapsed, loss: 1.6796379e-06\n",
      "step: 21760 train: 0.08214926719665527 elapsed, loss: 2.7241135e-06\n",
      "step: 21770 train: 0.0802006721496582 elapsed, loss: 2.6090947e-06\n",
      "step: 21780 train: 0.07948422431945801 elapsed, loss: 4.426562e-06\n",
      "step: 21790 train: 0.07948899269104004 elapsed, loss: 2.0577536e-06\n",
      "step: 21800 train: 0.07526826858520508 elapsed, loss: 1.9506526e-06\n",
      "step: 21810 train: 0.08508038520812988 elapsed, loss: 1.941804e-06\n",
      "step: 21820 train: 0.07826995849609375 elapsed, loss: 2.3939604e-06\n",
      "step: 21830 train: 0.08604192733764648 elapsed, loss: 2.237964e-06\n",
      "step: 21840 train: 0.08022093772888184 elapsed, loss: 1.9161935e-06\n",
      "step: 21850 train: 0.08035087585449219 elapsed, loss: 1.5153368e-05\n",
      "step: 21860 train: 0.08448433876037598 elapsed, loss: 2.2933727e-06\n",
      "step: 21870 train: 0.08835506439208984 elapsed, loss: 1.9269037e-06\n",
      "step: 21880 train: 0.0800929069519043 elapsed, loss: 1.7248069e-06\n",
      "step: 21890 train: 0.08499932289123535 elapsed, loss: 1.6512331e-06\n",
      "step: 21900 train: 0.08625578880310059 elapsed, loss: 1.8887188e-06\n",
      "step: 21910 train: 0.0739593505859375 elapsed, loss: 2.6202713e-06\n",
      "step: 21920 train: 0.0764622688293457 elapsed, loss: 7.598992e-06\n",
      "step: 21930 train: 0.08576607704162598 elapsed, loss: 0.0003647178\n",
      "step: 21940 train: 0.08498311042785645 elapsed, loss: 8.690778e-05\n",
      "step: 21950 train: 0.08202695846557617 elapsed, loss: 5.4456563e-05\n",
      "step: 21960 train: 0.08706402778625488 elapsed, loss: 1.7647402e-05\n",
      "step: 21970 train: 0.08861422538757324 elapsed, loss: 1.4229462e-05\n",
      "step: 21980 train: 0.08572840690612793 elapsed, loss: 1.4163451e-05\n",
      "step: 21990 train: 0.0847787857055664 elapsed, loss: 0.00030039187\n",
      "step: 22000 train: 0.07891678810119629 elapsed, loss: 0.00017269194\n",
      "step: 22010 train: 0.08002281188964844 elapsed, loss: 0.00015934894\n",
      "step: 22020 train: 0.08433771133422852 elapsed, loss: 5.542459e-05\n",
      "step: 22030 train: 0.08538198471069336 elapsed, loss: 7.049803e-05\n",
      "step: 22040 train: 0.0837409496307373 elapsed, loss: 4.7998794e-05\n",
      "step: 22050 train: 0.07945418357849121 elapsed, loss: 7.242377e-05\n",
      "step: 22060 train: 0.08974242210388184 elapsed, loss: 1.6146952e-05\n",
      "step: 22070 train: 0.08177590370178223 elapsed, loss: 2.1394213e-05\n",
      "step: 22080 train: 0.08508515357971191 elapsed, loss: 1.5118924e-05\n",
      "step: 22090 train: 0.07979321479797363 elapsed, loss: 1.2085522e-05\n",
      "step: 22100 train: 0.08041524887084961 elapsed, loss: 1.202794e-05\n",
      "step: 22110 train: 0.0813150405883789 elapsed, loss: 1.4997264e-05\n",
      "step: 22120 train: 0.07592391967773438 elapsed, loss: 9.061667e-06\n",
      "step: 22130 train: 0.07869744300842285 elapsed, loss: 5.952049e-06\n",
      "step: 22140 train: 0.07537627220153809 elapsed, loss: 4.8684547e-06\n",
      "step: 22150 train: 0.07322382926940918 elapsed, loss: 4.698034e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22160 train: 0.0775909423828125 elapsed, loss: 2.5099093e-06\n",
      "step: 22170 train: 0.0794680118560791 elapsed, loss: 2.592793e-06\n",
      "step: 22180 train: 0.07960081100463867 elapsed, loss: 1.9329566e-06\n",
      "step: 22190 train: 0.07888650894165039 elapsed, loss: 1.8440161e-06\n",
      "step: 22200 train: 0.07627463340759277 elapsed, loss: 2.2891868e-06\n",
      "step: 22210 train: 0.07836604118347168 elapsed, loss: 1.8542586e-06\n",
      "step: 22220 train: 0.07800722122192383 elapsed, loss: 1.5739333e-06\n",
      "step: 22230 train: 0.07812714576721191 elapsed, loss: 1.664737e-06\n",
      "step: 22240 train: 0.07552051544189453 elapsed, loss: 1.8612453e-06\n",
      "step: 22250 train: 0.07614588737487793 elapsed, loss: 1.5911628e-06\n",
      "step: 22260 train: 0.07419180870056152 elapsed, loss: 1.818405e-06\n",
      "step: 22270 train: 0.08600115776062012 elapsed, loss: 1.3159561e-06\n",
      "step: 22280 train: 0.08470797538757324 elapsed, loss: 1.3876693e-06\n",
      "step: 22290 train: 0.08197021484375 elapsed, loss: 1.3224768e-06\n",
      "step: 22300 train: 0.07869529724121094 elapsed, loss: 2.7292333e-06\n",
      "step: 22310 train: 0.07529735565185547 elapsed, loss: 4.332497e-06\n",
      "step: 22320 train: 0.0797884464263916 elapsed, loss: 1.8244526e-06\n",
      "step: 22330 train: 0.07700872421264648 elapsed, loss: 2.1899973e-06\n",
      "step: 22340 train: 0.07939720153808594 elapsed, loss: 1.5809183e-06\n",
      "step: 22350 train: 0.08448123931884766 elapsed, loss: 1.7601969e-06\n",
      "step: 22360 train: 0.07689785957336426 elapsed, loss: 1.8612453e-06\n",
      "step: 22370 train: 0.08284592628479004 elapsed, loss: 1.655424e-06\n",
      "step: 22380 train: 0.08217000961303711 elapsed, loss: 1.3895319e-06\n",
      "step: 22390 train: 0.08331108093261719 elapsed, loss: 1.5222449e-06\n",
      "step: 22400 train: 0.08135700225830078 elapsed, loss: 1.8831317e-06\n",
      "step: 22410 train: 0.07536935806274414 elapsed, loss: 5.1343345e-06\n",
      "step: 22420 train: 0.08303236961364746 elapsed, loss: 0.00014481175\n",
      "step: 22430 train: 0.07970833778381348 elapsed, loss: 0.00016345113\n",
      "step: 22440 train: 0.08513832092285156 elapsed, loss: 0.00015392908\n",
      "step: 22450 train: 0.07858610153198242 elapsed, loss: 5.035654e-05\n",
      "step: 22460 train: 0.08693122863769531 elapsed, loss: 2.8662453e-05\n",
      "step: 22470 train: 0.07836461067199707 elapsed, loss: 3.297811e-05\n",
      "step: 22480 train: 0.08452248573303223 elapsed, loss: 1.36533e-05\n",
      "step: 22490 train: 0.0866396427154541 elapsed, loss: 1.3536401e-05\n",
      "step: 22500 train: 0.08554220199584961 elapsed, loss: 1.0378358e-05\n",
      "step: 22510 train: 0.07865047454833984 elapsed, loss: 1.6169668e-05\n",
      "step: 22520 train: 0.08096098899841309 elapsed, loss: 1.0372378e-05\n",
      "step: 22530 train: 0.08854866027832031 elapsed, loss: 5.2204887e-06\n",
      "step: 22540 train: 0.09382152557373047 elapsed, loss: 3.2703274e-06\n",
      "step: 22550 train: 0.0783686637878418 elapsed, loss: 4.2430142e-06\n",
      "step: 22560 train: 0.08030200004577637 elapsed, loss: 3.1348206e-06\n",
      "step: 22570 train: 0.08165931701660156 elapsed, loss: 2.5965128e-06\n",
      "step: 22580 train: 0.07648611068725586 elapsed, loss: 3.6451852e-06\n",
      "step: 22590 train: 0.07577013969421387 elapsed, loss: 3.2963699e-06\n",
      "step: 22600 train: 0.07937264442443848 elapsed, loss: 2.0302746e-06\n",
      "step: 22610 train: 0.08290863037109375 elapsed, loss: 1.493839e-06\n",
      "step: 22620 train: 0.08645820617675781 elapsed, loss: 1.4626401e-06\n",
      "step: 22630 train: 0.07874321937561035 elapsed, loss: 1.9553074e-06\n",
      "step: 22640 train: 0.08295607566833496 elapsed, loss: 1.4947677e-06\n",
      "step: 22650 train: 0.08253026008605957 elapsed, loss: 1.7089743e-06\n",
      "step: 22660 train: 0.07473230361938477 elapsed, loss: 2.3371485e-06\n",
      "step: 22670 train: 0.08552289009094238 elapsed, loss: 1.4631057e-06\n",
      "step: 22680 train: 0.09171462059020996 elapsed, loss: 1.1175861e-06\n",
      "step: 22690 train: 0.07427215576171875 elapsed, loss: 2.849376e-06\n",
      "step: 22700 train: 0.08484005928039551 elapsed, loss: 1.5445966e-06\n",
      "step: 22710 train: 0.08308911323547363 elapsed, loss: 1.7131651e-06\n",
      "step: 22720 train: 0.0890645980834961 elapsed, loss: 1.4924406e-06\n",
      "step: 22730 train: 0.07856273651123047 elapsed, loss: 2.4670635e-06\n",
      "step: 22740 train: 0.0799248218536377 elapsed, loss: 1.3699741e-06\n",
      "step: 22750 train: 0.07419872283935547 elapsed, loss: 3.2586895e-06\n",
      "step: 22760 train: 0.08322834968566895 elapsed, loss: 1.5073438e-06\n",
      "step: 22770 train: 0.08198189735412598 elapsed, loss: 2.3469288e-06\n",
      "step: 22780 train: 0.0812520980834961 elapsed, loss: 2.2309791e-06\n",
      "step: 22790 train: 0.07649803161621094 elapsed, loss: 2.0428524e-06\n",
      "step: 22800 train: 0.08173298835754395 elapsed, loss: 1.3876693e-06\n",
      "step: 22810 train: 0.07972955703735352 elapsed, loss: 1.9422707e-06\n",
      "step: 22820 train: 0.08158373832702637 elapsed, loss: 2.1331828e-06\n",
      "step: 22830 train: 0.08251786231994629 elapsed, loss: 1.5404057e-06\n",
      "step: 22840 train: 0.07298803329467773 elapsed, loss: 2.6416917e-06\n",
      "step: 22850 train: 0.080596923828125 elapsed, loss: 1.7643881e-06\n",
      "step: 22860 train: 0.0750417709350586 elapsed, loss: 5.6850035e-06\n",
      "step: 22870 train: 0.073516845703125 elapsed, loss: 9.58408e-06\n",
      "step: 22880 train: 0.0785059928894043 elapsed, loss: 4.7641306e-06\n",
      "step: 22890 train: 0.07684946060180664 elapsed, loss: 4.9764967e-06\n",
      "step: 22900 train: 0.08211231231689453 elapsed, loss: 1.8696273e-06\n",
      "step: 22910 train: 0.08125948905944824 elapsed, loss: 1.7476225e-06\n",
      "step: 22920 train: 0.0788121223449707 elapsed, loss: 1.7387766e-06\n",
      "step: 22930 train: 0.07820677757263184 elapsed, loss: 2.0940738e-06\n",
      "step: 22940 train: 0.0806281566619873 elapsed, loss: 2.1723063e-06\n",
      "step: 22950 train: 0.07909035682678223 elapsed, loss: 2.6379662e-06\n",
      "step: 22960 train: 0.07894349098205566 elapsed, loss: 1.7569365e-06\n",
      "step: 22970 train: 0.07749319076538086 elapsed, loss: 2.6780128e-06\n",
      "step: 22980 train: 0.08254837989807129 elapsed, loss: 2.2225977e-06\n",
      "step: 22990 train: 0.08260703086853027 elapsed, loss: 1.5343514e-06\n",
      "step: 23000 train: 0.08311820030212402 elapsed, loss: 1.7732355e-06\n",
      "step: 23010 train: 0.08242344856262207 elapsed, loss: 1.4412203e-06\n",
      "step: 23020 train: 0.0782623291015625 elapsed, loss: 4.779967e-06\n",
      "step: 23030 train: 0.08114075660705566 elapsed, loss: 3.6712643e-06\n",
      "step: 23040 train: 0.07642960548400879 elapsed, loss: 3.3927838e-06\n",
      "step: 23050 train: 0.08083534240722656 elapsed, loss: 3.3327296e-06\n",
      "step: 23060 train: 0.08029580116271973 elapsed, loss: 1.9436675e-06\n",
      "step: 23070 train: 0.07844805717468262 elapsed, loss: 1.9590332e-06\n",
      "step: 23080 train: 0.07901906967163086 elapsed, loss: 1.8044352e-06\n",
      "step: 23090 train: 0.08011102676391602 elapsed, loss: 1.8337716e-06\n",
      "step: 23100 train: 0.07725906372070312 elapsed, loss: 1.7862747e-06\n",
      "step: 23110 train: 0.08049774169921875 elapsed, loss: 3.358342e-06\n",
      "step: 23120 train: 0.08078908920288086 elapsed, loss: 2.371609e-06\n",
      "step: 23130 train: 0.08428740501403809 elapsed, loss: 1.6191012e-06\n",
      "step: 23140 train: 0.07909512519836426 elapsed, loss: 0.010062983\n",
      "step: 23150 train: 0.08209967613220215 elapsed, loss: 6.243279e-05\n",
      "step: 23160 train: 0.08272242546081543 elapsed, loss: 0.00046992386\n",
      "step: 23170 train: 0.09220409393310547 elapsed, loss: 4.220112e-05\n",
      "step: 23180 train: 0.08638405799865723 elapsed, loss: 2.4636083e-05\n",
      "step: 23190 train: 0.08495807647705078 elapsed, loss: 1.4040692e-05\n",
      "step: 23200 train: 0.08101391792297363 elapsed, loss: 9.348943e-06\n",
      "step: 23210 train: 0.07992124557495117 elapsed, loss: 1.0276914e-05\n",
      "step: 23220 train: 0.07863378524780273 elapsed, loss: 1.048935e-05\n",
      "step: 23230 train: 0.08008027076721191 elapsed, loss: 5.180448e-06\n",
      "step: 23240 train: 0.07723140716552734 elapsed, loss: 7.4560926e-06\n",
      "step: 23250 train: 0.0770416259765625 elapsed, loss: 3.6479767e-06\n",
      "step: 23260 train: 0.07747721672058105 elapsed, loss: 3.4109582e-06\n",
      "step: 23270 train: 0.08037209510803223 elapsed, loss: 2.6975704e-06\n",
      "step: 23280 train: 0.07829856872558594 elapsed, loss: 2.8982674e-06\n",
      "step: 23290 train: 0.0847172737121582 elapsed, loss: 1.8784752e-06\n",
      "step: 23300 train: 0.08344912528991699 elapsed, loss: 1.6507669e-06\n",
      "step: 23310 train: 0.0742940902709961 elapsed, loss: 2.9969892e-06\n",
      "step: 23320 train: 0.07964324951171875 elapsed, loss: 1.789534e-06\n",
      "step: 23330 train: 0.08012795448303223 elapsed, loss: 2.0195696e-06\n",
      "step: 23340 train: 0.07846736907958984 elapsed, loss: 1.6875532e-06\n",
      "step: 23350 train: 0.07596230506896973 elapsed, loss: 2.3497232e-06\n",
      "step: 23360 train: 0.08143186569213867 elapsed, loss: 1.260544e-06\n",
      "step: 23370 train: 0.0786447525024414 elapsed, loss: 1.8281837e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 23380 train: 0.07909655570983887 elapsed, loss: 2.2761453e-06\n",
      "step: 23390 train: 0.07932066917419434 elapsed, loss: 2.3241112e-06\n",
      "step: 23400 train: 0.08145999908447266 elapsed, loss: 2.4270216e-06\n",
      "step: 23410 train: 0.07905411720275879 elapsed, loss: 2.9601993e-06\n",
      "step: 23420 train: 0.07842063903808594 elapsed, loss: 1.5068783e-06\n",
      "step: 23430 train: 0.07752180099487305 elapsed, loss: 2.3241091e-06\n",
      "step: 23440 train: 0.08402585983276367 elapsed, loss: 3.0975589e-06\n",
      "step: 23450 train: 0.08243060111999512 elapsed, loss: 1.5608948e-06\n",
      "step: 23460 train: 0.07450270652770996 elapsed, loss: 2.8884906e-06\n",
      "step: 23470 train: 0.08032798767089844 elapsed, loss: 4.7305693e-06\n",
      "step: 23480 train: 0.07741379737854004 elapsed, loss: 2.404201e-06\n",
      "step: 23490 train: 0.0826716423034668 elapsed, loss: 1.5557725e-06\n",
      "step: 23500 train: 0.07726263999938965 elapsed, loss: 2.2016404e-06\n",
      "step: 23510 train: 0.08012223243713379 elapsed, loss: 1.7825481e-06\n",
      "step: 23520 train: 0.07705259323120117 elapsed, loss: 2.4880233e-06\n",
      "step: 23530 train: 0.08340001106262207 elapsed, loss: 2.0097914e-06\n",
      "step: 23540 train: 0.08529496192932129 elapsed, loss: 1.8505351e-06\n",
      "step: 23550 train: 0.07483863830566406 elapsed, loss: 3.003974e-06\n",
      "step: 23560 train: 0.07739901542663574 elapsed, loss: 1.8500698e-06\n",
      "step: 23570 train: 0.08117818832397461 elapsed, loss: 1.8812689e-06\n",
      "step: 23580 train: 0.0805368423461914 elapsed, loss: 1.6544925e-06\n",
      "step: 23590 train: 0.07826042175292969 elapsed, loss: 3.0072342e-06\n",
      "step: 23600 train: 0.07815313339233398 elapsed, loss: 2.1452984e-06\n",
      "step: 23610 train: 0.07993054389953613 elapsed, loss: 1.9660192e-06\n",
      "step: 23620 train: 0.07901716232299805 elapsed, loss: 4.597714e-06\n",
      "step: 23630 train: 0.0767827033996582 elapsed, loss: 0.005329185\n",
      "step: 23640 train: 0.080902099609375 elapsed, loss: 0.0015732835\n",
      "step: 23650 train: 0.08191561698913574 elapsed, loss: 3.6212157e-05\n",
      "step: 23660 train: 0.0751035213470459 elapsed, loss: 5.107599e-05\n",
      "step: 23670 train: 0.07904577255249023 elapsed, loss: 9.53824e-05\n",
      "step: 23680 train: 0.0872797966003418 elapsed, loss: 1.065172e-05\n",
      "step: 23690 train: 0.08223915100097656 elapsed, loss: 2.2177182e-05\n",
      "step: 23700 train: 0.0816640853881836 elapsed, loss: 8.336181e-06\n",
      "step: 23710 train: 0.0837702751159668 elapsed, loss: 9.516931e-06\n",
      "step: 23720 train: 0.08310961723327637 elapsed, loss: 5.4630946e-06\n",
      "step: 23730 train: 0.08604121208190918 elapsed, loss: 5.518966e-06\n",
      "step: 23740 train: 0.08318352699279785 elapsed, loss: 3.4426248e-06\n",
      "step: 23750 train: 0.0827031135559082 elapsed, loss: 4.658919e-06\n",
      "step: 23760 train: 0.07939815521240234 elapsed, loss: 3.277755e-06\n",
      "step: 23770 train: 0.07813453674316406 elapsed, loss: 2.9839457e-06\n",
      "step: 23780 train: 0.08437323570251465 elapsed, loss: 2.024226e-06\n",
      "step: 23790 train: 0.07917428016662598 elapsed, loss: 0.014486534\n",
      "step: 23800 train: 0.07939362525939941 elapsed, loss: 0.000109715984\n",
      "step: 23810 train: 0.08996224403381348 elapsed, loss: 5.7825557e-05\n",
      "step: 23820 train: 0.0782327651977539 elapsed, loss: 8.794136e-05\n",
      "step: 23830 train: 0.08405876159667969 elapsed, loss: 1.91986e-05\n",
      "step: 23840 train: 0.08159852027893066 elapsed, loss: 1.10010715e-05\n",
      "step: 23850 train: 0.07865238189697266 elapsed, loss: 8.504729e-06\n",
      "step: 23860 train: 0.08319878578186035 elapsed, loss: 8.281679e-06\n",
      "step: 23870 train: 0.0771636962890625 elapsed, loss: 8.733834e-06\n",
      "step: 23880 train: 0.07516598701477051 elapsed, loss: 8.072631e-06\n",
      "step: 23890 train: 0.07920265197753906 elapsed, loss: 5.8505807e-06\n",
      "step: 23900 train: 0.07738494873046875 elapsed, loss: 7.5105763e-06\n",
      "step: 23910 train: 0.08383464813232422 elapsed, loss: 2.373003e-06\n",
      "step: 23920 train: 0.08972859382629395 elapsed, loss: 3.2665537e-06\n",
      "step: 23930 train: 0.0841214656829834 elapsed, loss: 2.8689317e-06\n",
      "step: 23940 train: 0.0833740234375 elapsed, loss: 2.6635585e-06\n",
      "step: 23950 train: 0.08340907096862793 elapsed, loss: 1.6791716e-06\n",
      "step: 23960 train: 0.07295894622802734 elapsed, loss: 3.72295e-06\n",
      "step: 23970 train: 0.07999801635742188 elapsed, loss: 1.798847e-06\n",
      "step: 23980 train: 0.08088374137878418 elapsed, loss: 1.083127e-06\n",
      "step: 23990 train: 0.08405900001525879 elapsed, loss: 1.7527414e-06\n",
      "step: 24000 train: 0.08443522453308105 elapsed, loss: 1.4035015e-06\n",
      "step: 24010 train: 0.0800635814666748 elapsed, loss: 1.7993129e-06\n",
      "step: 24020 train: 0.07503747940063477 elapsed, loss: 3.0109231e-06\n",
      "step: 24030 train: 0.07714653015136719 elapsed, loss: 1.915728e-06\n",
      "step: 24040 train: 0.07700848579406738 elapsed, loss: 1.5785895e-06\n",
      "step: 24050 train: 0.08062934875488281 elapsed, loss: 1.2433145e-06\n",
      "step: 24060 train: 0.08989787101745605 elapsed, loss: 1.1804502e-06\n",
      "step: 24070 train: 0.07585382461547852 elapsed, loss: 2.6523996e-06\n",
      "step: 24080 train: 0.08679771423339844 elapsed, loss: 1.5720707e-06\n",
      "step: 24090 train: 0.08350157737731934 elapsed, loss: 1.5939536e-06\n",
      "step: 24100 train: 0.08116292953491211 elapsed, loss: 1.8076948e-06\n",
      "step: 24110 train: 0.0888357162475586 elapsed, loss: 1.2838268e-06\n",
      "step: 24120 train: 0.08396577835083008 elapsed, loss: 1.8156093e-06\n",
      "step: 24130 train: 0.0857856273651123 elapsed, loss: 1.242849e-06\n",
      "step: 24140 train: 0.0852057933807373 elapsed, loss: 1.6996617e-06\n",
      "step: 24150 train: 0.0800936222076416 elapsed, loss: 2.3404095e-06\n",
      "step: 24160 train: 0.07392454147338867 elapsed, loss: 1.4944316e-05\n",
      "step: 24170 train: 0.0843813419342041 elapsed, loss: 3.906408e-06\n",
      "step: 24180 train: 0.08291149139404297 elapsed, loss: 2.132723e-06\n",
      "step: 24190 train: 0.07365036010742188 elapsed, loss: 3.140413e-06\n",
      "step: 24200 train: 0.0750265121459961 elapsed, loss: 2.3697464e-06\n",
      "step: 24210 train: 0.07732295989990234 elapsed, loss: 2.3092105e-06\n",
      "step: 24220 train: 0.08186149597167969 elapsed, loss: 1.60001e-06\n",
      "step: 24230 train: 0.07634758949279785 elapsed, loss: 2.0530977e-06\n",
      "step: 24240 train: 0.07854199409484863 elapsed, loss: 2.3939608e-06\n",
      "step: 24250 train: 0.08417844772338867 elapsed, loss: 2.0391262e-06\n",
      "step: 24260 train: 0.0782158374786377 elapsed, loss: 2.9783632e-06\n",
      "step: 24270 train: 0.07873702049255371 elapsed, loss: 1.7629912e-06\n",
      "step: 24280 train: 0.0839235782623291 elapsed, loss: 1.629345e-06\n",
      "step: 24290 train: 0.08717513084411621 elapsed, loss: 2.4400576e-06\n",
      "step: 24300 train: 0.07884860038757324 elapsed, loss: 1.7946561e-06\n",
      "step: 24310 train: 0.07731890678405762 elapsed, loss: 2.2351703e-06\n",
      "step: 24320 train: 0.07588481903076172 elapsed, loss: 2.397686e-06\n",
      "step: 24330 train: 0.07541060447692871 elapsed, loss: 2.4954732e-06\n",
      "step: 24340 train: 0.07956790924072266 elapsed, loss: 1.7494865e-06\n",
      "step: 24350 train: 0.08301973342895508 elapsed, loss: 2.1504195e-06\n",
      "step: 24360 train: 0.0823521614074707 elapsed, loss: 3.3266738e-06\n",
      "step: 24370 train: 0.08644914627075195 elapsed, loss: 1.6968663e-06\n",
      "step: 24380 train: 0.08137845993041992 elapsed, loss: 1.4645032e-06\n",
      "step: 24390 train: 0.07696795463562012 elapsed, loss: 1.8100228e-06\n",
      "step: 24400 train: 0.07521533966064453 elapsed, loss: 2.3241116e-06\n",
      "step: 24410 train: 0.07762312889099121 elapsed, loss: 3.3606675e-06\n",
      "step: 24420 train: 0.07968425750732422 elapsed, loss: 2.4493738e-06\n",
      "step: 24430 train: 0.07674193382263184 elapsed, loss: 2.9643898e-06\n",
      "step: 24440 train: 0.0775144100189209 elapsed, loss: 1.9352858e-06\n",
      "step: 24450 train: 0.0833137035369873 elapsed, loss: 1.4253877e-06\n",
      "step: 24460 train: 0.07557559013366699 elapsed, loss: 2.3143325e-06\n",
      "step: 24470 train: 0.08473992347717285 elapsed, loss: 1.6945396e-06\n",
      "step: 24480 train: 0.07485127449035645 elapsed, loss: 2.960668e-06\n",
      "step: 24490 train: 0.07812118530273438 elapsed, loss: 1.6670651e-06\n",
      "step: 24500 train: 0.08000922203063965 elapsed, loss: 1.9008266e-06\n",
      "step: 24510 train: 0.07964134216308594 elapsed, loss: 2.9345906e-06\n",
      "step: 24520 train: 0.07911109924316406 elapsed, loss: 6.048441e-06\n",
      "step: 24530 train: 0.0842278003692627 elapsed, loss: 1.7662505e-06\n",
      "step: 24540 train: 0.08436274528503418 elapsed, loss: 2.0014095e-06\n",
      "step: 24550 train: 0.0753779411315918 elapsed, loss: 3.829087e-06\n",
      "step: 24560 train: 0.0798945426940918 elapsed, loss: 2.3622956e-06\n",
      "step: 24570 train: 0.07568049430847168 elapsed, loss: 2.7851065e-06\n",
      "step: 24580 train: 0.08266329765319824 elapsed, loss: 1.7592661e-06\n",
      "step: 24590 train: 0.08101963996887207 elapsed, loss: 1.9757972e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24600 train: 0.07223963737487793 elapsed, loss: 0.00012691472\n",
      "step: 24610 train: 0.07593536376953125 elapsed, loss: 0.00019780241\n",
      "step: 24620 train: 0.07895326614379883 elapsed, loss: 0.00010376833\n",
      "step: 24630 train: 0.07590222358703613 elapsed, loss: 6.992358e-05\n",
      "step: 24640 train: 0.07947993278503418 elapsed, loss: 7.259341e-05\n",
      "step: 24650 train: 0.07640218734741211 elapsed, loss: 5.58811e-05\n",
      "step: 24660 train: 0.07641196250915527 elapsed, loss: 4.2856125e-05\n",
      "step: 24670 train: 0.08379745483398438 elapsed, loss: 2.3216267e-05\n",
      "step: 24680 train: 0.07640671730041504 elapsed, loss: 2.5837868e-05\n",
      "step: 24690 train: 0.07938599586486816 elapsed, loss: 1.19695505e-05\n",
      "step: 24700 train: 0.08131742477416992 elapsed, loss: 9.806488e-06\n",
      "step: 24710 train: 0.07939362525939941 elapsed, loss: 6.6202506e-06\n",
      "step: 24720 train: 0.0743570327758789 elapsed, loss: 8.105679e-06\n",
      "step: 24730 train: 0.08025646209716797 elapsed, loss: 4.9010105e-06\n",
      "step: 24740 train: 0.08157825469970703 elapsed, loss: 3.0421545e-06\n",
      "step: 24750 train: 0.08032512664794922 elapsed, loss: 4.473586e-06\n",
      "step: 24760 train: 0.07700634002685547 elapsed, loss: 4.2044308e-06\n",
      "step: 24770 train: 0.08282208442687988 elapsed, loss: 2.2686968e-06\n",
      "step: 24780 train: 0.07694649696350098 elapsed, loss: 3.9618317e-06\n",
      "step: 24790 train: 0.07622003555297852 elapsed, loss: 2.199777e-06\n",
      "step: 24800 train: 0.08190321922302246 elapsed, loss: 1.6344686e-06\n",
      "step: 24810 train: 0.08449816703796387 elapsed, loss: 2.2291154e-06\n",
      "step: 24820 train: 0.07624363899230957 elapsed, loss: 2.162062e-06\n",
      "step: 24830 train: 0.07625794410705566 elapsed, loss: 2.7320293e-06\n",
      "step: 24840 train: 0.07547235488891602 elapsed, loss: 3.3778974e-06\n",
      "step: 24850 train: 0.07660603523254395 elapsed, loss: 1.8700927e-06\n",
      "step: 24860 train: 0.0733194351196289 elapsed, loss: 0.015738396\n",
      "step: 24870 train: 0.07945799827575684 elapsed, loss: 0.00010762742\n",
      "step: 24880 train: 0.07904601097106934 elapsed, loss: 5.275332e-05\n",
      "step: 24890 train: 0.07590079307556152 elapsed, loss: 2.0185329e-05\n",
      "step: 24900 train: 0.08172607421875 elapsed, loss: 0.0005423702\n",
      "step: 24910 train: 0.0801243782043457 elapsed, loss: 1.2910421e-05\n",
      "step: 24920 train: 0.08117341995239258 elapsed, loss: 8.363575e-06\n",
      "step: 24930 train: 0.08075904846191406 elapsed, loss: 7.1632026e-06\n",
      "step: 24940 train: 0.07553339004516602 elapsed, loss: 9.280442e-06\n",
      "step: 24950 train: 0.0761115550994873 elapsed, loss: 1.0920094e-05\n",
      "step: 24960 train: 0.07851195335388184 elapsed, loss: 5.8197893e-06\n",
      "step: 24970 train: 0.08039093017578125 elapsed, loss: 3.5175917e-06\n",
      "step: 24980 train: 0.08025121688842773 elapsed, loss: 3.071015e-06\n",
      "step: 24990 train: 0.08133196830749512 elapsed, loss: 2.0675318e-06\n",
      "step: 25000 train: 0.08428788185119629 elapsed, loss: 2.0530963e-06\n",
      "step: 25010 train: 0.07948017120361328 elapsed, loss: 2.0260877e-06\n",
      "step: 25020 train: 0.07948780059814453 elapsed, loss: 2.8982558e-06\n",
      "step: 25030 train: 0.07963180541992188 elapsed, loss: 1.8449471e-06\n",
      "step: 25040 train: 0.07950401306152344 elapsed, loss: 2.1057162e-06\n",
      "step: 25050 train: 0.07660222053527832 elapsed, loss: 1.5813831e-06\n",
      "step: 25060 train: 0.08797001838684082 elapsed, loss: 9.993084e-07\n",
      "step: 25070 train: 0.07671809196472168 elapsed, loss: 1.7732359e-06\n",
      "step: 25080 train: 0.07706379890441895 elapsed, loss: 1.7657853e-06\n",
      "step: 25090 train: 0.07697296142578125 elapsed, loss: 1.2936058e-06\n",
      "step: 25100 train: 0.07626032829284668 elapsed, loss: 1.6293469e-06\n",
      "step: 25110 train: 0.07580089569091797 elapsed, loss: 2.1108397e-06\n",
      "step: 25120 train: 0.07598114013671875 elapsed, loss: 1.8714903e-06\n",
      "step: 25130 train: 0.08095216751098633 elapsed, loss: 1.1175862e-06\n",
      "step: 25140 train: 0.08323502540588379 elapsed, loss: 1.1408691e-06\n",
      "step: 25150 train: 0.07510614395141602 elapsed, loss: 1.9902313e-06\n",
      "step: 25160 train: 0.07587242126464844 elapsed, loss: 1.845879e-06\n",
      "step: 25170 train: 0.07611608505249023 elapsed, loss: 1.8659025e-06\n",
      "step: 25180 train: 0.08588433265686035 elapsed, loss: 1.945064e-06\n",
      "step: 25190 train: 0.08551859855651855 elapsed, loss: 1.8649711e-06\n",
      "step: 25200 train: 0.08036065101623535 elapsed, loss: 1.2856897e-06\n",
      "step: 25210 train: 0.08041048049926758 elapsed, loss: 1.9804547e-06\n",
      "step: 25220 train: 0.08063125610351562 elapsed, loss: 2.2076963e-06\n",
      "step: 25230 train: 0.08131051063537598 elapsed, loss: 1.4272499e-06\n",
      "step: 25240 train: 0.07972908020019531 elapsed, loss: 1.2377268e-06\n",
      "step: 25250 train: 0.08072280883789062 elapsed, loss: 1.4551899e-06\n",
      "step: 25260 train: 0.08002686500549316 elapsed, loss: 2.4199953e-06\n",
      "step: 25270 train: 0.07583498954772949 elapsed, loss: 2.4996652e-06\n",
      "step: 25280 train: 0.07644391059875488 elapsed, loss: 2.7012943e-06\n",
      "step: 25290 train: 0.08279037475585938 elapsed, loss: 1.7387767e-06\n",
      "step: 25300 train: 0.07924437522888184 elapsed, loss: 1.8756813e-06\n",
      "step: 25310 train: 0.0748739242553711 elapsed, loss: 0.008091265\n",
      "step: 25320 train: 0.07322144508361816 elapsed, loss: 0.000111549845\n",
      "step: 25330 train: 0.07703185081481934 elapsed, loss: 3.384743e-05\n",
      "step: 25340 train: 0.07811617851257324 elapsed, loss: 1.9379164e-05\n",
      "step: 25350 train: 0.08129048347473145 elapsed, loss: 1.1902421e-05\n",
      "step: 25360 train: 0.08413028717041016 elapsed, loss: 2.162196e-05\n",
      "step: 25370 train: 0.07480931282043457 elapsed, loss: 2.514297e-05\n",
      "step: 25380 train: 0.0768423080444336 elapsed, loss: 9.728913e-06\n",
      "step: 25390 train: 0.08051037788391113 elapsed, loss: 4.658449e-06\n",
      "step: 25400 train: 0.07975554466247559 elapsed, loss: 4.9480814e-06\n",
      "step: 25410 train: 0.07608628273010254 elapsed, loss: 4.4391318e-06\n",
      "step: 25420 train: 0.07941985130310059 elapsed, loss: 3.343433e-06\n",
      "step: 25430 train: 0.07931041717529297 elapsed, loss: 2.7078145e-06\n",
      "step: 25440 train: 0.07586193084716797 elapsed, loss: 2.6347066e-06\n",
      "step: 25450 train: 0.0819542407989502 elapsed, loss: 2.2961715e-06\n",
      "step: 25460 train: 0.07534408569335938 elapsed, loss: 3.064509e-06\n",
      "step: 25470 train: 0.08319306373596191 elapsed, loss: 1.729929e-06\n",
      "step: 25480 train: 0.0756533145904541 elapsed, loss: 2.168108e-06\n",
      "step: 25490 train: 0.07944679260253906 elapsed, loss: 1.5138633e-06\n",
      "step: 25500 train: 0.07655835151672363 elapsed, loss: 1.6274843e-06\n",
      "step: 25510 train: 0.08091425895690918 elapsed, loss: 1.2442458e-06\n",
      "step: 25520 train: 0.0803072452545166 elapsed, loss: 2.9499538e-06\n",
      "step: 25530 train: 0.08035683631896973 elapsed, loss: 1.4724194e-06\n",
      "step: 25540 train: 0.08021283149719238 elapsed, loss: 1.4733507e-06\n",
      "step: 25550 train: 0.08032369613647461 elapsed, loss: 1.9101399e-06\n",
      "step: 25560 train: 0.07961392402648926 elapsed, loss: 1.53994e-06\n",
      "step: 25570 train: 0.08251261711120605 elapsed, loss: 1.3005908e-06\n",
      "step: 25580 train: 0.08282661437988281 elapsed, loss: 1.3406375e-06\n",
      "step: 25590 train: 0.0762166976928711 elapsed, loss: 1.8547255e-06\n",
      "step: 25600 train: 0.08048796653747559 elapsed, loss: 1.4258528e-06\n",
      "step: 25610 train: 0.08003497123718262 elapsed, loss: 1.8412086e-06\n",
      "step: 25620 train: 0.07690548896789551 elapsed, loss: 1.9478584e-06\n",
      "step: 25630 train: 0.07956123352050781 elapsed, loss: 1.5161914e-06\n",
      "step: 25640 train: 0.07630777359008789 elapsed, loss: 2.808863e-06\n",
      "step: 25650 train: 0.07736992835998535 elapsed, loss: 1.6223623e-06\n",
      "step: 25660 train: 0.08130764961242676 elapsed, loss: 1.7699764e-06\n",
      "step: 25670 train: 0.08325648307800293 elapsed, loss: 1.3438971e-06\n",
      "step: 25680 train: 0.07656598091125488 elapsed, loss: 1.4715643e-05\n",
      "step: 25690 train: 0.07877683639526367 elapsed, loss: 0.001786168\n",
      "step: 25700 train: 0.07597541809082031 elapsed, loss: 3.822876e-05\n",
      "step: 25710 train: 0.07625937461853027 elapsed, loss: 4.9555365e-05\n",
      "step: 25720 train: 0.0780637264251709 elapsed, loss: 2.0397254e-05\n",
      "step: 25730 train: 0.07963275909423828 elapsed, loss: 1.2018047e-05\n",
      "step: 25740 train: 0.07653522491455078 elapsed, loss: 1.300695e-05\n",
      "step: 25750 train: 0.0763406753540039 elapsed, loss: 1.2631713e-05\n",
      "step: 25760 train: 0.08352041244506836 elapsed, loss: 5.83469e-06\n",
      "step: 25770 train: 0.0764768123626709 elapsed, loss: 8.416214e-06\n",
      "step: 25780 train: 0.07540512084960938 elapsed, loss: 5.86171e-06\n",
      "step: 25790 train: 0.07961082458496094 elapsed, loss: 5.2798914e-06\n",
      "step: 25800 train: 0.07910490036010742 elapsed, loss: 3.1324923e-06\n",
      "step: 25810 train: 0.08389878273010254 elapsed, loss: 2.9527093e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25820 train: 0.07882094383239746 elapsed, loss: 2.812567e-06\n",
      "step: 25830 train: 0.08074665069580078 elapsed, loss: 2.1518167e-06\n",
      "step: 25840 train: 0.0855255126953125 elapsed, loss: 6.0872862e-06\n",
      "step: 25850 train: 0.08377838134765625 elapsed, loss: 5.655846e-06\n",
      "step: 25860 train: 0.07981157302856445 elapsed, loss: 3.2307291e-06\n",
      "step: 25870 train: 0.08509302139282227 elapsed, loss: 1.6735845e-06\n",
      "step: 25880 train: 0.08701419830322266 elapsed, loss: 1.6447134e-06\n",
      "step: 25890 train: 0.07650208473205566 elapsed, loss: 3.1938816e-06\n",
      "step: 25900 train: 0.08229780197143555 elapsed, loss: 1.9879049e-06\n",
      "step: 25910 train: 0.08046269416809082 elapsed, loss: 1.6642715e-06\n",
      "step: 25920 train: 0.07622838020324707 elapsed, loss: 1.810954e-06\n",
      "step: 25930 train: 0.08086085319519043 elapsed, loss: 1.3774247e-06\n",
      "step: 25940 train: 0.08027338981628418 elapsed, loss: 1.2963999e-06\n",
      "step: 25950 train: 0.08053827285766602 elapsed, loss: 1.3136291e-06\n",
      "step: 25960 train: 0.07739043235778809 elapsed, loss: 2.0079276e-06\n",
      "step: 25970 train: 0.07629919052124023 elapsed, loss: 1.6586837e-06\n",
      "step: 25980 train: 0.07967066764831543 elapsed, loss: 1.6638057e-06\n",
      "step: 25990 train: 0.07687568664550781 elapsed, loss: 1.7178211e-06\n",
      "step: 26000 train: 0.07989358901977539 elapsed, loss: 2.6165458e-06\n",
      "step: 26010 train: 0.07669734954833984 elapsed, loss: 2.4340075e-06\n",
      "step: 26020 train: 0.07780170440673828 elapsed, loss: 2.5965228e-06\n",
      "step: 26030 train: 0.08475184440612793 elapsed, loss: 1.3820813e-06\n",
      "step: 26040 train: 0.08099699020385742 elapsed, loss: 1.4081581e-06\n",
      "step: 26050 train: 0.0769808292388916 elapsed, loss: 2.1094418e-06\n",
      "step: 26060 train: 0.08010411262512207 elapsed, loss: 1.968346e-06\n",
      "step: 26070 train: 0.07693791389465332 elapsed, loss: 2.2570562e-06\n",
      "step: 26080 train: 0.0778653621673584 elapsed, loss: 2.1997798e-06\n",
      "step: 26090 train: 0.0790104866027832 elapsed, loss: 1.8482065e-06\n",
      "step: 26100 train: 0.07947397232055664 elapsed, loss: 9.566193e-05\n",
      "step: 26110 train: 0.07961249351501465 elapsed, loss: 0.0004428848\n",
      "step: 26120 train: 0.07626557350158691 elapsed, loss: 3.7012094e-05\n",
      "step: 26130 train: 0.08004212379455566 elapsed, loss: 2.7438122e-05\n",
      "step: 26140 train: 0.07983851432800293 elapsed, loss: 1.3631337e-05\n",
      "step: 26150 train: 0.0714254379272461 elapsed, loss: 2.2922679e-05\n",
      "step: 26160 train: 0.07928109169006348 elapsed, loss: 1.7418075e-05\n",
      "step: 26170 train: 0.0806269645690918 elapsed, loss: 7.4998156e-06\n",
      "step: 26180 train: 0.0816645622253418 elapsed, loss: 7.2181188e-06\n",
      "step: 26190 train: 0.07644176483154297 elapsed, loss: 5.2647347e-06\n",
      "step: 26200 train: 0.07561087608337402 elapsed, loss: 4.457752e-06\n",
      "step: 26210 train: 0.07616257667541504 elapsed, loss: 4.3939344e-06\n",
      "step: 26220 train: 0.07895565032958984 elapsed, loss: 2.7171272e-06\n",
      "step: 26230 train: 0.07494592666625977 elapsed, loss: 4.3739374e-06\n",
      "step: 26240 train: 0.08741998672485352 elapsed, loss: 1.9785916e-06\n",
      "step: 26250 train: 0.08076357841491699 elapsed, loss: 1.9958213e-06\n",
      "step: 26260 train: 0.07573843002319336 elapsed, loss: 1.9296974e-06\n",
      "step: 26270 train: 0.07507038116455078 elapsed, loss: 1.8007099e-06\n",
      "step: 26280 train: 0.07673811912536621 elapsed, loss: 1.5627575e-06\n",
      "step: 26290 train: 0.07558035850524902 elapsed, loss: 3.2032601e-06\n",
      "step: 26300 train: 0.07631897926330566 elapsed, loss: 1.886857e-06\n",
      "step: 26310 train: 0.07959675788879395 elapsed, loss: 1.3490192e-06\n",
      "step: 26320 train: 0.08027124404907227 elapsed, loss: 1.5846422e-06\n",
      "step: 26330 train: 0.07603096961975098 elapsed, loss: 2.4586855e-06\n",
      "step: 26340 train: 0.07666134834289551 elapsed, loss: 1.7299292e-06\n",
      "step: 26350 train: 0.08150506019592285 elapsed, loss: 1.6051325e-06\n",
      "step: 26360 train: 0.08481907844543457 elapsed, loss: 1.5306271e-06\n",
      "step: 26370 train: 0.0790395736694336 elapsed, loss: 2.207231e-06\n",
      "step: 26380 train: 0.07629179954528809 elapsed, loss: 2.8088612e-06\n",
      "step: 26390 train: 0.08054018020629883 elapsed, loss: 2.1611304e-06\n",
      "step: 26400 train: 0.07818174362182617 elapsed, loss: 0.00089947076\n",
      "step: 26410 train: 0.08323812484741211 elapsed, loss: 0.0058248313\n",
      "step: 26420 train: 0.08441734313964844 elapsed, loss: 0.00030237035\n",
      "step: 26430 train: 0.07626676559448242 elapsed, loss: 8.342457e-05\n",
      "step: 26440 train: 0.07965826988220215 elapsed, loss: 0.0001557756\n",
      "step: 26450 train: 0.07699751853942871 elapsed, loss: 1.7374594e-05\n",
      "step: 26460 train: 0.07952570915222168 elapsed, loss: 1.4153886e-05\n",
      "step: 26470 train: 0.08089447021484375 elapsed, loss: 1.2920752e-05\n",
      "step: 26480 train: 0.07528138160705566 elapsed, loss: 1.4012385e-05\n",
      "step: 26490 train: 0.08060979843139648 elapsed, loss: 6.2239233e-06\n",
      "step: 26500 train: 0.0760030746459961 elapsed, loss: 7.988335e-06\n",
      "step: 26510 train: 0.07851409912109375 elapsed, loss: 4.5634515e-06\n",
      "step: 26520 train: 0.07952332496643066 elapsed, loss: 4.7701915e-06\n",
      "step: 26530 train: 0.08646774291992188 elapsed, loss: 2.6863927e-06\n",
      "step: 26540 train: 0.07985782623291016 elapsed, loss: 2.2859272e-06\n",
      "step: 26550 train: 0.07465219497680664 elapsed, loss: 4.4474777e-06\n",
      "step: 26560 train: 0.08186793327331543 elapsed, loss: 3.277315e-06\n",
      "step: 26570 train: 0.07949972152709961 elapsed, loss: 4.0340065e-06\n",
      "step: 26580 train: 0.08382368087768555 elapsed, loss: 2.0102564e-06\n",
      "step: 26590 train: 0.08010315895080566 elapsed, loss: 1.7527466e-06\n",
      "step: 26600 train: 0.08474254608154297 elapsed, loss: 1.2502992e-06\n",
      "step: 26610 train: 0.08060693740844727 elapsed, loss: 1.176725e-06\n",
      "step: 26620 train: 0.08025765419006348 elapsed, loss: 1.0821959e-06\n",
      "step: 26630 train: 0.07872915267944336 elapsed, loss: 1.5492514e-06\n",
      "step: 26640 train: 0.07612752914428711 elapsed, loss: 1.682432e-06\n",
      "step: 26650 train: 0.07664227485656738 elapsed, loss: 1.3848751e-06\n",
      "step: 26660 train: 0.07822251319885254 elapsed, loss: 1.4235248e-06\n",
      "step: 26670 train: 0.07634997367858887 elapsed, loss: 1.8496044e-06\n",
      "step: 26680 train: 0.0800027847290039 elapsed, loss: 1.361592e-06\n",
      "step: 26690 train: 0.07246947288513184 elapsed, loss: 2.3762504e-06\n",
      "step: 26700 train: 0.08412694931030273 elapsed, loss: 1.5068779e-06\n",
      "step: 26710 train: 0.08368253707885742 elapsed, loss: 1.4794036e-06\n",
      "step: 26720 train: 0.07892107963562012 elapsed, loss: 1.3061788e-06\n",
      "step: 26730 train: 0.0794229507446289 elapsed, loss: 1.3089726e-06\n",
      "step: 26740 train: 0.07933926582336426 elapsed, loss: 1.7057152e-06\n",
      "step: 26750 train: 0.0805351734161377 elapsed, loss: 1.1958172e-06\n",
      "step: 26760 train: 0.07610774040222168 elapsed, loss: 2.0093253e-06\n",
      "step: 26770 train: 0.07953500747680664 elapsed, loss: 5.1356487e-06\n",
      "step: 26780 train: 0.07567000389099121 elapsed, loss: 3.0281844e-06\n",
      "step: 26790 train: 0.08252334594726562 elapsed, loss: 1.7276013e-06\n",
      "step: 26800 train: 0.08124065399169922 elapsed, loss: 1.2582154e-06\n",
      "step: 26810 train: 0.07660508155822754 elapsed, loss: 1.7164253e-06\n",
      "step: 26820 train: 0.07653403282165527 elapsed, loss: 2.5457637e-06\n",
      "step: 26830 train: 0.07903861999511719 elapsed, loss: 1.9865083e-06\n",
      "step: 26840 train: 0.08752632141113281 elapsed, loss: 1.8360988e-06\n",
      "step: 26850 train: 0.07617783546447754 elapsed, loss: 2.5979198e-06\n",
      "step: 26860 train: 0.08370184898376465 elapsed, loss: 1.6572862e-06\n",
      "step: 26870 train: 0.0874021053314209 elapsed, loss: 1.4351665e-06\n",
      "step: 26880 train: 0.07841253280639648 elapsed, loss: 2.9383168e-06\n",
      "step: 26890 train: 0.07926273345947266 elapsed, loss: 0.0001567449\n",
      "step: 26900 train: 0.07651233673095703 elapsed, loss: 3.9042727e-05\n",
      "step: 26910 train: 0.08326601982116699 elapsed, loss: 1.6879201e-05\n",
      "step: 26920 train: 0.08088088035583496 elapsed, loss: 2.248829e-05\n",
      "step: 26930 train: 0.07695341110229492 elapsed, loss: 1.818252e-05\n",
      "step: 26940 train: 0.08479547500610352 elapsed, loss: 9.902584e-06\n",
      "step: 26950 train: 0.07933735847473145 elapsed, loss: 1.2153132e-05\n",
      "step: 26960 train: 0.07637310028076172 elapsed, loss: 1.0261558e-05\n",
      "step: 26970 train: 0.08038735389709473 elapsed, loss: 6.443308e-06\n",
      "step: 26980 train: 0.07656311988830566 elapsed, loss: 7.246552e-06\n",
      "step: 26990 train: 0.07764840126037598 elapsed, loss: 5.3718304e-06\n",
      "step: 27000 train: 0.07925963401794434 elapsed, loss: 5.8551946e-06\n",
      "step: 27010 train: 0.07996916770935059 elapsed, loss: 2.6267899e-06\n",
      "step: 27020 train: 0.08149886131286621 elapsed, loss: 3.908735e-06\n",
      "step: 27030 train: 0.07786989212036133 elapsed, loss: 2.843763e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27040 train: 0.08012247085571289 elapsed, loss: 1.7252725e-06\n",
      "step: 27050 train: 0.08387303352355957 elapsed, loss: 1.6014069e-06\n",
      "step: 27060 train: 0.07979059219360352 elapsed, loss: 1.4901143e-06\n",
      "step: 27070 train: 0.07611703872680664 elapsed, loss: 2.0135153e-06\n",
      "step: 27080 train: 0.07654953002929688 elapsed, loss: 1.9180554e-06\n",
      "step: 27090 train: 0.07676935195922852 elapsed, loss: 2.3073474e-06\n",
      "step: 27100 train: 0.0758979320526123 elapsed, loss: 2.267754e-06\n",
      "step: 27110 train: 0.07537364959716797 elapsed, loss: 3.044021e-06\n",
      "step: 27120 train: 0.07623052597045898 elapsed, loss: 2.178825e-06\n",
      "step: 27130 train: 0.07982897758483887 elapsed, loss: 1.6842841e-06\n",
      "step: 27140 train: 0.07975411415100098 elapsed, loss: 1.4225939e-06\n",
      "step: 27150 train: 0.07544517517089844 elapsed, loss: 2.9676526e-06\n",
      "step: 27160 train: 0.08008766174316406 elapsed, loss: 0.0049314788\n",
      "step: 27170 train: 0.0874941349029541 elapsed, loss: 0.00012717681\n",
      "step: 27180 train: 0.08376836776733398 elapsed, loss: 2.3553677e-05\n",
      "step: 27190 train: 0.07985663414001465 elapsed, loss: 0.00011425787\n",
      "step: 27200 train: 0.07942724227905273 elapsed, loss: 2.886815e-05\n",
      "step: 27210 train: 0.07624244689941406 elapsed, loss: 2.0113124e-05\n",
      "step: 27220 train: 0.07607221603393555 elapsed, loss: 1.5242131e-05\n",
      "step: 27230 train: 0.07650160789489746 elapsed, loss: 2.0881307e-05\n",
      "step: 27240 train: 0.0769956111907959 elapsed, loss: 8.559569e-05\n",
      "step: 27250 train: 0.07458996772766113 elapsed, loss: 9.767564e-06\n",
      "step: 27260 train: 0.081787109375 elapsed, loss: 5.4877833e-06\n",
      "step: 27270 train: 0.07991147041320801 elapsed, loss: 2.6626437e-06\n",
      "step: 27280 train: 0.07684731483459473 elapsed, loss: 2.9220146e-06\n",
      "step: 27290 train: 0.07679200172424316 elapsed, loss: 2.8987338e-06\n",
      "step: 27300 train: 0.07984256744384766 elapsed, loss: 1.725738e-06\n",
      "step: 27310 train: 0.07689905166625977 elapsed, loss: 1.6521624e-06\n",
      "step: 27320 train: 0.07190966606140137 elapsed, loss: 3.6745232e-06\n",
      "step: 27330 train: 0.07895112037658691 elapsed, loss: 1.6787062e-06\n",
      "step: 27340 train: 0.08101010322570801 elapsed, loss: 1.1008224e-06\n",
      "step: 27350 train: 0.07600116729736328 elapsed, loss: 1.8551918e-06\n",
      "step: 27360 train: 0.07381200790405273 elapsed, loss: 2.7590363e-06\n",
      "step: 27370 train: 0.07267498970031738 elapsed, loss: 1.7401741e-06\n",
      "step: 27380 train: 0.08195257186889648 elapsed, loss: 1.109204e-06\n",
      "step: 27390 train: 0.08149862289428711 elapsed, loss: 1.1106013e-06\n",
      "step: 27400 train: 0.08105731010437012 elapsed, loss: 1.5194507e-06\n",
      "step: 27410 train: 0.07688307762145996 elapsed, loss: 1.6321399e-06\n",
      "step: 27420 train: 0.07575511932373047 elapsed, loss: 1.4333041e-06\n",
      "step: 27430 train: 0.07274699211120605 elapsed, loss: 2.771605e-06\n",
      "step: 27440 train: 0.07833647727966309 elapsed, loss: 1.3862723e-06\n",
      "step: 27450 train: 0.07901263236999512 elapsed, loss: 1.2814983e-06\n",
      "step: 27460 train: 0.07971572875976562 elapsed, loss: 1.5003591e-06\n",
      "step: 27470 train: 0.07521677017211914 elapsed, loss: 1.9548434e-06\n",
      "step: 27480 train: 0.08025979995727539 elapsed, loss: 1.5385431e-06\n",
      "step: 27490 train: 0.07947850227355957 elapsed, loss: 1.3955854e-06\n",
      "step: 27500 train: 0.0765995979309082 elapsed, loss: 2.1383055e-06\n",
      "step: 27510 train: 0.08002877235412598 elapsed, loss: 0.03495092\n",
      "step: 27520 train: 0.0792851448059082 elapsed, loss: 0.0003185416\n",
      "step: 27530 train: 0.07641339302062988 elapsed, loss: 0.0014183416\n",
      "step: 27540 train: 0.07635283470153809 elapsed, loss: 8.247273e-05\n",
      "step: 27550 train: 0.08080816268920898 elapsed, loss: 3.0035677e-05\n",
      "step: 27560 train: 0.08008337020874023 elapsed, loss: 4.3735305e-05\n",
      "step: 27570 train: 0.0801396369934082 elapsed, loss: 4.174688e-05\n",
      "step: 27580 train: 0.08000779151916504 elapsed, loss: 1.49956995e-05\n",
      "step: 27590 train: 0.08197522163391113 elapsed, loss: 9.897372e-06\n",
      "step: 27600 train: 0.08357763290405273 elapsed, loss: 6.8321033e-06\n",
      "step: 27610 train: 0.08149313926696777 elapsed, loss: 8.019542e-06\n",
      "step: 27620 train: 0.08012700080871582 elapsed, loss: 4.3012938e-06\n",
      "step: 27630 train: 0.0837852954864502 elapsed, loss: 7.7480145e-06\n",
      "step: 27640 train: 0.08321762084960938 elapsed, loss: 3.137614e-06\n",
      "step: 27650 train: 0.07950711250305176 elapsed, loss: 2.8088505e-06\n",
      "step: 27660 train: 0.08085012435913086 elapsed, loss: 3.4281873e-06\n",
      "step: 27670 train: 0.07704043388366699 elapsed, loss: 2.435869e-06\n",
      "step: 27680 train: 0.0767984390258789 elapsed, loss: 3.2689154e-06\n",
      "step: 27690 train: 0.07587313652038574 elapsed, loss: 2.4740527e-06\n",
      "step: 27700 train: 0.07907819747924805 elapsed, loss: 1.5334201e-06\n",
      "step: 27710 train: 0.07622098922729492 elapsed, loss: 2.2305117e-06\n",
      "step: 27720 train: 0.07601284980773926 elapsed, loss: 1.5320237e-06\n",
      "step: 27730 train: 0.07883596420288086 elapsed, loss: 1.1348156e-06\n",
      "step: 27740 train: 0.07899713516235352 elapsed, loss: 1.4617093e-06\n",
      "step: 27750 train: 0.08328008651733398 elapsed, loss: 1.041218e-06\n",
      "step: 27760 train: 0.07964801788330078 elapsed, loss: 1.6940737e-06\n",
      "step: 27770 train: 0.0755012035369873 elapsed, loss: 1.5497192e-06\n",
      "step: 27780 train: 0.07718491554260254 elapsed, loss: 2.1862752e-06\n",
      "step: 27790 train: 0.07143187522888184 elapsed, loss: 4.4939552e-05\n",
      "step: 27800 train: 0.08022809028625488 elapsed, loss: 2.7843445e-05\n",
      "step: 27810 train: 0.07625412940979004 elapsed, loss: 4.285924e-06\n",
      "step: 27820 train: 0.07599830627441406 elapsed, loss: 3.935228e-06\n",
      "step: 27830 train: 0.08032965660095215 elapsed, loss: 3.1809213e-06\n",
      "step: 27840 train: 0.08124732971191406 elapsed, loss: 1.4356322e-06\n",
      "step: 27850 train: 0.07686257362365723 elapsed, loss: 2.3366838e-06\n",
      "step: 27860 train: 0.08153462409973145 elapsed, loss: 1.6121171e-06\n",
      "step: 27870 train: 0.0842742919921875 elapsed, loss: 1.3704389e-06\n",
      "step: 27880 train: 0.08019375801086426 elapsed, loss: 1.15101e-05\n",
      "step: 27890 train: 0.08381199836730957 elapsed, loss: 2.1187548e-06\n",
      "step: 27900 train: 0.08573651313781738 elapsed, loss: 1.9106042e-06\n",
      "step: 27910 train: 0.08621597290039062 elapsed, loss: 2.0284165e-06\n",
      "step: 27920 train: 0.08187675476074219 elapsed, loss: 1.5138626e-06\n",
      "step: 27930 train: 0.0796499252319336 elapsed, loss: 1.8938393e-06\n",
      "step: 27940 train: 0.07600235939025879 elapsed, loss: 1.8100229e-06\n",
      "step: 27950 train: 0.0860910415649414 elapsed, loss: 1.2605437e-06\n",
      "step: 27960 train: 0.08471536636352539 elapsed, loss: 1.8840618e-06\n",
      "step: 27970 train: 0.08352994918823242 elapsed, loss: 1.2768421e-06\n",
      "step: 27980 train: 0.0745229721069336 elapsed, loss: 1.8640385e-06\n",
      "step: 27990 train: 0.07793617248535156 elapsed, loss: 1.2028017e-06\n",
      "step: 28000 train: 0.07986140251159668 elapsed, loss: 1.2223595e-06\n",
      "step: 28010 train: 0.0788581371307373 elapsed, loss: 1.1674119e-06\n",
      "step: 28020 train: 0.08270025253295898 elapsed, loss: 1.1441286e-06\n",
      "step: 28030 train: 0.0807793140411377 elapsed, loss: 1.1199145e-06\n",
      "step: 28040 train: 0.08056497573852539 elapsed, loss: 5.7261685e-05\n",
      "step: 28050 train: 0.07604598999023438 elapsed, loss: 5.1949784e-05\n",
      "step: 28060 train: 0.07982754707336426 elapsed, loss: 2.33992e-05\n",
      "step: 28070 train: 0.07655477523803711 elapsed, loss: 2.0361185e-05\n",
      "step: 28080 train: 0.07941961288452148 elapsed, loss: 1.1974262e-05\n",
      "step: 28090 train: 0.07736420631408691 elapsed, loss: 1.3701502e-05\n",
      "step: 28100 train: 0.08332276344299316 elapsed, loss: 1.1353564e-05\n",
      "step: 28110 train: 0.07591891288757324 elapsed, loss: 1.2816847e-05\n",
      "step: 28120 train: 0.07522416114807129 elapsed, loss: 8.704864e-06\n",
      "step: 28130 train: 0.07990860939025879 elapsed, loss: 4.0000136e-06\n",
      "step: 28140 train: 0.08057117462158203 elapsed, loss: 4.382782e-06\n",
      "step: 28150 train: 0.07596778869628906 elapsed, loss: 3.558107e-06\n",
      "step: 28160 train: 0.08515691757202148 elapsed, loss: 3.3256933e-06\n",
      "step: 28170 train: 0.07928705215454102 elapsed, loss: 2.6072287e-06\n",
      "step: 28180 train: 0.07494711875915527 elapsed, loss: 3.2121227e-06\n",
      "step: 28190 train: 0.08370518684387207 elapsed, loss: 1.4859236e-06\n",
      "step: 28200 train: 0.0766746997833252 elapsed, loss: 2.7464641e-06\n",
      "step: 28210 train: 0.07376360893249512 elapsed, loss: 1.9031552e-06\n",
      "step: 28220 train: 0.07625913619995117 elapsed, loss: 1.9506517e-06\n",
      "step: 28230 train: 0.07734274864196777 elapsed, loss: 1.9357512e-06\n",
      "step: 28240 train: 0.08050274848937988 elapsed, loss: 1.1897637e-06\n",
      "step: 28250 train: 0.07768464088439941 elapsed, loss: 1.626551e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28260 train: 0.07706141471862793 elapsed, loss: 1.747159e-06\n",
      "step: 28270 train: 0.07875990867614746 elapsed, loss: 1.5292297e-06\n",
      "step: 28280 train: 0.07959437370300293 elapsed, loss: 1.7443618e-06\n",
      "step: 28290 train: 0.07950186729431152 elapsed, loss: 1.6596143e-06\n",
      "step: 28300 train: 0.07564544677734375 elapsed, loss: 1.9283007e-06\n",
      "step: 28310 train: 0.07830548286437988 elapsed, loss: 1.269856e-06\n",
      "step: 28320 train: 0.0787045955657959 elapsed, loss: 1.4477396e-06\n",
      "step: 28330 train: 0.07576394081115723 elapsed, loss: 1.9241097e-06\n",
      "step: 28340 train: 0.07670331001281738 elapsed, loss: 2.3040884e-06\n",
      "step: 28350 train: 0.07235145568847656 elapsed, loss: 2.1513508e-06\n",
      "step: 28360 train: 0.08168983459472656 elapsed, loss: 1.700593e-06\n",
      "step: 28370 train: 0.07560896873474121 elapsed, loss: 2.1974517e-06\n",
      "step: 28380 train: 0.0774545669555664 elapsed, loss: 1.4975647e-06\n",
      "step: 28390 train: 0.08101749420166016 elapsed, loss: 1.523642e-06\n",
      "step: 28400 train: 0.07850050926208496 elapsed, loss: 2.6929138e-06\n",
      "step: 28410 train: 0.07656502723693848 elapsed, loss: 0.0013175991\n",
      "step: 28420 train: 0.08414912223815918 elapsed, loss: 0.01403258\n",
      "step: 28430 train: 0.08123040199279785 elapsed, loss: 0.00021146599\n",
      "step: 28440 train: 0.0761575698852539 elapsed, loss: 5.3515745e-05\n",
      "step: 28450 train: 0.07812809944152832 elapsed, loss: 0.00057263375\n",
      "step: 28460 train: 0.0752265453338623 elapsed, loss: 7.614422e-05\n",
      "step: 28470 train: 0.07803153991699219 elapsed, loss: 2.8637163e-05\n",
      "step: 28480 train: 0.07677149772644043 elapsed, loss: 1.5006408e-05\n",
      "step: 28490 train: 0.08003735542297363 elapsed, loss: 1.3634948e-05\n",
      "step: 28500 train: 0.0757758617401123 elapsed, loss: 1.2731896e-05\n",
      "step: 28510 train: 0.07693052291870117 elapsed, loss: 8.571302e-06\n",
      "step: 28520 train: 0.08180379867553711 elapsed, loss: 6.3944126e-06\n",
      "step: 28530 train: 0.0761864185333252 elapsed, loss: 6.3776506e-06\n",
      "step: 28540 train: 0.07695531845092773 elapsed, loss: 4.380925e-06\n",
      "step: 28550 train: 0.07594966888427734 elapsed, loss: 5.4831335e-06\n",
      "step: 28560 train: 0.08010125160217285 elapsed, loss: 2.5024588e-06\n",
      "step: 28570 train: 0.07935404777526855 elapsed, loss: 3.049141e-06\n",
      "step: 28580 train: 0.07538247108459473 elapsed, loss: 3.4971054e-06\n",
      "step: 28590 train: 0.08062005043029785 elapsed, loss: 2.865207e-06\n",
      "step: 28600 train: 0.07703614234924316 elapsed, loss: 1.905483e-06\n",
      "step: 28610 train: 0.07632756233215332 elapsed, loss: 1.833306e-06\n",
      "step: 28620 train: 0.0799098014831543 elapsed, loss: 2.2151444e-06\n",
      "step: 28630 train: 0.07694411277770996 elapsed, loss: 1.6982646e-06\n",
      "step: 28640 train: 0.0791482925415039 elapsed, loss: 1.8142136e-06\n",
      "step: 28650 train: 0.07950592041015625 elapsed, loss: 1.631674e-06\n",
      "step: 28660 train: 0.0756680965423584 elapsed, loss: 2.0200357e-06\n",
      "step: 28670 train: 0.0799262523651123 elapsed, loss: 1.3117665e-06\n",
      "step: 28680 train: 0.08005857467651367 elapsed, loss: 1.2191001e-06\n",
      "step: 28690 train: 0.07658004760742188 elapsed, loss: 1.4966337e-06\n",
      "step: 28700 train: 0.07802248001098633 elapsed, loss: 1.7196824e-06\n",
      "step: 28710 train: 0.07996439933776855 elapsed, loss: 1.3527444e-06\n",
      "step: 28720 train: 0.08007550239562988 elapsed, loss: 1.3061788e-06\n",
      "step: 28730 train: 0.07707715034484863 elapsed, loss: 1.7271354e-06\n",
      "step: 28740 train: 0.0753791332244873 elapsed, loss: 3.073355e-06\n",
      "step: 28750 train: 0.07608222961425781 elapsed, loss: 2.1718406e-06\n",
      "step: 28760 train: 0.07901740074157715 elapsed, loss: 1.3150253e-06\n",
      "step: 28770 train: 0.07480478286743164 elapsed, loss: 1.9781264e-06\n",
      "step: 28780 train: 0.08028268814086914 elapsed, loss: 1.2624066e-06\n",
      "step: 28790 train: 0.07339763641357422 elapsed, loss: 3.1278396e-06\n",
      "step: 28800 train: 0.08088135719299316 elapsed, loss: 1.532024e-06\n",
      "step: 28810 train: 0.07709074020385742 elapsed, loss: 2.1378478e-06\n",
      "step: 28820 train: 0.07865381240844727 elapsed, loss: 1.5487877e-06\n",
      "step: 28830 train: 0.07925033569335938 elapsed, loss: 1.7438992e-06\n",
      "step: 28840 train: 0.08557295799255371 elapsed, loss: 1.5939567e-06\n",
      "step: 28850 train: 0.08157086372375488 elapsed, loss: 1.5390088e-06\n",
      "step: 28860 train: 0.08194470405578613 elapsed, loss: 9.9416575e-06\n",
      "step: 28870 train: 0.08595061302185059 elapsed, loss: 2.4340068e-06\n",
      "step: 28880 train: 0.08195328712463379 elapsed, loss: 1.5189851e-06\n",
      "step: 28890 train: 0.08346366882324219 elapsed, loss: 1.534818e-06\n",
      "step: 28900 train: 0.08241033554077148 elapsed, loss: 1.96695e-06\n",
      "step: 28910 train: 0.08397102355957031 elapsed, loss: 1.781617e-06\n",
      "step: 28920 train: 0.08669304847717285 elapsed, loss: 1.4188686e-06\n",
      "step: 28930 train: 0.07912278175354004 elapsed, loss: 2.734823e-06\n",
      "step: 28940 train: 0.08339762687683105 elapsed, loss: 1.5618259e-06\n",
      "step: 28950 train: 0.08452939987182617 elapsed, loss: 1.4095551e-06\n",
      "step: 28960 train: 0.08495330810546875 elapsed, loss: 1.8975666e-06\n",
      "step: 28970 train: 0.08547163009643555 elapsed, loss: 1.5934907e-06\n",
      "step: 28980 train: 0.0817110538482666 elapsed, loss: 2.6556602e-06\n",
      "step: 28990 train: 0.08341765403747559 elapsed, loss: 1.4714881e-06\n",
      "step: 29000 train: 0.08248114585876465 elapsed, loss: 2.809315e-06\n",
      "step: 29010 train: 0.07904243469238281 elapsed, loss: 0.0014457116\n",
      "step: 29020 train: 0.07875490188598633 elapsed, loss: 3.4967397e-05\n",
      "step: 29030 train: 0.08681750297546387 elapsed, loss: 0.00010420126\n",
      "step: 29040 train: 0.08487772941589355 elapsed, loss: 4.0491246e-05\n",
      "step: 29050 train: 0.08049821853637695 elapsed, loss: 0.00031105714\n",
      "step: 29060 train: 0.08105158805847168 elapsed, loss: 0.00015460188\n",
      "step: 29070 train: 0.08782005310058594 elapsed, loss: 5.544327e-05\n",
      "step: 29080 train: 0.07756948471069336 elapsed, loss: 5.136723e-05\n",
      "step: 29090 train: 0.0779118537902832 elapsed, loss: 2.4880756e-05\n",
      "step: 29100 train: 0.08275413513183594 elapsed, loss: 2.0948368e-05\n",
      "step: 29110 train: 0.0827329158782959 elapsed, loss: 1.3129664e-05\n",
      "step: 29120 train: 0.0827951431274414 elapsed, loss: 9.65811e-06\n",
      "step: 29130 train: 0.0773320198059082 elapsed, loss: 1.2555907e-05\n",
      "step: 29140 train: 0.08499670028686523 elapsed, loss: 4.8623915e-06\n",
      "step: 29150 train: 0.08499765396118164 elapsed, loss: 3.980914e-06\n",
      "step: 29160 train: 0.08373403549194336 elapsed, loss: 6.578785e-06\n",
      "step: 29170 train: 0.08264398574829102 elapsed, loss: 2.3497223e-06\n",
      "step: 29180 train: 0.08302974700927734 elapsed, loss: 4.2491283e-06\n",
      "step: 29190 train: 0.08487868309020996 elapsed, loss: 1.6768438e-06\n",
      "step: 29200 train: 0.08377599716186523 elapsed, loss: 1.3485537e-06\n",
      "step: 29210 train: 0.08361196517944336 elapsed, loss: 1.3015217e-06\n",
      "step: 29220 train: 0.07927274703979492 elapsed, loss: 1.6568206e-06\n",
      "step: 29230 train: 0.0860452651977539 elapsed, loss: 1.6228275e-06\n",
      "step: 29240 train: 0.08243703842163086 elapsed, loss: 1.2638036e-06\n",
      "step: 29250 train: 0.0802907943725586 elapsed, loss: 2.1234125e-06\n",
      "step: 29260 train: 0.08223176002502441 elapsed, loss: 1.334115e-06\n",
      "step: 29270 train: 0.08151507377624512 elapsed, loss: 1.8523746e-06\n",
      "step: 29280 train: 0.08212733268737793 elapsed, loss: 2.0121195e-06\n",
      "step: 29290 train: 0.07943034172058105 elapsed, loss: 1.7243417e-06\n",
      "step: 29300 train: 0.07852482795715332 elapsed, loss: 1.9506508e-06\n",
      "step: 29310 train: 0.07870078086853027 elapsed, loss: 7.1049594e-06\n",
      "step: 29320 train: 0.08533167839050293 elapsed, loss: 1.2018706e-06\n",
      "step: 29330 train: 0.0847475528717041 elapsed, loss: 1.5334203e-06\n",
      "step: 29340 train: 0.08320331573486328 elapsed, loss: 1.1390064e-06\n",
      "step: 29350 train: 0.0831298828125 elapsed, loss: 1.2689256e-06\n",
      "step: 29360 train: 0.08431673049926758 elapsed, loss: 1.2987281e-06\n",
      "step: 29370 train: 0.0804591178894043 elapsed, loss: 1.9501774e-06\n",
      "step: 29380 train: 0.08358883857727051 elapsed, loss: 1.2181689e-06\n",
      "step: 29390 train: 0.07728075981140137 elapsed, loss: 1.789067e-06\n",
      "step: 29400 train: 0.0803837776184082 elapsed, loss: 1.386738e-06\n",
      "step: 29410 train: 0.08106470108032227 elapsed, loss: 2.0600828e-06\n",
      "step: 29420 train: 0.08215832710266113 elapsed, loss: 1.3904631e-06\n",
      "step: 29430 train: 0.08155989646911621 elapsed, loss: 1.8714903e-06\n",
      "step: 29440 train: 0.09103250503540039 elapsed, loss: 2.6943103e-06\n",
      "step: 29450 train: 0.08422493934631348 elapsed, loss: 1.309904e-06\n",
      "step: 29460 train: 0.08042788505554199 elapsed, loss: 1.7266698e-06\n",
      "step: 29470 train: 0.08828210830688477 elapsed, loss: 1.2521621e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 29480 train: 0.08488869667053223 elapsed, loss: 1.2689256e-06\n",
      "step: 29490 train: 0.08477282524108887 elapsed, loss: 1.2395892e-06\n",
      "step: 29500 train: 0.08491969108581543 elapsed, loss: 1.3047818e-06\n",
      "step: 29510 train: 0.08241009712219238 elapsed, loss: 1.963691e-06\n",
      "step: 29520 train: 0.08801150321960449 elapsed, loss: 1.9362133e-06\n",
      "step: 29530 train: 0.0800924301147461 elapsed, loss: 1.7159599e-06\n",
      "step: 29540 train: 0.08208775520324707 elapsed, loss: 0.0016761434\n",
      "step: 29550 train: 0.08491873741149902 elapsed, loss: 0.000104152394\n",
      "step: 29560 train: 0.08349466323852539 elapsed, loss: 5.3521842e-05\n",
      "step: 29570 train: 0.07817840576171875 elapsed, loss: 4.7562175e-05\n",
      "step: 29580 train: 0.08185243606567383 elapsed, loss: 3.2824646e-05\n",
      "step: 29590 train: 0.08463644981384277 elapsed, loss: 2.012359e-05\n",
      "step: 29600 train: 0.08950471878051758 elapsed, loss: 1.5190039e-05\n",
      "step: 29610 train: 0.08579182624816895 elapsed, loss: 1.1091197e-05\n",
      "step: 29620 train: 0.08046293258666992 elapsed, loss: 1.1192662e-05\n",
      "step: 29630 train: 0.07977461814880371 elapsed, loss: 9.405074e-06\n",
      "step: 29640 train: 0.08206820487976074 elapsed, loss: 4.172307e-06\n",
      "step: 29650 train: 0.0845797061920166 elapsed, loss: 2.9867424e-06\n",
      "step: 29660 train: 0.08841824531555176 elapsed, loss: 2.9858097e-06\n",
      "step: 29670 train: 0.0851585865020752 elapsed, loss: 3.4658906e-06\n",
      "step: 29680 train: 0.08392119407653809 elapsed, loss: 2.088953e-06\n",
      "step: 29690 train: 0.08240675926208496 elapsed, loss: 2.0968694e-06\n",
      "step: 29700 train: 0.08582401275634766 elapsed, loss: 1.4090897e-06\n",
      "step: 29710 train: 0.07968807220458984 elapsed, loss: 3.709447e-06\n",
      "step: 29720 train: 0.08238935470581055 elapsed, loss: 1.6572866e-06\n",
      "step: 29730 train: 0.08098483085632324 elapsed, loss: 1.4323728e-06\n",
      "step: 29740 train: 0.08144688606262207 elapsed, loss: 1.9958206e-06\n",
      "step: 29750 train: 0.08141469955444336 elapsed, loss: 2.325508e-06\n",
      "step: 29760 train: 0.08125185966491699 elapsed, loss: 1.6977987e-06\n",
      "step: 29770 train: 0.08418655395507812 elapsed, loss: 1.3839435e-06\n",
      "step: 29780 train: 0.0825204849243164 elapsed, loss: 1.4873203e-06\n",
      "step: 29790 train: 0.08443069458007812 elapsed, loss: 1.3383075e-06\n",
      "step: 29800 train: 0.07565522193908691 elapsed, loss: 2.0931438e-06\n",
      "step: 29810 train: 0.08539605140686035 elapsed, loss: 1.6740494e-06\n",
      "step: 29820 train: 0.08651137351989746 elapsed, loss: 1.5394743e-06\n",
      "step: 29830 train: 0.0863044261932373 elapsed, loss: 1.4584496e-06\n",
      "step: 29840 train: 0.08427786827087402 elapsed, loss: 0.0003457048\n",
      "step: 29850 train: 0.08560013771057129 elapsed, loss: 1.6429436e-05\n",
      "step: 29860 train: 0.08146452903747559 elapsed, loss: 1.3488698e-05\n",
      "step: 29870 train: 0.07504129409790039 elapsed, loss: 1.5888869e-05\n",
      "step: 29880 train: 0.07725286483764648 elapsed, loss: 7.0891438e-06\n",
      "step: 29890 train: 0.08503961563110352 elapsed, loss: 4.2901156e-06\n",
      "step: 29900 train: 0.08239984512329102 elapsed, loss: 4.9150303e-06\n",
      "step: 29910 train: 0.08956313133239746 elapsed, loss: 3.2484334e-06\n",
      "step: 29920 train: 0.08267712593078613 elapsed, loss: 2.4470428e-06\n",
      "step: 29930 train: 0.08129167556762695 elapsed, loss: 2.373002e-06\n",
      "step: 29940 train: 0.08197593688964844 elapsed, loss: 2.0549596e-06\n",
      "step: 29950 train: 0.07922506332397461 elapsed, loss: 1.9608954e-06\n",
      "step: 29960 train: 0.08596205711364746 elapsed, loss: 1.4053642e-06\n",
      "step: 29970 train: 0.08031034469604492 elapsed, loss: 1.6698592e-06\n",
      "step: 29980 train: 0.08145380020141602 elapsed, loss: 1.3192168e-06\n",
      "step: 29990 train: 0.08335423469543457 elapsed, loss: 1.3089725e-06\n",
      "step: 30000 train: 0.07919478416442871 elapsed, loss: 2.3525154e-06\n",
      "step: 30010 train: 0.08350682258605957 elapsed, loss: 2.4642752e-06\n",
      "step: 30020 train: 0.07760190963745117 elapsed, loss: 1.1706713e-06\n",
      "step: 30030 train: 0.08247756958007812 elapsed, loss: 1.3657827e-06\n",
      "step: 30040 train: 0.0767512321472168 elapsed, loss: 2.1895357e-06\n",
      "step: 30050 train: 0.08140134811401367 elapsed, loss: 1.5986135e-06\n",
      "step: 30060 train: 0.08142638206481934 elapsed, loss: 2.4982678e-06\n",
      "step: 30070 train: 0.08177614212036133 elapsed, loss: 1.6530953e-06\n",
      "step: 30080 train: 0.08391499519348145 elapsed, loss: 1.7536769e-06\n",
      "step: 30090 train: 0.0861811637878418 elapsed, loss: 1.3089725e-06\n",
      "step: 30100 train: 0.08310127258300781 elapsed, loss: 1.6405229e-06\n",
      "step: 30110 train: 0.08463644981384277 elapsed, loss: 1.3057131e-06\n",
      "step: 30120 train: 0.0782771110534668 elapsed, loss: 2.4279539e-06\n",
      "step: 30130 train: 0.07995414733886719 elapsed, loss: 1.6665995e-06\n",
      "step: 30140 train: 0.08403611183166504 elapsed, loss: 1.1352813e-06\n",
      "step: 30150 train: 0.08530139923095703 elapsed, loss: 1.3015215e-06\n",
      "step: 30160 train: 0.0818026065826416 elapsed, loss: 1.6945389e-06\n",
      "step: 30170 train: 0.08271908760070801 elapsed, loss: 0.0028600078\n",
      "step: 30180 train: 0.08040809631347656 elapsed, loss: 0.00025352387\n",
      "step: 30190 train: 0.07938337326049805 elapsed, loss: 0.00051884854\n",
      "step: 30200 train: 0.08363509178161621 elapsed, loss: 5.9643724e-05\n",
      "step: 30210 train: 0.08325052261352539 elapsed, loss: 5.7147285e-05\n",
      "step: 30220 train: 0.08275341987609863 elapsed, loss: 3.387475e-05\n",
      "step: 30230 train: 0.0787649154663086 elapsed, loss: 4.5417113e-05\n",
      "step: 30240 train: 0.08184957504272461 elapsed, loss: 1.7921177e-05\n",
      "step: 30250 train: 0.07738757133483887 elapsed, loss: 2.4840723e-05\n",
      "step: 30260 train: 0.08455157279968262 elapsed, loss: 9.519266e-06\n",
      "step: 30270 train: 0.08870625495910645 elapsed, loss: 5.419777e-06\n",
      "step: 30280 train: 0.0821988582611084 elapsed, loss: 5.4528605e-06\n",
      "step: 30290 train: 0.0755312442779541 elapsed, loss: 6.6244584e-06\n",
      "step: 30300 train: 0.08106398582458496 elapsed, loss: 3.0551957e-06\n",
      "step: 30310 train: 0.07924723625183105 elapsed, loss: 2.4745193e-06\n",
      "step: 30320 train: 0.08708858489990234 elapsed, loss: 2.1876685e-06\n",
      "step: 30330 train: 0.0867307186126709 elapsed, loss: 1.774632e-06\n",
      "step: 30340 train: 0.07921910285949707 elapsed, loss: 3.9017636e-06\n",
      "step: 30350 train: 0.08538126945495605 elapsed, loss: 1.357867e-06\n",
      "step: 30360 train: 0.07867980003356934 elapsed, loss: 2.4139815e-06\n",
      "step: 30370 train: 0.0840914249420166 elapsed, loss: 1.1916256e-06\n",
      "step: 30380 train: 0.07919812202453613 elapsed, loss: 1.7294634e-06\n",
      "step: 30390 train: 0.07834196090698242 elapsed, loss: 3.4584516e-06\n",
      "step: 30400 train: 0.07955455780029297 elapsed, loss: 1.9404076e-06\n",
      "step: 30410 train: 0.08938074111938477 elapsed, loss: 1.1161892e-06\n",
      "step: 30420 train: 0.08490848541259766 elapsed, loss: 1.1678773e-06\n",
      "step: 30430 train: 0.08171963691711426 elapsed, loss: 1.198611e-06\n",
      "step: 30440 train: 0.08841323852539062 elapsed, loss: 1.2624052e-06\n",
      "step: 30450 train: 0.08357024192810059 elapsed, loss: 1.355073e-06\n",
      "step: 30460 train: 0.07923102378845215 elapsed, loss: 1.443083e-06\n",
      "step: 30470 train: 0.08407092094421387 elapsed, loss: 1.6759127e-06\n",
      "step: 30480 train: 0.08666539192199707 elapsed, loss: 1.2260851e-06\n",
      "step: 30490 train: 0.07940411567687988 elapsed, loss: 1.5767274e-06\n",
      "step: 30500 train: 0.08145928382873535 elapsed, loss: 1.3965168e-06\n",
      "step: 30510 train: 0.07904505729675293 elapsed, loss: 2.3855787e-06\n",
      "step: 30520 train: 0.08002233505249023 elapsed, loss: 1.3923253e-06\n",
      "step: 30530 train: 0.07953047752380371 elapsed, loss: 2.0735865e-06\n",
      "step: 30540 train: 0.08374977111816406 elapsed, loss: 1.6381939e-06\n",
      "step: 30550 train: 0.08471536636352539 elapsed, loss: 1.5706737e-06\n",
      "step: 30560 train: 0.08593893051147461 elapsed, loss: 1.3029191e-06\n",
      "step: 30570 train: 0.08257293701171875 elapsed, loss: 0.00079141406\n",
      "step: 30580 train: 0.08091521263122559 elapsed, loss: 0.00011489243\n",
      "step: 30590 train: 0.07959628105163574 elapsed, loss: 7.254326e-05\n",
      "step: 30600 train: 0.08617043495178223 elapsed, loss: 5.4336822e-05\n",
      "step: 30610 train: 0.08375287055969238 elapsed, loss: 6.2158106e-05\n",
      "step: 30620 train: 0.08475923538208008 elapsed, loss: 0.00017591727\n",
      "step: 30630 train: 0.08149886131286621 elapsed, loss: 5.8019243e-05\n",
      "step: 30640 train: 0.08527445793151855 elapsed, loss: 2.7392201e-05\n",
      "step: 30650 train: 0.08202409744262695 elapsed, loss: 4.142727e-05\n",
      "step: 30660 train: 0.08389687538146973 elapsed, loss: 2.0395346e-05\n",
      "step: 30670 train: 0.07895827293395996 elapsed, loss: 1.2166923e-05\n",
      "step: 30680 train: 0.08294367790222168 elapsed, loss: 8.620222e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30690 train: 0.08098745346069336 elapsed, loss: 5.6344566e-06\n",
      "step: 30700 train: 0.08555746078491211 elapsed, loss: 3.3117713e-06\n",
      "step: 30710 train: 0.08177638053894043 elapsed, loss: 0.0021366142\n",
      "step: 30720 train: 0.08394765853881836 elapsed, loss: 0.00033114344\n",
      "step: 30730 train: 0.08011293411254883 elapsed, loss: 0.00014185606\n",
      "step: 30740 train: 0.0838632583618164 elapsed, loss: 2.4502335e-05\n",
      "step: 30750 train: 0.08121991157531738 elapsed, loss: 2.910475e-05\n",
      "step: 30760 train: 0.08181023597717285 elapsed, loss: 2.2079745e-05\n",
      "step: 30770 train: 0.08010172843933105 elapsed, loss: 1.1175646e-05\n",
      "step: 30780 train: 0.08452391624450684 elapsed, loss: 9.144531e-06\n",
      "step: 30790 train: 0.07844996452331543 elapsed, loss: 9.910072e-06\n",
      "step: 30800 train: 0.08699727058410645 elapsed, loss: 4.806991e-06\n",
      "step: 30810 train: 0.08275079727172852 elapsed, loss: 4.0498385e-06\n",
      "step: 30820 train: 0.08499574661254883 elapsed, loss: 3.727133e-06\n",
      "step: 30830 train: 0.08340859413146973 elapsed, loss: 3.2745215e-06\n",
      "step: 30840 train: 0.0823814868927002 elapsed, loss: 2.5769577e-06\n",
      "step: 30850 train: 0.08701372146606445 elapsed, loss: 1.7681127e-06\n",
      "step: 30860 train: 0.08489656448364258 elapsed, loss: 1.586506e-06\n",
      "step: 30870 train: 0.07846665382385254 elapsed, loss: 1.2703226e-06\n",
      "step: 30880 train: 0.0837855339050293 elapsed, loss: 1.6172396e-06\n",
      "step: 30890 train: 0.08124399185180664 elapsed, loss: 3.0388949e-06\n",
      "step: 30900 train: 0.08623051643371582 elapsed, loss: 1.0477368e-06\n",
      "step: 30910 train: 0.07712221145629883 elapsed, loss: 1.6954698e-06\n",
      "step: 30920 train: 0.07586026191711426 elapsed, loss: 1.1282964e-06\n",
      "step: 30930 train: 0.07774543762207031 elapsed, loss: 1.4570525e-06\n",
      "step: 30940 train: 0.07944655418395996 elapsed, loss: 1.0533248e-06\n",
      "step: 30950 train: 0.08466362953186035 elapsed, loss: 1.3890522e-06\n",
      "step: 30960 train: 0.08352184295654297 elapsed, loss: 9.844072e-07\n",
      "step: 30970 train: 0.08708810806274414 elapsed, loss: 9.0291644e-07\n",
      "step: 30980 train: 0.07966470718383789 elapsed, loss: 1.4379602e-06\n",
      "step: 30990 train: 0.08366203308105469 elapsed, loss: 1.0230568e-06\n",
      "step: 31000 train: 0.0810694694519043 elapsed, loss: 1.2363298e-06\n",
      "step: 31010 train: 0.08569526672363281 elapsed, loss: 2.3795226e-06\n",
      "step: 31020 train: 0.07904863357543945 elapsed, loss: 1.4821981e-06\n",
      "step: 31030 train: 0.07889008522033691 elapsed, loss: 1.5571691e-06\n",
      "step: 31040 train: 0.07931065559387207 elapsed, loss: 1.5851092e-06\n",
      "step: 31050 train: 0.08842158317565918 elapsed, loss: 8.6473244e-07\n",
      "step: 31060 train: 0.08528614044189453 elapsed, loss: 9.713686e-07\n",
      "step: 31070 train: 0.08056068420410156 elapsed, loss: 1.36718e-06\n",
      "step: 31080 train: 0.0825343132019043 elapsed, loss: 1.1716027e-06\n",
      "step: 31090 train: 0.08152627944946289 elapsed, loss: 1.41654e-06\n",
      "step: 31100 train: 0.08102536201477051 elapsed, loss: 2.3990815e-06\n",
      "step: 31110 train: 0.08214163780212402 elapsed, loss: 1.4081583e-06\n",
      "step: 31120 train: 0.08069920539855957 elapsed, loss: 1.379753e-06\n",
      "step: 31130 train: 0.08466815948486328 elapsed, loss: 1.0374926e-06\n",
      "step: 31140 train: 0.07947850227355957 elapsed, loss: 1.3140949e-06\n",
      "step: 31150 train: 0.0859534740447998 elapsed, loss: 1.2512307e-06\n",
      "step: 31160 train: 0.08492255210876465 elapsed, loss: 1.3886006e-06\n",
      "step: 31170 train: 0.08161783218383789 elapsed, loss: 2.1904639e-06\n",
      "step: 31180 train: 0.08620238304138184 elapsed, loss: 1.13435e-06\n",
      "step: 31190 train: 0.08407711982727051 elapsed, loss: 1.3080414e-06\n",
      "step: 31200 train: 0.07918381690979004 elapsed, loss: 1.8118856e-06\n",
      "step: 31210 train: 0.08440041542053223 elapsed, loss: 1.2279479e-06\n",
      "step: 31220 train: 0.07857799530029297 elapsed, loss: 4.6822074e-06\n",
      "step: 31230 train: 0.08746337890625 elapsed, loss: 1.250765e-06\n",
      "step: 31240 train: 0.07845711708068848 elapsed, loss: 1.3490196e-06\n",
      "step: 31250 train: 0.08384585380554199 elapsed, loss: 1.3480881e-06\n",
      "step: 31260 train: 0.07653069496154785 elapsed, loss: 4.4628787e-06\n",
      "step: 31270 train: 0.0817105770111084 elapsed, loss: 1.8295806e-06\n",
      "step: 31280 train: 0.0792703628540039 elapsed, loss: 2.4926626e-06\n",
      "step: 31290 train: 0.07756495475769043 elapsed, loss: 2.6565922e-06\n",
      "step: 31300 train: 0.0774681568145752 elapsed, loss: 1.9189872e-06\n",
      "step: 31310 train: 0.09018874168395996 elapsed, loss: 1.3108354e-06\n",
      "step: 31320 train: 0.08229851722717285 elapsed, loss: 3.7122377e-06\n",
      "step: 31330 train: 0.08569526672363281 elapsed, loss: 1.4272504e-06\n",
      "step: 31340 train: 0.08716511726379395 elapsed, loss: 1.3271331e-06\n",
      "step: 31350 train: 0.07983541488647461 elapsed, loss: 0.0056988397\n",
      "step: 31360 train: 0.08506536483764648 elapsed, loss: 0.00031530694\n",
      "step: 31370 train: 0.0865011215209961 elapsed, loss: 0.00014766736\n",
      "step: 31380 train: 0.07726573944091797 elapsed, loss: 0.00010802302\n",
      "step: 31390 train: 0.08533620834350586 elapsed, loss: 2.827107e-05\n",
      "step: 31400 train: 0.0891718864440918 elapsed, loss: 1.5596628e-05\n",
      "step: 31410 train: 0.08640289306640625 elapsed, loss: 1.5652577e-05\n",
      "step: 31420 train: 0.0894777774810791 elapsed, loss: 1.1542519e-05\n",
      "step: 31430 train: 0.08716988563537598 elapsed, loss: 1.0196364e-05\n",
      "step: 31440 train: 0.0842442512512207 elapsed, loss: 1.0734217e-05\n",
      "step: 31450 train: 0.08061003684997559 elapsed, loss: 6.7385395e-06\n",
      "step: 31460 train: 0.08895683288574219 elapsed, loss: 8.768291e-06\n",
      "step: 31470 train: 0.08575105667114258 elapsed, loss: 4.199306e-06\n",
      "step: 31480 train: 0.08338022232055664 elapsed, loss: 2.8060654e-06\n",
      "step: 31490 train: 0.09288525581359863 elapsed, loss: 2.3189853e-06\n",
      "step: 31500 train: 0.07978248596191406 elapsed, loss: 4.7070753e-06\n",
      "step: 31510 train: 0.0878746509552002 elapsed, loss: 1.6591478e-06\n",
      "step: 31520 train: 0.07926654815673828 elapsed, loss: 1.6326064e-06\n",
      "step: 31530 train: 0.0849905014038086 elapsed, loss: 1.722013e-06\n",
      "step: 31540 train: 0.08656692504882812 elapsed, loss: 1.3052472e-06\n",
      "step: 31550 train: 0.08490180969238281 elapsed, loss: 2.008313e-06\n",
      "step: 31560 train: 0.08113646507263184 elapsed, loss: 0.00029878042\n",
      "step: 31570 train: 0.08470702171325684 elapsed, loss: 6.6447465e-05\n",
      "step: 31580 train: 0.08514714241027832 elapsed, loss: 3.8795126e-05\n",
      "step: 31590 train: 0.0827946662902832 elapsed, loss: 2.8125645e-05\n",
      "step: 31600 train: 0.08245253562927246 elapsed, loss: 2.048886e-05\n",
      "step: 31610 train: 0.08956456184387207 elapsed, loss: 1.2455224e-05\n",
      "step: 31620 train: 0.08491039276123047 elapsed, loss: 1.05479785e-05\n",
      "step: 31630 train: 0.08090639114379883 elapsed, loss: 3.7579874e-05\n",
      "step: 31640 train: 0.07778501510620117 elapsed, loss: 1.7786535e-05\n",
      "step: 31650 train: 0.0819997787475586 elapsed, loss: 6.0393377e-06\n",
      "step: 31660 train: 0.0806131362915039 elapsed, loss: 6.1768915e-06\n",
      "step: 31670 train: 0.08696103096008301 elapsed, loss: 4.6044183e-06\n",
      "step: 31680 train: 0.08194112777709961 elapsed, loss: 2.669164e-06\n",
      "step: 31690 train: 0.08468985557556152 elapsed, loss: 1.9851095e-06\n",
      "step: 31700 train: 0.08826923370361328 elapsed, loss: 6.7670303e-06\n",
      "step: 31710 train: 0.08074522018432617 elapsed, loss: 4.0186387e-06\n",
      "step: 31720 train: 0.0771474838256836 elapsed, loss: 5.0542585e-06\n",
      "step: 31730 train: 0.0888984203338623 elapsed, loss: 1.5036183e-06\n",
      "step: 31740 train: 0.0860586166381836 elapsed, loss: 1.5450603e-06\n",
      "step: 31750 train: 0.0871126651763916 elapsed, loss: 1.1771906e-06\n",
      "step: 31760 train: 0.07811307907104492 elapsed, loss: 1.6582176e-06\n",
      "step: 31770 train: 0.08291435241699219 elapsed, loss: 1.76951e-06\n",
      "step: 31780 train: 0.08485078811645508 elapsed, loss: 9.2340554e-07\n",
      "step: 31790 train: 0.07961344718933105 elapsed, loss: 1.334118e-06\n",
      "step: 31800 train: 0.0776209831237793 elapsed, loss: 2.1275866e-06\n",
      "step: 31810 train: 0.08004570007324219 elapsed, loss: 1.0938376e-06\n",
      "step: 31820 train: 0.08768582344055176 elapsed, loss: 9.3411575e-07\n",
      "step: 31830 train: 0.0786893367767334 elapsed, loss: 1.265666e-06\n",
      "step: 31840 train: 0.07884073257446289 elapsed, loss: 1.5655512e-06\n",
      "step: 31850 train: 0.0766761302947998 elapsed, loss: 1.4686941e-06\n",
      "step: 31860 train: 0.08109498023986816 elapsed, loss: 1.0509963e-06\n",
      "step: 31870 train: 0.08933186531066895 elapsed, loss: 8.498313e-07\n",
      "step: 31880 train: 0.08500337600708008 elapsed, loss: 1.1627546e-06\n",
      "step: 31890 train: 0.08586406707763672 elapsed, loss: 9.872012e-07\n",
      "step: 31900 train: 0.08102107048034668 elapsed, loss: 1.138075e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31910 train: 0.08137321472167969 elapsed, loss: 2.3283023e-06\n",
      "step: 31920 train: 0.0774533748626709 elapsed, loss: 1.5464595e-06\n",
      "step: 31930 train: 0.07919526100158691 elapsed, loss: 5.507678e-05\n",
      "step: 31940 train: 0.07958364486694336 elapsed, loss: 3.316062e-05\n",
      "step: 31950 train: 0.08263778686523438 elapsed, loss: 6.697991e-06\n",
      "step: 31960 train: 0.08396005630493164 elapsed, loss: 6.171679e-06\n",
      "step: 31970 train: 0.08074569702148438 elapsed, loss: 4.241685e-06\n",
      "step: 31980 train: 0.07565641403198242 elapsed, loss: 3.6135184e-06\n",
      "step: 31990 train: 0.07915711402893066 elapsed, loss: 3.3541469e-06\n",
      "step: 32000 train: 0.08096504211425781 elapsed, loss: 2.2812665e-06\n",
      "step: 32010 train: 0.08083200454711914 elapsed, loss: 2.020966e-06\n",
      "step: 32020 train: 0.07809638977050781 elapsed, loss: 2.1508852e-06\n",
      "step: 32030 train: 0.0804893970489502 elapsed, loss: 1.4840605e-06\n",
      "step: 32040 train: 0.08607792854309082 elapsed, loss: 1.1795189e-06\n",
      "step: 32050 train: 0.08580756187438965 elapsed, loss: 1.1776563e-06\n",
      "step: 32060 train: 0.08017182350158691 elapsed, loss: 1.0747453e-06\n",
      "step: 32070 train: 0.08359289169311523 elapsed, loss: 1.0896464e-06\n",
      "step: 32080 train: 0.07963848114013672 elapsed, loss: 1.9059478e-06\n",
      "step: 32090 train: 0.0843503475189209 elapsed, loss: 1.7737012e-06\n",
      "step: 32100 train: 0.0825035572052002 elapsed, loss: 1.1725342e-06\n",
      "step: 32110 train: 0.08222723007202148 elapsed, loss: 2.0028008e-06\n",
      "step: 32120 train: 0.08385396003723145 elapsed, loss: 1.0323699e-06\n",
      "step: 32130 train: 0.07947564125061035 elapsed, loss: 1.1497167e-06\n",
      "step: 32140 train: 0.07950830459594727 elapsed, loss: 2.1285337e-06\n",
      "step: 32150 train: 0.08371424674987793 elapsed, loss: 1.1036159e-06\n",
      "step: 32160 train: 0.07712411880493164 elapsed, loss: 4.525741e-06\n",
      "step: 32170 train: 0.07970166206359863 elapsed, loss: 1.908742e-06\n",
      "step: 32180 train: 0.08137679100036621 elapsed, loss: 1.2181688e-06\n",
      "step: 32190 train: 0.08371901512145996 elapsed, loss: 1.3480881e-06\n",
      "step: 32200 train: 0.08280515670776367 elapsed, loss: 1.3820807e-06\n",
      "step: 32210 train: 0.08037590980529785 elapsed, loss: 1.9799882e-06\n",
      "step: 32220 train: 0.07844996452331543 elapsed, loss: 4.034803e-06\n",
      "step: 32230 train: 0.08101248741149902 elapsed, loss: 0.00029682566\n",
      "step: 32240 train: 0.07975244522094727 elapsed, loss: 3.5518533e-05\n",
      "step: 32250 train: 0.08060860633850098 elapsed, loss: 3.5687597e-05\n",
      "step: 32260 train: 0.08140206336975098 elapsed, loss: 2.0072213e-05\n",
      "step: 32270 train: 0.07916784286499023 elapsed, loss: 2.3990133e-05\n",
      "step: 32280 train: 0.07974743843078613 elapsed, loss: 8.726345e-06\n",
      "step: 32290 train: 0.08021950721740723 elapsed, loss: 9.737245e-06\n",
      "step: 32300 train: 0.08461189270019531 elapsed, loss: 4.5885763e-06\n",
      "step: 32310 train: 0.08455610275268555 elapsed, loss: 3.4831253e-06\n",
      "step: 32320 train: 0.07765650749206543 elapsed, loss: 5.3448357e-06\n",
      "step: 32330 train: 0.08460211753845215 elapsed, loss: 3.0309413e-06\n",
      "step: 32340 train: 0.0880587100982666 elapsed, loss: 1.883131e-06\n",
      "step: 32350 train: 0.08087372779846191 elapsed, loss: 1.8663675e-06\n",
      "step: 32360 train: 0.08396553993225098 elapsed, loss: 2.917361e-06\n",
      "step: 32370 train: 0.08160161972045898 elapsed, loss: 2.0679968e-06\n",
      "step: 32380 train: 0.08344554901123047 elapsed, loss: 1.6712559e-06\n",
      "step: 32390 train: 0.08131694793701172 elapsed, loss: 3.1376135e-06\n",
      "step: 32400 train: 0.08358097076416016 elapsed, loss: 1.6619429e-06\n",
      "step: 32410 train: 0.08327388763427734 elapsed, loss: 2.0773114e-06\n",
      "step: 32420 train: 0.07311868667602539 elapsed, loss: 1.6557933e-05\n",
      "step: 32430 train: 0.08008718490600586 elapsed, loss: 0.00241757\n",
      "step: 32440 train: 0.08250188827514648 elapsed, loss: 5.912223e-05\n",
      "step: 32450 train: 0.0897364616394043 elapsed, loss: 2.4531691e-05\n",
      "step: 32460 train: 0.08239603042602539 elapsed, loss: 0.00034326583\n",
      "step: 32470 train: 0.08241677284240723 elapsed, loss: 1.4991983e-05\n",
      "step: 32480 train: 0.08238410949707031 elapsed, loss: 1.5056492e-05\n",
      "step: 32490 train: 0.08119344711303711 elapsed, loss: 1.0798518e-05\n",
      "step: 32500 train: 0.08393430709838867 elapsed, loss: 1.2341436e-05\n",
      "step: 32510 train: 0.08361673355102539 elapsed, loss: 4.5448332e-06\n",
      "step: 32520 train: 0.08864951133728027 elapsed, loss: 3.493845e-06\n",
      "step: 32530 train: 0.08072137832641602 elapsed, loss: 6.316637e-06\n",
      "step: 32540 train: 0.08345866203308105 elapsed, loss: 4.427023e-06\n",
      "step: 32550 train: 0.08019137382507324 elapsed, loss: 4.4638055e-06\n",
      "step: 32560 train: 0.0848386287689209 elapsed, loss: 2.3073417e-06\n",
      "step: 32570 train: 0.08556914329528809 elapsed, loss: 1.8826657e-06\n",
      "step: 32580 train: 0.07694888114929199 elapsed, loss: 2.7720741e-06\n",
      "step: 32590 train: 0.07588362693786621 elapsed, loss: 1.6251558e-06\n",
      "step: 32600 train: 0.08213520050048828 elapsed, loss: 1.467297e-06\n",
      "step: 32610 train: 0.0793924331665039 elapsed, loss: 1.5450604e-06\n",
      "step: 32620 train: 0.0828711986541748 elapsed, loss: 1.2842927e-06\n",
      "step: 32630 train: 0.08694148063659668 elapsed, loss: 1.1245711e-06\n",
      "step: 32640 train: 0.08157944679260254 elapsed, loss: 1.3792875e-06\n",
      "step: 32650 train: 0.08055663108825684 elapsed, loss: 2.073586e-06\n",
      "step: 32660 train: 0.08014535903930664 elapsed, loss: 1.8440157e-06\n",
      "step: 32670 train: 0.07932043075561523 elapsed, loss: 1.1166545e-06\n",
      "step: 32680 train: 0.07905387878417969 elapsed, loss: 1.383012e-06\n",
      "step: 32690 train: 0.07496023178100586 elapsed, loss: 1.7490217e-06\n",
      "step: 32700 train: 0.08049821853637695 elapsed, loss: 1.9986096e-06\n",
      "step: 32710 train: 0.0813148021697998 elapsed, loss: 1.7140951e-06\n",
      "step: 32720 train: 0.0838766098022461 elapsed, loss: 2.18255e-06\n",
      "step: 32730 train: 0.08422374725341797 elapsed, loss: 1.1846412e-06\n",
      "step: 32740 train: 0.08184051513671875 elapsed, loss: 0.0027527886\n",
      "step: 32750 train: 0.07600975036621094 elapsed, loss: 0.00032437604\n",
      "step: 32760 train: 0.08351778984069824 elapsed, loss: 5.4766002e-05\n",
      "step: 32770 train: 0.08095264434814453 elapsed, loss: 0.00010255769\n",
      "step: 32780 train: 0.08135414123535156 elapsed, loss: 2.3461856e-05\n",
      "step: 32790 train: 0.08418726921081543 elapsed, loss: 1.3944371e-05\n",
      "step: 32800 train: 0.08414721488952637 elapsed, loss: 1.3894158e-05\n",
      "step: 32810 train: 0.08581066131591797 elapsed, loss: 1.1134556e-05\n",
      "step: 32820 train: 0.08144593238830566 elapsed, loss: 1.9694095e-05\n",
      "step: 32830 train: 0.07761836051940918 elapsed, loss: 8.800864e-06\n",
      "step: 32840 train: 0.082733154296875 elapsed, loss: 6.58764e-06\n",
      "step: 32850 train: 0.0826115608215332 elapsed, loss: 4.5168563e-06\n",
      "step: 32860 train: 0.08272480964660645 elapsed, loss: 3.2107137e-06\n",
      "step: 32870 train: 0.08485269546508789 elapsed, loss: 2.3953571e-06\n",
      "step: 32880 train: 0.08670282363891602 elapsed, loss: 2.0028056e-06\n",
      "step: 32890 train: 0.08581256866455078 elapsed, loss: 1.7671816e-06\n",
      "step: 32900 train: 0.08165645599365234 elapsed, loss: 3.6288054e-06\n",
      "step: 32910 train: 0.08014559745788574 elapsed, loss: 0.00015800903\n",
      "step: 32920 train: 0.07933902740478516 elapsed, loss: 0.00034542504\n",
      "step: 32930 train: 0.08197569847106934 elapsed, loss: 9.6883785e-05\n",
      "step: 32940 train: 0.08036518096923828 elapsed, loss: 2.3088445e-05\n",
      "step: 32950 train: 0.08296394348144531 elapsed, loss: 1.1645279e-05\n",
      "step: 32960 train: 0.07970571517944336 elapsed, loss: 1.7864804e-05\n",
      "step: 32970 train: 0.0791616439819336 elapsed, loss: 1.2983752e-05\n",
      "step: 32980 train: 0.08017969131469727 elapsed, loss: 9.64466e-06\n",
      "step: 32990 train: 0.08232808113098145 elapsed, loss: 5.554378e-06\n",
      "step: 33000 train: 0.08392667770385742 elapsed, loss: 3.4510044e-06\n",
      "step: 33010 train: 0.08324193954467773 elapsed, loss: 3.0849942e-06\n",
      "step: 33020 train: 0.07738447189331055 elapsed, loss: 7.716368e-06\n",
      "step: 33030 train: 0.08739328384399414 elapsed, loss: 2.0102557e-06\n",
      "step: 33040 train: 0.08310198783874512 elapsed, loss: 2.566252e-06\n",
      "step: 33050 train: 0.08668732643127441 elapsed, loss: 1.4323721e-06\n",
      "step: 33060 train: 0.08591723442077637 elapsed, loss: 1.9189874e-06\n",
      "step: 33070 train: 0.08223366737365723 elapsed, loss: 1.4798699e-06\n",
      "step: 33080 train: 0.07886648178100586 elapsed, loss: 9.607898e-06\n",
      "step: 33090 train: 0.07889199256896973 elapsed, loss: 2.3925622e-06\n",
      "step: 33100 train: 0.08589410781860352 elapsed, loss: 1.138075e-06\n",
      "step: 33110 train: 0.08552289009094238 elapsed, loss: 1.4468079e-06\n",
      "step: 33120 train: 0.08476948738098145 elapsed, loss: 8.5402223e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 33130 train: 0.08376169204711914 elapsed, loss: 2.756241e-06\n",
      "step: 33140 train: 0.07963037490844727 elapsed, loss: 1.0537905e-06\n",
      "step: 33150 train: 0.08108782768249512 elapsed, loss: 1.0584472e-06\n",
      "step: 33160 train: 0.08016347885131836 elapsed, loss: 1.2093212e-06\n",
      "step: 33170 train: 0.08132576942443848 elapsed, loss: 1.0021024e-06\n",
      "step: 33180 train: 0.0762016773223877 elapsed, loss: 1.5408714e-06\n",
      "step: 33190 train: 0.0759432315826416 elapsed, loss: 1.534818e-06\n",
      "step: 33200 train: 0.08049273490905762 elapsed, loss: 1.4672969e-06\n",
      "step: 33210 train: 0.09195566177368164 elapsed, loss: 9.676435e-07\n",
      "step: 33220 train: 0.078857421875 elapsed, loss: 1.4384264e-06\n",
      "step: 33230 train: 0.08640241622924805 elapsed, loss: 9.4575745e-07\n",
      "step: 33240 train: 0.07731151580810547 elapsed, loss: 1.4458767e-06\n",
      "step: 33250 train: 0.0823819637298584 elapsed, loss: 1.4416859e-06\n",
      "step: 33260 train: 0.08315730094909668 elapsed, loss: 1.0542565e-06\n",
      "step: 33270 train: 0.08834028244018555 elapsed, loss: 1.1217771e-06\n",
      "step: 33280 train: 0.08308744430541992 elapsed, loss: 1.893842e-06\n",
      "step: 33290 train: 0.09409070014953613 elapsed, loss: 1.0933716e-06\n",
      "step: 33300 train: 0.08604311943054199 elapsed, loss: 1.2009392e-06\n",
      "step: 33310 train: 0.08406591415405273 elapsed, loss: 1.5776585e-06\n",
      "step: 33320 train: 0.08158373832702637 elapsed, loss: 0.000106237225\n",
      "step: 33330 train: 0.08433079719543457 elapsed, loss: 7.807436e-05\n",
      "step: 33340 train: 0.08051705360412598 elapsed, loss: 7.5917895e-05\n",
      "step: 33350 train: 0.08277034759521484 elapsed, loss: 5.8471604e-05\n",
      "step: 33360 train: 0.08668923377990723 elapsed, loss: 1.6120215e-05\n",
      "step: 33370 train: 0.07909989356994629 elapsed, loss: 2.3714223e-05\n",
      "step: 33380 train: 0.08032965660095215 elapsed, loss: 3.1990163e-05\n",
      "step: 33390 train: 0.08278298377990723 elapsed, loss: 7.468221e-06\n",
      "step: 33400 train: 0.08652997016906738 elapsed, loss: 7.157634e-06\n",
      "step: 33410 train: 0.08698010444641113 elapsed, loss: 4.630492e-06\n",
      "step: 33420 train: 0.0823056697845459 elapsed, loss: 4.363204e-06\n",
      "step: 33430 train: 0.07640266418457031 elapsed, loss: 4.842379e-06\n",
      "step: 33440 train: 0.07846236228942871 elapsed, loss: 4.092217e-06\n",
      "step: 33450 train: 0.08692002296447754 elapsed, loss: 1.7927935e-06\n",
      "step: 33460 train: 0.08329391479492188 elapsed, loss: 1.8472751e-06\n",
      "step: 33470 train: 0.08543968200683594 elapsed, loss: 1.5799865e-06\n",
      "step: 33480 train: 0.08636641502380371 elapsed, loss: 1.6558893e-06\n",
      "step: 33490 train: 0.08265995979309082 elapsed, loss: 1.7532126e-06\n",
      "step: 33500 train: 0.08333396911621094 elapsed, loss: 1.2991936e-06\n",
      "step: 33510 train: 0.08423638343811035 elapsed, loss: 1.6856915e-06\n",
      "step: 33520 train: 0.08306694030761719 elapsed, loss: 1.1320217e-06\n",
      "step: 33530 train: 0.08020257949829102 elapsed, loss: 1.5608948e-06\n",
      "step: 33540 train: 0.08838677406311035 elapsed, loss: 9.266654e-07\n",
      "step: 33550 train: 0.08022952079772949 elapsed, loss: 1.5366804e-06\n",
      "step: 33560 train: 0.08063173294067383 elapsed, loss: 1.2433146e-06\n",
      "step: 33570 train: 0.07979559898376465 elapsed, loss: 1.8938418e-06\n",
      "step: 33580 train: 0.08145833015441895 elapsed, loss: 1.0491342e-06\n",
      "step: 33590 train: 0.08092546463012695 elapsed, loss: 1.4239909e-06\n",
      "step: 33600 train: 0.08215761184692383 elapsed, loss: 1.3792873e-06\n",
      "step: 33610 train: 0.08645915985107422 elapsed, loss: 1.6679962e-06\n",
      "step: 33620 train: 0.08603262901306152 elapsed, loss: 1.0207289e-06\n",
      "step: 33630 train: 0.08391356468200684 elapsed, loss: 1.3033846e-06\n",
      "step: 33640 train: 0.08369684219360352 elapsed, loss: 1.0831274e-06\n",
      "step: 33650 train: 0.08375287055969238 elapsed, loss: 2.3692623e-06\n",
      "step: 33660 train: 0.080810546875 elapsed, loss: 1.8104886e-06\n",
      "step: 33670 train: 0.08261823654174805 elapsed, loss: 1.2419175e-06\n",
      "step: 33680 train: 0.08029031753540039 elapsed, loss: 2.2407587e-06\n",
      "step: 33690 train: 0.07872986793518066 elapsed, loss: 2.0079287e-06\n",
      "step: 33700 train: 0.08517813682556152 elapsed, loss: 0.0006014195\n",
      "step: 33710 train: 0.08478641510009766 elapsed, loss: 0.00022150163\n",
      "step: 33720 train: 0.08147192001342773 elapsed, loss: 4.1743544e-05\n",
      "step: 33730 train: 0.08591771125793457 elapsed, loss: 1.6306301e-05\n",
      "step: 33740 train: 0.08383536338806152 elapsed, loss: 0.00011180291\n",
      "step: 33750 train: 0.0843353271484375 elapsed, loss: 5.873513e-05\n",
      "step: 33760 train: 0.08178901672363281 elapsed, loss: 1.0017945e-05\n",
      "step: 33770 train: 0.08236002922058105 elapsed, loss: 1.0037698e-05\n",
      "step: 33780 train: 0.0818483829498291 elapsed, loss: 9.635377e-06\n",
      "step: 33790 train: 0.07992172241210938 elapsed, loss: 7.746132e-06\n",
      "step: 33800 train: 0.08271670341491699 elapsed, loss: 4.2141874e-06\n",
      "step: 33810 train: 0.08563089370727539 elapsed, loss: 3.0644896e-06\n",
      "step: 33820 train: 0.08939170837402344 elapsed, loss: 2.2230608e-06\n",
      "step: 33830 train: 0.0832071304321289 elapsed, loss: 2.1662518e-06\n",
      "step: 33840 train: 0.07826948165893555 elapsed, loss: 2.7418073e-06\n",
      "step: 33850 train: 0.08403635025024414 elapsed, loss: 1.4551883e-06\n",
      "step: 33860 train: 0.08459687232971191 elapsed, loss: 1.2540246e-06\n",
      "step: 33870 train: 0.07941436767578125 elapsed, loss: 1.9804547e-06\n",
      "step: 33880 train: 0.08046245574951172 elapsed, loss: 1.7853431e-06\n",
      "step: 33890 train: 0.0818178653717041 elapsed, loss: 2.2659033e-06\n",
      "step: 33900 train: 0.08416366577148438 elapsed, loss: 1.1892976e-06\n",
      "step: 33910 train: 0.08340787887573242 elapsed, loss: 1.5581008e-06\n",
      "step: 33920 train: 0.07945060729980469 elapsed, loss: 1.4710224e-06\n",
      "step: 33930 train: 0.08000707626342773 elapsed, loss: 1.6428509e-06\n",
      "step: 33940 train: 0.08359122276306152 elapsed, loss: 1.4714878e-06\n",
      "step: 33950 train: 0.08744335174560547 elapsed, loss: 0.026647162\n",
      "step: 33960 train: 0.08133625984191895 elapsed, loss: 0.0012812733\n",
      "step: 33970 train: 0.0799555778503418 elapsed, loss: 9.3281946e-05\n",
      "step: 33980 train: 0.08160638809204102 elapsed, loss: 7.72114e-05\n",
      "step: 33990 train: 0.0877234935760498 elapsed, loss: 3.2434284e-05\n",
      "step: 34000 train: 0.08572244644165039 elapsed, loss: 2.2937738e-05\n",
      "step: 34010 train: 0.08482193946838379 elapsed, loss: 2.4305546e-05\n",
      "step: 34020 train: 0.08822941780090332 elapsed, loss: 1.9558807e-05\n",
      "step: 34030 train: 0.08291244506835938 elapsed, loss: 7.9864785e-06\n",
      "step: 34040 train: 0.08374977111816406 elapsed, loss: 7.2265248e-06\n",
      "step: 34050 train: 0.08353185653686523 elapsed, loss: 5.5455284e-06\n",
      "step: 34060 train: 0.08314085006713867 elapsed, loss: 5.6651993e-06\n",
      "step: 34070 train: 0.08627438545227051 elapsed, loss: 3.36672e-06\n",
      "step: 34080 train: 0.08469557762145996 elapsed, loss: 2.5634595e-06\n",
      "step: 34090 train: 0.08137083053588867 elapsed, loss: 2.6482098e-06\n",
      "step: 34100 train: 0.08109450340270996 elapsed, loss: 2.0405246e-06\n",
      "step: 34110 train: 0.08274483680725098 elapsed, loss: 2.2319095e-06\n",
      "step: 34120 train: 0.08075666427612305 elapsed, loss: 2.2514682e-06\n",
      "step: 34130 train: 0.08022260665893555 elapsed, loss: 1.5739332e-06\n",
      "step: 34140 train: 0.0802011489868164 elapsed, loss: 1.8533293e-06\n",
      "step: 34150 train: 0.08203959465026855 elapsed, loss: 1.7420366e-06\n",
      "step: 34160 train: 0.08542060852050781 elapsed, loss: 1.0733481e-06\n",
      "step: 34170 train: 0.08715987205505371 elapsed, loss: 9.760254e-07\n",
      "step: 34180 train: 0.08095860481262207 elapsed, loss: 1.605598e-06\n",
      "step: 34190 train: 0.0846703052520752 elapsed, loss: 1.691745e-06\n",
      "step: 34200 train: 0.0835270881652832 elapsed, loss: 1.2200314e-06\n",
      "step: 34210 train: 0.08502578735351562 elapsed, loss: 0.00026813662\n",
      "step: 34220 train: 0.08038663864135742 elapsed, loss: 1.8316621e-05\n",
      "step: 34230 train: 0.07747197151184082 elapsed, loss: 1.5374237e-05\n",
      "step: 34240 train: 0.08092164993286133 elapsed, loss: 1.0553405e-05\n",
      "step: 34250 train: 0.07961535453796387 elapsed, loss: 9.216856e-06\n",
      "step: 34260 train: 0.07995009422302246 elapsed, loss: 6.3264056e-06\n",
      "step: 34270 train: 0.07714223861694336 elapsed, loss: 7.5469197e-06\n",
      "step: 34280 train: 0.08579444885253906 elapsed, loss: 2.7157294e-06\n",
      "step: 34290 train: 0.08559584617614746 elapsed, loss: 2.7464616e-06\n",
      "step: 34300 train: 0.07867264747619629 elapsed, loss: 2.750649e-06\n",
      "step: 34310 train: 0.08066487312316895 elapsed, loss: 2.5676509e-06\n",
      "step: 34320 train: 0.08854293823242188 elapsed, loss: 1.6535504e-06\n",
      "step: 34330 train: 0.08399057388305664 elapsed, loss: 1.1660148e-06\n",
      "step: 34340 train: 0.08113384246826172 elapsed, loss: 1.5329547e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 34350 train: 0.07851123809814453 elapsed, loss: 1.3839438e-06\n",
      "step: 34360 train: 0.07883548736572266 elapsed, loss: 2.223058e-06\n",
      "step: 34370 train: 0.07584619522094727 elapsed, loss: 1.5050156e-06\n",
      "step: 34380 train: 0.07567930221557617 elapsed, loss: 2.3506536e-06\n",
      "step: 34390 train: 0.08269476890563965 elapsed, loss: 1.1161892e-06\n",
      "step: 34400 train: 0.08383011817932129 elapsed, loss: 1.168808e-06\n",
      "step: 34410 train: 0.08543515205383301 elapsed, loss: 1.3317897e-06\n",
      "step: 34420 train: 0.08524346351623535 elapsed, loss: 1.2721853e-06\n",
      "step: 34430 train: 0.08172821998596191 elapsed, loss: 1.0542565e-06\n",
      "step: 34440 train: 0.07894182205200195 elapsed, loss: 1.1804503e-06\n",
      "step: 34450 train: 0.08169078826904297 elapsed, loss: 2.1974517e-06\n",
      "step: 34460 train: 0.0802621841430664 elapsed, loss: 1.3872028e-06\n",
      "step: 34470 train: 0.07922887802124023 elapsed, loss: 1.8090916e-06\n",
      "step: 34480 train: 0.07947611808776855 elapsed, loss: 1.7872046e-06\n",
      "step: 34490 train: 0.07645058631896973 elapsed, loss: 1.4924419e-06\n",
      "step: 34500 train: 0.08369827270507812 elapsed, loss: 1.4426171e-06\n",
      "step: 34510 train: 0.08185124397277832 elapsed, loss: 2.0414561e-06\n",
      "step: 34520 train: 0.07972168922424316 elapsed, loss: 1.763457e-06\n",
      "step: 34530 train: 0.08467864990234375 elapsed, loss: 1.5087394e-06\n",
      "step: 34540 train: 0.08476662635803223 elapsed, loss: 1.5273667e-06\n",
      "step: 34550 train: 0.08649945259094238 elapsed, loss: 1.1436632e-06\n",
      "step: 34560 train: 0.08179950714111328 elapsed, loss: 1.3038504e-06\n",
      "step: 34570 train: 0.07740950584411621 elapsed, loss: 1.6749817e-06\n",
      "step: 34580 train: 0.07999038696289062 elapsed, loss: 1.8803379e-06\n",
      "step: 34590 train: 0.07752823829650879 elapsed, loss: 1.5650858e-06\n",
      "step: 34600 train: 0.08144116401672363 elapsed, loss: 2.1192209e-06\n",
      "step: 34610 train: 0.08339047431945801 elapsed, loss: 1.1981454e-06\n",
      "step: 34620 train: 0.08381772041320801 elapsed, loss: 1.8076931e-06\n",
      "step: 34630 train: 0.0814211368560791 elapsed, loss: 1.8146798e-06\n",
      "step: 34640 train: 0.07915830612182617 elapsed, loss: 1.7001273e-06\n",
      "step: 34650 train: 0.08367395401000977 elapsed, loss: 1.4705569e-06\n",
      "step: 34660 train: 0.08152365684509277 elapsed, loss: 2.0093257e-06\n",
      "step: 34670 train: 0.08046150207519531 elapsed, loss: 2.8689171e-06\n",
      "step: 34680 train: 0.08318161964416504 elapsed, loss: 0.02434132\n",
      "step: 34690 train: 0.07960963249206543 elapsed, loss: 0.00020443424\n",
      "step: 34700 train: 0.07521772384643555 elapsed, loss: 3.8975602e-05\n",
      "step: 34710 train: 0.08335638046264648 elapsed, loss: 1.911255e-05\n",
      "step: 34720 train: 0.08406591415405273 elapsed, loss: 1.7869723e-05\n",
      "step: 34730 train: 0.07802557945251465 elapsed, loss: 1.398082e-05\n",
      "step: 34740 train: 0.08627009391784668 elapsed, loss: 6.882891e-06\n",
      "step: 34750 train: 0.08586859703063965 elapsed, loss: 5.6768476e-06\n",
      "step: 34760 train: 0.07996654510498047 elapsed, loss: 8.37204e-06\n",
      "step: 34770 train: 0.07993221282958984 elapsed, loss: 9.210225e-06\n",
      "step: 34780 train: 0.07927441596984863 elapsed, loss: 6.5527042e-06\n",
      "step: 34790 train: 0.0835568904876709 elapsed, loss: 2.8391264e-06\n",
      "step: 34800 train: 0.08897686004638672 elapsed, loss: 1.8854587e-06\n",
      "step: 34810 train: 0.08743977546691895 elapsed, loss: 2.1015248e-06\n",
      "step: 34820 train: 0.08138322830200195 elapsed, loss: 1.967881e-06\n",
      "step: 34830 train: 0.08251786231994629 elapsed, loss: 3.0770625e-06\n",
      "step: 34840 train: 0.08224725723266602 elapsed, loss: 0.0001530099\n",
      "step: 34850 train: 0.08192944526672363 elapsed, loss: 0.0007583614\n",
      "step: 34860 train: 0.07959127426147461 elapsed, loss: 0.0003755244\n",
      "step: 34870 train: 0.08125662803649902 elapsed, loss: 0.0002560473\n",
      "step: 34880 train: 0.08280277252197266 elapsed, loss: 4.3772434e-05\n",
      "step: 34890 train: 0.08429408073425293 elapsed, loss: 0.00011519374\n",
      "step: 34900 train: 0.07875537872314453 elapsed, loss: 7.10763e-05\n",
      "step: 34910 train: 0.0829010009765625 elapsed, loss: 2.2261262e-05\n",
      "step: 34920 train: 0.08205127716064453 elapsed, loss: 2.3281704e-05\n",
      "step: 34930 train: 0.08144283294677734 elapsed, loss: 1.0291256e-05\n",
      "step: 34940 train: 0.08444643020629883 elapsed, loss: 8.359933e-06\n",
      "step: 34950 train: 0.07942080497741699 elapsed, loss: 9.204579e-06\n",
      "step: 34960 train: 0.08480501174926758 elapsed, loss: 7.19159e-06\n",
      "step: 34970 train: 0.08685612678527832 elapsed, loss: 3.5026908e-06\n",
      "step: 34980 train: 0.08138656616210938 elapsed, loss: 2.9429705e-06\n",
      "step: 34990 train: 0.08732461929321289 elapsed, loss: 2.3664832e-06\n",
      "step: 35000 train: 0.0848081111907959 elapsed, loss: 1.9455294e-06\n",
      "step: 35010 train: 0.08153200149536133 elapsed, loss: 2.0922087e-06\n",
      "step: 35020 train: 0.08389139175415039 elapsed, loss: 1.6684593e-06\n",
      "step: 35030 train: 0.08355712890625 elapsed, loss: 1.2689256e-06\n",
      "step: 35040 train: 0.08019351959228516 elapsed, loss: 1.7816176e-06\n",
      "step: 35050 train: 0.08081221580505371 elapsed, loss: 1.814212e-06\n",
      "step: 35060 train: 0.08604025840759277 elapsed, loss: 9.718343e-07\n",
      "step: 35070 train: 0.08664155006408691 elapsed, loss: 9.699717e-07\n",
      "step: 35080 train: 0.08569645881652832 elapsed, loss: 9.4529173e-07\n",
      "step: 35090 train: 0.0821692943572998 elapsed, loss: 1.0258509e-06\n",
      "step: 35100 train: 0.0818181037902832 elapsed, loss: 1.1092043e-06\n",
      "step: 35110 train: 0.08370804786682129 elapsed, loss: 8.8754973e-07\n",
      "step: 35120 train: 0.08552980422973633 elapsed, loss: 8.69389e-07\n",
      "step: 35130 train: 0.08642172813415527 elapsed, loss: 1.2787046e-06\n",
      "step: 35140 train: 0.08517169952392578 elapsed, loss: 1.1124639e-06\n",
      "step: 35150 train: 0.07857418060302734 elapsed, loss: 1.9059485e-06\n",
      "step: 35160 train: 0.07833409309387207 elapsed, loss: 1.2968655e-06\n",
      "step: 35170 train: 0.0857541561126709 elapsed, loss: 1.0193312e-06\n",
      "step: 35180 train: 0.08481717109680176 elapsed, loss: 1.1418001e-06\n",
      "step: 35190 train: 0.0841665267944336 elapsed, loss: 1.5906969e-06\n",
      "step: 35200 train: 0.09130215644836426 elapsed, loss: 1.1431877e-06\n",
      "step: 35210 train: 0.08067488670349121 elapsed, loss: 1.2945372e-06\n",
      "step: 35220 train: 0.08358621597290039 elapsed, loss: 9.913922e-07\n",
      "step: 35230 train: 0.08339667320251465 elapsed, loss: 1.3960505e-06\n",
      "step: 35240 train: 0.07941961288452148 elapsed, loss: 1.2521621e-06\n",
      "step: 35250 train: 0.08416581153869629 elapsed, loss: 1.1632208e-06\n",
      "step: 35260 train: 0.08103370666503906 elapsed, loss: 1.2824297e-06\n",
      "step: 35270 train: 0.08207178115844727 elapsed, loss: 0.00071892026\n",
      "step: 35280 train: 0.0825965404510498 elapsed, loss: 0.0006527775\n",
      "step: 35290 train: 0.07996010780334473 elapsed, loss: 0.00012552406\n",
      "step: 35300 train: 0.0813760757446289 elapsed, loss: 0.00016796665\n",
      "step: 35310 train: 0.08375740051269531 elapsed, loss: 1.3027572e-05\n",
      "step: 35320 train: 0.08133792877197266 elapsed, loss: 1.0844585e-05\n",
      "step: 35330 train: 0.08103775978088379 elapsed, loss: 8.2504985e-06\n",
      "step: 35340 train: 0.08025527000427246 elapsed, loss: 9.30687e-06\n",
      "step: 35350 train: 0.07880496978759766 elapsed, loss: 5.383479e-06\n",
      "step: 35360 train: 0.0891869068145752 elapsed, loss: 3.674979e-06\n",
      "step: 35370 train: 0.0834953784942627 elapsed, loss: 2.6742805e-06\n",
      "step: 35380 train: 0.0871267318725586 elapsed, loss: 2.179756e-06\n",
      "step: 35390 train: 0.08246302604675293 elapsed, loss: 2.71247e-06\n",
      "step: 35400 train: 0.08495926856994629 elapsed, loss: 1.7750979e-06\n",
      "step: 35410 train: 0.08252787590026855 elapsed, loss: 1.7611287e-06\n",
      "step: 35420 train: 0.0852205753326416 elapsed, loss: 1.1515792e-06\n",
      "step: 35430 train: 0.08546185493469238 elapsed, loss: 1.5650851e-06\n",
      "step: 35440 train: 0.08609747886657715 elapsed, loss: 1.6097889e-06\n",
      "step: 35450 train: 0.08500123023986816 elapsed, loss: 1.1343495e-06\n",
      "step: 35460 train: 0.08278942108154297 elapsed, loss: 1.2698567e-06\n",
      "step: 35470 train: 0.0792701244354248 elapsed, loss: 1.5571695e-06\n",
      "step: 35480 train: 0.08108854293823242 elapsed, loss: 1.5171222e-06\n",
      "step: 35490 train: 0.08689117431640625 elapsed, loss: 1.1594955e-06\n",
      "step: 35500 train: 0.0802299976348877 elapsed, loss: 1.798847e-06\n",
      "step: 35510 train: 0.07779383659362793 elapsed, loss: 1.4021045e-06\n",
      "step: 35520 train: 0.08145952224731445 elapsed, loss: 1.246108e-06\n",
      "step: 35530 train: 0.08238792419433594 elapsed, loss: 1.24378e-06\n",
      "step: 35540 train: 0.07827067375183105 elapsed, loss: 2.665898e-06\n",
      "step: 35550 train: 0.08448624610900879 elapsed, loss: 1.4449446e-06\n",
      "step: 35560 train: 0.08376669883728027 elapsed, loss: 1.7425023e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 35570 train: 0.08280754089355469 elapsed, loss: 1.3336526e-06\n",
      "step: 35580 train: 0.08145570755004883 elapsed, loss: 1.8379625e-06\n",
      "step: 35590 train: 0.08480119705200195 elapsed, loss: 1.0812647e-06\n",
      "step: 35600 train: 0.0810847282409668 elapsed, loss: 1.8253896e-06\n",
      "step: 35610 train: 0.0872654914855957 elapsed, loss: 1.5809153e-06\n",
      "step: 35620 train: 0.08319687843322754 elapsed, loss: 1.41654e-06\n",
      "step: 35630 train: 0.0821080207824707 elapsed, loss: 2.1359824e-06\n",
      "step: 35640 train: 0.08315086364746094 elapsed, loss: 1.5608904e-06\n",
      "step: 35650 train: 0.07999801635742188 elapsed, loss: 2.0056002e-06\n",
      "step: 35660 train: 0.08099794387817383 elapsed, loss: 0.000319739\n",
      "step: 35670 train: 0.08181047439575195 elapsed, loss: 0.0063165952\n",
      "step: 35680 train: 0.08209395408630371 elapsed, loss: 0.00014372397\n",
      "step: 35690 train: 0.08250927925109863 elapsed, loss: 6.971842e-05\n",
      "step: 35700 train: 0.08293652534484863 elapsed, loss: 8.3924606e-05\n",
      "step: 35710 train: 0.09020853042602539 elapsed, loss: 8.369224e-05\n",
      "step: 35720 train: 0.07773876190185547 elapsed, loss: 4.0226838e-05\n",
      "step: 35730 train: 0.08210992813110352 elapsed, loss: 2.066117e-05\n",
      "step: 35740 train: 0.07965493202209473 elapsed, loss: 1.2341162e-05\n",
      "step: 35750 train: 0.07665348052978516 elapsed, loss: 1.3759733e-05\n",
      "step: 35760 train: 0.08374881744384766 elapsed, loss: 1.4313686e-05\n",
      "step: 35770 train: 0.08284807205200195 elapsed, loss: 4.624915e-06\n",
      "step: 35780 train: 0.08921980857849121 elapsed, loss: 3.462176e-06\n",
      "step: 35790 train: 0.08730101585388184 elapsed, loss: 2.7846381e-06\n",
      "step: 35800 train: 0.08183908462524414 elapsed, loss: 2.2062973e-06\n",
      "step: 35810 train: 0.08353567123413086 elapsed, loss: 1.8128162e-06\n",
      "step: 35820 train: 0.08698105812072754 elapsed, loss: 1.6908116e-06\n",
      "step: 35830 train: 0.08066439628601074 elapsed, loss: 1.8812685e-06\n",
      "step: 35840 train: 0.07918739318847656 elapsed, loss: 1.7513496e-06\n",
      "step: 35850 train: 0.08043742179870605 elapsed, loss: 1.3303925e-06\n",
      "step: 35860 train: 0.08938241004943848 elapsed, loss: 1.4840609e-06\n",
      "step: 35870 train: 0.08669805526733398 elapsed, loss: 1.1296933e-06\n",
      "step: 35880 train: 0.0921175479888916 elapsed, loss: 1.1268992e-06\n",
      "step: 35890 train: 0.08036231994628906 elapsed, loss: 1.1436632e-06\n",
      "step: 35900 train: 0.08182930946350098 elapsed, loss: 1.2768412e-06\n",
      "step: 35910 train: 0.08278656005859375 elapsed, loss: 1.3937229e-06\n",
      "step: 35920 train: 0.08632826805114746 elapsed, loss: 1.8039657e-06\n",
      "step: 35930 train: 0.08146071434020996 elapsed, loss: 1.3518134e-06\n",
      "step: 35940 train: 0.08554768562316895 elapsed, loss: 2.6146822e-06\n",
      "step: 35950 train: 0.08510875701904297 elapsed, loss: 1.4388918e-06\n",
      "step: 35960 train: 0.08227086067199707 elapsed, loss: 1.5604281e-06\n",
      "step: 35970 train: 0.08429813385009766 elapsed, loss: 1.5459937e-06\n",
      "step: 35980 train: 0.08371353149414062 elapsed, loss: 1.1716027e-06\n",
      "step: 35990 train: 0.08425498008728027 elapsed, loss: 1.5520474e-06\n",
      "step: 36000 train: 0.07838273048400879 elapsed, loss: 1.6656686e-06\n",
      "step: 36010 train: 0.08551979064941406 elapsed, loss: 1.0374927e-06\n",
      "step: 36020 train: 0.07993936538696289 elapsed, loss: 1.8430849e-06\n",
      "step: 36030 train: 0.08209419250488281 elapsed, loss: 1.6544927e-06\n",
      "step: 36040 train: 0.0831303596496582 elapsed, loss: 0.03743304\n",
      "step: 36050 train: 0.07928991317749023 elapsed, loss: 0.00018095627\n",
      "step: 36060 train: 0.08083820343017578 elapsed, loss: 7.4639654e-05\n",
      "step: 36070 train: 0.08609938621520996 elapsed, loss: 4.4780027e-05\n",
      "step: 36080 train: 0.08400678634643555 elapsed, loss: 2.218845e-05\n",
      "step: 36090 train: 0.0801093578338623 elapsed, loss: 0.0003110531\n",
      "step: 36100 train: 0.0891876220703125 elapsed, loss: 4.27367e-05\n",
      "step: 36110 train: 0.08205652236938477 elapsed, loss: 2.7910964e-05\n",
      "step: 36120 train: 0.08213424682617188 elapsed, loss: 1.7953547e-05\n",
      "step: 36130 train: 0.0822150707244873 elapsed, loss: 1.1238028e-05\n",
      "step: 36140 train: 0.08328843116760254 elapsed, loss: 6.9428597e-06\n",
      "step: 36150 train: 0.07906603813171387 elapsed, loss: 9.3354865e-06\n",
      "step: 36160 train: 0.09453034400939941 elapsed, loss: 4.9671116e-06\n",
      "step: 36170 train: 0.08096194267272949 elapsed, loss: 4.222598e-06\n",
      "step: 36180 train: 0.08482003211975098 elapsed, loss: 2.4908168e-06\n",
      "step: 36190 train: 0.08217477798461914 elapsed, loss: 3.7508876e-06\n",
      "step: 36200 train: 0.07727408409118652 elapsed, loss: 2.7073472e-06\n",
      "step: 36210 train: 0.0770421028137207 elapsed, loss: 2.3189891e-06\n",
      "step: 36220 train: 0.08394503593444824 elapsed, loss: 1.3606607e-06\n",
      "step: 36230 train: 0.07941412925720215 elapsed, loss: 1.2572841e-06\n",
      "step: 36240 train: 0.08283042907714844 elapsed, loss: 1.6447132e-06\n",
      "step: 36250 train: 0.08085441589355469 elapsed, loss: 1.4808005e-06\n",
      "step: 36260 train: 0.08283472061157227 elapsed, loss: 1.3178199e-06\n",
      "step: 36270 train: 0.08287167549133301 elapsed, loss: 1.0957001e-06\n",
      "step: 36280 train: 0.07802915573120117 elapsed, loss: 1.4482046e-06\n",
      "step: 36290 train: 0.08387303352355957 elapsed, loss: 2.3757998e-06\n",
      "step: 36300 train: 0.08301520347595215 elapsed, loss: 1.2903463e-06\n",
      "step: 36310 train: 0.07946538925170898 elapsed, loss: 1.1692746e-06\n",
      "step: 36320 train: 0.08600163459777832 elapsed, loss: 1.0374924e-06\n",
      "step: 36330 train: 0.07754945755004883 elapsed, loss: 0.008625465\n",
      "step: 36340 train: 0.08004331588745117 elapsed, loss: 5.6637633e-05\n",
      "step: 36350 train: 0.07894325256347656 elapsed, loss: 6.716259e-05\n",
      "step: 36360 train: 0.08370590209960938 elapsed, loss: 3.0551044e-05\n",
      "step: 36370 train: 0.08059930801391602 elapsed, loss: 1.7808441e-05\n",
      "step: 36380 train: 0.07980632781982422 elapsed, loss: 1.3344839e-05\n",
      "step: 36390 train: 0.07789015769958496 elapsed, loss: 1.3822813e-05\n",
      "step: 36400 train: 0.07845830917358398 elapsed, loss: 8.367377e-06\n",
      "step: 36410 train: 0.08358287811279297 elapsed, loss: 3.5318113e-05\n",
      "step: 36420 train: 0.08704829216003418 elapsed, loss: 9.5133055e-06\n",
      "step: 36430 train: 0.07638120651245117 elapsed, loss: 4.022828e-06\n",
      "step: 36440 train: 0.08186173439025879 elapsed, loss: 2.8954587e-06\n",
      "step: 36450 train: 0.0871436595916748 elapsed, loss: 2.1886033e-06\n",
      "step: 36460 train: 0.0876932144165039 elapsed, loss: 1.6968665e-06\n",
      "step: 36470 train: 0.0796499252319336 elapsed, loss: 2.6617126e-06\n",
      "step: 36480 train: 0.08242106437683105 elapsed, loss: 1.3993106e-06\n",
      "step: 36490 train: 0.08701109886169434 elapsed, loss: 1.3480878e-06\n",
      "step: 36500 train: 0.08478760719299316 elapsed, loss: 1.901758e-06\n",
      "step: 36510 train: 0.08683991432189941 elapsed, loss: 1.1818472e-06\n",
      "step: 36520 train: 0.08323836326599121 elapsed, loss: 8.908094e-07\n",
      "step: 36530 train: 0.08690428733825684 elapsed, loss: 9.406351e-07\n",
      "step: 36540 train: 0.08336091041564941 elapsed, loss: 1.0137438e-06\n",
      "step: 36550 train: 0.08796381950378418 elapsed, loss: 1.409555e-06\n",
      "step: 36560 train: 0.08054637908935547 elapsed, loss: 1.7085088e-06\n",
      "step: 36570 train: 0.08741474151611328 elapsed, loss: 9.909263e-07\n",
      "step: 36580 train: 0.08199024200439453 elapsed, loss: 1.3350437e-06\n",
      "step: 36590 train: 0.08310723304748535 elapsed, loss: 1.2395892e-06\n",
      "step: 36600 train: 0.08214688301086426 elapsed, loss: 1.8719556e-06\n",
      "step: 36610 train: 0.07952737808227539 elapsed, loss: 2.1825501e-06\n",
      "step: 36620 train: 0.0863502025604248 elapsed, loss: 1.1851068e-06\n",
      "step: 36630 train: 0.0809028148651123 elapsed, loss: 2.087537e-06\n",
      "step: 36640 train: 0.08276009559631348 elapsed, loss: 7.509076e-06\n",
      "step: 36650 train: 0.08345842361450195 elapsed, loss: 0.0037945092\n",
      "step: 36660 train: 0.08639955520629883 elapsed, loss: 5.7971083e-05\n",
      "step: 36670 train: 0.08598113059997559 elapsed, loss: 0.00016982682\n",
      "step: 36680 train: 0.0855104923248291 elapsed, loss: 5.228709e-05\n",
      "step: 36690 train: 0.0850379467010498 elapsed, loss: 1.8135694e-05\n",
      "step: 36700 train: 0.08380436897277832 elapsed, loss: 1.4517101e-05\n",
      "step: 36710 train: 0.07993841171264648 elapsed, loss: 1.1326509e-05\n",
      "step: 36720 train: 0.08387899398803711 elapsed, loss: 1.1379942e-05\n",
      "step: 36730 train: 0.08485269546508789 elapsed, loss: 7.807102e-06\n",
      "step: 36740 train: 0.08531403541564941 elapsed, loss: 5.068231e-06\n",
      "step: 36750 train: 0.08685588836669922 elapsed, loss: 5.2742594e-06\n",
      "step: 36760 train: 0.08500552177429199 elapsed, loss: 3.0263127e-06\n",
      "step: 36770 train: 0.08435773849487305 elapsed, loss: 3.3951237e-06\n",
      "step: 36780 train: 0.08225011825561523 elapsed, loss: 2.6286514e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 36790 train: 0.07854747772216797 elapsed, loss: 2.012584e-06\n",
      "step: 36800 train: 0.08461546897888184 elapsed, loss: 1.4174709e-06\n",
      "step: 36810 train: 0.0862874984741211 elapsed, loss: 1.2251533e-06\n",
      "step: 36820 train: 0.08161234855651855 elapsed, loss: 1.0156064e-06\n",
      "step: 36830 train: 0.08627796173095703 elapsed, loss: 1.1175861e-06\n",
      "step: 36840 train: 0.0773162841796875 elapsed, loss: 1.1352812e-06\n",
      "step: 36850 train: 0.07988166809082031 elapsed, loss: 1.1464564e-06\n",
      "step: 36860 train: 0.09032893180847168 elapsed, loss: 2.399922e-05\n",
      "step: 36870 train: 0.08931398391723633 elapsed, loss: 2.9811508e-06\n",
      "step: 36880 train: 0.0816948413848877 elapsed, loss: 2.7813885e-06\n",
      "step: 36890 train: 0.08300995826721191 elapsed, loss: 1.8142132e-06\n",
      "step: 36900 train: 0.08403563499450684 elapsed, loss: 1.6861542e-06\n",
      "step: 36910 train: 0.08095598220825195 elapsed, loss: 1.8128163e-06\n",
      "step: 36920 train: 0.08013558387756348 elapsed, loss: 1.2516964e-06\n",
      "step: 36930 train: 0.08018326759338379 elapsed, loss: 1.3336524e-06\n",
      "step: 36940 train: 0.08190011978149414 elapsed, loss: 1.2442459e-06\n",
      "step: 36950 train: 0.08946776390075684 elapsed, loss: 1.0500653e-06\n",
      "step: 36960 train: 0.077056884765625 elapsed, loss: 1.6172396e-06\n",
      "step: 36970 train: 0.08464312553405762 elapsed, loss: 1.1059444e-06\n",
      "step: 36980 train: 0.08474397659301758 elapsed, loss: 1.0714857e-06\n",
      "step: 36990 train: 0.0840902328491211 elapsed, loss: 1.0118812e-06\n",
      "step: 37000 train: 0.08391451835632324 elapsed, loss: 1.0761423e-06\n",
      "step: 37010 train: 0.08262395858764648 elapsed, loss: 1.5902314e-06\n",
      "step: 37020 train: 0.08513355255126953 elapsed, loss: 1.6572862e-06\n",
      "step: 37030 train: 0.07969164848327637 elapsed, loss: 1.8160765e-06\n",
      "step: 37040 train: 0.08304572105407715 elapsed, loss: 1.2684598e-06\n",
      "step: 37050 train: 0.08200526237487793 elapsed, loss: 1.9324912e-06\n",
      "step: 37060 train: 0.08264708518981934 elapsed, loss: 1.0919748e-06\n",
      "step: 37070 train: 0.07859134674072266 elapsed, loss: 1.7527456e-06\n",
      "step: 37080 train: 0.08289790153503418 elapsed, loss: 1.0542562e-06\n",
      "step: 37090 train: 0.07800436019897461 elapsed, loss: 1.8868571e-06\n",
      "step: 37100 train: 0.0827188491821289 elapsed, loss: 1.6433166e-06\n",
      "step: 37110 train: 0.08228421211242676 elapsed, loss: 1.8794035e-06\n",
      "step: 37120 train: 0.08441162109375 elapsed, loss: 1.2489025e-06\n",
      "step: 37130 train: 0.08077645301818848 elapsed, loss: 1.7117688e-06\n",
      "step: 37140 train: 0.0811464786529541 elapsed, loss: 1.6773099e-06\n",
      "step: 37150 train: 0.08157920837402344 elapsed, loss: 1.0854556e-06\n",
      "step: 37160 train: 0.07697582244873047 elapsed, loss: 4.894733e-06\n",
      "step: 37170 train: 0.07927632331848145 elapsed, loss: 0.004350475\n",
      "step: 37180 train: 0.07963085174560547 elapsed, loss: 6.298038e-05\n",
      "step: 37190 train: 0.08397126197814941 elapsed, loss: 3.4718945e-05\n",
      "step: 37200 train: 0.0810239315032959 elapsed, loss: 1.4371293e-05\n",
      "step: 37210 train: 0.08097600936889648 elapsed, loss: 9.290243e-06\n",
      "step: 37220 train: 0.07959198951721191 elapsed, loss: 1.8331373e-05\n",
      "step: 37230 train: 0.07944202423095703 elapsed, loss: 1.641846e-05\n",
      "step: 37240 train: 0.08092689514160156 elapsed, loss: 5.4407556e-06\n",
      "step: 37250 train: 0.0850532054901123 elapsed, loss: 6.883073e-06\n",
      "step: 37260 train: 0.08432865142822266 elapsed, loss: 3.1362192e-06\n",
      "step: 37270 train: 0.08802461624145508 elapsed, loss: 2.6179314e-06\n",
      "step: 37280 train: 0.08310580253601074 elapsed, loss: 7.3930696e-06\n",
      "step: 37290 train: 0.08752894401550293 elapsed, loss: 2.21468e-06\n",
      "step: 37300 train: 0.08451104164123535 elapsed, loss: 1.5138631e-06\n",
      "step: 37310 train: 0.08693385124206543 elapsed, loss: 1.3792867e-06\n",
      "step: 37320 train: 0.08143186569213867 elapsed, loss: 1.5650857e-06\n",
      "step: 37330 train: 0.08431553840637207 elapsed, loss: 1.1967484e-06\n",
      "step: 37340 train: 0.08098149299621582 elapsed, loss: 1.2940716e-06\n",
      "step: 37350 train: 0.08084607124328613 elapsed, loss: 1.4482047e-06\n",
      "step: 37360 train: 0.07878732681274414 elapsed, loss: 1.8295805e-06\n",
      "step: 37370 train: 0.08273577690124512 elapsed, loss: 1.7629914e-06\n",
      "step: 37380 train: 0.08167624473571777 elapsed, loss: 1.4058296e-06\n",
      "step: 37390 train: 0.08848834037780762 elapsed, loss: 9.2992497e-07\n",
      "step: 37400 train: 0.0821385383605957 elapsed, loss: 1.327599e-06\n",
      "step: 37410 train: 0.08008456230163574 elapsed, loss: 1.8808032e-06\n",
      "step: 37420 train: 0.07756996154785156 elapsed, loss: 1.3425001e-06\n",
      "step: 37430 train: 0.08074474334716797 elapsed, loss: 1.7867321e-06\n",
      "step: 37440 train: 0.08440518379211426 elapsed, loss: 1.2824299e-06\n",
      "step: 37450 train: 0.08008456230163574 elapsed, loss: 1.1469226e-06\n",
      "step: 37460 train: 0.0829927921295166 elapsed, loss: 1.1408692e-06\n",
      "step: 37470 train: 0.08321070671081543 elapsed, loss: 2.949016e-06\n",
      "step: 37480 train: 0.0836176872253418 elapsed, loss: 1.5255048e-06\n",
      "step: 37490 train: 0.07623291015625 elapsed, loss: 2.5448344e-06\n",
      "step: 37500 train: 0.08289313316345215 elapsed, loss: 1.3136291e-06\n",
      "step: 37510 train: 0.08085513114929199 elapsed, loss: 1.1660147e-06\n",
      "step: 37520 train: 0.08108878135681152 elapsed, loss: 1.34576e-06\n",
      "step: 37530 train: 0.08457303047180176 elapsed, loss: 1.2842925e-06\n",
      "step: 37540 train: 0.08097362518310547 elapsed, loss: 1.3606607e-06\n",
      "step: 37550 train: 0.08581209182739258 elapsed, loss: 0.000846954\n",
      "step: 37560 train: 0.08078503608703613 elapsed, loss: 0.00016228792\n",
      "step: 37570 train: 0.09069466590881348 elapsed, loss: 0.005374034\n",
      "step: 37580 train: 0.08272862434387207 elapsed, loss: 0.00017281095\n",
      "step: 37590 train: 0.08576273918151855 elapsed, loss: 5.3603086e-05\n",
      "step: 37600 train: 0.08080649375915527 elapsed, loss: 7.185328e-05\n",
      "step: 37610 train: 0.08295345306396484 elapsed, loss: 2.8793083e-05\n",
      "step: 37620 train: 0.08690023422241211 elapsed, loss: 1.8712424e-05\n",
      "step: 37630 train: 0.08575820922851562 elapsed, loss: 1.269649e-05\n",
      "step: 37640 train: 0.08084726333618164 elapsed, loss: 2.3214685e-05\n",
      "step: 37650 train: 0.08507204055786133 elapsed, loss: 1.0456821e-05\n",
      "step: 37660 train: 0.08486247062683105 elapsed, loss: 5.182774e-06\n",
      "step: 37670 train: 0.08165669441223145 elapsed, loss: 5.0621697e-06\n",
      "step: 37680 train: 0.08185076713562012 elapsed, loss: 3.8169575e-05\n",
      "step: 37690 train: 0.08733391761779785 elapsed, loss: 5.8331443e-06\n",
      "step: 37700 train: 0.08272671699523926 elapsed, loss: 3.4253794e-06\n",
      "step: 37710 train: 0.07963991165161133 elapsed, loss: 2.7930273e-06\n",
      "step: 37720 train: 0.08319449424743652 elapsed, loss: 2.6858083e-06\n",
      "step: 37730 train: 0.08371615409851074 elapsed, loss: 1.1553044e-06\n",
      "step: 37740 train: 0.08107781410217285 elapsed, loss: 1.5259693e-06\n",
      "step: 37750 train: 0.07787513732910156 elapsed, loss: 1.7327233e-06\n",
      "step: 37760 train: 0.08688473701477051 elapsed, loss: 1.1362117e-06\n",
      "step: 37770 train: 0.08206987380981445 elapsed, loss: 1.3904626e-06\n",
      "step: 37780 train: 0.08623909950256348 elapsed, loss: 1.1594955e-06\n",
      "step: 37790 train: 0.07498931884765625 elapsed, loss: 2.006997e-06\n",
      "step: 37800 train: 0.08631181716918945 elapsed, loss: 8.051278e-07\n",
      "step: 37810 train: 0.07960081100463867 elapsed, loss: 1.2391235e-06\n",
      "step: 37820 train: 0.07959270477294922 elapsed, loss: 1.4463424e-06\n",
      "step: 37830 train: 0.0819242000579834 elapsed, loss: 1.2796279e-06\n",
      "step: 37840 train: 0.08147120475769043 elapsed, loss: 9.136269e-07\n",
      "step: 37850 train: 0.07666349411010742 elapsed, loss: 2.3115301e-06\n",
      "step: 37860 train: 0.08762907981872559 elapsed, loss: 8.46106e-07\n",
      "step: 37870 train: 0.0861058235168457 elapsed, loss: 9.965142e-07\n",
      "step: 37880 train: 0.08132243156433105 elapsed, loss: 9.88132e-07\n",
      "step: 37890 train: 0.08222055435180664 elapsed, loss: 9.2666534e-07\n",
      "step: 37900 train: 0.08380746841430664 elapsed, loss: 1.0468056e-06\n",
      "step: 37910 train: 0.08899927139282227 elapsed, loss: 0.012063811\n",
      "step: 37920 train: 0.08033514022827148 elapsed, loss: 9.3170034e-05\n",
      "step: 37930 train: 0.07743334770202637 elapsed, loss: 5.5512843e-05\n",
      "step: 37940 train: 0.08181214332580566 elapsed, loss: 4.5643006e-05\n",
      "step: 37950 train: 0.08203315734863281 elapsed, loss: 2.2241602e-05\n",
      "step: 37960 train: 0.08179974555969238 elapsed, loss: 1.7938928e-05\n",
      "step: 37970 train: 0.08183932304382324 elapsed, loss: 1.28913025e-05\n",
      "step: 37980 train: 0.08138895034790039 elapsed, loss: 9.904894e-06\n",
      "step: 37990 train: 0.08615827560424805 elapsed, loss: 5.1841635e-06\n",
      "step: 38000 train: 0.08058476448059082 elapsed, loss: 7.1603986e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38010 train: 0.08037686347961426 elapsed, loss: 3.3448343e-06\n",
      "step: 38020 train: 0.07552242279052734 elapsed, loss: 4.5113075e-06\n",
      "step: 38030 train: 0.08087730407714844 elapsed, loss: 2.960667e-06\n",
      "step: 38040 train: 0.08161020278930664 elapsed, loss: 2.1252738e-06\n",
      "step: 38050 train: 0.08379554748535156 elapsed, loss: 1.3401718e-06\n",
      "step: 38060 train: 0.08211803436279297 elapsed, loss: 1.840756e-06\n",
      "step: 38070 train: 0.08420991897583008 elapsed, loss: 1.1180517e-06\n",
      "step: 38080 train: 0.07956099510192871 elapsed, loss: 1.7937243e-06\n",
      "step: 38090 train: 0.08048629760742188 elapsed, loss: 1.6125828e-06\n",
      "step: 38100 train: 0.08231234550476074 elapsed, loss: 1.1660142e-06\n",
      "step: 38110 train: 0.08394861221313477 elapsed, loss: 1.1008224e-06\n",
      "step: 38120 train: 0.08318233489990234 elapsed, loss: 1.5161909e-06\n",
      "step: 38130 train: 0.08256769180297852 elapsed, loss: 1.0225915e-06\n",
      "step: 38140 train: 0.08232998847961426 elapsed, loss: 1.2493682e-06\n",
      "step: 38150 train: 0.09382963180541992 elapsed, loss: 1.0323702e-06\n",
      "step: 38160 train: 0.0858914852142334 elapsed, loss: 1.8440151e-06\n",
      "step: 38170 train: 0.08700275421142578 elapsed, loss: 1.1017535e-06\n",
      "step: 38180 train: 0.0800778865814209 elapsed, loss: 8.301032e-06\n",
      "step: 38190 train: 0.08179068565368652 elapsed, loss: 0.00017715275\n",
      "step: 38200 train: 0.08610129356384277 elapsed, loss: 6.616365e-06\n",
      "step: 38210 train: 0.0867159366607666 elapsed, loss: 2.1671808e-06\n",
      "step: 38220 train: 0.08505082130432129 elapsed, loss: 1.7299292e-06\n",
      "step: 38230 train: 0.07827162742614746 elapsed, loss: 2.4614787e-06\n",
      "step: 38240 train: 0.08509254455566406 elapsed, loss: 1.2665973e-06\n",
      "step: 38250 train: 0.08469700813293457 elapsed, loss: 1.4631057e-06\n",
      "step: 38260 train: 0.08777976036071777 elapsed, loss: 1.1054789e-06\n",
      "step: 38270 train: 0.08502006530761719 elapsed, loss: 1.1175862e-06\n",
      "step: 38280 train: 0.08009839057922363 elapsed, loss: 1.2414515e-06\n",
      "step: 38290 train: 0.07987737655639648 elapsed, loss: 1.1585644e-06\n",
      "step: 38300 train: 0.07851624488830566 elapsed, loss: 6.3388798e-06\n",
      "step: 38310 train: 0.07791900634765625 elapsed, loss: 1.5310923e-06\n",
      "step: 38320 train: 0.0792243480682373 elapsed, loss: 1.5413366e-06\n",
      "step: 38330 train: 0.08087706565856934 elapsed, loss: 1.8840456e-06\n",
      "step: 38340 train: 0.08334827423095703 elapsed, loss: 1.1390059e-06\n",
      "step: 38350 train: 0.07918024063110352 elapsed, loss: 1.1436632e-06\n",
      "step: 38360 train: 0.08716964721679688 elapsed, loss: 1.1380752e-06\n",
      "step: 38370 train: 0.08388400077819824 elapsed, loss: 1.7108376e-06\n",
      "step: 38380 train: 0.08283066749572754 elapsed, loss: 1.6381935e-06\n",
      "step: 38390 train: 0.07634925842285156 elapsed, loss: 1.8705591e-06\n",
      "step: 38400 train: 0.0836782455444336 elapsed, loss: 1.1390065e-06\n",
      "step: 38410 train: 0.08130025863647461 elapsed, loss: 1.2754449e-06\n",
      "step: 38420 train: 0.0857841968536377 elapsed, loss: 1.003965e-06\n",
      "step: 38430 train: 0.0771636962890625 elapsed, loss: 1.9478587e-06\n",
      "step: 38440 train: 0.08187103271484375 elapsed, loss: 1.2586811e-06\n",
      "step: 38450 train: 0.0842132568359375 elapsed, loss: 1.1557704e-06\n",
      "step: 38460 train: 0.0804910659790039 elapsed, loss: 2.3394753e-06\n",
      "step: 38470 train: 0.07594728469848633 elapsed, loss: 2.6104913e-06\n",
      "step: 38480 train: 0.08646655082702637 elapsed, loss: 0.005861938\n",
      "step: 38490 train: 0.07992315292358398 elapsed, loss: 0.00039118464\n",
      "step: 38500 train: 0.08057951927185059 elapsed, loss: 4.789085e-05\n",
      "step: 38510 train: 0.07952022552490234 elapsed, loss: 0.000101557336\n",
      "step: 38520 train: 0.08042597770690918 elapsed, loss: 1.6931404e-05\n",
      "step: 38530 train: 0.0807344913482666 elapsed, loss: 1.4747981e-05\n",
      "step: 38540 train: 0.07960629463195801 elapsed, loss: 1.0903805e-05\n",
      "step: 38550 train: 0.08249759674072266 elapsed, loss: 7.852355e-06\n",
      "step: 38560 train: 0.08445405960083008 elapsed, loss: 6.4409733e-06\n",
      "step: 38570 train: 0.0867469310760498 elapsed, loss: 3.6768442e-06\n",
      "step: 38580 train: 0.08170223236083984 elapsed, loss: 6.061938e-06\n",
      "step: 38590 train: 0.0802452564239502 elapsed, loss: 4.451697e-06\n",
      "step: 38600 train: 0.08209896087646484 elapsed, loss: 4.775319e-06\n",
      "step: 38610 train: 0.08077883720397949 elapsed, loss: 2.393493e-06\n",
      "step: 38620 train: 0.07851362228393555 elapsed, loss: 2.1425035e-06\n",
      "step: 38630 train: 0.08211326599121094 elapsed, loss: 2.3539128e-06\n",
      "step: 38640 train: 0.0849754810333252 elapsed, loss: 1.1869695e-06\n",
      "step: 38650 train: 0.08540654182434082 elapsed, loss: 9.9308545e-06\n",
      "step: 38660 train: 0.08602619171142578 elapsed, loss: 7.799723e-06\n",
      "step: 38670 train: 0.08250021934509277 elapsed, loss: 5.6991603e-06\n",
      "step: 38680 train: 0.08600473403930664 elapsed, loss: 3.0551842e-06\n",
      "step: 38690 train: 0.08707952499389648 elapsed, loss: 0.0008034656\n",
      "step: 38700 train: 0.08119678497314453 elapsed, loss: 0.000596011\n",
      "step: 38710 train: 0.08515715599060059 elapsed, loss: 4.9635146e-05\n",
      "step: 38720 train: 0.0806574821472168 elapsed, loss: 2.8923972e-05\n",
      "step: 38730 train: 0.08244538307189941 elapsed, loss: 3.3242097e-05\n",
      "step: 38740 train: 0.08271288871765137 elapsed, loss: 1.7839739e-05\n",
      "step: 38750 train: 0.07677102088928223 elapsed, loss: 1.2424403e-05\n",
      "step: 38760 train: 0.0816950798034668 elapsed, loss: 2.598531e-05\n",
      "step: 38770 train: 0.0856938362121582 elapsed, loss: 6.981566e-06\n",
      "step: 38780 train: 0.08003592491149902 elapsed, loss: 5.2917385e-06\n",
      "step: 38790 train: 0.08280563354492188 elapsed, loss: 5.1154802e-06\n",
      "step: 38800 train: 0.07871031761169434 elapsed, loss: 3.90873e-06\n",
      "step: 38810 train: 0.08040547370910645 elapsed, loss: 2.7804547e-06\n",
      "step: 38820 train: 0.08248257637023926 elapsed, loss: 1.2171138e-05\n",
      "step: 38830 train: 0.08386492729187012 elapsed, loss: 2.1811534e-06\n",
      "step: 38840 train: 0.08503961563110352 elapsed, loss: 1.7471561e-06\n",
      "step: 38850 train: 0.08579397201538086 elapsed, loss: 1.5594937e-06\n",
      "step: 38860 train: 0.08542156219482422 elapsed, loss: 1.1362124e-06\n",
      "step: 38870 train: 0.07745218276977539 elapsed, loss: 1.6642713e-06\n",
      "step: 38880 train: 0.08333444595336914 elapsed, loss: 1.4551902e-06\n",
      "step: 38890 train: 0.08452844619750977 elapsed, loss: 9.383068e-07\n",
      "step: 38900 train: 0.07811903953552246 elapsed, loss: 1.5101376e-06\n",
      "step: 38910 train: 0.0826253890991211 elapsed, loss: 1.0053618e-06\n",
      "step: 38920 train: 0.08039188385009766 elapsed, loss: 1.1967406e-06\n",
      "step: 38930 train: 0.08224773406982422 elapsed, loss: 1.1865039e-06\n",
      "step: 38940 train: 0.08222031593322754 elapsed, loss: 8.80099e-07\n",
      "step: 38950 train: 0.08025860786437988 elapsed, loss: 1.0947685e-06\n",
      "step: 38960 train: 0.08099579811096191 elapsed, loss: 1.1138609e-06\n",
      "step: 38970 train: 0.07899165153503418 elapsed, loss: 1.4123492e-06\n",
      "step: 38980 train: 0.08113908767700195 elapsed, loss: 9.527423e-07\n",
      "step: 38990 train: 0.08139991760253906 elapsed, loss: 1.3955854e-06\n",
      "step: 39000 train: 0.0792388916015625 elapsed, loss: 1.657752e-06\n",
      "step: 39010 train: 0.07844066619873047 elapsed, loss: 1.3480881e-06\n",
      "step: 39020 train: 0.0761728286743164 elapsed, loss: 1.5664828e-06\n",
      "step: 39030 train: 0.07935881614685059 elapsed, loss: 1.1594955e-06\n",
      "step: 39040 train: 0.07661151885986328 elapsed, loss: 1.5711391e-06\n",
      "step: 39050 train: 0.07801342010498047 elapsed, loss: 1.2493681e-06\n",
      "step: 39060 train: 0.08080625534057617 elapsed, loss: 1.4374948e-06\n",
      "step: 39070 train: 0.07985663414001465 elapsed, loss: 1.6116519e-06\n",
      "step: 39080 train: 0.08444094657897949 elapsed, loss: 1.1608925e-06\n",
      "step: 39090 train: 0.08895468711853027 elapsed, loss: 1.0128124e-06\n",
      "step: 39100 train: 0.08633184432983398 elapsed, loss: 8.437778e-07\n",
      "step: 39110 train: 0.08365631103515625 elapsed, loss: 1.6428503e-06\n",
      "step: 39120 train: 0.08568882942199707 elapsed, loss: 1.0910435e-06\n",
      "step: 39130 train: 0.08400893211364746 elapsed, loss: 1.0738139e-06\n",
      "step: 39140 train: 0.08446455001831055 elapsed, loss: 1.0807989e-06\n",
      "step: 39150 train: 0.07964324951171875 elapsed, loss: 2.1806886e-06\n",
      "step: 39160 train: 0.08291196823120117 elapsed, loss: 1.4356311e-06\n",
      "step: 39170 train: 0.08536100387573242 elapsed, loss: 1.2409862e-06\n",
      "step: 39180 train: 0.08356761932373047 elapsed, loss: 2.348791e-06\n",
      "step: 39190 train: 0.0812525749206543 elapsed, loss: 1.4151433e-06\n",
      "step: 39200 train: 0.08332109451293945 elapsed, loss: 1.7597315e-06\n",
      "step: 39210 train: 0.08243179321289062 elapsed, loss: 1.944597e-06\n",
      "step: 39220 train: 0.0815572738647461 elapsed, loss: 1.6321404e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 39230 train: 0.07876753807067871 elapsed, loss: 2.3795255e-06\n",
      "step: 39240 train: 0.08690309524536133 elapsed, loss: 1.4966338e-06\n",
      "step: 39250 train: 0.08354640007019043 elapsed, loss: 1.7764944e-06\n",
      "step: 39260 train: 0.08579492568969727 elapsed, loss: 1.817008e-06\n",
      "step: 39270 train: 0.08439040184020996 elapsed, loss: 2.1648552e-06\n",
      "step: 39280 train: 0.08498644828796387 elapsed, loss: 1.4197998e-06\n",
      "step: 39290 train: 0.07910466194152832 elapsed, loss: 1.950187e-06\n",
      "step: 39300 train: 0.08298730850219727 elapsed, loss: 1.7504169e-06\n",
      "step: 39310 train: 0.08347558975219727 elapsed, loss: 1.7634572e-06\n",
      "step: 39320 train: 0.0818781852722168 elapsed, loss: 2.243551e-06\n",
      "step: 39330 train: 0.08444023132324219 elapsed, loss: 1.7955873e-06\n",
      "step: 39340 train: 0.08115983009338379 elapsed, loss: 1.954378e-06\n",
      "step: 39350 train: 0.08124852180480957 elapsed, loss: 4.054961e-06\n",
      "step: 39360 train: 0.08288717269897461 elapsed, loss: 1.8435363e-06\n",
      "step: 39370 train: 0.08280467987060547 elapsed, loss: 1.6060609e-06\n",
      "step: 39380 train: 0.08199882507324219 elapsed, loss: 1.8519326e-06\n",
      "step: 39390 train: 0.08086967468261719 elapsed, loss: 2.2598506e-06\n",
      "step: 39400 train: 0.07522368431091309 elapsed, loss: 2.149489e-06\n",
      "step: 39410 train: 0.08170437812805176 elapsed, loss: 2.447034e-06\n",
      "step: 39420 train: 0.08356714248657227 elapsed, loss: 1.8808031e-06\n",
      "step: 39430 train: 0.08430957794189453 elapsed, loss: 2.2137501e-06\n",
      "step: 39440 train: 0.08774185180664062 elapsed, loss: 1.4961681e-06\n",
      "step: 39450 train: 0.08428192138671875 elapsed, loss: 1.5948879e-06\n",
      "step: 39460 train: 0.07990550994873047 elapsed, loss: 5.8853993e-06\n",
      "step: 39470 train: 0.07968783378601074 elapsed, loss: 4.602544e-06\n",
      "step: 39480 train: 0.08233213424682617 elapsed, loss: 2.0405241e-06\n",
      "step: 39490 train: 0.0841531753540039 elapsed, loss: 2.4973372e-06\n",
      "step: 39500 train: 0.08023738861083984 elapsed, loss: 3.0994352e-06\n",
      "step: 39510 train: 0.08178853988647461 elapsed, loss: 2.305485e-06\n",
      "step: 39520 train: 0.07925581932067871 elapsed, loss: 3.3653257e-06\n",
      "step: 39530 train: 0.08205103874206543 elapsed, loss: 1.9143308e-06\n",
      "step: 39540 train: 0.08661103248596191 elapsed, loss: 3.3918002e-06\n",
      "step: 39550 train: 0.08281612396240234 elapsed, loss: 0.0008219672\n",
      "step: 39560 train: 0.08438658714294434 elapsed, loss: 7.005118e-05\n",
      "step: 39570 train: 0.08037686347961426 elapsed, loss: 5.6147204e-05\n",
      "step: 39580 train: 0.07695960998535156 elapsed, loss: 3.0956493e-05\n",
      "step: 39590 train: 0.07676887512207031 elapsed, loss: 1.9446059e-05\n",
      "step: 39600 train: 0.0844109058380127 elapsed, loss: 1.1458271e-05\n",
      "step: 39610 train: 0.08455300331115723 elapsed, loss: 9.852797e-06\n",
      "step: 39620 train: 0.08761143684387207 elapsed, loss: 4.915498e-06\n",
      "step: 39630 train: 0.08038210868835449 elapsed, loss: 7.4519257e-06\n",
      "step: 39640 train: 0.0816640853881836 elapsed, loss: 7.884057e-06\n",
      "step: 39650 train: 0.08215212821960449 elapsed, loss: 2.915961e-06\n",
      "step: 39660 train: 0.08597111701965332 elapsed, loss: 2.1392448e-06\n",
      "step: 39670 train: 0.0844273567199707 elapsed, loss: 1.954843e-06\n",
      "step: 39680 train: 0.08175921440124512 elapsed, loss: 2.16532e-06\n",
      "step: 39690 train: 0.07874894142150879 elapsed, loss: 2.1997782e-06\n",
      "step: 39700 train: 0.08022022247314453 elapsed, loss: 2.7497247e-06\n",
      "step: 39710 train: 0.08123517036437988 elapsed, loss: 1.6498357e-06\n",
      "step: 39720 train: 0.08104920387268066 elapsed, loss: 2.0577538e-06\n",
      "step: 39730 train: 0.07940793037414551 elapsed, loss: 2.378592e-06\n",
      "step: 39740 train: 0.08070540428161621 elapsed, loss: 2.1513513e-06\n",
      "step: 39750 train: 0.08701705932617188 elapsed, loss: 1.3173546e-06\n",
      "step: 39760 train: 0.08333349227905273 elapsed, loss: 1.50455e-06\n",
      "step: 39770 train: 0.08423161506652832 elapsed, loss: 1.6358655e-06\n",
      "step: 39780 train: 0.08273458480834961 elapsed, loss: 1.6759127e-06\n",
      "step: 39790 train: 0.08540773391723633 elapsed, loss: 1.4817326e-06\n",
      "step: 39800 train: 0.07863879203796387 elapsed, loss: 2.417242e-06\n",
      "step: 39810 train: 0.08210515975952148 elapsed, loss: 0.016530132\n",
      "step: 39820 train: 0.08627533912658691 elapsed, loss: 6.920556e-05\n",
      "step: 39830 train: 0.08152341842651367 elapsed, loss: 3.425208e-05\n",
      "step: 39840 train: 0.0754251480102539 elapsed, loss: 0.000100471494\n",
      "step: 39850 train: 0.08103370666503906 elapsed, loss: 2.2550563e-05\n",
      "step: 39860 train: 0.08295536041259766 elapsed, loss: 2.1354248e-05\n",
      "step: 39870 train: 0.080810546875 elapsed, loss: 1.000544e-05\n",
      "step: 39880 train: 0.08191895484924316 elapsed, loss: 8.016243e-06\n",
      "step: 39890 train: 0.08269238471984863 elapsed, loss: 6.010253e-06\n",
      "step: 39900 train: 0.07976531982421875 elapsed, loss: 4.724103e-06\n",
      "step: 39910 train: 0.08504247665405273 elapsed, loss: 3.8412213e-06\n",
      "step: 39920 train: 0.08933496475219727 elapsed, loss: 2.8796399e-06\n",
      "step: 39930 train: 0.08127331733703613 elapsed, loss: 3.1413397e-06\n",
      "step: 39940 train: 0.07983875274658203 elapsed, loss: 2.3837156e-06\n",
      "step: 39950 train: 0.08280014991760254 elapsed, loss: 1.7206161e-06\n",
      "step: 39960 train: 0.08057379722595215 elapsed, loss: 2.5918644e-06\n",
      "step: 39970 train: 0.08287334442138672 elapsed, loss: 1.419334e-06\n",
      "step: 39980 train: 0.08766555786132812 elapsed, loss: 1.1385404e-06\n",
      "step: 39990 train: 0.07624077796936035 elapsed, loss: 2.3748626e-06\n",
      "step: 40000 train: 0.08638739585876465 elapsed, loss: 1.139472e-06\n",
      "step: 40010 train: 0.08173608779907227 elapsed, loss: 1.3629887e-06\n",
      "step: 40020 train: 0.08444046974182129 elapsed, loss: 1.9115353e-06\n",
      "step: 40030 train: 0.08319473266601562 elapsed, loss: 1.6898827e-06\n",
      "step: 40040 train: 0.08229541778564453 elapsed, loss: 1.4067614e-06\n",
      "step: 40050 train: 0.07767200469970703 elapsed, loss: 2.2747513e-06\n",
      "step: 40060 train: 0.08356475830078125 elapsed, loss: 1.2931393e-06\n",
      "step: 40070 train: 0.08633279800415039 elapsed, loss: 4.0065283e-06\n",
      "step: 40080 train: 0.07848024368286133 elapsed, loss: 1.7592653e-06\n",
      "step: 40090 train: 0.08246994018554688 elapsed, loss: 1.56462e-06\n",
      "step: 40100 train: 0.07966208457946777 elapsed, loss: 1.8221303e-06\n",
      "step: 40110 train: 0.08263087272644043 elapsed, loss: 1.7220132e-06\n",
      "step: 40120 train: 0.08619928359985352 elapsed, loss: 1.5310925e-06\n",
      "step: 40130 train: 0.08228421211242676 elapsed, loss: 1.400242e-06\n",
      "step: 40140 train: 0.0794687271118164 elapsed, loss: 0.0010328473\n",
      "step: 40150 train: 0.08366823196411133 elapsed, loss: 3.1494732e-05\n",
      "step: 40160 train: 0.0822300910949707 elapsed, loss: 1.716581e-05\n",
      "step: 40170 train: 0.07866930961608887 elapsed, loss: 2.417956e-05\n",
      "step: 40180 train: 0.08109593391418457 elapsed, loss: 1.9616753e-05\n",
      "step: 40190 train: 0.07723569869995117 elapsed, loss: 1.1421575e-05\n",
      "step: 40200 train: 0.08247566223144531 elapsed, loss: 8.711932e-06\n",
      "step: 40210 train: 0.08537483215332031 elapsed, loss: 7.983186e-06\n",
      "step: 40220 train: 0.08780956268310547 elapsed, loss: 3.6265587e-06\n",
      "step: 40230 train: 0.07933616638183594 elapsed, loss: 7.805345e-06\n",
      "step: 40240 train: 0.08770942687988281 elapsed, loss: 3.0202532e-06\n",
      "step: 40250 train: 0.08199334144592285 elapsed, loss: 2.3348207e-06\n",
      "step: 40260 train: 0.0810856819152832 elapsed, loss: 2.0256234e-06\n",
      "step: 40270 train: 0.0757913589477539 elapsed, loss: 3.116657e-06\n",
      "step: 40280 train: 0.07876753807067871 elapsed, loss: 2.059616e-06\n",
      "step: 40290 train: 0.08100366592407227 elapsed, loss: 1.585571e-06\n",
      "step: 40300 train: 0.08735871315002441 elapsed, loss: 1.2349326e-06\n",
      "step: 40310 train: 0.0792231559753418 elapsed, loss: 1.7220134e-06\n",
      "step: 40320 train: 0.0805213451385498 elapsed, loss: 1.4323728e-06\n",
      "step: 40330 train: 0.07852673530578613 elapsed, loss: 1.2381922e-06\n",
      "step: 40340 train: 0.09208893775939941 elapsed, loss: 1.013278e-06\n",
      "step: 40350 train: 0.07746577262878418 elapsed, loss: 2.651003e-06\n",
      "step: 40360 train: 0.08321189880371094 elapsed, loss: 1.3159577e-06\n",
      "step: 40370 train: 0.07770299911499023 elapsed, loss: 1.8398255e-06\n",
      "step: 40380 train: 0.0895991325378418 elapsed, loss: 1.2493676e-06\n",
      "step: 40390 train: 0.09419703483581543 elapsed, loss: 1.3974482e-06\n",
      "step: 40400 train: 0.08519625663757324 elapsed, loss: 1.548321e-06\n",
      "step: 40410 train: 0.0812382698059082 elapsed, loss: 1.484061e-06\n",
      "step: 40420 train: 0.07820248603820801 elapsed, loss: 1.9790577e-06\n",
      "step: 40430 train: 0.07761573791503906 elapsed, loss: 1.3629892e-06\n",
      "step: 40440 train: 0.07630276679992676 elapsed, loss: 2.1695123e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 40450 train: 0.07909536361694336 elapsed, loss: 2.0340053e-06\n",
      "step: 40460 train: 0.0834047794342041 elapsed, loss: 1.4207312e-06\n",
      "step: 40470 train: 0.08268308639526367 elapsed, loss: 1.6265531e-06\n",
      "step: 40480 train: 0.08334207534790039 elapsed, loss: 1.5851091e-06\n",
      "step: 40490 train: 0.08489155769348145 elapsed, loss: 1.314095e-06\n",
      "step: 40500 train: 0.08030581474304199 elapsed, loss: 1.9823174e-06\n",
      "step: 40510 train: 0.08351802825927734 elapsed, loss: 3.626824e-05\n",
      "step: 40520 train: 0.08568215370178223 elapsed, loss: 0.00084813585\n",
      "step: 40530 train: 0.0824587345123291 elapsed, loss: 5.421035e-05\n",
      "step: 40540 train: 0.08748459815979004 elapsed, loss: 2.1980157e-05\n",
      "step: 40550 train: 0.08132767677307129 elapsed, loss: 1.6518858e-05\n",
      "step: 40560 train: 0.07806158065795898 elapsed, loss: 2.7123791e-05\n",
      "step: 40570 train: 0.07659029960632324 elapsed, loss: 0.0031450978\n",
      "step: 40580 train: 0.08224844932556152 elapsed, loss: 0.00011977379\n",
      "step: 40590 train: 0.07778739929199219 elapsed, loss: 2.6954385e-05\n",
      "step: 40600 train: 0.08505749702453613 elapsed, loss: 1.749181e-05\n",
      "step: 40610 train: 0.08300566673278809 elapsed, loss: 1.3063532e-05\n",
      "step: 40620 train: 0.09168076515197754 elapsed, loss: 8.423212e-06\n",
      "step: 40630 train: 0.08630800247192383 elapsed, loss: 6.8771606e-06\n",
      "step: 40640 train: 0.08435845375061035 elapsed, loss: 5.2907903e-06\n",
      "step: 40650 train: 0.08342289924621582 elapsed, loss: 4.36928e-06\n",
      "step: 40660 train: 0.08044123649597168 elapsed, loss: 3.239584e-06\n",
      "step: 40670 train: 0.08610415458679199 elapsed, loss: 3.0919807e-06\n",
      "step: 40680 train: 0.08596992492675781 elapsed, loss: 2.1927917e-06\n",
      "step: 40690 train: 0.08566403388977051 elapsed, loss: 2.1886035e-06\n",
      "step: 40700 train: 0.09091377258300781 elapsed, loss: 1.4044324e-06\n",
      "step: 40710 train: 0.08617019653320312 elapsed, loss: 1.3411029e-06\n",
      "step: 40720 train: 0.08702969551086426 elapsed, loss: 1.1469226e-06\n",
      "step: 40730 train: 0.08166193962097168 elapsed, loss: 2.0568218e-06\n",
      "step: 40740 train: 0.08197450637817383 elapsed, loss: 1.3951196e-06\n",
      "step: 40750 train: 0.08352327346801758 elapsed, loss: 1.0435458e-06\n",
      "step: 40760 train: 0.08712625503540039 elapsed, loss: 8.4331197e-07\n",
      "step: 40770 train: 0.08022403717041016 elapsed, loss: 1.5133975e-06\n",
      "step: 40780 train: 0.08127260208129883 elapsed, loss: 1.3192171e-06\n",
      "step: 40790 train: 0.08480405807495117 elapsed, loss: 1.1147922e-06\n",
      "step: 40800 train: 0.08310294151306152 elapsed, loss: 1.4952361e-06\n",
      "step: 40810 train: 0.08158707618713379 elapsed, loss: 1.2149076e-06\n",
      "step: 40820 train: 0.08840394020080566 elapsed, loss: 9.676432e-07\n",
      "step: 40830 train: 0.08047056198120117 elapsed, loss: 1.6633398e-06\n",
      "step: 40840 train: 0.0839376449584961 elapsed, loss: 9.234057e-07\n",
      "step: 40850 train: 0.0806424617767334 elapsed, loss: 1.7103719e-06\n",
      "step: 40860 train: 0.08583474159240723 elapsed, loss: 8.889467e-07\n",
      "step: 40870 train: 0.08141589164733887 elapsed, loss: 1.3303905e-06\n",
      "step: 40880 train: 0.08228898048400879 elapsed, loss: 1.0961651e-06\n",
      "step: 40890 train: 0.08230733871459961 elapsed, loss: 1.5236402e-06\n",
      "step: 40900 train: 0.08238410949707031 elapsed, loss: 1.5068783e-06\n",
      "step: 40910 train: 0.08188390731811523 elapsed, loss: 2.427916e-06\n",
      "step: 40920 train: 0.0881192684173584 elapsed, loss: 0.021164495\n",
      "step: 40930 train: 0.07863402366638184 elapsed, loss: 0.00017677929\n",
      "step: 40940 train: 0.07703185081481934 elapsed, loss: 8.127693e-05\n",
      "step: 40950 train: 0.08036375045776367 elapsed, loss: 6.604386e-05\n",
      "step: 40960 train: 0.08412623405456543 elapsed, loss: 3.3766657e-05\n",
      "step: 40970 train: 0.07994318008422852 elapsed, loss: 1.3905152e-05\n",
      "step: 40980 train: 0.07781124114990234 elapsed, loss: 1.27015355e-05\n",
      "step: 40990 train: 0.08237290382385254 elapsed, loss: 8.488441e-06\n",
      "step: 41000 train: 0.08808755874633789 elapsed, loss: 6.5662393e-06\n",
      "step: 41010 train: 0.07787489891052246 elapsed, loss: 7.048202e-06\n",
      "step: 41020 train: 0.07880640029907227 elapsed, loss: 7.0877095e-06\n",
      "step: 41030 train: 0.08329176902770996 elapsed, loss: 3.1688069e-06\n",
      "step: 41040 train: 0.08460736274719238 elapsed, loss: 2.2747508e-06\n",
      "step: 41050 train: 0.08392190933227539 elapsed, loss: 2.0097905e-06\n",
      "step: 41060 train: 0.08100199699401855 elapsed, loss: 1.8421526e-06\n",
      "step: 41070 train: 0.0804448127746582 elapsed, loss: 1.5781242e-06\n",
      "step: 41080 train: 0.07912516593933105 elapsed, loss: 1.8742827e-06\n",
      "step: 41090 train: 0.07982563972473145 elapsed, loss: 2.106648e-06\n",
      "step: 41100 train: 0.08612179756164551 elapsed, loss: 1.282895e-06\n",
      "step: 41110 train: 0.07691597938537598 elapsed, loss: 1.9958168e-06\n",
      "step: 41120 train: 0.0841822624206543 elapsed, loss: 1.1189832e-06\n",
      "step: 41130 train: 0.08094668388366699 elapsed, loss: 1.2652007e-06\n",
      "step: 41140 train: 0.07791948318481445 elapsed, loss: 1.8305119e-06\n",
      "step: 41150 train: 0.0832669734954834 elapsed, loss: 1.0030337e-06\n",
      "step: 41160 train: 0.0821375846862793 elapsed, loss: 1.5385431e-06\n",
      "step: 41170 train: 0.08215093612670898 elapsed, loss: 9.3923813e-07\n",
      "step: 41180 train: 0.07750129699707031 elapsed, loss: 1.928766e-06\n",
      "step: 41190 train: 0.08152580261230469 elapsed, loss: 1.2279456e-06\n",
      "step: 41200 train: 0.08426451683044434 elapsed, loss: 1.0854556e-06\n",
      "step: 41210 train: 0.08224964141845703 elapsed, loss: 1.223755e-06\n",
      "step: 41220 train: 0.07857179641723633 elapsed, loss: 1.328996e-06\n",
      "step: 41230 train: 0.08063554763793945 elapsed, loss: 1.0603098e-06\n",
      "step: 41240 train: 0.07952427864074707 elapsed, loss: 1.6740502e-06\n",
      "step: 41250 train: 0.07896113395690918 elapsed, loss: 1.2763762e-06\n",
      "step: 41260 train: 0.07986569404602051 elapsed, loss: 2.9792939e-06\n",
      "step: 41270 train: 0.08329510688781738 elapsed, loss: 1.1692746e-06\n",
      "step: 41280 train: 0.07786965370178223 elapsed, loss: 1.8835971e-06\n",
      "step: 41290 train: 0.0890963077545166 elapsed, loss: 1.1422655e-06\n",
      "step: 41300 train: 0.08292818069458008 elapsed, loss: 2.3506532e-06\n",
      "step: 41310 train: 0.07850074768066406 elapsed, loss: 2.0121192e-06\n",
      "step: 41320 train: 0.07823348045349121 elapsed, loss: 1.949721e-06\n",
      "step: 41330 train: 0.08116459846496582 elapsed, loss: 1.459381e-06\n",
      "step: 41340 train: 0.07764172554016113 elapsed, loss: 2.185811e-06\n",
      "step: 41350 train: 0.07366704940795898 elapsed, loss: 2.2533316e-06\n",
      "step: 41360 train: 0.08282613754272461 elapsed, loss: 1.3215454e-06\n",
      "step: 41370 train: 0.0842123031616211 elapsed, loss: 1.4337695e-06\n",
      "step: 41380 train: 0.0802309513092041 elapsed, loss: 1.6852259e-06\n",
      "step: 41390 train: 0.0791616439819336 elapsed, loss: 2.084762e-06\n",
      "step: 41400 train: 0.08148360252380371 elapsed, loss: 1.975798e-06\n",
      "step: 41410 train: 0.07989382743835449 elapsed, loss: 1.6051323e-06\n",
      "step: 41420 train: 0.08201932907104492 elapsed, loss: 1.9818508e-06\n",
      "step: 41430 train: 0.08113861083984375 elapsed, loss: 2.5317959e-06\n",
      "step: 41440 train: 0.08393979072570801 elapsed, loss: 0.09175647\n",
      "step: 41450 train: 0.08398795127868652 elapsed, loss: 0.00036665378\n",
      "step: 41460 train: 0.08215093612670898 elapsed, loss: 6.764628e-05\n",
      "step: 41470 train: 0.08322024345397949 elapsed, loss: 3.0751376e-05\n",
      "step: 41480 train: 0.08175492286682129 elapsed, loss: 2.8835526e-05\n",
      "step: 41490 train: 0.08577203750610352 elapsed, loss: 2.1108552e-05\n",
      "step: 41500 train: 0.07903909683227539 elapsed, loss: 1.7487157e-05\n",
      "step: 41510 train: 0.08313798904418945 elapsed, loss: 1.2233868e-05\n",
      "step: 41520 train: 0.07850933074951172 elapsed, loss: 1.25074075e-05\n",
      "step: 41530 train: 0.08399486541748047 elapsed, loss: 6.980641e-06\n",
      "step: 41540 train: 0.08269977569580078 elapsed, loss: 1.522906e-05\n",
      "step: 41550 train: 0.08617854118347168 elapsed, loss: 3.490586e-06\n",
      "step: 41560 train: 0.08298969268798828 elapsed, loss: 2.952733e-06\n",
      "step: 41570 train: 0.08107757568359375 elapsed, loss: 2.5788263e-06\n",
      "step: 41580 train: 0.07799243927001953 elapsed, loss: 3.0537997e-06\n",
      "step: 41590 train: 0.08900904655456543 elapsed, loss: 1.5450602e-06\n",
      "step: 41600 train: 0.08381938934326172 elapsed, loss: 1.5422684e-06\n",
      "step: 41610 train: 0.08110499382019043 elapsed, loss: 1.5157257e-06\n",
      "step: 41620 train: 0.0843658447265625 elapsed, loss: 1.1399371e-06\n",
      "step: 41630 train: 0.08573460578918457 elapsed, loss: 9.91392e-07\n",
      "step: 41640 train: 0.07708048820495605 elapsed, loss: 2.8582194e-06\n",
      "step: 41650 train: 0.07961869239807129 elapsed, loss: 1.3993106e-06\n",
      "step: 41660 train: 0.07938051223754883 elapsed, loss: 2.0633388e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41670 train: 0.08672070503234863 elapsed, loss: 1.0845242e-06\n",
      "step: 41680 train: 0.08300232887268066 elapsed, loss: 2.3124699e-06\n",
      "step: 41690 train: 0.08280634880065918 elapsed, loss: 2.2356307e-06\n",
      "step: 41700 train: 0.08645296096801758 elapsed, loss: 1.0649663e-06\n",
      "step: 41710 train: 0.08178853988647461 elapsed, loss: 1.280567e-06\n",
      "step: 41720 train: 0.07919597625732422 elapsed, loss: 2.1550763e-06\n",
      "step: 41730 train: 0.08407831192016602 elapsed, loss: 1.4011734e-06\n",
      "step: 41740 train: 0.09038734436035156 elapsed, loss: 1.0528593e-06\n",
      "step: 41750 train: 0.0839226245880127 elapsed, loss: 2.739012e-06\n",
      "step: 41760 train: 0.08048510551452637 elapsed, loss: 2.0135167e-06\n",
      "step: 41770 train: 0.08373904228210449 elapsed, loss: 7.289884e-06\n",
      "step: 41780 train: 0.08176493644714355 elapsed, loss: 2.9634598e-06\n",
      "step: 41790 train: 0.07652544975280762 elapsed, loss: 2.7958245e-06\n",
      "step: 41800 train: 0.08003664016723633 elapsed, loss: 2.276614e-06\n",
      "step: 41810 train: 0.07996582984924316 elapsed, loss: 2.055889e-06\n",
      "step: 41820 train: 0.07752728462219238 elapsed, loss: 1.5324896e-06\n",
      "step: 41830 train: 0.07469034194946289 elapsed, loss: 2.033074e-06\n",
      "step: 41840 train: 0.07697749137878418 elapsed, loss: 1.4519301e-06\n",
      "step: 41850 train: 0.08333659172058105 elapsed, loss: 1.6791726e-06\n",
      "step: 41860 train: 0.07842826843261719 elapsed, loss: 1.8957046e-06\n",
      "step: 41870 train: 0.07499909400939941 elapsed, loss: 2.836802e-06\n",
      "step: 41880 train: 0.08394074440002441 elapsed, loss: 1.5092066e-06\n",
      "step: 41890 train: 0.08114433288574219 elapsed, loss: 0.0028251265\n",
      "step: 41900 train: 0.08103370666503906 elapsed, loss: 4.405291e-05\n",
      "step: 41910 train: 0.08594202995300293 elapsed, loss: 2.777452e-05\n",
      "step: 41920 train: 0.08281087875366211 elapsed, loss: 2.0213407e-05\n",
      "step: 41930 train: 0.08053898811340332 elapsed, loss: 9.381585e-06\n",
      "step: 41940 train: 0.08289241790771484 elapsed, loss: 7.1165286e-06\n",
      "step: 41950 train: 0.0864567756652832 elapsed, loss: 4.982525e-06\n",
      "step: 41960 train: 0.08390307426452637 elapsed, loss: 6.4111036e-06\n",
      "step: 41970 train: 0.08291125297546387 elapsed, loss: 0.0008513986\n",
      "step: 41980 train: 0.08015131950378418 elapsed, loss: 6.857991e-05\n",
      "step: 41990 train: 0.08433771133422852 elapsed, loss: 4.766104e-05\n",
      "step: 42000 train: 0.0822141170501709 elapsed, loss: 2.7065773e-05\n",
      "step: 42010 train: 0.07779359817504883 elapsed, loss: 1.6043045e-05\n",
      "step: 42020 train: 0.08387374877929688 elapsed, loss: 1.2714379e-05\n",
      "step: 42030 train: 0.07837343215942383 elapsed, loss: 1.0636017e-05\n",
      "step: 42040 train: 0.0774683952331543 elapsed, loss: 1.4847805e-05\n",
      "step: 42050 train: 0.08208107948303223 elapsed, loss: 9.800107e-06\n",
      "step: 42060 train: 0.08025455474853516 elapsed, loss: 6.934576e-06\n",
      "step: 42070 train: 0.07974505424499512 elapsed, loss: 5.2773116e-06\n",
      "step: 42080 train: 0.08152270317077637 elapsed, loss: 4.0931473e-06\n",
      "step: 42090 train: 0.07901191711425781 elapsed, loss: 3.398387e-06\n",
      "step: 42100 train: 0.07908177375793457 elapsed, loss: 2.9639273e-06\n",
      "step: 42110 train: 0.09015297889709473 elapsed, loss: 1.5557714e-06\n",
      "step: 42120 train: 0.07871460914611816 elapsed, loss: 2.1611297e-06\n",
      "step: 42130 train: 0.0854649543762207 elapsed, loss: 1.2945371e-06\n",
      "step: 42140 train: 0.08377552032470703 elapsed, loss: 1.2880107e-06\n",
      "step: 42150 train: 0.08137941360473633 elapsed, loss: 1.6954702e-06\n",
      "step: 42160 train: 0.08211088180541992 elapsed, loss: 1.0784706e-06\n",
      "step: 42170 train: 0.08248209953308105 elapsed, loss: 0.00022558257\n",
      "step: 42180 train: 0.08172774314880371 elapsed, loss: 0.0002447446\n",
      "step: 42190 train: 0.08402395248413086 elapsed, loss: 0.00029925778\n",
      "step: 42200 train: 0.07902741432189941 elapsed, loss: 0.00017494027\n",
      "step: 42210 train: 0.08319997787475586 elapsed, loss: 6.493171e-05\n",
      "step: 42220 train: 0.07995152473449707 elapsed, loss: 2.2893317e-05\n",
      "step: 42230 train: 0.07661056518554688 elapsed, loss: 2.0355099e-05\n",
      "step: 42240 train: 0.08735084533691406 elapsed, loss: 1.0155304e-05\n",
      "step: 42250 train: 0.08986663818359375 elapsed, loss: 7.3233587e-06\n",
      "step: 42260 train: 0.08882904052734375 elapsed, loss: 6.2904214e-06\n",
      "step: 42270 train: 0.08029460906982422 elapsed, loss: 4.0274876e-06\n",
      "step: 42280 train: 0.08087682723999023 elapsed, loss: 4.9834493e-06\n",
      "step: 42290 train: 0.08653736114501953 elapsed, loss: 2.52853e-06\n",
      "step: 42300 train: 0.07732295989990234 elapsed, loss: 4.1252742e-06\n",
      "step: 42310 train: 0.08149886131286621 elapsed, loss: 1.5972155e-06\n",
      "step: 42320 train: 0.08245158195495605 elapsed, loss: 1.6181698e-06\n",
      "step: 42330 train: 0.08548164367675781 elapsed, loss: 1.0500652e-06\n",
      "step: 42340 train: 0.08595156669616699 elapsed, loss: 1.0216598e-06\n",
      "step: 42350 train: 0.0814211368560791 elapsed, loss: 1.0626381e-06\n",
      "step: 42360 train: 0.07915282249450684 elapsed, loss: 1.4095551e-06\n",
      "step: 42370 train: 0.08503127098083496 elapsed, loss: 8.5681586e-07\n",
      "step: 42380 train: 0.08722281455993652 elapsed, loss: 6.728802e-07\n",
      "step: 42390 train: 0.08297228813171387 elapsed, loss: 9.154894e-07\n",
      "step: 42400 train: 0.08212733268737793 elapsed, loss: 8.2468557e-07\n",
      "step: 42410 train: 0.08686470985412598 elapsed, loss: 7.865014e-07\n",
      "step: 42420 train: 0.08266425132751465 elapsed, loss: 8.582132e-07\n",
      "step: 42430 train: 0.083221435546875 elapsed, loss: 8.6007446e-07\n",
      "step: 42440 train: 0.08098793029785156 elapsed, loss: 1.0468058e-06\n",
      "step: 42450 train: 0.08045601844787598 elapsed, loss: 1.4803352e-06\n",
      "step: 42460 train: 0.08553528785705566 elapsed, loss: 6.617044e-07\n",
      "step: 42470 train: 0.0840616226196289 elapsed, loss: 1.0621725e-06\n",
      "step: 42480 train: 0.08516621589660645 elapsed, loss: 7.497142e-07\n",
      "step: 42490 train: 0.07713913917541504 elapsed, loss: 1.3490194e-06\n",
      "step: 42500 train: 0.07954835891723633 elapsed, loss: 1.2577498e-06\n",
      "step: 42510 train: 0.07693290710449219 elapsed, loss: 1.0323702e-06\n",
      "step: 42520 train: 0.0811758041381836 elapsed, loss: 1.1441289e-06\n",
      "step: 42530 train: 0.08242011070251465 elapsed, loss: 7.865014e-07\n",
      "step: 42540 train: 0.08438968658447266 elapsed, loss: 9.019853e-07\n",
      "step: 42550 train: 0.08476662635803223 elapsed, loss: 8.6659514e-07\n",
      "step: 42560 train: 0.07991814613342285 elapsed, loss: 1.3094384e-06\n",
      "step: 42570 train: 0.08779788017272949 elapsed, loss: 9.18749e-07\n",
      "step: 42580 train: 0.08162593841552734 elapsed, loss: 1.8528606e-06\n",
      "step: 42590 train: 0.08445048332214355 elapsed, loss: 8.8103025e-07\n",
      "step: 42600 train: 0.08441615104675293 elapsed, loss: 1.5115344e-06\n",
      "step: 42610 train: 0.08275890350341797 elapsed, loss: 9.75094e-07\n",
      "step: 42620 train: 0.08393526077270508 elapsed, loss: 1.2968649e-06\n",
      "step: 42630 train: 0.08324837684631348 elapsed, loss: 8.190977e-07\n",
      "step: 42640 train: 0.08071279525756836 elapsed, loss: 1.3671802e-06\n",
      "step: 42650 train: 0.0866401195526123 elapsed, loss: 8.5122826e-07\n",
      "step: 42660 train: 0.07404971122741699 elapsed, loss: 2.4554229e-06\n",
      "step: 42670 train: 0.07915925979614258 elapsed, loss: 2.231911e-06\n",
      "step: 42680 train: 0.08053255081176758 elapsed, loss: 1.2367946e-06\n",
      "step: 42690 train: 0.07951021194458008 elapsed, loss: 3.4732986e-06\n",
      "step: 42700 train: 0.08092331886291504 elapsed, loss: 1.2153749e-06\n",
      "step: 42710 train: 0.07576227188110352 elapsed, loss: 1.8840633e-06\n",
      "step: 42720 train: 0.0807185173034668 elapsed, loss: 1.4016391e-06\n",
      "step: 42730 train: 0.07916569709777832 elapsed, loss: 1.2391235e-06\n",
      "step: 42740 train: 0.07858395576477051 elapsed, loss: 1.3541417e-06\n",
      "step: 42750 train: 0.08156442642211914 elapsed, loss: 1.2554217e-06\n",
      "step: 42760 train: 0.07882881164550781 elapsed, loss: 1.2777734e-06\n",
      "step: 42770 train: 0.08318448066711426 elapsed, loss: 2.7250433e-06\n",
      "step: 42780 train: 0.09187531471252441 elapsed, loss: 0.0025425577\n",
      "step: 42790 train: 0.08148455619812012 elapsed, loss: 0.0001314683\n",
      "step: 42800 train: 0.08493185043334961 elapsed, loss: 6.9379246e-05\n",
      "step: 42810 train: 0.08647370338439941 elapsed, loss: 2.5003203e-05\n",
      "step: 42820 train: 0.07643818855285645 elapsed, loss: 4.7221158e-05\n",
      "step: 42830 train: 0.07874202728271484 elapsed, loss: 3.24151e-05\n",
      "step: 42840 train: 0.08243703842163086 elapsed, loss: 2.2732052e-05\n",
      "step: 42850 train: 0.08047103881835938 elapsed, loss: 1.8313374e-05\n",
      "step: 42860 train: 0.07861685752868652 elapsed, loss: 1.08543045e-05\n",
      "step: 42870 train: 0.08734321594238281 elapsed, loss: 4.979635e-06\n",
      "step: 42880 train: 0.08641242980957031 elapsed, loss: 3.4272473e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 42890 train: 0.08183717727661133 elapsed, loss: 4.517362e-06\n",
      "step: 42900 train: 0.07976770401000977 elapsed, loss: 3.0328433e-06\n",
      "step: 42910 train: 0.08707499504089355 elapsed, loss: 2.114548e-06\n",
      "step: 42920 train: 0.08274006843566895 elapsed, loss: 1.9450642e-06\n",
      "step: 42930 train: 0.08342576026916504 elapsed, loss: 1.2102525e-06\n",
      "step: 42940 train: 0.08202052116394043 elapsed, loss: 1.3029189e-06\n",
      "step: 42950 train: 0.0802912712097168 elapsed, loss: 1.3122321e-06\n",
      "step: 42960 train: 0.08325958251953125 elapsed, loss: 1.2628723e-06\n",
      "step: 42970 train: 0.08464455604553223 elapsed, loss: 1.2400549e-06\n",
      "step: 42980 train: 0.08342385292053223 elapsed, loss: 0.00033444248\n",
      "step: 42990 train: 0.08172392845153809 elapsed, loss: 0.00011773753\n",
      "step: 43000 train: 0.08401274681091309 elapsed, loss: 2.3210148e-05\n",
      "step: 43010 train: 0.07688784599304199 elapsed, loss: 5.3874355e-05\n",
      "step: 43020 train: 0.08118176460266113 elapsed, loss: 1.4024725e-05\n",
      "step: 43030 train: 0.07675337791442871 elapsed, loss: 4.4775064e-05\n",
      "step: 43040 train: 0.08128690719604492 elapsed, loss: 1.0640713e-05\n",
      "step: 43050 train: 0.08191967010498047 elapsed, loss: 7.6162905e-06\n",
      "step: 43060 train: 0.08498311042785645 elapsed, loss: 5.84634e-06\n",
      "step: 43070 train: 0.08397150039672852 elapsed, loss: 5.493357e-06\n",
      "step: 43080 train: 0.07915258407592773 elapsed, loss: 4.4088524e-06\n",
      "step: 43090 train: 0.08298254013061523 elapsed, loss: 3.2264124e-06\n",
      "step: 43100 train: 0.07816219329833984 elapsed, loss: 2.61282e-06\n",
      "step: 43110 train: 0.08090734481811523 elapsed, loss: 1.4519301e-06\n",
      "step: 43120 train: 0.0848538875579834 elapsed, loss: 1.2209628e-06\n",
      "step: 43130 train: 0.08209061622619629 elapsed, loss: 1.2842918e-06\n",
      "step: 43140 train: 0.07744216918945312 elapsed, loss: 1.0686919e-06\n",
      "step: 43150 train: 0.08222222328186035 elapsed, loss: 9.248026e-07\n",
      "step: 43160 train: 0.08361291885375977 elapsed, loss: 9.25734e-07\n",
      "step: 43170 train: 0.07805418968200684 elapsed, loss: 1.1627552e-06\n",
      "step: 43180 train: 0.08382272720336914 elapsed, loss: 1.4500677e-06\n",
      "step: 43190 train: 0.08134937286376953 elapsed, loss: 1.2898806e-06\n",
      "step: 43200 train: 0.08026123046875 elapsed, loss: 1.995355e-06\n",
      "step: 43210 train: 0.08304715156555176 elapsed, loss: 1.0430804e-06\n",
      "step: 43220 train: 0.08166670799255371 elapsed, loss: 0.0001115415\n",
      "step: 43230 train: 0.0826871395111084 elapsed, loss: 0.048222803\n",
      "step: 43240 train: 0.07910513877868652 elapsed, loss: 6.9653615e-05\n",
      "step: 43250 train: 0.08016610145568848 elapsed, loss: 5.820224e-05\n",
      "step: 43260 train: 0.08888936042785645 elapsed, loss: 1.3834404e-05\n",
      "step: 43270 train: 0.09293007850646973 elapsed, loss: 9.222685e-06\n",
      "step: 43280 train: 0.08846473693847656 elapsed, loss: 1.7166407e-05\n",
      "step: 43290 train: 0.08115243911743164 elapsed, loss: 1.6383467e-05\n",
      "step: 43300 train: 0.08368086814880371 elapsed, loss: 7.425796e-06\n",
      "step: 43310 train: 0.08083105087280273 elapsed, loss: 5.4491356e-06\n",
      "step: 43320 train: 0.08245635032653809 elapsed, loss: 4.4391045e-06\n",
      "step: 43330 train: 0.07422685623168945 elapsed, loss: 8.911256e-06\n",
      "step: 43340 train: 0.08447837829589844 elapsed, loss: 2.7548444e-06\n",
      "step: 43350 train: 0.08152008056640625 elapsed, loss: 3.4663663e-06\n",
      "step: 43360 train: 0.0834963321685791 elapsed, loss: 1.7308599e-06\n",
      "step: 43370 train: 0.07623124122619629 elapsed, loss: 3.249838e-06\n",
      "step: 43380 train: 0.0839836597442627 elapsed, loss: 1.431441e-06\n",
      "step: 43390 train: 0.08142876625061035 elapsed, loss: 1.4146774e-06\n",
      "step: 43400 train: 0.07712292671203613 elapsed, loss: 1.2833611e-06\n",
      "step: 43410 train: 0.08562874794006348 elapsed, loss: 1.6442475e-06\n",
      "step: 43420 train: 0.08012127876281738 elapsed, loss: 1.3033847e-06\n",
      "step: 43430 train: 0.08214569091796875 elapsed, loss: 1.0253854e-06\n",
      "step: 43440 train: 0.07845783233642578 elapsed, loss: 1.2828956e-06\n",
      "step: 43450 train: 0.08590555191040039 elapsed, loss: 9.145579e-07\n",
      "step: 43460 train: 0.08037185668945312 elapsed, loss: 1.2535589e-06\n",
      "step: 43470 train: 0.08089613914489746 elapsed, loss: 8.4377774e-07\n",
      "step: 43480 train: 0.0820925235748291 elapsed, loss: 1.3667145e-06\n",
      "step: 43490 train: 0.0808265209197998 elapsed, loss: 1.0798676e-06\n",
      "step: 43500 train: 0.08398890495300293 elapsed, loss: 1.104082e-06\n",
      "step: 43510 train: 0.08082151412963867 elapsed, loss: 1.1674119e-06\n",
      "step: 43520 train: 0.08343172073364258 elapsed, loss: 1.323408e-06\n",
      "step: 43530 train: 0.07955813407897949 elapsed, loss: 9.97911e-07\n",
      "step: 43540 train: 0.07980895042419434 elapsed, loss: 1.4994264e-06\n",
      "step: 43550 train: 0.08333826065063477 elapsed, loss: 1.0929061e-06\n",
      "step: 43560 train: 0.08234357833862305 elapsed, loss: 1.3848698e-06\n",
      "step: 43570 train: 0.08114171028137207 elapsed, loss: 1.5227106e-06\n",
      "step: 43580 train: 0.08876347541809082 elapsed, loss: 9.0803894e-07\n",
      "step: 43590 train: 0.08367323875427246 elapsed, loss: 1.1054788e-06\n",
      "step: 43600 train: 0.0782461166381836 elapsed, loss: 1.4947711e-06\n",
      "step: 43610 train: 0.0813910961151123 elapsed, loss: 1.6270153e-06\n",
      "step: 43620 train: 0.07679915428161621 elapsed, loss: 0.0001779986\n",
      "step: 43630 train: 0.08545517921447754 elapsed, loss: 0.00022588897\n",
      "step: 43640 train: 0.08150291442871094 elapsed, loss: 4.271735e-05\n",
      "step: 43650 train: 0.08177447319030762 elapsed, loss: 2.604447e-05\n",
      "step: 43660 train: 0.0830695629119873 elapsed, loss: 1.4529708e-05\n",
      "step: 43670 train: 0.07897067070007324 elapsed, loss: 3.7140853e-05\n",
      "step: 43680 train: 0.08619523048400879 elapsed, loss: 8.276119e-06\n",
      "step: 43690 train: 0.08034276962280273 elapsed, loss: 9.246058e-06\n",
      "step: 43700 train: 0.07773637771606445 elapsed, loss: 1.9387886e-05\n",
      "step: 43710 train: 0.08047008514404297 elapsed, loss: 6.8539007e-06\n",
      "step: 43720 train: 0.0825357437133789 elapsed, loss: 2.7073443e-06\n",
      "step: 43730 train: 0.08130002021789551 elapsed, loss: 2.498733e-06\n",
      "step: 43740 train: 0.0773935317993164 elapsed, loss: 3.0151493e-06\n",
      "step: 43750 train: 0.0828242301940918 elapsed, loss: 1.7979155e-06\n",
      "step: 43760 train: 0.08102846145629883 elapsed, loss: 1.7206129e-06\n",
      "step: 43770 train: 0.08418631553649902 elapsed, loss: 1.5734587e-06\n",
      "step: 43780 train: 0.08355093002319336 elapsed, loss: 1.2111836e-06\n",
      "step: 43790 train: 0.08173465728759766 elapsed, loss: 1.0868525e-06\n",
      "step: 43800 train: 0.0763864517211914 elapsed, loss: 1.3173536e-06\n",
      "step: 43810 train: 0.08111405372619629 elapsed, loss: 1.2661317e-06\n",
      "step: 43820 train: 0.0791933536529541 elapsed, loss: 2.0321427e-06\n",
      "step: 43830 train: 0.08268284797668457 elapsed, loss: 1.7094367e-06\n",
      "step: 43840 train: 0.07670879364013672 elapsed, loss: 0.00021040221\n",
      "step: 43850 train: 0.0784451961517334 elapsed, loss: 0.0006356742\n",
      "step: 43860 train: 0.08371329307556152 elapsed, loss: 2.820713e-05\n",
      "step: 43870 train: 0.08218574523925781 elapsed, loss: 1.27973135e-05\n",
      "step: 43880 train: 0.07819128036499023 elapsed, loss: 1.2548406e-05\n",
      "step: 43890 train: 0.08244967460632324 elapsed, loss: 8.194586e-06\n",
      "step: 43900 train: 0.08165264129638672 elapsed, loss: 4.1471512e-06\n",
      "step: 43910 train: 0.0825810432434082 elapsed, loss: 3.5981482e-06\n",
      "step: 43920 train: 0.07743573188781738 elapsed, loss: 2.5313284e-06\n",
      "step: 43930 train: 0.0884561538696289 elapsed, loss: 1.5851092e-06\n",
      "step: 43940 train: 0.0885012149810791 elapsed, loss: 1.5972162e-06\n",
      "step: 43950 train: 0.0831756591796875 elapsed, loss: 1.424922e-06\n",
      "step: 43960 train: 0.08006858825683594 elapsed, loss: 1.578587e-06\n",
      "step: 43970 train: 0.08304381370544434 elapsed, loss: 9.918574e-07\n",
      "step: 43980 train: 0.07514405250549316 elapsed, loss: 1.4021032e-06\n",
      "step: 43990 train: 0.08334088325500488 elapsed, loss: 1.1175861e-06\n",
      "step: 44000 train: 0.07675600051879883 elapsed, loss: 0.002813368\n",
      "step: 44010 train: 0.07574629783630371 elapsed, loss: 0.00032001408\n",
      "step: 44020 train: 0.0812067985534668 elapsed, loss: 3.4034576e-05\n",
      "step: 44030 train: 0.07892489433288574 elapsed, loss: 5.033639e-05\n",
      "step: 44040 train: 0.08742237091064453 elapsed, loss: 1.7033939e-05\n",
      "step: 44050 train: 0.08141970634460449 elapsed, loss: 1.1293894e-05\n",
      "step: 44060 train: 0.08551931381225586 elapsed, loss: 1.5117752e-05\n",
      "step: 44070 train: 0.08269858360290527 elapsed, loss: 8.989966e-06\n",
      "step: 44080 train: 0.08189773559570312 elapsed, loss: 1.1027731e-05\n",
      "step: 44090 train: 0.07686376571655273 elapsed, loss: 5.2875525e-06\n",
      "step: 44100 train: 0.08178448677062988 elapsed, loss: 5.485192e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 44110 train: 0.08315324783325195 elapsed, loss: 3.7173509e-06\n",
      "step: 44120 train: 0.0808875560760498 elapsed, loss: 2.9206167e-06\n",
      "step: 44130 train: 0.08175182342529297 elapsed, loss: 2.8442496e-06\n",
      "step: 44140 train: 0.0839691162109375 elapsed, loss: 1.9934914e-06\n",
      "step: 44150 train: 0.0780797004699707 elapsed, loss: 1.6414514e-06\n",
      "step: 44160 train: 0.08067083358764648 elapsed, loss: 1.3541414e-06\n",
      "step: 44170 train: 0.0800631046295166 elapsed, loss: 1.1823126e-06\n",
      "step: 44180 train: 0.08365106582641602 elapsed, loss: 1.0696232e-06\n",
      "step: 44190 train: 0.08068418502807617 elapsed, loss: 1.1739307e-06\n",
      "step: 44200 train: 0.07960343360900879 elapsed, loss: 9.1222967e-07\n",
      "step: 44210 train: 0.07743549346923828 elapsed, loss: 1.3532103e-06\n",
      "step: 44220 train: 0.07742571830749512 elapsed, loss: 9.69506e-07\n",
      "step: 44230 train: 0.08265972137451172 elapsed, loss: 8.950003e-07\n",
      "step: 44240 train: 0.0785224437713623 elapsed, loss: 2.2216655e-06\n",
      "step: 44250 train: 0.07902073860168457 elapsed, loss: 1.304316e-06\n",
      "step: 44260 train: 0.08472180366516113 elapsed, loss: 7.0035424e-07\n",
      "step: 44270 train: 0.07988667488098145 elapsed, loss: 9.327189e-07\n",
      "step: 44280 train: 0.08777022361755371 elapsed, loss: 1.1185175e-06\n",
      "step: 44290 train: 0.08787965774536133 elapsed, loss: 8.47503e-07\n",
      "step: 44300 train: 0.08000731468200684 elapsed, loss: 1.1008201e-06\n",
      "step: 44310 train: 0.07542157173156738 elapsed, loss: 1.3266679e-06\n",
      "step: 44320 train: 0.08275127410888672 elapsed, loss: 1.3788218e-06\n",
      "step: 44330 train: 0.0778963565826416 elapsed, loss: 1.7629909e-06\n",
      "step: 44340 train: 0.08316779136657715 elapsed, loss: 1.3792868e-06\n",
      "step: 44350 train: 0.07951235771179199 elapsed, loss: 1.0561184e-06\n",
      "step: 44360 train: 0.0829627513885498 elapsed, loss: 1.2814983e-06\n",
      "step: 44370 train: 0.08195018768310547 elapsed, loss: 1.2568186e-06\n",
      "step: 44380 train: 0.0820457935333252 elapsed, loss: 1.027248e-06\n",
      "step: 44390 train: 0.07968378067016602 elapsed, loss: 1.2023365e-06\n",
      "step: 44400 train: 0.0788724422454834 elapsed, loss: 1.0821958e-06\n",
      "step: 44410 train: 0.08254408836364746 elapsed, loss: 2.18301e-06\n",
      "step: 44420 train: 0.08329343795776367 elapsed, loss: 1.3681098e-06\n",
      "step: 44430 train: 0.08260655403137207 elapsed, loss: 1.3452941e-06\n",
      "step: 44440 train: 0.0795588493347168 elapsed, loss: 2.1327253e-06\n",
      "step: 44450 train: 0.08325839042663574 elapsed, loss: 9.881326e-07\n",
      "step: 44460 train: 0.08019256591796875 elapsed, loss: 5.3717125e-05\n",
      "step: 44470 train: 0.07956624031066895 elapsed, loss: 4.1136327e-06\n",
      "step: 44480 train: 0.08210945129394531 elapsed, loss: 3.25356e-06\n",
      "step: 44490 train: 0.07326030731201172 elapsed, loss: 4.1373874e-06\n",
      "step: 44500 train: 0.07784867286682129 elapsed, loss: 2.1094413e-06\n",
      "step: 44510 train: 0.08021879196166992 elapsed, loss: 1.4989316e-05\n",
      "step: 44520 train: 0.08335137367248535 elapsed, loss: 1.9762635e-06\n",
      "step: 44530 train: 0.07419514656066895 elapsed, loss: 2.550886e-06\n",
      "step: 44540 train: 0.0778350830078125 elapsed, loss: 1.601873e-06\n",
      "step: 44550 train: 0.07333230972290039 elapsed, loss: 2.5625282e-06\n",
      "step: 44560 train: 0.07672762870788574 elapsed, loss: 2.020035e-06\n",
      "step: 44570 train: 0.08185911178588867 elapsed, loss: 1.1804503e-06\n",
      "step: 44580 train: 0.07947826385498047 elapsed, loss: 1.1548389e-06\n",
      "step: 44590 train: 0.08443331718444824 elapsed, loss: 1.2801016e-06\n",
      "step: 44600 train: 0.0832357406616211 elapsed, loss: 1.661477e-06\n",
      "step: 44610 train: 0.07835841178894043 elapsed, loss: 1.5343522e-06\n",
      "step: 44620 train: 0.07843995094299316 elapsed, loss: 1.322011e-06\n",
      "step: 44630 train: 0.07795524597167969 elapsed, loss: 6.5126587e-06\n",
      "step: 44640 train: 0.08180379867553711 elapsed, loss: 0.00050428236\n",
      "step: 44650 train: 0.07957577705383301 elapsed, loss: 0.012584337\n",
      "step: 44660 train: 0.08377718925476074 elapsed, loss: 0.0003851853\n",
      "step: 44670 train: 0.07650494575500488 elapsed, loss: 0.00013151696\n",
      "step: 44680 train: 0.08591723442077637 elapsed, loss: 3.8774324e-05\n",
      "step: 44690 train: 0.08527207374572754 elapsed, loss: 2.9687682e-05\n",
      "step: 44700 train: 0.07902812957763672 elapsed, loss: 2.3970928e-05\n",
      "step: 44710 train: 0.08886146545410156 elapsed, loss: 1.7851406e-05\n",
      "step: 44720 train: 0.08423805236816406 elapsed, loss: 1.5502286e-05\n",
      "step: 44730 train: 0.08081960678100586 elapsed, loss: 1.3623112e-05\n",
      "step: 44740 train: 0.08303976058959961 elapsed, loss: 5.8621717e-06\n",
      "step: 44750 train: 0.07551383972167969 elapsed, loss: 7.189289e-06\n",
      "step: 44760 train: 0.07991290092468262 elapsed, loss: 7.1175796e-06\n",
      "step: 44770 train: 0.0816342830657959 elapsed, loss: 3.925511e-06\n",
      "step: 44780 train: 0.0789785385131836 elapsed, loss: 4.9209366e-06\n",
      "step: 44790 train: 0.08325982093811035 elapsed, loss: 1.7886018e-06\n",
      "step: 44800 train: 0.0823664665222168 elapsed, loss: 2.3511138e-06\n",
      "step: 44810 train: 0.08290910720825195 elapsed, loss: 1.4593776e-06\n",
      "step: 44820 train: 0.08537077903747559 elapsed, loss: 1.2190999e-06\n",
      "step: 44830 train: 0.08431649208068848 elapsed, loss: 1.2284133e-06\n",
      "step: 44840 train: 0.08806037902832031 elapsed, loss: 9.913919e-07\n",
      "step: 44850 train: 0.08218193054199219 elapsed, loss: 1.3592639e-06\n",
      "step: 44860 train: 0.08698844909667969 elapsed, loss: 9.355125e-07\n",
      "step: 44870 train: 0.08893632888793945 elapsed, loss: 9.685739e-07\n",
      "step: 44880 train: 0.08143377304077148 elapsed, loss: 1.731792e-06\n",
      "step: 44890 train: 0.08531641960144043 elapsed, loss: 1.3951194e-06\n",
      "step: 44900 train: 0.07900786399841309 elapsed, loss: 1.8877863e-06\n",
      "step: 44910 train: 0.07674503326416016 elapsed, loss: 1.7937249e-06\n",
      "step: 44920 train: 0.08044552803039551 elapsed, loss: 1.7057151e-06\n",
      "step: 44930 train: 0.07906746864318848 elapsed, loss: 1.5762578e-06\n",
      "step: 44940 train: 0.08072710037231445 elapsed, loss: 1.2861552e-06\n",
      "step: 44950 train: 0.0861046314239502 elapsed, loss: 1.1413349e-06\n",
      "step: 44960 train: 0.0778353214263916 elapsed, loss: 2.0251573e-06\n",
      "step: 44970 train: 0.07408952713012695 elapsed, loss: 1.85007e-06\n",
      "step: 44980 train: 0.08443522453308105 elapsed, loss: 1.2475048e-06\n",
      "step: 44990 train: 0.07884001731872559 elapsed, loss: 1.2502994e-06\n",
      "step: 45000 train: 0.081573486328125 elapsed, loss: 2.1913982e-06\n",
      "step: 45010 train: 0.09087109565734863 elapsed, loss: 1.2447115e-06\n",
      "step: 45020 train: 0.08363771438598633 elapsed, loss: 1.283827e-06\n",
      "step: 45030 train: 0.07949113845825195 elapsed, loss: 1.5525119e-06\n",
      "step: 45040 train: 0.07596564292907715 elapsed, loss: 2.1965207e-06\n",
      "step: 45050 train: 0.07978510856628418 elapsed, loss: 1.1851065e-06\n",
      "step: 45060 train: 0.08396029472351074 elapsed, loss: 4.207449e-06\n",
      "step: 45070 train: 0.0797271728515625 elapsed, loss: 7.758844e-06\n",
      "step: 45080 train: 0.08212018013000488 elapsed, loss: 7.203193e-06\n",
      "step: 45090 train: 0.07567691802978516 elapsed, loss: 4.0931445e-06\n",
      "step: 45100 train: 0.07773518562316895 elapsed, loss: 4.0744126e-06\n",
      "step: 45110 train: 0.08176302909851074 elapsed, loss: 2.61701e-06\n",
      "step: 45120 train: 0.07604336738586426 elapsed, loss: 2.536918e-06\n",
      "step: 45130 train: 0.07598066329956055 elapsed, loss: 2.4028077e-06\n",
      "step: 45140 train: 0.08148980140686035 elapsed, loss: 3.0212025e-06\n",
      "step: 45150 train: 0.08798098564147949 elapsed, loss: 1.0221258e-06\n",
      "step: 45160 train: 0.0799098014831543 elapsed, loss: 1.9366826e-06\n",
      "step: 45170 train: 0.07715129852294922 elapsed, loss: 2.574168e-06\n",
      "step: 45180 train: 0.08215856552124023 elapsed, loss: 2.2244603e-06\n",
      "step: 45190 train: 0.08567309379577637 elapsed, loss: 1.0011711e-06\n",
      "step: 45200 train: 0.08404421806335449 elapsed, loss: 1.9697432e-06\n",
      "step: 45210 train: 0.07698607444763184 elapsed, loss: 1.9818517e-06\n",
      "step: 45220 train: 0.08482551574707031 elapsed, loss: 2.0833647e-06\n",
      "step: 45230 train: 0.08234143257141113 elapsed, loss: 1.9394763e-06\n",
      "step: 45240 train: 0.08415484428405762 elapsed, loss: 1.1613583e-06\n",
      "step: 45250 train: 0.07293224334716797 elapsed, loss: 2.8093268e-06\n",
      "step: 45260 train: 0.07995057106018066 elapsed, loss: 3.4724242e-06\n",
      "step: 45270 train: 0.08283829689025879 elapsed, loss: 1.6810352e-06\n",
      "step: 45280 train: 0.08458089828491211 elapsed, loss: 1.0598443e-06\n",
      "step: 45290 train: 0.07956361770629883 elapsed, loss: 2.029334e-06\n",
      "step: 45300 train: 0.07822108268737793 elapsed, loss: 1.2167718e-06\n",
      "step: 45310 train: 0.0797264575958252 elapsed, loss: 1.7457621e-06\n",
      "step: 45320 train: 0.08656167984008789 elapsed, loss: 0.02514907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 45330 train: 0.0847780704498291 elapsed, loss: 0.00017109014\n",
      "step: 45340 train: 0.08285140991210938 elapsed, loss: 5.3205382e-05\n",
      "step: 45350 train: 0.08075594902038574 elapsed, loss: 3.9617727e-05\n",
      "step: 45360 train: 0.0838921070098877 elapsed, loss: 2.576275e-05\n",
      "step: 45370 train: 0.07996535301208496 elapsed, loss: 2.0093365e-05\n",
      "step: 45380 train: 0.08691883087158203 elapsed, loss: 1.2595336e-05\n",
      "step: 45390 train: 0.07956814765930176 elapsed, loss: 1.3849103e-05\n",
      "step: 45400 train: 0.08000373840332031 elapsed, loss: 8.940068e-06\n",
      "step: 45410 train: 0.07773756980895996 elapsed, loss: 8.364044e-06\n",
      "step: 45420 train: 0.07921600341796875 elapsed, loss: 5.116198e-06\n",
      "step: 45430 train: 0.08011627197265625 elapsed, loss: 4.475891e-06\n",
      "step: 45440 train: 0.08381772041320801 elapsed, loss: 3.0659066e-06\n",
      "step: 45450 train: 0.07223105430603027 elapsed, loss: 3.5655576e-06\n",
      "step: 45460 train: 0.08342552185058594 elapsed, loss: 1.8011735e-06\n",
      "step: 45470 train: 0.07608771324157715 elapsed, loss: 1.8561233e-06\n",
      "step: 45480 train: 0.07967758178710938 elapsed, loss: 1.2209628e-06\n",
      "step: 45490 train: 0.07884693145751953 elapsed, loss: 4.0037353e-06\n",
      "step: 45500 train: 0.0899357795715332 elapsed, loss: 1.3443628e-06\n",
      "step: 45510 train: 0.08054304122924805 elapsed, loss: 1.1688089e-06\n",
      "step: 45520 train: 0.08916306495666504 elapsed, loss: 1.129693e-06\n",
      "step: 45530 train: 0.07839751243591309 elapsed, loss: 1.7662466e-06\n",
      "step: 45540 train: 0.07897615432739258 elapsed, loss: 1.4780073e-06\n",
      "step: 45550 train: 0.0826268196105957 elapsed, loss: 1.6447138e-06\n",
      "step: 45560 train: 0.08226513862609863 elapsed, loss: 1.3825465e-06\n",
      "step: 45570 train: 0.0840754508972168 elapsed, loss: 5.59859e-06\n",
      "step: 45580 train: 0.07884645462036133 elapsed, loss: 1.5371461e-06\n",
      "step: 45590 train: 0.08807802200317383 elapsed, loss: 1.0309733e-06\n",
      "step: 45600 train: 0.08140826225280762 elapsed, loss: 1.256353e-06\n",
      "step: 45610 train: 0.0821232795715332 elapsed, loss: 9.816133e-07\n",
      "step: 45620 train: 0.07827305793762207 elapsed, loss: 2.376266e-06\n",
      "step: 45630 train: 0.08234286308288574 elapsed, loss: 1.3653175e-06\n",
      "step: 45640 train: 0.08072352409362793 elapsed, loss: 1.2661319e-06\n",
      "step: 45650 train: 0.07956957817077637 elapsed, loss: 1.8780095e-06\n",
      "step: 45660 train: 0.08740353584289551 elapsed, loss: 1.7243401e-06\n",
      "step: 45670 train: 0.08173727989196777 elapsed, loss: 2.4214337e-06\n",
      "step: 45680 train: 0.08252787590026855 elapsed, loss: 1.3839435e-06\n",
      "step: 45690 train: 0.07826995849609375 elapsed, loss: 2.4386638e-06\n",
      "step: 45700 train: 0.07434868812561035 elapsed, loss: 2.1820836e-06\n",
      "step: 45710 train: 0.08275890350341797 elapsed, loss: 1.7820832e-06\n",
      "step: 45720 train: 0.08054733276367188 elapsed, loss: 1.6959361e-06\n",
      "step: 45730 train: 0.08154821395874023 elapsed, loss: 1.8808024e-06\n",
      "step: 45740 train: 0.08061695098876953 elapsed, loss: 6.0959355e-06\n",
      "step: 45750 train: 0.08208537101745605 elapsed, loss: 2.7273722e-06\n",
      "step: 45760 train: 0.0861515998840332 elapsed, loss: 3.591627e-06\n",
      "step: 45770 train: 0.0782003402709961 elapsed, loss: 2.2458808e-06\n",
      "step: 45780 train: 0.0864098072052002 elapsed, loss: 1.2749795e-06\n",
      "step: 45790 train: 0.08432865142822266 elapsed, loss: 1.1860383e-06\n",
      "step: 45800 train: 0.08717775344848633 elapsed, loss: 1.204198e-06\n",
      "step: 45810 train: 0.0853421688079834 elapsed, loss: 1.8039675e-06\n",
      "step: 45820 train: 0.07995820045471191 elapsed, loss: 2.0558916e-06\n",
      "step: 45830 train: 0.08669328689575195 elapsed, loss: 1.6204992e-06\n",
      "step: 45840 train: 0.08178496360778809 elapsed, loss: 2.3902353e-06\n",
      "step: 45850 train: 0.08078241348266602 elapsed, loss: 1.693608e-06\n",
      "step: 45860 train: 0.0780799388885498 elapsed, loss: 2.598849e-06\n",
      "step: 45870 train: 0.08311939239501953 elapsed, loss: 2.4246942e-06\n",
      "step: 45880 train: 0.07887077331542969 elapsed, loss: 1.6163082e-06\n",
      "step: 45890 train: 0.0825965404510498 elapsed, loss: 2.2062993e-06\n",
      "step: 45900 train: 0.08263444900512695 elapsed, loss: 7.0244328e-06\n",
      "step: 45910 train: 0.08009171485900879 elapsed, loss: 1.2050632e-05\n",
      "step: 45920 train: 0.08477091789245605 elapsed, loss: 1.7876712e-06\n",
      "step: 45930 train: 0.0826730728149414 elapsed, loss: 2.2901186e-06\n",
      "step: 45940 train: 0.07641911506652832 elapsed, loss: 2.1774288e-06\n",
      "step: 45950 train: 0.07625627517700195 elapsed, loss: 3.2870933e-06\n",
      "step: 45960 train: 0.07870745658874512 elapsed, loss: 1.6596143e-06\n",
      "step: 45970 train: 0.08042001724243164 elapsed, loss: 2.0326088e-06\n",
      "step: 45980 train: 0.07899785041809082 elapsed, loss: 1.7289967e-06\n",
      "step: 45990 train: 0.08485746383666992 elapsed, loss: 0.0023136693\n",
      "step: 46000 train: 0.08385324478149414 elapsed, loss: 5.0179966e-05\n",
      "step: 46010 train: 0.07540225982666016 elapsed, loss: 7.5464064e-05\n",
      "step: 46020 train: 0.07771468162536621 elapsed, loss: 2.0549429e-05\n",
      "step: 46030 train: 0.08779597282409668 elapsed, loss: 1.0622022e-05\n",
      "step: 46040 train: 0.08365797996520996 elapsed, loss: 1.2946471e-05\n",
      "step: 46050 train: 0.08156895637512207 elapsed, loss: 1.2371982e-05\n",
      "step: 46060 train: 0.08193206787109375 elapsed, loss: 1.6476759e-05\n",
      "step: 46070 train: 0.07879424095153809 elapsed, loss: 6.2661397e-06\n",
      "step: 46080 train: 0.08526182174682617 elapsed, loss: 3.6922156e-06\n",
      "step: 46090 train: 0.08594059944152832 elapsed, loss: 2.8139812e-06\n",
      "step: 46100 train: 0.08645057678222656 elapsed, loss: 2.382318e-06\n",
      "step: 46110 train: 0.08448147773742676 elapsed, loss: 2.3995474e-06\n",
      "step: 46120 train: 0.07978439331054688 elapsed, loss: 1.6675302e-06\n",
      "step: 46130 train: 0.08107757568359375 elapsed, loss: 1.7345859e-06\n",
      "step: 46140 train: 0.07540082931518555 elapsed, loss: 1.7792888e-06\n",
      "step: 46150 train: 0.07990360260009766 elapsed, loss: 1.3220111e-06\n",
      "step: 46160 train: 0.08261275291442871 elapsed, loss: 1.1688087e-06\n",
      "step: 46170 train: 0.07783317565917969 elapsed, loss: 1.5841779e-06\n",
      "step: 46180 train: 0.07740902900695801 elapsed, loss: 1.5785901e-06\n",
      "step: 46190 train: 0.0823359489440918 elapsed, loss: 1.245176e-06\n",
      "step: 46200 train: 0.08689498901367188 elapsed, loss: 1.2391233e-06\n",
      "step: 46210 train: 0.08196759223937988 elapsed, loss: 1.186504e-06\n",
      "step: 46220 train: 0.08116364479064941 elapsed, loss: 1.5203822e-06\n",
      "step: 46230 train: 0.08454728126525879 elapsed, loss: 1.1641516e-06\n",
      "step: 46240 train: 0.08525919914245605 elapsed, loss: 1.8239846e-06\n",
      "step: 46250 train: 0.07976627349853516 elapsed, loss: 1.35647e-06\n",
      "step: 46260 train: 0.09107303619384766 elapsed, loss: 1.0509966e-06\n",
      "step: 46270 train: 0.07721376419067383 elapsed, loss: 1.9143308e-06\n",
      "step: 46280 train: 0.0876765251159668 elapsed, loss: 1.3434316e-06\n",
      "step: 46290 train: 0.08322548866271973 elapsed, loss: 1.0156066e-06\n",
      "step: 46300 train: 0.08082985877990723 elapsed, loss: 3.605437e-06\n",
      "step: 46310 train: 0.08019661903381348 elapsed, loss: 0.020796755\n",
      "step: 46320 train: 0.07787203788757324 elapsed, loss: 0.00019672823\n",
      "step: 46330 train: 0.07334494590759277 elapsed, loss: 0.0005944668\n",
      "step: 46340 train: 0.08740854263305664 elapsed, loss: 3.484205e-05\n",
      "step: 46350 train: 0.08179831504821777 elapsed, loss: 5.190148e-05\n",
      "step: 46360 train: 0.0875101089477539 elapsed, loss: 2.4427542e-05\n",
      "step: 46370 train: 0.07805371284484863 elapsed, loss: 2.8460194e-05\n",
      "step: 46380 train: 0.0813283920288086 elapsed, loss: 1.7295915e-05\n",
      "step: 46390 train: 0.07415962219238281 elapsed, loss: 1.4574891e-05\n",
      "step: 46400 train: 0.07958507537841797 elapsed, loss: 6.3967204e-06\n",
      "step: 46410 train: 0.08011102676391602 elapsed, loss: 6.041926e-06\n",
      "step: 46420 train: 0.07550406455993652 elapsed, loss: 5.0449466e-06\n",
      "step: 46430 train: 0.08279705047607422 elapsed, loss: 3.6018762e-06\n",
      "step: 46440 train: 0.08397221565246582 elapsed, loss: 2.6617126e-06\n",
      "step: 46450 train: 0.08200645446777344 elapsed, loss: 2.1480919e-06\n",
      "step: 46460 train: 0.07815408706665039 elapsed, loss: 3.463573e-06\n",
      "step: 46470 train: 0.08491992950439453 elapsed, loss: 1.7252728e-06\n",
      "step: 46480 train: 0.07805848121643066 elapsed, loss: 1.9501858e-06\n",
      "step: 46490 train: 0.08186626434326172 elapsed, loss: 1.8859241e-06\n",
      "step: 46500 train: 0.08610916137695312 elapsed, loss: 1.1469227e-06\n",
      "step: 46510 train: 0.07824110984802246 elapsed, loss: 1.5930246e-06\n",
      "step: 46520 train: 0.08051705360412598 elapsed, loss: 9.142125e-06\n",
      "step: 46530 train: 0.07959651947021484 elapsed, loss: 1.928766e-06\n",
      "step: 46540 train: 0.0823521614074707 elapsed, loss: 1.2093213e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 46550 train: 0.08086061477661133 elapsed, loss: 1.5101376e-06\n",
      "step: 46560 train: 0.08944559097290039 elapsed, loss: 1.948324e-06\n",
      "step: 46570 train: 0.07788610458374023 elapsed, loss: 1.4980303e-06\n",
      "step: 46580 train: 0.07658243179321289 elapsed, loss: 1.2912776e-06\n",
      "step: 46590 train: 0.08112025260925293 elapsed, loss: 1.8002443e-06\n",
      "step: 46600 train: 0.08895468711853027 elapsed, loss: 1.0952344e-06\n",
      "step: 46610 train: 0.08058691024780273 elapsed, loss: 1.9771946e-06\n",
      "step: 46620 train: 0.08555293083190918 elapsed, loss: 1.4756753e-06\n",
      "step: 46630 train: 0.08254265785217285 elapsed, loss: 1.3741651e-06\n",
      "step: 46640 train: 0.07825827598571777 elapsed, loss: 1.4351665e-06\n",
      "step: 46650 train: 0.07655882835388184 elapsed, loss: 1.5315584e-06\n",
      "step: 46660 train: 0.07651853561401367 elapsed, loss: 2.3636926e-06\n",
      "step: 46670 train: 0.07525992393493652 elapsed, loss: 3.0281888e-06\n",
      "step: 46680 train: 0.08562850952148438 elapsed, loss: 1.35647e-06\n",
      "step: 46690 train: 0.08541488647460938 elapsed, loss: 1.3029187e-06\n",
      "step: 46700 train: 0.09154391288757324 elapsed, loss: 1.6801039e-06\n",
      "step: 46710 train: 0.08134293556213379 elapsed, loss: 1.5995449e-06\n",
      "step: 46720 train: 0.07794070243835449 elapsed, loss: 2.7548472e-06\n",
      "step: 46730 train: 0.0834505558013916 elapsed, loss: 1.6144458e-06\n",
      "step: 46740 train: 0.08138227462768555 elapsed, loss: 0.030689938\n",
      "step: 46750 train: 0.08768343925476074 elapsed, loss: 7.3524316e-05\n",
      "step: 46760 train: 0.08701944351196289 elapsed, loss: 4.7124486e-05\n",
      "step: 46770 train: 0.08448576927185059 elapsed, loss: 2.578599e-05\n",
      "step: 46780 train: 0.08093595504760742 elapsed, loss: 3.6401056e-05\n",
      "step: 46790 train: 0.0841679573059082 elapsed, loss: 1.540176e-05\n",
      "step: 46800 train: 0.0782771110534668 elapsed, loss: 1.3613841e-05\n",
      "step: 46810 train: 0.07727646827697754 elapsed, loss: 1.659297e-05\n",
      "step: 46820 train: 0.08150506019592285 elapsed, loss: 5.9939302e-06\n",
      "step: 46830 train: 0.08079004287719727 elapsed, loss: 6.6267767e-06\n",
      "step: 46840 train: 0.08145952224731445 elapsed, loss: 3.7606626e-06\n",
      "step: 46850 train: 0.0896444320678711 elapsed, loss: 3.057056e-06\n",
      "step: 46860 train: 0.08354401588439941 elapsed, loss: 2.7674187e-06\n",
      "step: 46870 train: 0.08343195915222168 elapsed, loss: 2.1504202e-06\n",
      "step: 46880 train: 0.0786905288696289 elapsed, loss: 2.9838902e-06\n",
      "step: 46890 train: 0.08073711395263672 elapsed, loss: 1.4482025e-06\n",
      "step: 46900 train: 0.0828714370727539 elapsed, loss: 1.1911604e-06\n",
      "step: 46910 train: 0.08361268043518066 elapsed, loss: 1.143663e-06\n",
      "step: 46920 train: 0.08474087715148926 elapsed, loss: 1.1092043e-06\n",
      "step: 46930 train: 0.08187460899353027 elapsed, loss: 1.1287615e-06\n",
      "step: 46940 train: 0.07563495635986328 elapsed, loss: 2.27708e-06\n",
      "step: 46950 train: 0.0906074047088623 elapsed, loss: 1.5059469e-06\n",
      "step: 46960 train: 0.08275127410888672 elapsed, loss: 1.3913943e-06\n",
      "step: 46970 train: 0.08318948745727539 elapsed, loss: 1.4780064e-06\n",
      "step: 46980 train: 0.08105707168579102 elapsed, loss: 1.4891831e-06\n",
      "step: 46990 train: 0.08086514472961426 elapsed, loss: 1.2805675e-06\n",
      "step: 47000 train: 0.07834434509277344 elapsed, loss: 2.2798667e-06\n",
      "step: 47010 train: 0.08660054206848145 elapsed, loss: 1.0021024e-06\n",
      "step: 47020 train: 0.0814201831817627 elapsed, loss: 1.3099038e-06\n",
      "step: 47030 train: 0.08525991439819336 elapsed, loss: 1.1757938e-06\n",
      "step: 47040 train: 0.07790279388427734 elapsed, loss: 2.148092e-06\n",
      "step: 47050 train: 0.0814213752746582 elapsed, loss: 1.580452e-06\n",
      "step: 47060 train: 0.07429909706115723 elapsed, loss: 3.3862807e-06\n",
      "step: 47070 train: 0.08435177803039551 elapsed, loss: 1.0956999e-06\n",
      "step: 47080 train: 0.08833599090576172 elapsed, loss: 1.1548391e-06\n",
      "step: 47090 train: 0.07942390441894531 elapsed, loss: 2.8787122e-06\n",
      "step: 47100 train: 0.08042383193969727 elapsed, loss: 2.2342385e-06\n",
      "step: 47110 train: 0.08242225646972656 elapsed, loss: 0.04807934\n",
      "step: 47120 train: 0.0788276195526123 elapsed, loss: 9.742647e-05\n",
      "step: 47130 train: 0.08158230781555176 elapsed, loss: 4.296068e-05\n",
      "step: 47140 train: 0.08484029769897461 elapsed, loss: 4.029157e-05\n",
      "step: 47150 train: 0.08494353294372559 elapsed, loss: 1.54888e-05\n",
      "step: 47160 train: 0.0773317813873291 elapsed, loss: 1.47970095e-05\n",
      "step: 47170 train: 0.08317971229553223 elapsed, loss: 1.3816636e-05\n",
      "step: 47180 train: 0.08368062973022461 elapsed, loss: 9.6227795e-06\n",
      "step: 47190 train: 0.08023190498352051 elapsed, loss: 7.3242854e-06\n",
      "step: 47200 train: 0.08167839050292969 elapsed, loss: 9.634699e-06\n",
      "step: 47210 train: 0.07644772529602051 elapsed, loss: 7.016537e-06\n",
      "step: 47220 train: 0.07898449897766113 elapsed, loss: 4.8572347e-06\n",
      "step: 47230 train: 0.08549046516418457 elapsed, loss: 3.2414448e-06\n",
      "step: 47240 train: 0.08110570907592773 elapsed, loss: 2.4251594e-06\n",
      "step: 47250 train: 0.0833740234375 elapsed, loss: 1.9073436e-06\n",
      "step: 47260 train: 0.07848381996154785 elapsed, loss: 2.5327272e-06\n",
      "step: 47270 train: 0.08540797233581543 elapsed, loss: 1.6312094e-06\n",
      "step: 47280 train: 0.08312344551086426 elapsed, loss: 1.4584496e-06\n",
      "step: 47290 train: 0.0835573673248291 elapsed, loss: 1.5632233e-06\n",
      "step: 47300 train: 0.08518767356872559 elapsed, loss: 1.2065271e-06\n",
      "step: 47310 train: 0.08139419555664062 elapsed, loss: 1.5157257e-06\n",
      "step: 47320 train: 0.08912158012390137 elapsed, loss: 1.7890679e-06\n",
      "step: 47330 train: 0.08383655548095703 elapsed, loss: 1.9827828e-06\n",
      "step: 47340 train: 0.08133149147033691 elapsed, loss: 1.917125e-06\n",
      "step: 47350 train: 0.08692359924316406 elapsed, loss: 1.2335352e-06\n",
      "step: 47360 train: 0.0828084945678711 elapsed, loss: 1.6563553e-06\n",
      "step: 47370 train: 0.07899689674377441 elapsed, loss: 1.4761446e-06\n",
      "step: 47380 train: 0.0788733959197998 elapsed, loss: 1.6298127e-06\n",
      "step: 47390 train: 0.08209085464477539 elapsed, loss: 1.0896466e-06\n",
      "step: 47400 train: 0.08991003036499023 elapsed, loss: 1.9115053e-06\n",
      "step: 47410 train: 0.07731842994689941 elapsed, loss: 0.002724663\n",
      "step: 47420 train: 0.07570123672485352 elapsed, loss: 6.5691565e-05\n",
      "step: 47430 train: 0.08440709114074707 elapsed, loss: 3.120697e-05\n",
      "step: 47440 train: 0.07691764831542969 elapsed, loss: 4.465536e-05\n",
      "step: 47450 train: 0.07736921310424805 elapsed, loss: 1.839355e-05\n",
      "step: 47460 train: 0.08439421653747559 elapsed, loss: 1.3960273e-05\n",
      "step: 47470 train: 0.08008384704589844 elapsed, loss: 7.924096e-06\n",
      "step: 47480 train: 0.07706618309020996 elapsed, loss: 1.0412336e-05\n",
      "step: 47490 train: 0.08751630783081055 elapsed, loss: 4.0135205e-06\n",
      "step: 47500 train: 0.0814063549041748 elapsed, loss: 3.8598473e-06\n",
      "step: 47510 train: 0.08071732521057129 elapsed, loss: 3.4337756e-06\n",
      "step: 47520 train: 0.07733678817749023 elapsed, loss: 3.6270253e-06\n",
      "step: 47530 train: 0.0847327709197998 elapsed, loss: 1.8831307e-06\n",
      "step: 47540 train: 0.07589054107666016 elapsed, loss: 3.2524817e-06\n",
      "step: 47550 train: 0.08571839332580566 elapsed, loss: 1.1292276e-06\n",
      "step: 47560 train: 0.08045697212219238 elapsed, loss: 1.5506503e-06\n",
      "step: 47570 train: 0.08267617225646973 elapsed, loss: 2.2314446e-06\n",
      "step: 47580 train: 0.08534932136535645 elapsed, loss: 1.6745148e-06\n",
      "step: 47590 train: 0.0830988883972168 elapsed, loss: 1.6367974e-06\n",
      "step: 47600 train: 0.0813593864440918 elapsed, loss: 1.1641522e-06\n",
      "step: 47610 train: 0.08809161186218262 elapsed, loss: 1.1250366e-06\n",
      "step: 47620 train: 0.07993602752685547 elapsed, loss: 2.39163e-06\n",
      "step: 47630 train: 0.0746150016784668 elapsed, loss: 1.9972185e-06\n",
      "step: 47640 train: 0.07874321937561035 elapsed, loss: 1.9497213e-06\n",
      "step: 47650 train: 0.07786440849304199 elapsed, loss: 2.1676494e-06\n",
      "step: 47660 train: 0.07453417778015137 elapsed, loss: 2.0777775e-06\n",
      "step: 47670 train: 0.08416533470153809 elapsed, loss: 1.2014043e-06\n",
      "step: 47680 train: 0.08436298370361328 elapsed, loss: 1.5296953e-06\n",
      "step: 47690 train: 0.0839684009552002 elapsed, loss: 2.1439005e-06\n",
      "step: 47700 train: 0.07721424102783203 elapsed, loss: 1.5501848e-06\n",
      "step: 47710 train: 0.07792091369628906 elapsed, loss: 1.8668338e-06\n",
      "step: 47720 train: 0.07632255554199219 elapsed, loss: 1.5026874e-06\n",
      "step: 47730 train: 0.08580803871154785 elapsed, loss: 1.5343522e-06\n",
      "step: 47740 train: 0.07968974113464355 elapsed, loss: 1.2745135e-06\n",
      "step: 47750 train: 0.08697247505187988 elapsed, loss: 1.3862724e-06\n",
      "step: 47760 train: 0.07999873161315918 elapsed, loss: 1.5622917e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 47770 train: 0.07822656631469727 elapsed, loss: 0.005564222\n",
      "step: 47780 train: 0.08070564270019531 elapsed, loss: 5.3127485e-05\n",
      "step: 47790 train: 0.07990503311157227 elapsed, loss: 0.00013047163\n",
      "step: 47800 train: 0.08195710182189941 elapsed, loss: 3.0297677e-05\n",
      "step: 47810 train: 0.07705807685852051 elapsed, loss: 3.675636e-05\n",
      "step: 47820 train: 0.07552242279052734 elapsed, loss: 2.0303156e-05\n",
      "step: 47830 train: 0.08763790130615234 elapsed, loss: 9.606723e-06\n",
      "step: 47840 train: 0.08610177040100098 elapsed, loss: 6.0535162e-06\n",
      "step: 47850 train: 0.0812530517578125 elapsed, loss: 1.4320324e-05\n",
      "step: 47860 train: 0.08570551872253418 elapsed, loss: 4.899147e-06\n",
      "step: 47870 train: 0.08140993118286133 elapsed, loss: 4.047043e-06\n",
      "step: 47880 train: 0.08104705810546875 elapsed, loss: 3.2195703e-06\n",
      "step: 47890 train: 0.08325004577636719 elapsed, loss: 3.4007135e-06\n",
      "step: 47900 train: 0.08466529846191406 elapsed, loss: 2.710607e-06\n",
      "step: 47910 train: 0.08666181564331055 elapsed, loss: 2.1271312e-06\n",
      "step: 47920 train: 0.07774567604064941 elapsed, loss: 1.4887175e-06\n",
      "step: 47930 train: 0.0814054012298584 elapsed, loss: 1.4146774e-06\n",
      "step: 47940 train: 0.08278203010559082 elapsed, loss: 1.0961655e-06\n",
      "step: 47950 train: 0.08407950401306152 elapsed, loss: 6.31291e-06\n",
      "step: 47960 train: 0.08881020545959473 elapsed, loss: 1.7699757e-06\n",
      "step: 47970 train: 0.07565903663635254 elapsed, loss: 2.4838278e-06\n",
      "step: 47980 train: 0.08602738380432129 elapsed, loss: 1.3979138e-06\n",
      "step: 47990 train: 0.08170819282531738 elapsed, loss: 1.2209629e-06\n",
      "step: 48000 train: 0.07474446296691895 elapsed, loss: 1.7136304e-06\n",
      "step: 48010 train: 0.08133816719055176 elapsed, loss: 1.6377286e-06\n",
      "step: 48020 train: 0.07871437072753906 elapsed, loss: 1.7653194e-06\n",
      "step: 48030 train: 0.07738304138183594 elapsed, loss: 1.3918602e-06\n",
      "step: 48040 train: 0.08258509635925293 elapsed, loss: 1.3010565e-06\n",
      "step: 48050 train: 0.07759284973144531 elapsed, loss: 1.3229422e-06\n",
      "step: 48060 train: 0.07998061180114746 elapsed, loss: 0.0006833188\n",
      "step: 48070 train: 0.08320450782775879 elapsed, loss: 0.009993529\n",
      "step: 48080 train: 0.08679842948913574 elapsed, loss: 8.750259e-05\n",
      "step: 48090 train: 0.07939910888671875 elapsed, loss: 3.646835e-05\n",
      "step: 48100 train: 0.07628011703491211 elapsed, loss: 5.6310688e-05\n",
      "step: 48110 train: 0.0835256576538086 elapsed, loss: 6.022017e-05\n",
      "step: 48120 train: 0.08221817016601562 elapsed, loss: 1.9290293e-05\n",
      "step: 48130 train: 0.08307623863220215 elapsed, loss: 1.13009355e-05\n",
      "step: 48140 train: 0.08187627792358398 elapsed, loss: 8.09122e-06\n",
      "step: 48150 train: 0.07874131202697754 elapsed, loss: 9.281075e-06\n",
      "step: 48160 train: 0.08489656448364258 elapsed, loss: 5.7550737e-06\n",
      "step: 48170 train: 0.08287358283996582 elapsed, loss: 4.929346e-06\n",
      "step: 48180 train: 0.08597683906555176 elapsed, loss: 2.880106e-06\n",
      "step: 48190 train: 0.07593369483947754 elapsed, loss: 7.1203785e-06\n",
      "step: 48200 train: 0.08097362518310547 elapsed, loss: 2.06008e-06\n",
      "step: 48210 train: 0.08452844619750977 elapsed, loss: 1.8868566e-06\n",
      "step: 48220 train: 0.0784294605255127 elapsed, loss: 1.4277159e-06\n",
      "step: 48230 train: 0.0780019760131836 elapsed, loss: 2.797165e-06\n",
      "step: 48240 train: 0.08286595344543457 elapsed, loss: 1.1459914e-06\n",
      "step: 48250 train: 0.08685636520385742 elapsed, loss: 1.2787041e-06\n",
      "step: 48260 train: 0.08278155326843262 elapsed, loss: 1.3182847e-06\n",
      "step: 48270 train: 0.08124613761901855 elapsed, loss: 1.0533249e-06\n",
      "step: 48280 train: 0.08167910575866699 elapsed, loss: 1.6381939e-06\n",
      "step: 48290 train: 0.07844114303588867 elapsed, loss: 1.0728828e-06\n",
      "step: 48300 train: 0.08466053009033203 elapsed, loss: 2.7822698e-06\n",
      "step: 48310 train: 0.08032727241516113 elapsed, loss: 1.8570547e-06\n",
      "step: 48320 train: 0.0808877944946289 elapsed, loss: 3.784879e-06\n",
      "step: 48330 train: 0.08234858512878418 elapsed, loss: 2.1164265e-06\n",
      "step: 48340 train: 0.07776665687561035 elapsed, loss: 2.2556594e-06\n",
      "step: 48350 train: 0.0778038501739502 elapsed, loss: 1.8477417e-06\n",
      "step: 48360 train: 0.08339929580688477 elapsed, loss: 2.158774e-06\n",
      "step: 48370 train: 0.0794687271118164 elapsed, loss: 1.4477391e-06\n",
      "step: 48380 train: 0.08453178405761719 elapsed, loss: 1.2721855e-06\n",
      "step: 48390 train: 0.0774071216583252 elapsed, loss: 1.5199169e-06\n",
      "step: 48400 train: 0.08066082000732422 elapsed, loss: 1.5250364e-06\n",
      "step: 48410 train: 0.0844571590423584 elapsed, loss: 1.8021054e-06\n",
      "step: 48420 train: 0.08088302612304688 elapsed, loss: 2.0437847e-06\n",
      "step: 48430 train: 0.08314967155456543 elapsed, loss: 1.5450606e-06\n",
      "step: 48440 train: 0.0751655101776123 elapsed, loss: 1.5129309e-06\n",
      "step: 48450 train: 0.08340859413146973 elapsed, loss: 0.00014025613\n",
      "step: 48460 train: 0.08151412010192871 elapsed, loss: 0.000976468\n",
      "step: 48470 train: 0.08157038688659668 elapsed, loss: 4.766429e-05\n",
      "step: 48480 train: 0.08037662506103516 elapsed, loss: 2.6345646e-05\n",
      "step: 48490 train: 0.07754755020141602 elapsed, loss: 3.5631565e-05\n",
      "step: 48500 train: 0.0762946605682373 elapsed, loss: 1.8133203e-05\n",
      "step: 48510 train: 0.07969284057617188 elapsed, loss: 1.9780851e-05\n",
      "step: 48520 train: 0.08026981353759766 elapsed, loss: 1.4074677e-05\n",
      "step: 48530 train: 0.08222627639770508 elapsed, loss: 6.2896397e-06\n",
      "step: 48540 train: 0.07578253746032715 elapsed, loss: 1.0310096e-05\n",
      "step: 48550 train: 0.08120131492614746 elapsed, loss: 5.881167e-06\n",
      "step: 48560 train: 0.08829665184020996 elapsed, loss: 2.6738207e-06\n",
      "step: 48570 train: 0.08235287666320801 elapsed, loss: 2.917357e-06\n",
      "step: 48580 train: 0.07992982864379883 elapsed, loss: 3.1399418e-06\n",
      "step: 48590 train: 0.07204914093017578 elapsed, loss: 2.7338906e-06\n",
      "step: 48600 train: 0.08119034767150879 elapsed, loss: 2.1052515e-06\n",
      "step: 48610 train: 0.08296465873718262 elapsed, loss: 1.4733496e-06\n",
      "step: 48620 train: 0.08655381202697754 elapsed, loss: 1.0416835e-06\n",
      "step: 48630 train: 0.08154892921447754 elapsed, loss: 1.0840583e-06\n",
      "step: 48640 train: 0.08129525184631348 elapsed, loss: 1.4500306e-06\n",
      "step: 48650 train: 0.08286571502685547 elapsed, loss: 1.4626405e-06\n",
      "step: 48660 train: 0.07937121391296387 elapsed, loss: 1.0626379e-06\n",
      "step: 48670 train: 0.08145618438720703 elapsed, loss: 1.6069945e-06\n",
      "step: 48680 train: 0.08338594436645508 elapsed, loss: 1.5338862e-06\n",
      "step: 48690 train: 0.08284926414489746 elapsed, loss: 1.414212e-06\n",
      "step: 48700 train: 0.07985353469848633 elapsed, loss: 1.6312096e-06\n",
      "step: 48710 train: 0.07407069206237793 elapsed, loss: 1.7550747e-06\n",
      "step: 48720 train: 0.08373665809631348 elapsed, loss: 1.034233e-06\n",
      "step: 48730 train: 0.08951807022094727 elapsed, loss: 9.28528e-07\n",
      "step: 48740 train: 0.08664131164550781 elapsed, loss: 1.0156064e-06\n",
      "step: 48750 train: 0.08207297325134277 elapsed, loss: 1.477076e-06\n",
      "step: 48760 train: 0.0804290771484375 elapsed, loss: 1.2470398e-06\n",
      "step: 48770 train: 0.07947540283203125 elapsed, loss: 3.032377e-06\n",
      "step: 48780 train: 0.0853583812713623 elapsed, loss: 1.1525105e-06\n",
      "step: 48790 train: 0.07710695266723633 elapsed, loss: 2.0791745e-06\n",
      "step: 48800 train: 0.08496809005737305 elapsed, loss: 1.3187512e-06\n",
      "step: 48810 train: 0.08389019966125488 elapsed, loss: 1.3667144e-06\n",
      "step: 48820 train: 0.08457303047180176 elapsed, loss: 1.5711394e-06\n",
      "step: 48830 train: 0.07810020446777344 elapsed, loss: 0.0011580192\n",
      "step: 48840 train: 0.08865141868591309 elapsed, loss: 3.3136257e-05\n",
      "step: 48850 train: 0.08167695999145508 elapsed, loss: 2.2975457e-05\n",
      "step: 48860 train: 0.07668685913085938 elapsed, loss: 1.4145922e-05\n",
      "step: 48870 train: 0.07816624641418457 elapsed, loss: 0.027004257\n",
      "step: 48880 train: 0.0773169994354248 elapsed, loss: 0.00044803962\n",
      "step: 48890 train: 0.08059287071228027 elapsed, loss: 0.00014975213\n",
      "step: 48900 train: 0.09168052673339844 elapsed, loss: 2.682798e-05\n",
      "step: 48910 train: 0.08616328239440918 elapsed, loss: 1.8253208e-05\n",
      "step: 48920 train: 0.08175086975097656 elapsed, loss: 2.5807783e-05\n",
      "step: 48930 train: 0.08225154876708984 elapsed, loss: 4.184795e-05\n",
      "step: 48940 train: 0.08015060424804688 elapsed, loss: 1.2414594e-05\n",
      "step: 48950 train: 0.07959342002868652 elapsed, loss: 1.0199748e-05\n",
      "step: 48960 train: 0.08281850814819336 elapsed, loss: 5.6633376e-06\n",
      "step: 48970 train: 0.08399128913879395 elapsed, loss: 3.3932638e-06\n",
      "step: 48980 train: 0.08227157592773438 elapsed, loss: 3.2661374e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 48990 train: 0.08483052253723145 elapsed, loss: 2.3650884e-06\n",
      "step: 49000 train: 0.08110713958740234 elapsed, loss: 2.002805e-06\n",
      "step: 49010 train: 0.08164358139038086 elapsed, loss: 2.5355207e-06\n",
      "step: 49020 train: 0.0801689624786377 elapsed, loss: 2.0582193e-06\n",
      "step: 49030 train: 0.07997274398803711 elapsed, loss: 1.3504159e-06\n",
      "step: 49040 train: 0.08410191535949707 elapsed, loss: 1.1231739e-06\n",
      "step: 49050 train: 0.08259344100952148 elapsed, loss: 1.1539075e-06\n",
      "step: 49060 train: 0.0812978744506836 elapsed, loss: 1.386738e-06\n",
      "step: 49070 train: 0.07818245887756348 elapsed, loss: 1.4263189e-06\n",
      "step: 49080 train: 0.0841684341430664 elapsed, loss: 1.020263e-06\n",
      "step: 49090 train: 0.0825657844543457 elapsed, loss: 1.117586e-06\n",
      "step: 49100 train: 0.08083128929138184 elapsed, loss: 1.0402865e-06\n",
      "step: 49110 train: 0.08135795593261719 elapsed, loss: 1.298728e-06\n",
      "step: 49120 train: 0.08245658874511719 elapsed, loss: 9.792844e-07\n",
      "step: 49130 train: 0.07964491844177246 elapsed, loss: 1.132953e-06\n",
      "step: 49140 train: 0.0835275650024414 elapsed, loss: 1.7429672e-06\n",
      "step: 49150 train: 0.0860745906829834 elapsed, loss: 1.0030335e-06\n",
      "step: 49160 train: 0.0876920223236084 elapsed, loss: 1.2610096e-06\n",
      "step: 49170 train: 0.07878351211547852 elapsed, loss: 1.6088578e-06\n",
      "step: 49180 train: 0.08170318603515625 elapsed, loss: 1.5459938e-06\n",
      "step: 49190 train: 0.07981133460998535 elapsed, loss: 1.0682259e-06\n",
      "step: 49200 train: 0.07404422760009766 elapsed, loss: 2.4507706e-06\n",
      "step: 49210 train: 0.07539772987365723 elapsed, loss: 1.9692789e-06\n",
      "step: 49220 train: 0.08189606666564941 elapsed, loss: 1.6251556e-06\n",
      "step: 49230 train: 0.08702492713928223 elapsed, loss: 1.310369e-06\n",
      "step: 49240 train: 0.08261609077453613 elapsed, loss: 1.4696247e-06\n",
      "step: 49250 train: 0.08156514167785645 elapsed, loss: 1.6721874e-06\n",
      "step: 49260 train: 0.08326101303100586 elapsed, loss: 1.0523938e-06\n",
      "step: 49270 train: 0.08019614219665527 elapsed, loss: 1.1073416e-06\n",
      "step: 49280 train: 0.07529997825622559 elapsed, loss: 1.5599635e-06\n",
      "step: 49290 train: 0.0863497257232666 elapsed, loss: 1.2489024e-06\n",
      "step: 49300 train: 0.08006048202514648 elapsed, loss: 1.9138652e-06\n",
      "step: 49310 train: 0.07583355903625488 elapsed, loss: 4.106645e-06\n",
      "step: 49320 train: 0.07854819297790527 elapsed, loss: 2.416775e-06\n",
      "step: 49330 train: 0.08256125450134277 elapsed, loss: 1.3140948e-06\n",
      "step: 49340 train: 0.08132719993591309 elapsed, loss: 1.3564701e-06\n",
      "step: 49350 train: 0.07808732986450195 elapsed, loss: 1.5282985e-06\n",
      "step: 49360 train: 0.08100104331970215 elapsed, loss: 3.1194545e-06\n",
      "step: 49370 train: 0.07821393013000488 elapsed, loss: 2.1629933e-06\n",
      "step: 49380 train: 0.0828392505645752 elapsed, loss: 1.637263e-06\n",
      "step: 49390 train: 0.08600926399230957 elapsed, loss: 1.520848e-06\n",
      "step: 49400 train: 0.08164286613464355 elapsed, loss: 2.529002e-06\n",
      "step: 49410 train: 0.08764266967773438 elapsed, loss: 0.00031247173\n",
      "step: 49420 train: 0.08595800399780273 elapsed, loss: 6.981455e-05\n",
      "step: 49430 train: 0.08138656616210938 elapsed, loss: 5.9060905e-05\n",
      "step: 49440 train: 0.08331894874572754 elapsed, loss: 2.0419267e-05\n",
      "step: 49450 train: 0.0764317512512207 elapsed, loss: 2.1443519e-05\n",
      "step: 49460 train: 0.07967448234558105 elapsed, loss: 1.1496368e-05\n",
      "step: 49470 train: 0.09232282638549805 elapsed, loss: 5.255426e-06\n",
      "step: 49480 train: 0.07741141319274902 elapsed, loss: 9.497071e-06\n",
      "step: 49490 train: 0.08880805969238281 elapsed, loss: 5.779295e-06\n",
      "step: 49500 train: 0.07857370376586914 elapsed, loss: 3.9916326e-06\n",
      "step: 49510 train: 0.08730888366699219 elapsed, loss: 2.4246929e-06\n",
      "step: 49520 train: 0.08085036277770996 elapsed, loss: 1.9096742e-06\n",
      "step: 49530 train: 0.07867145538330078 elapsed, loss: 2.4442506e-06\n",
      "step: 49540 train: 0.07794046401977539 elapsed, loss: 1.5194505e-06\n",
      "step: 49550 train: 0.07866573333740234 elapsed, loss: 2.3045081e-06\n",
      "step: 49560 train: 0.08131837844848633 elapsed, loss: 1.2335355e-06\n",
      "step: 49570 train: 0.0796513557434082 elapsed, loss: 1.3597296e-06\n",
      "step: 49580 train: 0.0883479118347168 elapsed, loss: 1.5199128e-06\n",
      "step: 49590 train: 0.08394670486450195 elapsed, loss: 1.464503e-06\n",
      "step: 49600 train: 0.08014273643493652 elapsed, loss: 1.8505355e-06\n",
      "step: 49610 train: 0.07711458206176758 elapsed, loss: 1.7816178e-06\n",
      "step: 49620 train: 0.08357858657836914 elapsed, loss: 1.3043159e-06\n",
      "step: 49630 train: 0.0791473388671875 elapsed, loss: 1.5618255e-06\n",
      "step: 49640 train: 0.08285188674926758 elapsed, loss: 1.2940693e-06\n",
      "step: 49650 train: 0.08211064338684082 elapsed, loss: 1.7238758e-06\n",
      "step: 49660 train: 0.08434557914733887 elapsed, loss: 1.7126895e-06\n",
      "step: 49670 train: 0.08290934562683105 elapsed, loss: 1.2773078e-06\n",
      "step: 49680 train: 0.08326935768127441 elapsed, loss: 1.5264361e-06\n",
      "step: 49690 train: 0.07908487319946289 elapsed, loss: 1.3792874e-06\n",
      "step: 49700 train: 0.08024334907531738 elapsed, loss: 2.05496e-06\n",
      "step: 49710 train: 0.08549880981445312 elapsed, loss: 1.146922e-06\n",
      "step: 49720 train: 0.0808250904083252 elapsed, loss: 1.5320238e-06\n",
      "step: 49730 train: 0.07606983184814453 elapsed, loss: 4.803265e-06\n",
      "step: 49740 train: 0.07995486259460449 elapsed, loss: 1.3844093e-06\n",
      "step: 49750 train: 0.08647656440734863 elapsed, loss: 1.7229447e-06\n",
      "step: 49760 train: 0.07716059684753418 elapsed, loss: 2.9574082e-06\n",
      "step: 49770 train: 0.08457136154174805 elapsed, loss: 1.4253878e-06\n",
      "step: 49780 train: 0.08615422248840332 elapsed, loss: 2.1164274e-06\n",
      "step: 49790 train: 0.07822895050048828 elapsed, loss: 3.0607844e-06\n",
      "step: 49800 train: 0.08554577827453613 elapsed, loss: 1.3434317e-06\n",
      "step: 49810 train: 0.08387613296508789 elapsed, loss: 1.7462278e-06\n",
      "step: 49820 train: 0.07620811462402344 elapsed, loss: 2.7781298e-06\n",
      "step: 49830 train: 0.08013224601745605 elapsed, loss: 1.3606607e-06\n",
      "step: 49840 train: 0.07743668556213379 elapsed, loss: 1.4817326e-06\n",
      "step: 49850 train: 0.07725834846496582 elapsed, loss: 2.6030411e-06\n",
      "step: 49860 train: 0.0830225944519043 elapsed, loss: 1.40583e-06\n",
      "step: 49870 train: 0.0772247314453125 elapsed, loss: 2.0340053e-06\n",
      "step: 49880 train: 0.07851982116699219 elapsed, loss: 2.3515818e-06\n",
      "step: 49890 train: 0.07912254333496094 elapsed, loss: 0.00043872988\n",
      "step: 49900 train: 0.08005976676940918 elapsed, loss: 0.00013224142\n",
      "step: 49910 train: 0.0818185806274414 elapsed, loss: 6.194569e-05\n",
      "step: 49920 train: 0.0782325267791748 elapsed, loss: 2.992947e-05\n",
      "step: 49930 train: 0.07960057258605957 elapsed, loss: 1.8056879e-05\n",
      "step: 49940 train: 0.08048319816589355 elapsed, loss: 1.3971756e-05\n",
      "step: 49950 train: 0.07754206657409668 elapsed, loss: 1.3235282e-05\n",
      "step: 49960 train: 0.08015942573547363 elapsed, loss: 1.2968037e-05\n",
      "step: 49970 train: 0.0763087272644043 elapsed, loss: 1.4092535e-05\n",
      "step: 49980 train: 0.0858161449432373 elapsed, loss: 5.678674e-06\n",
      "step: 49990 train: 0.07477259635925293 elapsed, loss: 6.808856e-06\n",
      "step: 50000 train: 0.08682608604431152 elapsed, loss: 3.8347057e-06\n",
      "step: 50010 train: 0.08015251159667969 elapsed, loss: 3.3457625e-06\n",
      "step: 50020 train: 0.07919692993164062 elapsed, loss: 2.3753319e-06\n",
      "step: 50030 train: 0.08451271057128906 elapsed, loss: 3.5222529e-06\n",
      "step: 50040 train: 0.08878302574157715 elapsed, loss: 1.3723021e-06\n",
      "step: 50050 train: 0.08160114288330078 elapsed, loss: 1.4658999e-06\n",
      "step: 50060 train: 0.08018898963928223 elapsed, loss: 1.2917432e-06\n",
      "step: 50070 train: 0.08702540397644043 elapsed, loss: 1.2409861e-06\n",
      "step: 50080 train: 0.08512425422668457 elapsed, loss: 1.1408687e-06\n",
      "step: 50090 train: 0.08256053924560547 elapsed, loss: 1.2507651e-06\n",
      "step: 50100 train: 0.08723640441894531 elapsed, loss: 1.4062955e-06\n",
      "step: 50110 train: 0.08212947845458984 elapsed, loss: 1.4379596e-06\n",
      "step: 50120 train: 0.085296630859375 elapsed, loss: 1.1920919e-06\n",
      "step: 50130 train: 0.08059120178222656 elapsed, loss: 1.5334208e-06\n",
      "step: 50140 train: 0.08108234405517578 elapsed, loss: 1.9948895e-06\n",
      "step: 50150 train: 0.0787954330444336 elapsed, loss: 0.00024685496\n",
      "step: 50160 train: 0.09040999412536621 elapsed, loss: 0.0008681125\n",
      "step: 50170 train: 0.07993245124816895 elapsed, loss: 0.0144760115\n",
      "step: 50180 train: 0.08441829681396484 elapsed, loss: 8.4952146e-05\n",
      "step: 50190 train: 0.07799625396728516 elapsed, loss: 7.448878e-05\n",
      "step: 50200 train: 0.08210206031799316 elapsed, loss: 3.6434525e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50210 train: 0.08196377754211426 elapsed, loss: 2.2395234e-05\n",
      "step: 50220 train: 0.07774519920349121 elapsed, loss: 2.397547e-05\n",
      "step: 50230 train: 0.08679509162902832 elapsed, loss: 1.1147769e-05\n",
      "step: 50240 train: 0.08489346504211426 elapsed, loss: 1.3793788e-05\n",
      "step: 50250 train: 0.07703137397766113 elapsed, loss: 1.0602986e-05\n",
      "step: 50260 train: 0.07492566108703613 elapsed, loss: 1.1696333e-05\n",
      "step: 50270 train: 0.07723617553710938 elapsed, loss: 7.005824e-06\n",
      "step: 50280 train: 0.08375740051269531 elapsed, loss: 4.452148e-06\n",
      "step: 50290 train: 0.08067202568054199 elapsed, loss: 4.390212e-06\n",
      "step: 50300 train: 0.07908940315246582 elapsed, loss: 2.6584537e-06\n",
      "step: 50310 train: 0.08119583129882812 elapsed, loss: 2.605832e-06\n",
      "step: 50320 train: 0.07351517677307129 elapsed, loss: 2.4465794e-06\n",
      "step: 50330 train: 0.07902860641479492 elapsed, loss: 1.4738152e-06\n",
      "step: 50340 train: 0.08626556396484375 elapsed, loss: 1.7727692e-06\n",
      "step: 50350 train: 0.08109855651855469 elapsed, loss: 1.3015215e-06\n",
      "step: 50360 train: 0.08119630813598633 elapsed, loss: 1.0700887e-06\n",
      "step: 50370 train: 0.08808660507202148 elapsed, loss: 6.956976e-07\n",
      "step: 50380 train: 0.07946372032165527 elapsed, loss: 1.5594978e-06\n",
      "step: 50390 train: 0.07897377014160156 elapsed, loss: 1.964156e-06\n",
      "step: 50400 train: 0.0780935287475586 elapsed, loss: 9.960484e-07\n",
      "step: 50410 train: 0.08594655990600586 elapsed, loss: 8.367928e-07\n",
      "step: 50420 train: 0.08356189727783203 elapsed, loss: 9.2107734e-07\n",
      "step: 50430 train: 0.08492398262023926 elapsed, loss: 1.3397062e-06\n",
      "step: 50440 train: 0.07520914077758789 elapsed, loss: 1.5138631e-06\n",
      "step: 50450 train: 0.08840751647949219 elapsed, loss: 1.2316683e-06\n",
      "step: 50460 train: 0.08211660385131836 elapsed, loss: 1.0351641e-06\n",
      "step: 50470 train: 0.07885336875915527 elapsed, loss: 2.9061819e-06\n",
      "step: 50480 train: 0.07904934883117676 elapsed, loss: 1.2731168e-06\n",
      "step: 50490 train: 0.07911133766174316 elapsed, loss: 1.1594957e-06\n",
      "step: 50500 train: 0.08106565475463867 elapsed, loss: 1.3336528e-06\n",
      "step: 50510 train: 0.07780742645263672 elapsed, loss: 1.4337688e-06\n",
      "step: 50520 train: 0.08076262474060059 elapsed, loss: 1.4542586e-06\n",
      "step: 50530 train: 0.0830998420715332 elapsed, loss: 1.543199e-06\n",
      "step: 50540 train: 0.08154129981994629 elapsed, loss: 1.2102527e-06\n",
      "step: 50550 train: 0.08490800857543945 elapsed, loss: 9.131612e-07\n",
      "step: 50560 train: 0.08215904235839844 elapsed, loss: 1.0425412e-05\n",
      "step: 50570 train: 0.07535719871520996 elapsed, loss: 1.505514e-05\n",
      "step: 50580 train: 0.07840228080749512 elapsed, loss: 2.2295812e-06\n",
      "step: 50590 train: 0.08869004249572754 elapsed, loss: 1.3406374e-06\n",
      "step: 50600 train: 0.07850360870361328 elapsed, loss: 2.1439007e-06\n",
      "step: 50610 train: 0.0812830924987793 elapsed, loss: 1.3131637e-06\n",
      "step: 50620 train: 0.0827031135559082 elapsed, loss: 1.3126979e-06\n",
      "step: 50630 train: 0.0784616470336914 elapsed, loss: 1.3960512e-06\n",
      "step: 50640 train: 0.08014440536499023 elapsed, loss: 1.1888324e-06\n",
      "step: 50650 train: 0.07187128067016602 elapsed, loss: 2.2696297e-06\n",
      "step: 50660 train: 0.0789794921875 elapsed, loss: 1.6223619e-06\n",
      "step: 50670 train: 0.08052873611450195 elapsed, loss: 1.6228278e-06\n",
      "step: 50680 train: 0.08262515068054199 elapsed, loss: 2.0069524e-06\n",
      "step: 50690 train: 0.07819175720214844 elapsed, loss: 1.7676482e-06\n",
      "step: 50700 train: 0.08388328552246094 elapsed, loss: 1.157633e-06\n",
      "step: 50710 train: 0.08622622489929199 elapsed, loss: 1.3140948e-06\n",
      "step: 50720 train: 0.08154559135437012 elapsed, loss: 1.4468083e-06\n",
      "step: 50730 train: 0.08268332481384277 elapsed, loss: 1.2703229e-06\n",
      "step: 50740 train: 0.08041787147521973 elapsed, loss: 0.011024901\n",
      "step: 50750 train: 0.08254265785217285 elapsed, loss: 0.00015588186\n",
      "step: 50760 train: 0.08407783508300781 elapsed, loss: 2.7332984e-05\n",
      "step: 50770 train: 0.08127498626708984 elapsed, loss: 1.8387207e-05\n",
      "step: 50780 train: 0.08794021606445312 elapsed, loss: 1.8097126e-05\n",
      "step: 50790 train: 0.07732701301574707 elapsed, loss: 2.1587479e-05\n",
      "step: 50800 train: 0.0805056095123291 elapsed, loss: 1.134269e-05\n",
      "step: 50810 train: 0.08070135116577148 elapsed, loss: 8.206699e-06\n",
      "step: 50820 train: 0.08306670188903809 elapsed, loss: 4.2691627e-06\n",
      "step: 50830 train: 0.0780026912689209 elapsed, loss: 1.1350296e-05\n",
      "step: 50840 train: 0.08157706260681152 elapsed, loss: 3.6414588e-06\n",
      "step: 50850 train: 0.0816192626953125 elapsed, loss: 3.7699765e-06\n",
      "step: 50860 train: 0.08187150955200195 elapsed, loss: 2.7287674e-06\n",
      "step: 50870 train: 0.08327913284301758 elapsed, loss: 1.8081604e-06\n",
      "step: 50880 train: 0.0820627212524414 elapsed, loss: 2.1927938e-06\n",
      "step: 50890 train: 0.07629632949829102 elapsed, loss: 2.1974515e-06\n",
      "step: 50900 train: 0.07894635200500488 elapsed, loss: 2.3515854e-06\n",
      "step: 50910 train: 0.08234667778015137 elapsed, loss: 1.7499528e-06\n",
      "step: 50920 train: 0.07650160789489746 elapsed, loss: 1.2735825e-06\n",
      "step: 50930 train: 0.08275580406188965 elapsed, loss: 1.5469243e-06\n",
      "step: 50940 train: 0.0794532299041748 elapsed, loss: 1.8789406e-06\n",
      "step: 50950 train: 0.08541584014892578 elapsed, loss: 9.625212e-07\n",
      "step: 50960 train: 0.08223772048950195 elapsed, loss: 1.3671789e-06\n",
      "step: 50970 train: 0.08844375610351562 elapsed, loss: 9.904609e-07\n",
      "step: 50980 train: 0.08889222145080566 elapsed, loss: 1.1082693e-06\n",
      "step: 50990 train: 0.08623623847961426 elapsed, loss: 1.0966314e-06\n",
      "step: 51000 train: 0.0794670581817627 elapsed, loss: 2.1247984e-06\n",
      "step: 51010 train: 0.08335709571838379 elapsed, loss: 1.250765e-06\n",
      "step: 51020 train: 0.08272027969360352 elapsed, loss: 1.3899969e-06\n",
      "step: 51030 train: 0.07729578018188477 elapsed, loss: 2.243551e-06\n",
      "step: 51040 train: 0.07837820053100586 elapsed, loss: 3.6139822e-06\n",
      "step: 51050 train: 0.07770729064941406 elapsed, loss: 1.4984964e-06\n",
      "step: 51060 train: 0.08246445655822754 elapsed, loss: 1.5888285e-06\n",
      "step: 51070 train: 0.08339810371398926 elapsed, loss: 1.5120004e-06\n",
      "step: 51080 train: 0.08426928520202637 elapsed, loss: 2.6845599e-05\n",
      "step: 51090 train: 0.0802619457244873 elapsed, loss: 5.2982687e-06\n",
      "step: 51100 train: 0.08370804786682129 elapsed, loss: 1.8519322e-06\n",
      "step: 51110 train: 0.07836794853210449 elapsed, loss: 2.071255e-06\n",
      "step: 51120 train: 0.07585263252258301 elapsed, loss: 3.139451e-06\n",
      "step: 51130 train: 0.07683134078979492 elapsed, loss: 1.6740503e-06\n",
      "step: 51140 train: 0.08343243598937988 elapsed, loss: 1.3182851e-06\n",
      "step: 51150 train: 0.08513331413269043 elapsed, loss: 1.1832442e-06\n",
      "step: 51160 train: 0.08344388008117676 elapsed, loss: 1.6367969e-06\n",
      "step: 51170 train: 0.08050894737243652 elapsed, loss: 1.8873221e-06\n",
      "step: 51180 train: 0.07422471046447754 elapsed, loss: 3.0426238e-06\n",
      "step: 51190 train: 0.07942891120910645 elapsed, loss: 7.869576e-06\n",
      "step: 51200 train: 0.07920670509338379 elapsed, loss: 2.3790597e-06\n",
      "step: 51210 train: 0.0842583179473877 elapsed, loss: 1.4710224e-06\n",
      "step: 51220 train: 0.07738423347473145 elapsed, loss: 1.3341167e-06\n",
      "step: 51230 train: 0.078460693359375 elapsed, loss: 2.0805714e-06\n",
      "step: 51240 train: 0.0798943042755127 elapsed, loss: 1.7299251e-06\n",
      "step: 51250 train: 0.08292341232299805 elapsed, loss: 1.8360998e-06\n",
      "step: 51260 train: 0.08018183708190918 elapsed, loss: 1.5967507e-06\n",
      "step: 51270 train: 0.08502078056335449 elapsed, loss: 1.3373781e-06\n",
      "step: 51280 train: 0.08824872970581055 elapsed, loss: 1.3527448e-06\n",
      "step: 51290 train: 0.08078217506408691 elapsed, loss: 1.9529807e-06\n",
      "step: 51300 train: 0.07846689224243164 elapsed, loss: 1.6428504e-06\n",
      "step: 51310 train: 0.08229970932006836 elapsed, loss: 1.5422686e-06\n",
      "step: 51320 train: 0.08110523223876953 elapsed, loss: 0.00023533398\n",
      "step: 51330 train: 0.07845091819763184 elapsed, loss: 0.0021492366\n",
      "step: 51340 train: 0.07941865921020508 elapsed, loss: 0.00016956398\n",
      "step: 51350 train: 0.082550048828125 elapsed, loss: 5.8160982e-05\n",
      "step: 51360 train: 0.08317780494689941 elapsed, loss: 0.00030889863\n",
      "step: 51370 train: 0.07947516441345215 elapsed, loss: 0.00023828268\n",
      "step: 51380 train: 0.07421469688415527 elapsed, loss: 8.141073e-05\n",
      "step: 51390 train: 0.08464312553405762 elapsed, loss: 4.0492312e-05\n",
      "step: 51400 train: 0.08263993263244629 elapsed, loss: 1.9613139e-05\n",
      "step: 51410 train: 0.08469676971435547 elapsed, loss: 1.4371924e-05\n",
      "step: 51420 train: 0.08594179153442383 elapsed, loss: 1.17632235e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 51430 train: 0.08132362365722656 elapsed, loss: 1.0327305e-05\n",
      "step: 51440 train: 0.07561707496643066 elapsed, loss: 1.2333792e-05\n",
      "step: 51450 train: 0.07995891571044922 elapsed, loss: 4.8214315e-06\n",
      "step: 51460 train: 0.08126997947692871 elapsed, loss: 5.6484223e-06\n",
      "step: 51470 train: 0.07800769805908203 elapsed, loss: 3.6922e-06\n",
      "step: 51480 train: 0.08349776268005371 elapsed, loss: 2.841923e-06\n",
      "step: 51490 train: 0.08022260665893555 elapsed, loss: 1.7695106e-06\n",
      "step: 51500 train: 0.07625150680541992 elapsed, loss: 2.1764954e-06\n",
      "step: 51510 train: 0.08051228523254395 elapsed, loss: 1.4044313e-06\n",
      "step: 51520 train: 0.08284568786621094 elapsed, loss: 1.2409862e-06\n",
      "step: 51530 train: 0.0860600471496582 elapsed, loss: 3.2419225e-06\n",
      "step: 51540 train: 0.08468127250671387 elapsed, loss: 1.4300443e-06\n",
      "step: 51550 train: 0.0843045711517334 elapsed, loss: 9.5367363e-07\n",
      "step: 51560 train: 0.07643508911132812 elapsed, loss: 1.6549528e-06\n",
      "step: 51570 train: 0.0878453254699707 elapsed, loss: 1.2200315e-06\n",
      "step: 51580 train: 0.07572078704833984 elapsed, loss: 1.5171217e-06\n",
      "step: 51590 train: 0.08420395851135254 elapsed, loss: 9.53208e-07\n",
      "step: 51600 train: 0.08456635475158691 elapsed, loss: 3.43512e-06\n",
      "step: 51610 train: 0.08631110191345215 elapsed, loss: 4.4069893e-05\n",
      "step: 51620 train: 0.08147311210632324 elapsed, loss: 1.2685776e-05\n",
      "step: 51630 train: 0.08433771133422852 elapsed, loss: 1.2537726e-05\n",
      "step: 51640 train: 0.08334779739379883 elapsed, loss: 1.1143017e-05\n",
      "step: 51650 train: 0.0785672664642334 elapsed, loss: 6.2742806e-06\n",
      "step: 51660 train: 0.07787823677062988 elapsed, loss: 5.4416632e-06\n",
      "step: 51670 train: 0.08601975440979004 elapsed, loss: 3.8430708e-06\n",
      "step: 51680 train: 0.08003044128417969 elapsed, loss: 5.6414146e-06\n",
      "step: 51690 train: 0.0807654857635498 elapsed, loss: 2.5215506e-06\n",
      "step: 51700 train: 0.08286523818969727 elapsed, loss: 1.7974501e-06\n",
      "step: 51710 train: 0.08138418197631836 elapsed, loss: 2.1327241e-06\n",
      "step: 51720 train: 0.07295989990234375 elapsed, loss: 2.366021e-06\n",
      "step: 51730 train: 0.08411335945129395 elapsed, loss: 1.2614751e-06\n",
      "step: 51740 train: 0.09004569053649902 elapsed, loss: 1.0426139e-06\n",
      "step: 51750 train: 0.08455252647399902 elapsed, loss: 1.0896466e-06\n",
      "step: 51760 train: 0.08063292503356934 elapsed, loss: 1.0598442e-06\n",
      "step: 51770 train: 0.07600045204162598 elapsed, loss: 1.2866208e-06\n",
      "step: 51780 train: 0.07816481590270996 elapsed, loss: 1.2950029e-06\n",
      "step: 51790 train: 0.07927155494689941 elapsed, loss: 9.415664e-07\n",
      "step: 51800 train: 0.09052133560180664 elapsed, loss: 7.6647797e-07\n",
      "step: 51810 train: 0.08704590797424316 elapsed, loss: 8.8149625e-07\n",
      "step: 51820 train: 0.08408713340759277 elapsed, loss: 8.8009926e-07\n",
      "step: 51830 train: 0.08378124237060547 elapsed, loss: 9.508798e-07\n",
      "step: 51840 train: 0.0795443058013916 elapsed, loss: 1.7904654e-06\n",
      "step: 51850 train: 0.07582449913024902 elapsed, loss: 1.4328383e-06\n",
      "step: 51860 train: 0.0858609676361084 elapsed, loss: 8.782366e-07\n",
      "step: 51870 train: 0.08005404472351074 elapsed, loss: 1.8379618e-06\n",
      "step: 51880 train: 0.0814061164855957 elapsed, loss: 1.7480903e-06\n",
      "step: 51890 train: 0.08458900451660156 elapsed, loss: 1.0491342e-06\n",
      "step: 51900 train: 0.07639455795288086 elapsed, loss: 1.626553e-06\n",
      "step: 51910 train: 0.0846858024597168 elapsed, loss: 1.0975627e-06\n",
      "step: 51920 train: 0.07817482948303223 elapsed, loss: 1.1380748e-06\n",
      "step: 51930 train: 0.07942700386047363 elapsed, loss: 1.9064144e-06\n",
      "step: 51940 train: 0.08005571365356445 elapsed, loss: 1.7862745e-06\n",
      "step: 51950 train: 0.08414983749389648 elapsed, loss: 1.2004731e-06\n",
      "step: 51960 train: 0.07600283622741699 elapsed, loss: 2.214681e-06\n",
      "step: 51970 train: 0.08463120460510254 elapsed, loss: 1.0831274e-06\n",
      "step: 51980 train: 0.0770106315612793 elapsed, loss: 1.7099061e-06\n",
      "step: 51990 train: 0.0764777660369873 elapsed, loss: 1.4845268e-06\n",
      "step: 52000 train: 0.08132362365722656 elapsed, loss: 1.1855727e-06\n",
      "step: 52010 train: 0.08329510688781738 elapsed, loss: 6.553657e-06\n",
      "step: 52020 train: 0.08272171020507812 elapsed, loss: 2.305018e-06\n",
      "step: 52030 train: 0.08853936195373535 elapsed, loss: 1.8267867e-06\n",
      "step: 52040 train: 0.0815584659576416 elapsed, loss: 2.0428529e-06\n",
      "step: 52050 train: 0.07618832588195801 elapsed, loss: 2.5038562e-06\n",
      "step: 52060 train: 0.08188629150390625 elapsed, loss: 1.3387746e-06\n",
      "step: 52070 train: 0.08362984657287598 elapsed, loss: 1.8342375e-06\n",
      "step: 52080 train: 0.08993768692016602 elapsed, loss: 1.0910433e-06\n",
      "step: 52090 train: 0.07676815986633301 elapsed, loss: 2.0759146e-06\n",
      "step: 52100 train: 0.08952212333679199 elapsed, loss: 1.2000075e-06\n",
      "step: 52110 train: 0.07976007461547852 elapsed, loss: 1.7713726e-06\n",
      "step: 52120 train: 0.08320498466491699 elapsed, loss: 1.2693915e-06\n",
      "step: 52130 train: 0.0892324447631836 elapsed, loss: 1.7136306e-06\n",
      "step: 52140 train: 0.08886599540710449 elapsed, loss: 1.283827e-06\n",
      "step: 52150 train: 0.0791778564453125 elapsed, loss: 1.5893004e-06\n",
      "step: 52160 train: 0.08004426956176758 elapsed, loss: 1.4342351e-06\n",
      "step: 52170 train: 0.08005046844482422 elapsed, loss: 1.9189877e-06\n",
      "step: 52180 train: 0.08005976676940918 elapsed, loss: 2.687789e-06\n",
      "step: 52190 train: 0.08439397811889648 elapsed, loss: 1.9110712e-06\n",
      "step: 52200 train: 0.07675290107727051 elapsed, loss: 1.7764954e-06\n",
      "step: 52210 train: 0.08362102508544922 elapsed, loss: 1.8146794e-06\n",
      "step: 52220 train: 0.08174276351928711 elapsed, loss: 2.9359853e-06\n",
      "step: 52230 train: 0.07969236373901367 elapsed, loss: 1.711769e-06\n",
      "step: 52240 train: 0.07772231101989746 elapsed, loss: 1.8347022e-06\n",
      "step: 52250 train: 0.07742977142333984 elapsed, loss: 1.9492554e-06\n",
      "step: 52260 train: 0.08143138885498047 elapsed, loss: 4.155975e-06\n",
      "step: 52270 train: 0.08069491386413574 elapsed, loss: 2.1811543e-06\n",
      "step: 52280 train: 0.07735013961791992 elapsed, loss: 1.72667e-06\n",
      "step: 52290 train: 0.08275222778320312 elapsed, loss: 1.569277e-06\n",
      "step: 52300 train: 0.08559203147888184 elapsed, loss: 1.5641547e-06\n",
      "step: 52310 train: 0.08351922035217285 elapsed, loss: 1.6177048e-06\n",
      "step: 52320 train: 0.0870676040649414 elapsed, loss: 1.2358641e-06\n",
      "step: 52330 train: 0.08217930793762207 elapsed, loss: 2.2826682e-06\n",
      "step: 52340 train: 0.08026576042175293 elapsed, loss: 2.0498383e-06\n",
      "step: 52350 train: 0.08587002754211426 elapsed, loss: 1.9557738e-06\n",
      "step: 52360 train: 0.07940125465393066 elapsed, loss: 2.0097891e-06\n",
      "step: 52370 train: 0.0759577751159668 elapsed, loss: 3.8570615e-06\n",
      "step: 52380 train: 0.07788538932800293 elapsed, loss: 2.4307478e-06\n",
      "step: 52390 train: 0.08320212364196777 elapsed, loss: 2.0172401e-06\n",
      "step: 52400 train: 0.08595824241638184 elapsed, loss: 1.438892e-06\n",
      "step: 52410 train: 0.08530354499816895 elapsed, loss: 7.855594e-06\n",
      "step: 52420 train: 0.08133316040039062 elapsed, loss: 2.7138663e-06\n",
      "step: 52430 train: 0.07403278350830078 elapsed, loss: 3.353218e-06\n",
      "step: 52440 train: 0.08088541030883789 elapsed, loss: 1.8463446e-06\n",
      "step: 52450 train: 0.08158731460571289 elapsed, loss: 2.0088446e-06\n",
      "step: 52460 train: 0.07948565483093262 elapsed, loss: 2.730088e-06\n",
      "step: 52470 train: 0.08366942405700684 elapsed, loss: 0.00021592599\n",
      "step: 52480 train: 0.0823369026184082 elapsed, loss: 6.5559376e-05\n",
      "step: 52490 train: 0.0806429386138916 elapsed, loss: 2.4613799e-05\n",
      "step: 52500 train: 0.0865621566772461 elapsed, loss: 2.1880878e-05\n",
      "step: 52510 train: 0.08010292053222656 elapsed, loss: 2.8757879e-05\n",
      "step: 52520 train: 0.08316493034362793 elapsed, loss: 9.187379e-06\n",
      "step: 52530 train: 0.0788736343383789 elapsed, loss: 1.0497253e-05\n",
      "step: 52540 train: 0.08808040618896484 elapsed, loss: 7.1649565e-06\n",
      "step: 52550 train: 0.08427000045776367 elapsed, loss: 4.5536717e-06\n",
      "step: 52560 train: 0.07860493659973145 elapsed, loss: 4.895474e-06\n",
      "step: 52570 train: 0.0806722640991211 elapsed, loss: 3.3872088e-06\n",
      "step: 52580 train: 0.08205270767211914 elapsed, loss: 2.6798716e-06\n",
      "step: 52590 train: 0.07834768295288086 elapsed, loss: 3.15345e-06\n",
      "step: 52600 train: 0.08418965339660645 elapsed, loss: 2.0996636e-06\n",
      "step: 52610 train: 0.08216094970703125 elapsed, loss: 2.115029e-06\n",
      "step: 52620 train: 0.07590126991271973 elapsed, loss: 2.4363349e-06\n",
      "step: 52630 train: 0.08300328254699707 elapsed, loss: 1.3452936e-06\n",
      "step: 52640 train: 0.08428621292114258 elapsed, loss: 2.5518127e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 52650 train: 0.08353257179260254 elapsed, loss: 1.4039674e-06\n",
      "step: 52660 train: 0.07737302780151367 elapsed, loss: 2.1103738e-06\n",
      "step: 52670 train: 0.08233880996704102 elapsed, loss: 1.9995464e-06\n",
      "step: 52680 train: 0.08037757873535156 elapsed, loss: 1.6270176e-06\n",
      "step: 52690 train: 0.08371949195861816 elapsed, loss: 1.5138631e-06\n",
      "step: 52700 train: 0.08322858810424805 elapsed, loss: 1.3797533e-06\n",
      "step: 52710 train: 0.08610343933105469 elapsed, loss: 1.3383092e-06\n",
      "step: 52720 train: 0.08394956588745117 elapsed, loss: 1.2782391e-06\n",
      "step: 52730 train: 0.09073686599731445 elapsed, loss: 1.1636866e-06\n",
      "step: 52740 train: 0.08394932746887207 elapsed, loss: 1.280567e-06\n",
      "step: 52750 train: 0.08028888702392578 elapsed, loss: 1.3881345e-06\n",
      "step: 52760 train: 0.0803229808807373 elapsed, loss: 2.3753341e-06\n",
      "step: 52770 train: 0.0880270004272461 elapsed, loss: 1.3099041e-06\n",
      "step: 52780 train: 0.07984709739685059 elapsed, loss: 1.9073445e-06\n",
      "step: 52790 train: 0.07932138442993164 elapsed, loss: 2.0903508e-06\n",
      "step: 52800 train: 0.07932138442993164 elapsed, loss: 1.9497215e-06\n",
      "step: 52810 train: 0.08109617233276367 elapsed, loss: 1.4607779e-06\n",
      "step: 52820 train: 0.0775749683380127 elapsed, loss: 1.7313266e-06\n",
      "step: 52830 train: 0.07906460762023926 elapsed, loss: 2.8069999e-06\n",
      "step: 52840 train: 0.07968878746032715 elapsed, loss: 2.6700961e-06\n",
      "step: 52850 train: 0.07658195495605469 elapsed, loss: 2.1867409e-06\n",
      "step: 52860 train: 0.07903885841369629 elapsed, loss: 4.609563e-06\n",
      "step: 52870 train: 0.0817255973815918 elapsed, loss: 2.037731e-06\n",
      "step: 52880 train: 0.07687258720397949 elapsed, loss: 2.523878e-06\n",
      "step: 52890 train: 0.08740735054016113 elapsed, loss: 1.3187514e-06\n",
      "step: 52900 train: 0.07949185371398926 elapsed, loss: 2.3976859e-06\n",
      "step: 52910 train: 0.0826561450958252 elapsed, loss: 1.7434317e-06\n",
      "step: 52920 train: 0.08663034439086914 elapsed, loss: 1.5166572e-06\n",
      "step: 52930 train: 0.0800330638885498 elapsed, loss: 2.5504228e-06\n",
      "step: 52940 train: 0.08040571212768555 elapsed, loss: 1.9879053e-06\n",
      "step: 52950 train: 0.08400487899780273 elapsed, loss: 1.9101392e-06\n",
      "step: 52960 train: 0.07889747619628906 elapsed, loss: 2.249606e-06\n",
      "step: 52970 train: 0.08265376091003418 elapsed, loss: 2.9001299e-06\n",
      "step: 52980 train: 0.08639669418334961 elapsed, loss: 1.6926767e-06\n",
      "step: 52990 train: 0.0842428207397461 elapsed, loss: 2.3655543e-06\n",
      "step: 53000 train: 0.07956528663635254 elapsed, loss: 5.766696e-06\n",
      "step: 53010 train: 0.07732129096984863 elapsed, loss: 2.137848e-06\n",
      "step: 53020 train: 0.07862520217895508 elapsed, loss: 2.1266721e-06\n",
      "step: 53030 train: 0.08008551597595215 elapsed, loss: 1.6535614e-06\n",
      "step: 53040 train: 0.0783853530883789 elapsed, loss: 1.9394697e-06\n",
      "step: 53050 train: 0.07969069480895996 elapsed, loss: 1.8305118e-06\n",
      "step: 53060 train: 0.07865548133850098 elapsed, loss: 0.0009284264\n",
      "step: 53070 train: 0.07818293571472168 elapsed, loss: 6.7965084e-05\n",
      "step: 53080 train: 0.08204054832458496 elapsed, loss: 4.922253e-05\n",
      "step: 53090 train: 0.08223605155944824 elapsed, loss: 2.4682558e-05\n",
      "step: 53100 train: 0.08380293846130371 elapsed, loss: 1.6958567e-05\n",
      "step: 53110 train: 0.07797074317932129 elapsed, loss: 1.8071087e-05\n",
      "step: 53120 train: 0.08418464660644531 elapsed, loss: 9.232066e-06\n",
      "step: 53130 train: 0.0833137035369873 elapsed, loss: 1.0141088e-05\n",
      "step: 53140 train: 0.08076906204223633 elapsed, loss: 1.0902077e-05\n",
      "step: 53150 train: 0.08149361610412598 elapsed, loss: 4.753892e-06\n",
      "step: 53160 train: 0.08431339263916016 elapsed, loss: 3.7131672e-06\n",
      "step: 53170 train: 0.0834047794342041 elapsed, loss: 2.7380802e-06\n",
      "step: 53180 train: 0.08450484275817871 elapsed, loss: 3.251233e-06\n",
      "step: 53190 train: 0.07851076126098633 elapsed, loss: 3.2638109e-06\n",
      "step: 53200 train: 0.07821416854858398 elapsed, loss: 2.54949e-06\n",
      "step: 53210 train: 0.08545589447021484 elapsed, loss: 1.6791726e-06\n",
      "step: 53220 train: 0.07659649848937988 elapsed, loss: 2.1951241e-06\n",
      "step: 53230 train: 0.08582663536071777 elapsed, loss: 1.4360979e-06\n",
      "step: 53240 train: 0.07185673713684082 elapsed, loss: 2.552749e-06\n",
      "step: 53250 train: 0.07822728157043457 elapsed, loss: 1.4491363e-06\n",
      "step: 53260 train: 0.0825965404510498 elapsed, loss: 1.4477391e-06\n",
      "step: 53270 train: 0.08018875122070312 elapsed, loss: 0.00094577693\n",
      "step: 53280 train: 0.08421039581298828 elapsed, loss: 0.007646342\n",
      "step: 53290 train: 0.08345770835876465 elapsed, loss: 7.367012e-05\n",
      "step: 53300 train: 0.07923412322998047 elapsed, loss: 8.18273e-05\n",
      "step: 53310 train: 0.08732938766479492 elapsed, loss: 3.5121047e-05\n",
      "step: 53320 train: 0.07973217964172363 elapsed, loss: 3.618553e-05\n",
      "step: 53330 train: 0.080352783203125 elapsed, loss: 2.8728897e-05\n",
      "step: 53340 train: 0.07761812210083008 elapsed, loss: 2.0239913e-05\n",
      "step: 53350 train: 0.0792684555053711 elapsed, loss: 6.98065e-06\n",
      "step: 53360 train: 0.08474946022033691 elapsed, loss: 1.0825561e-05\n",
      "step: 53370 train: 0.08181905746459961 elapsed, loss: 4.7241065e-06\n",
      "step: 53380 train: 0.0792841911315918 elapsed, loss: 5.75135e-06\n",
      "step: 53390 train: 0.08755373954772949 elapsed, loss: 3.2093267e-06\n",
      "step: 53400 train: 0.0774998664855957 elapsed, loss: 3.4542618e-06\n",
      "step: 53410 train: 0.08446645736694336 elapsed, loss: 1.8631068e-06\n",
      "step: 53420 train: 0.07850885391235352 elapsed, loss: 1.5478562e-06\n",
      "step: 53430 train: 0.08170843124389648 elapsed, loss: 1.7904645e-06\n",
      "step: 53440 train: 0.08301997184753418 elapsed, loss: 1.4686939e-06\n",
      "step: 53450 train: 0.08730268478393555 elapsed, loss: 1.0402853e-06\n",
      "step: 53460 train: 0.08136129379272461 elapsed, loss: 2.5387785e-06\n",
      "step: 53470 train: 0.07767844200134277 elapsed, loss: 1.7578686e-06\n",
      "step: 53480 train: 0.0841073989868164 elapsed, loss: 2.502919e-06\n",
      "step: 53490 train: 0.0786750316619873 elapsed, loss: 1.5133974e-06\n",
      "step: 53500 train: 0.07900404930114746 elapsed, loss: 1.7280586e-06\n",
      "step: 53510 train: 0.08677983283996582 elapsed, loss: 9.881326e-07\n",
      "step: 53520 train: 0.09112930297851562 elapsed, loss: 9.91392e-07\n",
      "step: 53530 train: 0.08869194984436035 elapsed, loss: 1.0514623e-06\n",
      "step: 53540 train: 0.08120369911193848 elapsed, loss: 1.7434336e-06\n",
      "step: 53550 train: 0.08333706855773926 elapsed, loss: 1.0207286e-06\n",
      "step: 53560 train: 0.08125662803649902 elapsed, loss: 1.8347026e-06\n",
      "step: 53570 train: 0.07926082611083984 elapsed, loss: 1.7061731e-06\n",
      "step: 53580 train: 0.08214616775512695 elapsed, loss: 1.0528593e-06\n",
      "step: 53590 train: 0.07720804214477539 elapsed, loss: 1.1650834e-06\n",
      "step: 53600 train: 0.08198666572570801 elapsed, loss: 1.1492511e-06\n",
      "step: 53610 train: 0.0798346996307373 elapsed, loss: 1.585109e-06\n",
      "step: 53620 train: 0.0859687328338623 elapsed, loss: 1.6861563e-06\n",
      "step: 53630 train: 0.07890629768371582 elapsed, loss: 1.1408688e-06\n",
      "step: 53640 train: 0.08346915245056152 elapsed, loss: 1.1399379e-06\n",
      "step: 53650 train: 0.08564162254333496 elapsed, loss: 9.373755e-07\n",
      "step: 53660 train: 0.07519221305847168 elapsed, loss: 1.9543777e-06\n",
      "step: 53670 train: 0.08547711372375488 elapsed, loss: 1.1450602e-06\n",
      "step: 53680 train: 0.07890105247497559 elapsed, loss: 1.5888345e-06\n",
      "step: 53690 train: 0.07236456871032715 elapsed, loss: 2.0377306e-06\n",
      "step: 53700 train: 0.08312511444091797 elapsed, loss: 1.9613628e-06\n",
      "step: 53710 train: 0.08197760581970215 elapsed, loss: 1.3080413e-06\n",
      "step: 53720 train: 0.07884359359741211 elapsed, loss: 1.8831317e-06\n",
      "step: 53730 train: 0.07688379287719727 elapsed, loss: 1.6447141e-06\n",
      "step: 53740 train: 0.0885002613067627 elapsed, loss: 1.561826e-06\n",
      "step: 53750 train: 0.0821220874786377 elapsed, loss: 1.8104888e-06\n",
      "step: 53760 train: 0.08564162254333496 elapsed, loss: 1.1436629e-06\n",
      "step: 53770 train: 0.08124518394470215 elapsed, loss: 1.7681128e-06\n",
      "step: 53780 train: 0.083404541015625 elapsed, loss: 1.89617e-06\n",
      "step: 53790 train: 0.08477783203125 elapsed, loss: 0.0010855144\n",
      "step: 53800 train: 0.07949471473693848 elapsed, loss: 0.0010805144\n",
      "step: 53810 train: 0.08117318153381348 elapsed, loss: 6.978675e-05\n",
      "step: 53820 train: 0.09000158309936523 elapsed, loss: 6.368535e-05\n",
      "step: 53830 train: 0.07944393157958984 elapsed, loss: 2.2676637e-05\n",
      "step: 53840 train: 0.07700991630554199 elapsed, loss: 1.9766445e-05\n",
      "step: 53850 train: 0.08167600631713867 elapsed, loss: 1.9710234e-05\n",
      "step: 53860 train: 0.07936358451843262 elapsed, loss: 9.742933e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 53870 train: 0.08145380020141602 elapsed, loss: 1.0435646e-05\n",
      "step: 53880 train: 0.08072257041931152 elapsed, loss: 1.12664e-05\n",
      "step: 53890 train: 0.07980823516845703 elapsed, loss: 6.670527e-06\n",
      "step: 53900 train: 0.08038139343261719 elapsed, loss: 4.2607635e-06\n",
      "step: 53910 train: 0.08252477645874023 elapsed, loss: 3.2442517e-06\n",
      "step: 53920 train: 0.07762765884399414 elapsed, loss: 2.992333e-06\n",
      "step: 53930 train: 0.07761049270629883 elapsed, loss: 0.0010631965\n",
      "step: 53940 train: 0.08530616760253906 elapsed, loss: 3.134735e-05\n",
      "step: 53950 train: 0.07997918128967285 elapsed, loss: 2.741316e-05\n",
      "step: 53960 train: 0.07954025268554688 elapsed, loss: 1.3148648e-05\n",
      "step: 53970 train: 0.0838472843170166 elapsed, loss: 9.487272e-06\n",
      "step: 53980 train: 0.08158707618713379 elapsed, loss: 1.2892063e-05\n",
      "step: 53990 train: 0.08246874809265137 elapsed, loss: 6.1303717e-06\n",
      "step: 54000 train: 0.07909297943115234 elapsed, loss: 8.828285e-06\n",
      "step: 54010 train: 0.07889413833618164 elapsed, loss: 4.420037e-06\n",
      "step: 54020 train: 0.07699728012084961 elapsed, loss: 4.0284176e-06\n",
      "step: 54030 train: 0.08058929443359375 elapsed, loss: 4.2347046e-06\n",
      "step: 54040 train: 0.08558845520019531 elapsed, loss: 1.8398247e-06\n",
      "step: 54050 train: 0.07878899574279785 elapsed, loss: 2.8945437e-06\n",
      "step: 54060 train: 0.08545207977294922 elapsed, loss: 1.2884833e-06\n",
      "step: 54070 train: 0.08276247978210449 elapsed, loss: 1.4165398e-06\n",
      "step: 54080 train: 0.08445048332214355 elapsed, loss: 1.4328382e-06\n",
      "step: 54090 train: 0.07854390144348145 elapsed, loss: 1.0323704e-06\n",
      "step: 54100 train: 0.08123993873596191 elapsed, loss: 1.6470414e-06\n",
      "step: 54110 train: 0.08874392509460449 elapsed, loss: 1.0034989e-06\n",
      "step: 54120 train: 0.08223342895507812 elapsed, loss: 8.819616e-07\n",
      "step: 54130 train: 0.0813140869140625 elapsed, loss: 8.1583755e-07\n",
      "step: 54140 train: 0.07982945442199707 elapsed, loss: 1.2153748e-06\n",
      "step: 54150 train: 0.08200740814208984 elapsed, loss: 1.1087386e-06\n",
      "step: 54160 train: 0.08172988891601562 elapsed, loss: 1.0360955e-06\n",
      "step: 54170 train: 0.08379268646240234 elapsed, loss: 9.154895e-07\n",
      "step: 54180 train: 0.08140254020690918 elapsed, loss: 9.741627e-07\n",
      "step: 54190 train: 0.08111715316772461 elapsed, loss: 1.0984942e-06\n",
      "step: 54200 train: 0.0827333927154541 elapsed, loss: 1.0444774e-06\n",
      "step: 54210 train: 0.0787053108215332 elapsed, loss: 1.7075779e-06\n",
      "step: 54220 train: 0.08173322677612305 elapsed, loss: 1.0412177e-06\n",
      "step: 54230 train: 0.0791769027709961 elapsed, loss: 2.0135137e-06\n",
      "step: 54240 train: 0.08028626441955566 elapsed, loss: 1.1147922e-06\n",
      "step: 54250 train: 0.080291748046875 elapsed, loss: 8.977944e-07\n",
      "step: 54260 train: 0.07874870300292969 elapsed, loss: 1.5068783e-06\n",
      "step: 54270 train: 0.0828695297241211 elapsed, loss: 1.0309734e-06\n",
      "step: 54280 train: 0.0820000171661377 elapsed, loss: 1.3275992e-06\n",
      "step: 54290 train: 0.08104920387268066 elapsed, loss: 1.2335356e-06\n",
      "step: 54300 train: 0.0777125358581543 elapsed, loss: 1.2596128e-06\n",
      "step: 54310 train: 0.08049416542053223 elapsed, loss: 1.6526292e-06\n",
      "step: 54320 train: 0.07902288436889648 elapsed, loss: 2.4675337e-06\n",
      "step: 54330 train: 0.07796931266784668 elapsed, loss: 1.7210822e-06\n",
      "step: 54340 train: 0.07901835441589355 elapsed, loss: 1.5376108e-06\n",
      "step: 54350 train: 0.08274292945861816 elapsed, loss: 1.2265508e-06\n",
      "step: 54360 train: 0.08350157737731934 elapsed, loss: 1.0882496e-06\n",
      "step: 54370 train: 0.07978224754333496 elapsed, loss: 1.8957044e-06\n",
      "step: 54380 train: 0.07578682899475098 elapsed, loss: 1.5893002e-06\n",
      "step: 54390 train: 0.08071708679199219 elapsed, loss: 1.0919749e-06\n",
      "step: 54400 train: 0.07666516304016113 elapsed, loss: 2.1676483e-06\n",
      "step: 54410 train: 0.08196759223937988 elapsed, loss: 1.2731168e-06\n",
      "step: 54420 train: 0.08614492416381836 elapsed, loss: 9.937205e-07\n",
      "step: 54430 train: 0.07964468002319336 elapsed, loss: 2.7189892e-06\n",
      "step: 54440 train: 0.08515453338623047 elapsed, loss: 1.6349338e-06\n",
      "step: 54450 train: 0.08180880546569824 elapsed, loss: 2.6673026e-06\n",
      "step: 54460 train: 0.09101629257202148 elapsed, loss: 1.4295788e-06\n",
      "step: 54470 train: 0.09209156036376953 elapsed, loss: 1.2139778e-06\n",
      "step: 54480 train: 0.08163666725158691 elapsed, loss: 2.5792924e-06\n",
      "step: 54490 train: 0.08013772964477539 elapsed, loss: 2.235859e-05\n",
      "step: 54500 train: 0.07761526107788086 elapsed, loss: 5.4076986e-06\n",
      "step: 54510 train: 0.07521319389343262 elapsed, loss: 5.3709136e-06\n",
      "step: 54520 train: 0.0798940658569336 elapsed, loss: 2.7408769e-06\n",
      "step: 54530 train: 0.08105325698852539 elapsed, loss: 2.8069994e-06\n",
      "step: 54540 train: 0.08202600479125977 elapsed, loss: 2.0474993e-06\n",
      "step: 54550 train: 0.07973051071166992 elapsed, loss: 1.809092e-06\n",
      "step: 54560 train: 0.07780742645263672 elapsed, loss: 1.5092066e-06\n",
      "step: 54570 train: 0.07986712455749512 elapsed, loss: 0.0004209874\n",
      "step: 54580 train: 0.0878150463104248 elapsed, loss: 0.00012912578\n",
      "step: 54590 train: 0.08733010292053223 elapsed, loss: 9.1038404e-05\n",
      "step: 54600 train: 0.09383344650268555 elapsed, loss: 2.6614302e-05\n",
      "step: 54610 train: 0.07604861259460449 elapsed, loss: 4.2649783e-05\n",
      "step: 54620 train: 0.0833272933959961 elapsed, loss: 1.4977172e-05\n",
      "step: 54630 train: 0.0869898796081543 elapsed, loss: 1.9006231e-05\n",
      "step: 54640 train: 0.0856318473815918 elapsed, loss: 9.424876e-06\n",
      "step: 54650 train: 0.08263635635375977 elapsed, loss: 8.196371e-06\n",
      "step: 54660 train: 0.07994627952575684 elapsed, loss: 6.304532e-06\n",
      "step: 54670 train: 0.0811009407043457 elapsed, loss: 8.612792e-06\n",
      "step: 54680 train: 0.07625579833984375 elapsed, loss: 4.9113096e-06\n",
      "step: 54690 train: 0.0782470703125 elapsed, loss: 4.454965e-06\n",
      "step: 54700 train: 0.08090376853942871 elapsed, loss: 3.2731245e-06\n",
      "step: 54710 train: 0.08083486557006836 elapsed, loss: 1.7201465e-06\n",
      "step: 54720 train: 0.08627176284790039 elapsed, loss: 1.944133e-06\n",
      "step: 54730 train: 0.0791168212890625 elapsed, loss: 1.5567027e-06\n",
      "step: 54740 train: 0.08395791053771973 elapsed, loss: 1.8193359e-06\n",
      "step: 54750 train: 0.08454322814941406 elapsed, loss: 1.0761423e-06\n",
      "step: 54760 train: 0.0777275562286377 elapsed, loss: 2.102923e-06\n",
      "step: 54770 train: 0.08344721794128418 elapsed, loss: 9.2573396e-07\n",
      "step: 54780 train: 0.08298563957214355 elapsed, loss: 1.3741649e-06\n",
      "step: 54790 train: 0.07590031623840332 elapsed, loss: 1.3406375e-06\n",
      "step: 54800 train: 0.07840728759765625 elapsed, loss: 1.0491341e-06\n",
      "step: 54810 train: 0.0857245922088623 elapsed, loss: 3.0812676e-06\n",
      "step: 54820 train: 0.08598613739013672 elapsed, loss: 1.0016366e-06\n",
      "step: 54830 train: 0.08618283271789551 elapsed, loss: 1.2093211e-06\n",
      "step: 54840 train: 0.07699775695800781 elapsed, loss: 1.5608948e-06\n",
      "step: 54850 train: 0.08406186103820801 elapsed, loss: 1.4011723e-06\n",
      "step: 54860 train: 0.08052754402160645 elapsed, loss: 1.3965066e-06\n",
      "step: 54870 train: 0.0765838623046875 elapsed, loss: 1.3373779e-06\n",
      "step: 54880 train: 0.08055543899536133 elapsed, loss: 1.6214307e-06\n",
      "step: 54890 train: 0.0833590030670166 elapsed, loss: 1.300588e-06\n",
      "step: 54900 train: 0.07712841033935547 elapsed, loss: 1.8286494e-06\n",
      "step: 54910 train: 0.08165526390075684 elapsed, loss: 1.2875521e-06\n",
      "step: 54920 train: 0.07774734497070312 elapsed, loss: 1.4528619e-06\n",
      "step: 54930 train: 0.09198832511901855 elapsed, loss: 0.047982775\n",
      "step: 54940 train: 0.08406257629394531 elapsed, loss: 8.460361e-05\n",
      "step: 54950 train: 0.0789027214050293 elapsed, loss: 4.4604076e-05\n",
      "step: 54960 train: 0.0850973129272461 elapsed, loss: 3.0407129e-05\n",
      "step: 54970 train: 0.08121323585510254 elapsed, loss: 1.7681214e-05\n",
      "step: 54980 train: 0.08162784576416016 elapsed, loss: 1.4423592e-05\n",
      "step: 54990 train: 0.0846552848815918 elapsed, loss: 2.0425894e-05\n",
      "step: 55000 train: 0.08353257179260254 elapsed, loss: 1.0904914e-05\n",
      "step: 55010 train: 0.0810234546661377 elapsed, loss: 7.978098e-06\n",
      "step: 55020 train: 0.07560610771179199 elapsed, loss: 1.0721731e-05\n",
      "step: 55030 train: 0.08181405067443848 elapsed, loss: 7.39596e-06\n",
      "step: 55040 train: 0.07989740371704102 elapsed, loss: 4.1946582e-06\n",
      "step: 55050 train: 0.0776522159576416 elapsed, loss: 2.8065335e-06\n",
      "step: 55060 train: 0.08664083480834961 elapsed, loss: 1.8021067e-06\n",
      "step: 55070 train: 0.08178997039794922 elapsed, loss: 1.7699753e-06\n",
      "step: 55080 train: 0.0834352970123291 elapsed, loss: 1.3536758e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 55090 train: 0.0830986499786377 elapsed, loss: 1.2391233e-06\n",
      "step: 55100 train: 0.08025288581848145 elapsed, loss: 1.0631038e-06\n",
      "step: 55110 train: 0.0834648609161377 elapsed, loss: 1.1245708e-06\n",
      "step: 55120 train: 0.07987689971923828 elapsed, loss: 1.507344e-06\n",
      "step: 55130 train: 0.07871699333190918 elapsed, loss: 1.3341183e-06\n",
      "step: 55140 train: 0.08411335945129395 elapsed, loss: 1.5157254e-06\n",
      "step: 55150 train: 0.08234143257141113 elapsed, loss: 1.1981456e-06\n",
      "step: 55160 train: 0.07677245140075684 elapsed, loss: 1.7192194e-06\n",
      "step: 55170 train: 0.08025026321411133 elapsed, loss: 1.482664e-06\n",
      "step: 55180 train: 0.07475543022155762 elapsed, loss: 1.5958182e-06\n",
      "step: 55190 train: 0.08359241485595703 elapsed, loss: 1.2484352e-06\n",
      "step: 55200 train: 0.08272886276245117 elapsed, loss: 1.1757936e-06\n",
      "step: 55210 train: 0.08255934715270996 elapsed, loss: 3.4467696e-06\n",
      "step: 55220 train: 0.08452177047729492 elapsed, loss: 1.0654323e-06\n",
      "step: 55230 train: 0.07812881469726562 elapsed, loss: 1.6782406e-06\n",
      "step: 55240 train: 0.08584094047546387 elapsed, loss: 1.2945372e-06\n",
      "step: 55250 train: 0.08885955810546875 elapsed, loss: 1.1906949e-06\n",
      "step: 55260 train: 0.08463215827941895 elapsed, loss: 1.0989597e-06\n",
      "step: 55270 train: 0.07737112045288086 elapsed, loss: 2.973697e-06\n",
      "step: 55280 train: 0.07500433921813965 elapsed, loss: 2.167184e-06\n",
      "step: 55290 train: 0.08149123191833496 elapsed, loss: 1.2083899e-06\n",
      "step: 55300 train: 0.08588457107543945 elapsed, loss: 1.0333017e-06\n",
      "step: 55310 train: 0.08168792724609375 elapsed, loss: 1.378356e-06\n",
      "step: 55320 train: 0.08513379096984863 elapsed, loss: 1.3927915e-06\n",
      "step: 55330 train: 0.08379507064819336 elapsed, loss: 1.2675289e-06\n",
      "step: 55340 train: 0.0819697380065918 elapsed, loss: 1.7546038e-06\n",
      "step: 55350 train: 0.08530116081237793 elapsed, loss: 1.2693913e-06\n",
      "step: 55360 train: 0.08337807655334473 elapsed, loss: 2.4475114e-06\n",
      "step: 55370 train: 0.0841970443725586 elapsed, loss: 1.1418003e-06\n",
      "step: 55380 train: 0.08442902565002441 elapsed, loss: 8.585206e-06\n",
      "step: 55390 train: 0.07891559600830078 elapsed, loss: 0.010859593\n",
      "step: 55400 train: 0.08633995056152344 elapsed, loss: 0.0001227918\n",
      "step: 55410 train: 0.09118819236755371 elapsed, loss: 3.2410782e-05\n",
      "step: 55420 train: 0.07727813720703125 elapsed, loss: 8.994412e-05\n",
      "step: 55430 train: 0.08275008201599121 elapsed, loss: 1.5616088e-05\n",
      "step: 55440 train: 0.08136224746704102 elapsed, loss: 0.0011245958\n",
      "step: 55450 train: 0.08345723152160645 elapsed, loss: 2.9711277e-05\n",
      "step: 55460 train: 0.08243775367736816 elapsed, loss: 2.832707e-05\n",
      "step: 55470 train: 0.07990574836730957 elapsed, loss: 2.1396998e-05\n",
      "step: 55480 train: 0.08183884620666504 elapsed, loss: 1.4435642e-05\n",
      "step: 55490 train: 0.08115315437316895 elapsed, loss: 1.0296084e-05\n",
      "step: 55500 train: 0.07921457290649414 elapsed, loss: 7.556223e-06\n",
      "step: 55510 train: 0.08131837844848633 elapsed, loss: 6.160194e-06\n",
      "step: 55520 train: 0.08408880233764648 elapsed, loss: 6.5432323e-06\n",
      "step: 55530 train: 0.08087635040283203 elapsed, loss: 6.154559e-06\n",
      "step: 55540 train: 0.08192992210388184 elapsed, loss: 6.26961e-06\n",
      "step: 55550 train: 0.0782158374786377 elapsed, loss: 3.3108429e-06\n",
      "step: 55560 train: 0.08248424530029297 elapsed, loss: 2.0451812e-06\n",
      "step: 55570 train: 0.07993578910827637 elapsed, loss: 1.8016397e-06\n",
      "step: 55580 train: 0.08357906341552734 elapsed, loss: 1.4998926e-06\n",
      "step: 55590 train: 0.07785367965698242 elapsed, loss: 1.6326064e-06\n",
      "step: 55600 train: 0.0889592170715332 elapsed, loss: 1.4291124e-06\n",
      "step: 55610 train: 0.08189988136291504 elapsed, loss: 1.1683433e-06\n",
      "step: 55620 train: 0.08790159225463867 elapsed, loss: 8.518527e-06\n",
      "step: 55630 train: 0.08260560035705566 elapsed, loss: 0.00012168233\n",
      "step: 55640 train: 0.07996726036071777 elapsed, loss: 6.0591563e-05\n",
      "step: 55650 train: 0.07721328735351562 elapsed, loss: 3.1866915e-05\n",
      "step: 55660 train: 0.07744717597961426 elapsed, loss: 2.671011e-05\n",
      "step: 55670 train: 0.08413577079772949 elapsed, loss: 1.0891667e-05\n",
      "step: 55680 train: 0.08884525299072266 elapsed, loss: 9.232208e-06\n",
      "step: 55690 train: 0.08201169967651367 elapsed, loss: 1.1676348e-05\n",
      "step: 55700 train: 0.08189558982849121 elapsed, loss: 8.272866e-06\n",
      "step: 55710 train: 0.07973051071166992 elapsed, loss: 5.2575933e-06\n",
      "step: 55720 train: 0.08336806297302246 elapsed, loss: 3.946909e-06\n",
      "step: 55730 train: 0.07893013954162598 elapsed, loss: 3.6437882e-06\n",
      "step: 55740 train: 0.08469057083129883 elapsed, loss: 1.9944232e-06\n",
      "step: 55750 train: 0.07970762252807617 elapsed, loss: 2.1895353e-06\n",
      "step: 55760 train: 0.08002734184265137 elapsed, loss: 1.6083917e-06\n",
      "step: 55770 train: 0.08067512512207031 elapsed, loss: 1.3774245e-06\n",
      "step: 55780 train: 0.08080458641052246 elapsed, loss: 1.2186345e-06\n",
      "step: 55790 train: 0.08567547798156738 elapsed, loss: 1.1194485e-06\n",
      "step: 55800 train: 0.0778193473815918 elapsed, loss: 1.1241054e-06\n",
      "step: 55810 train: 0.08314156532287598 elapsed, loss: 7.8277617e-07\n",
      "step: 55820 train: 0.0819544792175293 elapsed, loss: 1.1473875e-06\n",
      "step: 55830 train: 0.07832551002502441 elapsed, loss: 9.4529173e-07\n",
      "step: 55840 train: 0.08520174026489258 elapsed, loss: 9.047793e-07\n",
      "step: 55850 train: 0.0810708999633789 elapsed, loss: 1.4672964e-06\n",
      "step: 55860 train: 0.08353638648986816 elapsed, loss: 8.442434e-07\n",
      "step: 55870 train: 0.08327889442443848 elapsed, loss: 1.0173982e-05\n",
      "step: 55880 train: 0.08621573448181152 elapsed, loss: 2.0624084e-06\n",
      "step: 55890 train: 0.08676886558532715 elapsed, loss: 1.2624007e-06\n",
      "step: 55900 train: 0.07642316818237305 elapsed, loss: 1.5348174e-06\n",
      "step: 55910 train: 0.07630610466003418 elapsed, loss: 1.3848754e-06\n",
      "step: 55920 train: 0.07973384857177734 elapsed, loss: 9.4296337e-07\n",
      "step: 55930 train: 0.0743722915649414 elapsed, loss: 1.3792874e-06\n",
      "step: 55940 train: 0.08300256729125977 elapsed, loss: 1.102685e-06\n",
      "step: 55950 train: 0.0821237564086914 elapsed, loss: 9.359784e-07\n",
      "step: 55960 train: 0.0860283374786377 elapsed, loss: 1.0165378e-06\n",
      "step: 55970 train: 0.08892154693603516 elapsed, loss: 7.655467e-07\n",
      "step: 55980 train: 0.08618617057800293 elapsed, loss: 1.1962829e-06\n",
      "step: 55990 train: 0.08687806129455566 elapsed, loss: 7.869671e-07\n",
      "step: 56000 train: 0.08464837074279785 elapsed, loss: 1.050531e-06\n",
      "step: 56010 train: 0.0747385025024414 elapsed, loss: 1.4789388e-06\n",
      "step: 56020 train: 0.07555389404296875 elapsed, loss: 2.2752165e-06\n",
      "step: 56030 train: 0.08003973960876465 elapsed, loss: 1.459379e-06\n",
      "step: 56040 train: 0.0755457878112793 elapsed, loss: 2.0717239e-06\n",
      "step: 56050 train: 0.08482623100280762 elapsed, loss: 1.2046647e-06\n",
      "step: 56060 train: 0.08094358444213867 elapsed, loss: 1.2274818e-06\n",
      "step: 56070 train: 0.08311986923217773 elapsed, loss: 1.4039674e-06\n",
      "step: 56080 train: 0.09065485000610352 elapsed, loss: 1.413746e-06\n",
      "step: 56090 train: 0.08515381813049316 elapsed, loss: 1.2330695e-06\n",
      "step: 56100 train: 0.08154296875 elapsed, loss: 0.013479171\n",
      "step: 56110 train: 0.08609914779663086 elapsed, loss: 0.0002517435\n",
      "step: 56120 train: 0.08264470100402832 elapsed, loss: 6.6070286e-05\n",
      "step: 56130 train: 0.07908821105957031 elapsed, loss: 9.83984e-05\n",
      "step: 56140 train: 0.08254647254943848 elapsed, loss: 2.9818864e-05\n",
      "step: 56150 train: 0.08471441268920898 elapsed, loss: 2.1942398e-05\n",
      "step: 56160 train: 0.08096981048583984 elapsed, loss: 1.7043107e-05\n",
      "step: 56170 train: 0.08108830451965332 elapsed, loss: 2.0071424e-05\n",
      "step: 56180 train: 0.07869458198547363 elapsed, loss: 1.66429e-05\n",
      "step: 56190 train: 0.07967090606689453 elapsed, loss: 7.928748e-06\n",
      "step: 56200 train: 0.0843660831451416 elapsed, loss: 5.712698e-06\n",
      "step: 56210 train: 0.0814504623413086 elapsed, loss: 4.1750986e-06\n",
      "step: 56220 train: 0.08652567863464355 elapsed, loss: 4.112219e-06\n",
      "step: 56230 train: 0.08318066596984863 elapsed, loss: 2.4712585e-06\n",
      "step: 56240 train: 0.08408761024475098 elapsed, loss: 3.982322e-06\n",
      "step: 56250 train: 0.08152055740356445 elapsed, loss: 2.2370327e-06\n",
      "step: 56260 train: 0.08343982696533203 elapsed, loss: 2.3269045e-06\n",
      "step: 56270 train: 0.07844996452331543 elapsed, loss: 1.3816152e-06\n",
      "step: 56280 train: 0.08395910263061523 elapsed, loss: 1.4160743e-06\n",
      "step: 56290 train: 0.07872843742370605 elapsed, loss: 1.3806842e-06\n",
      "step: 56300 train: 0.08192563056945801 elapsed, loss: 1.0849899e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 56310 train: 0.08281946182250977 elapsed, loss: 1.354141e-06\n",
      "step: 56320 train: 0.0796501636505127 elapsed, loss: 1.7103582e-06\n",
      "step: 56330 train: 0.081512451171875 elapsed, loss: 1.2940715e-06\n",
      "step: 56340 train: 0.07694745063781738 elapsed, loss: 1.4654343e-06\n",
      "step: 56350 train: 0.08050251007080078 elapsed, loss: 1.8649707e-06\n",
      "step: 56360 train: 0.07946991920471191 elapsed, loss: 1.3997764e-06\n",
      "step: 56370 train: 0.08218073844909668 elapsed, loss: 1.0044307e-06\n",
      "step: 56380 train: 0.07743668556213379 elapsed, loss: 1.6656685e-06\n",
      "step: 56390 train: 0.07985639572143555 elapsed, loss: 1.6191022e-06\n",
      "step: 56400 train: 0.0825645923614502 elapsed, loss: 0.00081722904\n",
      "step: 56410 train: 0.07909560203552246 elapsed, loss: 0.00017614987\n",
      "step: 56420 train: 0.08583235740661621 elapsed, loss: 5.5969125e-05\n",
      "step: 56430 train: 0.08379554748535156 elapsed, loss: 0.00019908565\n",
      "step: 56440 train: 0.0890810489654541 elapsed, loss: 5.421392e-05\n",
      "step: 56450 train: 0.08251595497131348 elapsed, loss: 1.32854375e-05\n",
      "step: 56460 train: 0.0874795913696289 elapsed, loss: 1.3061886e-05\n",
      "step: 56470 train: 0.08278059959411621 elapsed, loss: 7.2046428e-06\n",
      "step: 56480 train: 0.0875709056854248 elapsed, loss: 7.102665e-06\n",
      "step: 56490 train: 0.08559751510620117 elapsed, loss: 4.481967e-06\n",
      "step: 56500 train: 0.07877206802368164 elapsed, loss: 4.7688036e-06\n",
      "step: 56510 train: 0.08360838890075684 elapsed, loss: 4.1448284e-06\n",
      "step: 56520 train: 0.08379387855529785 elapsed, loss: 2.4726562e-06\n",
      "step: 56530 train: 0.08405542373657227 elapsed, loss: 2.059616e-06\n",
      "step: 56540 train: 0.08734583854675293 elapsed, loss: 1.4682281e-06\n",
      "step: 56550 train: 0.0863490104675293 elapsed, loss: 1.7057143e-06\n",
      "step: 56560 train: 0.08592557907104492 elapsed, loss: 1.196282e-06\n",
      "step: 56570 train: 0.08310914039611816 elapsed, loss: 1.1390064e-06\n",
      "step: 56580 train: 0.08855962753295898 elapsed, loss: 1.1823125e-06\n",
      "step: 56590 train: 0.08475780487060547 elapsed, loss: 9.709029e-07\n",
      "step: 56600 train: 0.0757906436920166 elapsed, loss: 1.0798677e-06\n",
      "step: 56610 train: 0.07648825645446777 elapsed, loss: 1.1390064e-06\n",
      "step: 56620 train: 0.08110880851745605 elapsed, loss: 9.909265e-07\n",
      "step: 56630 train: 0.08388948440551758 elapsed, loss: 7.185149e-07\n",
      "step: 56640 train: 0.08178520202636719 elapsed, loss: 1.042149e-06\n",
      "step: 56650 train: 0.0776979923248291 elapsed, loss: 8.870842e-07\n",
      "step: 56660 train: 0.07856917381286621 elapsed, loss: 1.1725341e-06\n",
      "step: 56670 train: 0.08501744270324707 elapsed, loss: 9.0803894e-07\n",
      "step: 56680 train: 0.07624363899230957 elapsed, loss: 1.4938397e-06\n",
      "step: 56690 train: 0.08000850677490234 elapsed, loss: 1.1990768e-06\n",
      "step: 56700 train: 0.07805252075195312 elapsed, loss: 9.3877253e-07\n",
      "step: 56710 train: 0.08300137519836426 elapsed, loss: 1.386738e-06\n",
      "step: 56720 train: 0.08615469932556152 elapsed, loss: 2.3650855e-06\n",
      "step: 56730 train: 0.08521819114685059 elapsed, loss: 9.592615e-07\n",
      "step: 56740 train: 0.08656978607177734 elapsed, loss: 9.294593e-07\n",
      "step: 56750 train: 0.07454991340637207 elapsed, loss: 1.5911625e-06\n",
      "step: 56760 train: 0.08146333694458008 elapsed, loss: 1.2526277e-06\n",
      "step: 56770 train: 0.08579683303833008 elapsed, loss: 1.0756768e-06\n",
      "step: 56780 train: 0.0792393684387207 elapsed, loss: 1.6754459e-06\n",
      "step: 56790 train: 0.08262324333190918 elapsed, loss: 0.0031168163\n",
      "step: 56800 train: 0.08683657646179199 elapsed, loss: 6.2760635e-05\n",
      "step: 56810 train: 0.08149933815002441 elapsed, loss: 4.9873684e-05\n",
      "step: 56820 train: 0.08176875114440918 elapsed, loss: 2.5671172e-05\n",
      "step: 56830 train: 0.08353996276855469 elapsed, loss: 1.4647882e-05\n",
      "step: 56840 train: 0.08295559883117676 elapsed, loss: 1.2646586e-05\n",
      "step: 56850 train: 0.07359457015991211 elapsed, loss: 2.161797e-05\n",
      "step: 56860 train: 0.085052490234375 elapsed, loss: 8.707732e-06\n",
      "step: 56870 train: 0.07936525344848633 elapsed, loss: 8.521525e-06\n",
      "step: 56880 train: 0.08658409118652344 elapsed, loss: 4.4680014e-06\n",
      "step: 56890 train: 0.08236384391784668 elapsed, loss: 4.244951e-06\n",
      "step: 56900 train: 0.08071374893188477 elapsed, loss: 3.836568e-06\n",
      "step: 56910 train: 0.08852815628051758 elapsed, loss: 2.0996604e-06\n",
      "step: 56920 train: 0.08156847953796387 elapsed, loss: 1.8402886e-06\n",
      "step: 56930 train: 0.0807962417602539 elapsed, loss: 1.6926756e-06\n",
      "step: 56940 train: 0.08548998832702637 elapsed, loss: 1.4011732e-06\n",
      "step: 56950 train: 0.08444714546203613 elapsed, loss: 1.6242245e-06\n",
      "step: 56960 train: 0.07979130744934082 elapsed, loss: 1.5944223e-06\n",
      "step: 56970 train: 0.08430051803588867 elapsed, loss: 1.0174687e-06\n",
      "step: 56980 train: 0.08879208564758301 elapsed, loss: 1.1306247e-06\n",
      "step: 56990 train: 0.08578920364379883 elapsed, loss: 1.0482026e-06\n",
      "step: 57000 train: 0.0815725326538086 elapsed, loss: 1.1399379e-06\n",
      "step: 57010 train: 0.08622169494628906 elapsed, loss: 1.1040814e-06\n",
      "step: 57020 train: 0.07940506935119629 elapsed, loss: 1.2177028e-06\n",
      "step: 57030 train: 0.08505964279174805 elapsed, loss: 1.154839e-06\n",
      "step: 57040 train: 0.086761474609375 elapsed, loss: 8.4610605e-07\n",
      "step: 57050 train: 0.0781245231628418 elapsed, loss: 1.5473906e-06\n",
      "step: 57060 train: 0.07990264892578125 elapsed, loss: 1.7229427e-06\n",
      "step: 57070 train: 0.08335590362548828 elapsed, loss: 1.1580985e-06\n",
      "step: 57080 train: 0.07715892791748047 elapsed, loss: 2.1290002e-06\n",
      "step: 57090 train: 0.07694792747497559 elapsed, loss: 1.2936058e-06\n",
      "step: 57100 train: 0.0779728889465332 elapsed, loss: 1.5515818e-06\n",
      "step: 57110 train: 0.07521295547485352 elapsed, loss: 1.5855744e-06\n",
      "step: 57120 train: 0.08189153671264648 elapsed, loss: 1.0156062e-06\n",
      "step: 57130 train: 0.0820615291595459 elapsed, loss: 1.414212e-06\n",
      "step: 57140 train: 0.08157205581665039 elapsed, loss: 1.0924404e-06\n",
      "step: 57150 train: 0.07978510856628418 elapsed, loss: 1.9688134e-06\n",
      "step: 57160 train: 0.07916426658630371 elapsed, loss: 1.5511159e-06\n",
      "step: 57170 train: 0.07916951179504395 elapsed, loss: 1.4398234e-06\n",
      "step: 57180 train: 0.07830142974853516 elapsed, loss: 3.8058374e-06\n",
      "step: 57190 train: 0.07489442825317383 elapsed, loss: 2.3255084e-06\n",
      "step: 57200 train: 0.08572244644165039 elapsed, loss: 1.2461084e-06\n",
      "step: 57210 train: 0.09243035316467285 elapsed, loss: 9.722877e-06\n",
      "step: 57220 train: 0.07993769645690918 elapsed, loss: 3.0212009e-06\n",
      "step: 57230 train: 0.0763094425201416 elapsed, loss: 1.5430547e-05\n",
      "step: 57240 train: 0.07538414001464844 elapsed, loss: 3.767187e-06\n",
      "step: 57250 train: 0.0908820629119873 elapsed, loss: 2.5732386e-06\n",
      "step: 57260 train: 0.08833003044128418 elapsed, loss: 1.9110714e-06\n",
      "step: 57270 train: 0.09176492691040039 elapsed, loss: 1.5329549e-06\n",
      "step: 57280 train: 0.08703446388244629 elapsed, loss: 1.3317883e-06\n",
      "step: 57290 train: 0.08004522323608398 elapsed, loss: 1.6079266e-06\n",
      "step: 57300 train: 0.08167123794555664 elapsed, loss: 1.5976805e-06\n",
      "step: 57310 train: 0.08950209617614746 elapsed, loss: 5.699122e-06\n",
      "step: 57320 train: 0.08709096908569336 elapsed, loss: 1.4742819e-06\n",
      "step: 57330 train: 0.08341646194458008 elapsed, loss: 1.4188684e-06\n",
      "step: 57340 train: 0.09095907211303711 elapsed, loss: 2.2686865e-05\n",
      "step: 57350 train: 0.08665013313293457 elapsed, loss: 0.001190954\n",
      "step: 57360 train: 0.08720922470092773 elapsed, loss: 0.012281619\n",
      "step: 57370 train: 0.07654500007629395 elapsed, loss: 0.00012719094\n",
      "step: 57380 train: 0.07854580879211426 elapsed, loss: 7.260134e-05\n",
      "step: 57390 train: 0.07953071594238281 elapsed, loss: 5.914548e-05\n",
      "step: 57400 train: 0.0823509693145752 elapsed, loss: 2.2289632e-05\n",
      "step: 57410 train: 0.08305954933166504 elapsed, loss: 1.4059957e-05\n",
      "step: 57420 train: 0.08481478691101074 elapsed, loss: 8.855786e-06\n",
      "step: 57430 train: 0.08248400688171387 elapsed, loss: 9.782818e-06\n",
      "step: 57440 train: 0.08434128761291504 elapsed, loss: 6.671488e-06\n",
      "step: 57450 train: 0.0818932056427002 elapsed, loss: 5.5925566e-06\n",
      "step: 57460 train: 0.08011484146118164 elapsed, loss: 3.5418095e-06\n",
      "step: 57470 train: 0.08064627647399902 elapsed, loss: 4.7227386e-06\n",
      "step: 57480 train: 0.07703089714050293 elapsed, loss: 3.6028082e-06\n",
      "step: 57490 train: 0.08246874809265137 elapsed, loss: 2.5671793e-06\n",
      "step: 57500 train: 0.08117556571960449 elapsed, loss: 1.8859251e-06\n",
      "step: 57510 train: 0.08273577690124512 elapsed, loss: 1.5394739e-06\n",
      "step: 57520 train: 0.09035158157348633 elapsed, loss: 1.0943029e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 57530 train: 0.07844185829162598 elapsed, loss: 1.0910436e-06\n",
      "step: 57540 train: 0.08375668525695801 elapsed, loss: 9.2154306e-07\n",
      "step: 57550 train: 0.08247518539428711 elapsed, loss: 1.118983e-06\n",
      "step: 57560 train: 0.08463501930236816 elapsed, loss: 9.830096e-07\n",
      "step: 57570 train: 0.08398866653442383 elapsed, loss: 1.5674143e-06\n",
      "step: 57580 train: 0.08521723747253418 elapsed, loss: 1.057514e-06\n",
      "step: 57590 train: 0.07350420951843262 elapsed, loss: 1.3867378e-06\n",
      "step: 57600 train: 0.0843656063079834 elapsed, loss: 7.9255494e-07\n",
      "step: 57610 train: 0.0809180736541748 elapsed, loss: 9.345816e-07\n",
      "step: 57620 train: 0.08175182342529297 elapsed, loss: 9.643838e-07\n",
      "step: 57630 train: 0.08202052116394043 elapsed, loss: 1.1441289e-06\n",
      "step: 57640 train: 0.07768511772155762 elapsed, loss: 1.2810322e-06\n",
      "step: 57650 train: 0.08066654205322266 elapsed, loss: 1.6544875e-06\n",
      "step: 57660 train: 0.07766532897949219 elapsed, loss: 1.0519279e-06\n",
      "step: 57670 train: 0.0784599781036377 elapsed, loss: 1.3378435e-06\n",
      "step: 57680 train: 0.08246397972106934 elapsed, loss: 1.2083892e-06\n",
      "step: 57690 train: 0.07845783233642578 elapsed, loss: 4.7893054e-06\n",
      "step: 57700 train: 0.08886051177978516 elapsed, loss: 1.6176992e-06\n",
      "step: 57710 train: 0.08898091316223145 elapsed, loss: 1.2712542e-06\n",
      "step: 57720 train: 0.08053755760192871 elapsed, loss: 1.1641522e-06\n",
      "step: 57730 train: 0.08626556396484375 elapsed, loss: 1.2018706e-06\n",
      "step: 57740 train: 0.07750105857849121 elapsed, loss: 1.8072292e-06\n",
      "step: 57750 train: 0.08205676078796387 elapsed, loss: 1.4402888e-06\n",
      "step: 57760 train: 0.08634686470031738 elapsed, loss: 9.1642073e-07\n",
      "step: 57770 train: 0.08665347099304199 elapsed, loss: 1.3639201e-06\n",
      "step: 57780 train: 0.0834348201751709 elapsed, loss: 1.0239884e-06\n",
      "step: 57790 train: 0.07712149620056152 elapsed, loss: 2.123412e-06\n",
      "step: 57800 train: 0.08312273025512695 elapsed, loss: 9.951175e-07\n",
      "step: 57810 train: 0.08491134643554688 elapsed, loss: 1.482664e-06\n",
      "step: 57820 train: 0.07934761047363281 elapsed, loss: 1.2209629e-06\n",
      "step: 57830 train: 0.08377909660339355 elapsed, loss: 1.4966339e-06\n",
      "step: 57840 train: 0.0846109390258789 elapsed, loss: 1.2721855e-06\n",
      "step: 57850 train: 0.08390974998474121 elapsed, loss: 1.5017558e-06\n",
      "step: 57860 train: 0.07804751396179199 elapsed, loss: 1.3364466e-06\n",
      "step: 57870 train: 0.0804142951965332 elapsed, loss: 2.3222478e-06\n",
      "step: 57880 train: 0.0842905044555664 elapsed, loss: 2.2905833e-06\n",
      "step: 57890 train: 0.08386635780334473 elapsed, loss: 1.4929085e-06\n",
      "step: 57900 train: 0.08043742179870605 elapsed, loss: 1.3983795e-06\n",
      "step: 57910 train: 0.08260917663574219 elapsed, loss: 1.2791702e-06\n",
      "step: 57920 train: 0.0778505802154541 elapsed, loss: 2.395823e-06\n",
      "step: 57930 train: 0.08608579635620117 elapsed, loss: 1.4500667e-06\n",
      "step: 57940 train: 0.07878684997558594 elapsed, loss: 1.9147956e-06\n",
      "step: 57950 train: 0.0828704833984375 elapsed, loss: 1.5432e-06\n",
      "step: 57960 train: 0.07920145988464355 elapsed, loss: 3.3513547e-06\n",
      "step: 57970 train: 0.08340096473693848 elapsed, loss: 2.230046e-06\n",
      "step: 57980 train: 0.08111214637756348 elapsed, loss: 5.5650858e-06\n",
      "step: 57990 train: 0.07929563522338867 elapsed, loss: 1.758335e-06\n",
      "step: 58000 train: 0.08239316940307617 elapsed, loss: 1.9352858e-06\n",
      "step: 58010 train: 0.08350586891174316 elapsed, loss: 2.487092e-06\n",
      "step: 58020 train: 0.08351588249206543 elapsed, loss: 1.9674162e-06\n",
      "step: 58030 train: 0.08178067207336426 elapsed, loss: 1.9273662e-06\n",
      "step: 58040 train: 0.08040618896484375 elapsed, loss: 2.085228e-06\n",
      "step: 58050 train: 0.07575225830078125 elapsed, loss: 2.2072313e-06\n",
      "step: 58060 train: 0.0766303539276123 elapsed, loss: 1.8812693e-06\n",
      "step: 58070 train: 0.08763289451599121 elapsed, loss: 1.4118837e-06\n",
      "step: 58080 train: 0.08143997192382812 elapsed, loss: 1.5986135e-06\n",
      "step: 58090 train: 0.0840597152709961 elapsed, loss: 3.3825427e-06\n",
      "step: 58100 train: 0.07778573036193848 elapsed, loss: 1.7858072e-06\n",
      "step: 58110 train: 0.08124804496765137 elapsed, loss: 2.530399e-06\n",
      "step: 58120 train: 0.08187222480773926 elapsed, loss: 1.7820835e-06\n",
      "step: 58130 train: 0.08260774612426758 elapsed, loss: 1.8523983e-06\n",
      "step: 58140 train: 0.07726407051086426 elapsed, loss: 2.550888e-06\n",
      "step: 58150 train: 0.08147978782653809 elapsed, loss: 1.5385434e-06\n",
      "step: 58160 train: 0.08747601509094238 elapsed, loss: 1.7490212e-06\n",
      "step: 58170 train: 0.08075714111328125 elapsed, loss: 1.7406385e-06\n",
      "step: 58180 train: 0.08252668380737305 elapsed, loss: 2.3846474e-06\n",
      "step: 58190 train: 0.08675479888916016 elapsed, loss: 1.5059468e-06\n",
      "step: 58200 train: 0.09186673164367676 elapsed, loss: 1.4705511e-06\n",
      "step: 58210 train: 0.08192729949951172 elapsed, loss: 2.7096758e-06\n",
      "step: 58220 train: 0.07666134834289551 elapsed, loss: 2.6975708e-06\n",
      "step: 58230 train: 0.08158683776855469 elapsed, loss: 0.00035424303\n",
      "step: 58240 train: 0.08564496040344238 elapsed, loss: 5.0263374e-05\n",
      "step: 58250 train: 0.08654189109802246 elapsed, loss: 1.0383677e-05\n",
      "step: 58260 train: 0.07844281196594238 elapsed, loss: 1.405113e-05\n",
      "step: 58270 train: 0.07560467720031738 elapsed, loss: 1.0607184e-05\n",
      "step: 58280 train: 0.08252191543579102 elapsed, loss: 8.4232815e-06\n",
      "step: 58290 train: 0.08112668991088867 elapsed, loss: 5.711303e-06\n",
      "step: 58300 train: 0.0828084945678711 elapsed, loss: 4.4461085e-06\n",
      "step: 58310 train: 0.07845211029052734 elapsed, loss: 3.6144538e-06\n",
      "step: 58320 train: 0.08481216430664062 elapsed, loss: 2.9769426e-06\n",
      "step: 58330 train: 0.08112406730651855 elapsed, loss: 4.140176e-06\n",
      "step: 58340 train: 0.08367776870727539 elapsed, loss: 1.549719e-06\n",
      "step: 58350 train: 0.08084487915039062 elapsed, loss: 2.3115388e-06\n",
      "step: 58360 train: 0.08117055892944336 elapsed, loss: 1.6167743e-06\n",
      "step: 58370 train: 0.08340215682983398 elapsed, loss: 2.255194e-06\n",
      "step: 58380 train: 0.08583760261535645 elapsed, loss: 1.5376113e-06\n",
      "step: 58390 train: 0.0753641128540039 elapsed, loss: 1.8891856e-06\n",
      "step: 58400 train: 0.08212161064147949 elapsed, loss: 2.4554217e-06\n",
      "step: 58410 train: 0.07881665229797363 elapsed, loss: 1.582781e-06\n",
      "step: 58420 train: 0.08270621299743652 elapsed, loss: 1.7546092e-06\n",
      "step: 58430 train: 0.08235049247741699 elapsed, loss: 1.6451795e-06\n",
      "step: 58440 train: 0.08716034889221191 elapsed, loss: 1.3019878e-06\n",
      "step: 58450 train: 0.08965349197387695 elapsed, loss: 1.2381923e-06\n",
      "step: 58460 train: 0.0814218521118164 elapsed, loss: 1.4291127e-06\n",
      "step: 58470 train: 0.0763239860534668 elapsed, loss: 2.0032721e-06\n",
      "step: 58480 train: 0.08042502403259277 elapsed, loss: 1.8859254e-06\n",
      "step: 58490 train: 0.07992434501647949 elapsed, loss: 1.8347031e-06\n",
      "step: 58500 train: 0.08327150344848633 elapsed, loss: 1.625156e-06\n",
      "step: 58510 train: 0.07835006713867188 elapsed, loss: 2.4833626e-06\n",
      "step: 58520 train: 0.08742070198059082 elapsed, loss: 1.2726497e-06\n",
      "step: 58530 train: 0.07995152473449707 elapsed, loss: 1.02535905e-05\n",
      "step: 58540 train: 0.07929682731628418 elapsed, loss: 3.904559e-06\n",
      "step: 58550 train: 0.07773447036743164 elapsed, loss: 2.706884e-06\n",
      "step: 58560 train: 0.08143782615661621 elapsed, loss: 1.3746305e-06\n",
      "step: 58570 train: 0.07690858840942383 elapsed, loss: 2.128069e-06\n",
      "step: 58580 train: 0.08058738708496094 elapsed, loss: 1.4393567e-06\n",
      "step: 58590 train: 0.07914495468139648 elapsed, loss: 2.1406408e-06\n",
      "step: 58600 train: 0.08172941207885742 elapsed, loss: 1.7178222e-06\n",
      "step: 58610 train: 0.0826106071472168 elapsed, loss: 1.9743975e-06\n",
      "step: 58620 train: 0.07698845863342285 elapsed, loss: 2.0395937e-06\n",
      "step: 58630 train: 0.08003067970275879 elapsed, loss: 2.0945417e-06\n",
      "step: 58640 train: 0.07362174987792969 elapsed, loss: 2.512238e-06\n",
      "step: 58650 train: 0.07636141777038574 elapsed, loss: 2.2542627e-06\n",
      "step: 58660 train: 0.08189535140991211 elapsed, loss: 1.4817326e-06\n",
      "step: 58670 train: 0.07977581024169922 elapsed, loss: 2.4517021e-06\n",
      "step: 58680 train: 0.08125185966491699 elapsed, loss: 2.0735863e-06\n",
      "step: 58690 train: 0.08772039413452148 elapsed, loss: 1.3378436e-06\n",
      "step: 58700 train: 0.07982635498046875 elapsed, loss: 1.4035018e-06\n",
      "step: 58710 train: 0.07926535606384277 elapsed, loss: 1.9753325e-06\n",
      "step: 58720 train: 0.07630586624145508 elapsed, loss: 2.3371506e-06\n",
      "step: 58730 train: 0.0818018913269043 elapsed, loss: 2.515024e-06\n",
      "step: 58740 train: 0.08644223213195801 elapsed, loss: 1.3899977e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 58750 train: 0.07715177536010742 elapsed, loss: 2.2063002e-06\n",
      "step: 58760 train: 0.07960319519042969 elapsed, loss: 0.14480981\n",
      "step: 58770 train: 0.07808518409729004 elapsed, loss: 0.00018542608\n",
      "step: 58780 train: 0.08552670478820801 elapsed, loss: 3.988766e-05\n",
      "step: 58790 train: 0.07791733741760254 elapsed, loss: 4.627735e-05\n",
      "step: 58800 train: 0.08744287490844727 elapsed, loss: 2.1759313e-05\n",
      "step: 58810 train: 0.09002065658569336 elapsed, loss: 1.5147609e-05\n",
      "step: 58820 train: 0.08664798736572266 elapsed, loss: 1.2836216e-05\n",
      "step: 58830 train: 0.07758641242980957 elapsed, loss: 1.6986642e-05\n",
      "step: 58840 train: 0.08473777770996094 elapsed, loss: 6.644403e-06\n",
      "step: 58850 train: 0.07696318626403809 elapsed, loss: 5.9012937e-06\n",
      "step: 58860 train: 0.08506441116333008 elapsed, loss: 4.423298e-06\n",
      "step: 58870 train: 0.0879971981048584 elapsed, loss: 5.0007147e-06\n",
      "step: 58880 train: 0.08315157890319824 elapsed, loss: 3.6316787e-06\n",
      "step: 58890 train: 0.07930183410644531 elapsed, loss: 3.6293502e-06\n",
      "step: 58900 train: 0.0820467472076416 elapsed, loss: 2.4489068e-06\n",
      "step: 58910 train: 0.07815766334533691 elapsed, loss: 3.0798742e-06\n",
      "step: 58920 train: 0.08073592185974121 elapsed, loss: 1.7876711e-06\n",
      "step: 58930 train: 0.07895946502685547 elapsed, loss: 1.5133976e-06\n",
      "step: 58940 train: 0.08158516883850098 elapsed, loss: 1.3066444e-06\n",
      "step: 58950 train: 0.07677960395812988 elapsed, loss: 1.6612343e-05\n",
      "step: 58960 train: 0.08031892776489258 elapsed, loss: 2.2081626e-06\n",
      "step: 58970 train: 0.08209228515625 elapsed, loss: 1.362989e-06\n",
      "step: 58980 train: 0.08411002159118652 elapsed, loss: 1.546925e-06\n",
      "step: 58990 train: 0.08149337768554688 elapsed, loss: 2.3255086e-06\n",
      "step: 59000 train: 0.07585549354553223 elapsed, loss: 2.1052517e-06\n",
      "step: 59010 train: 0.08069014549255371 elapsed, loss: 1.9916292e-06\n",
      "step: 59020 train: 0.08297276496887207 elapsed, loss: 1.3899976e-06\n",
      "step: 59030 train: 0.0806112289428711 elapsed, loss: 2.081969e-06\n",
      "step: 59040 train: 0.08666753768920898 elapsed, loss: 1.4342345e-06\n",
      "step: 59050 train: 0.0804445743560791 elapsed, loss: 1.6274842e-06\n",
      "step: 59060 train: 0.08127164840698242 elapsed, loss: 2.1243231e-06\n",
      "step: 59070 train: 0.0806422233581543 elapsed, loss: 2.0842972e-06\n",
      "step: 59080 train: 0.08185529708862305 elapsed, loss: 3.5036255e-06\n",
      "step: 59090 train: 0.08333611488342285 elapsed, loss: 3.4128211e-06\n",
      "step: 59100 train: 0.0841825008392334 elapsed, loss: 1.4952367e-06\n",
      "step: 59110 train: 0.08538055419921875 elapsed, loss: 2.5154955e-06\n",
      "step: 59120 train: 0.0856177806854248 elapsed, loss: 1.5255046e-06\n",
      "step: 59130 train: 0.07994437217712402 elapsed, loss: 2.363693e-06\n",
      "step: 59140 train: 0.07878470420837402 elapsed, loss: 2.2193349e-06\n",
      "step: 59150 train: 0.08830714225769043 elapsed, loss: 1.2698564e-06\n",
      "step: 59160 train: 0.08318877220153809 elapsed, loss: 2.276148e-06\n",
      "step: 59170 train: 0.08397841453552246 elapsed, loss: 2.275683e-06\n",
      "step: 59180 train: 0.07992935180664062 elapsed, loss: 1.4118835e-06\n",
      "step: 59190 train: 0.08086252212524414 elapsed, loss: 2.1355197e-06\n",
      "step: 59200 train: 0.07808923721313477 elapsed, loss: 2.1597343e-06\n",
      "step: 59210 train: 0.08475112915039062 elapsed, loss: 1.6959364e-06\n",
      "step: 59220 train: 0.08309054374694824 elapsed, loss: 1.5273674e-06\n",
      "step: 59230 train: 0.08441853523254395 elapsed, loss: 3.08686e-06\n",
      "step: 59240 train: 0.07947683334350586 elapsed, loss: 2.0023408e-06\n",
      "step: 59250 train: 0.08542323112487793 elapsed, loss: 1.6167742e-06\n",
      "step: 59260 train: 0.07853841781616211 elapsed, loss: 0.00023697311\n",
      "step: 59270 train: 0.07842493057250977 elapsed, loss: 0.000114215116\n",
      "step: 59280 train: 0.08037877082824707 elapsed, loss: 4.55495e-05\n",
      "step: 59290 train: 0.08195281028747559 elapsed, loss: 2.1794349e-05\n",
      "step: 59300 train: 0.08375811576843262 elapsed, loss: 2.6103837e-05\n",
      "step: 59310 train: 0.08432722091674805 elapsed, loss: 1.5503621e-05\n",
      "step: 59320 train: 0.08211445808410645 elapsed, loss: 1.1721891e-05\n",
      "step: 59330 train: 0.07780671119689941 elapsed, loss: 7.313145e-06\n",
      "step: 59340 train: 0.08682918548583984 elapsed, loss: 5.812353e-06\n",
      "step: 59350 train: 0.0774071216583252 elapsed, loss: 4.8312168e-06\n",
      "step: 59360 train: 0.07869791984558105 elapsed, loss: 4.9578584e-06\n",
      "step: 59370 train: 0.0935368537902832 elapsed, loss: 2.4652038e-06\n",
      "step: 59380 train: 0.0832357406616211 elapsed, loss: 2.5448342e-06\n",
      "step: 59390 train: 0.08365488052368164 elapsed, loss: 1.6591489e-06\n",
      "step: 59400 train: 0.07460641860961914 elapsed, loss: 3.3401789e-06\n",
      "step: 59410 train: 0.08199524879455566 elapsed, loss: 1.6149108e-06\n",
      "step: 59420 train: 0.08028817176818848 elapsed, loss: 1.753678e-06\n",
      "step: 59430 train: 0.08888721466064453 elapsed, loss: 1.2922089e-06\n",
      "step: 59440 train: 0.08469629287719727 elapsed, loss: 1.3504164e-06\n",
      "step: 59450 train: 0.08157920837402344 elapsed, loss: 1.836098e-06\n",
      "step: 59460 train: 0.09171319007873535 elapsed, loss: 1.4714881e-06\n",
      "step: 59470 train: 0.08309459686279297 elapsed, loss: 1.914796e-06\n",
      "step: 59480 train: 0.08463525772094727 elapsed, loss: 1.1431976e-06\n",
      "step: 59490 train: 0.07856535911560059 elapsed, loss: 1.758335e-06\n",
      "step: 59500 train: 0.07927894592285156 elapsed, loss: 1.5967505e-06\n",
      "step: 59510 train: 0.08634305000305176 elapsed, loss: 1.7667167e-06\n",
      "step: 59520 train: 0.07387256622314453 elapsed, loss: 2.1252747e-06\n",
      "step: 59530 train: 0.07893490791320801 elapsed, loss: 1.9487898e-06\n",
      "step: 59540 train: 0.08523750305175781 elapsed, loss: 0.000474222\n",
      "step: 59550 train: 0.08215165138244629 elapsed, loss: 0.00010818154\n",
      "step: 59560 train: 0.08660554885864258 elapsed, loss: 5.1616516e-05\n",
      "step: 59570 train: 0.07878303527832031 elapsed, loss: 3.8820137e-05\n",
      "step: 59580 train: 0.0777125358581543 elapsed, loss: 3.2906264e-05\n",
      "step: 59590 train: 0.08574414253234863 elapsed, loss: 1.5913156e-05\n",
      "step: 59600 train: 0.09031295776367188 elapsed, loss: 1.2453885e-05\n",
      "step: 59610 train: 0.0873713493347168 elapsed, loss: 8.981113e-06\n",
      "step: 59620 train: 0.07901191711425781 elapsed, loss: 1.0670974e-05\n",
      "step: 59630 train: 0.08480334281921387 elapsed, loss: 6.8754343e-06\n",
      "step: 59640 train: 0.08310413360595703 elapsed, loss: 6.6491198e-06\n",
      "step: 59650 train: 0.07775044441223145 elapsed, loss: 5.829581e-06\n",
      "step: 59660 train: 0.07653045654296875 elapsed, loss: 3.6107251e-06\n",
      "step: 59670 train: 0.07890701293945312 elapsed, loss: 2.3189891e-06\n",
      "step: 59680 train: 0.08095383644104004 elapsed, loss: 2.6863927e-06\n",
      "step: 59690 train: 0.0878899097442627 elapsed, loss: 1.4165402e-06\n",
      "step: 59700 train: 0.0872187614440918 elapsed, loss: 1.3462254e-06\n",
      "step: 59710 train: 0.08209347724914551 elapsed, loss: 1.1394721e-06\n",
      "step: 59720 train: 0.08673214912414551 elapsed, loss: 9.4296354e-07\n",
      "step: 59730 train: 0.0870966911315918 elapsed, loss: 4.2028325e-05\n",
      "step: 59740 train: 0.08258247375488281 elapsed, loss: 7.5070784e-05\n",
      "step: 59750 train: 0.08157849311828613 elapsed, loss: 1.9047962e-05\n",
      "step: 59760 train: 0.07765555381774902 elapsed, loss: 2.6455102e-05\n",
      "step: 59770 train: 0.07766342163085938 elapsed, loss: 1.8989827e-05\n",
      "step: 59780 train: 0.0814056396484375 elapsed, loss: 7.3658994e-06\n",
      "step: 59790 train: 0.08279156684875488 elapsed, loss: 4.979721e-06\n",
      "step: 59800 train: 0.08614778518676758 elapsed, loss: 2.9462312e-06\n",
      "step: 59810 train: 0.08240747451782227 elapsed, loss: 5.6832405e-06\n",
      "step: 59820 train: 0.08226990699768066 elapsed, loss: 6.5492623e-06\n",
      "step: 59830 train: 0.08000922203063965 elapsed, loss: 2.662177e-06\n",
      "step: 59840 train: 0.08538603782653809 elapsed, loss: 1.9199174e-06\n",
      "step: 59850 train: 0.07537555694580078 elapsed, loss: 1.8207329e-06\n",
      "step: 59860 train: 0.08103609085083008 elapsed, loss: 1.9115366e-06\n",
      "step: 59870 train: 0.08615589141845703 elapsed, loss: 1.18371e-06\n",
      "step: 59880 train: 0.08431768417358398 elapsed, loss: 9.797506e-07\n",
      "step: 59890 train: 0.08407235145568848 elapsed, loss: 1.027248e-06\n",
      "step: 59900 train: 0.08226633071899414 elapsed, loss: 1.1441288e-06\n",
      "step: 59910 train: 0.0759284496307373 elapsed, loss: 2.0530972e-06\n",
      "step: 59920 train: 0.07909655570983887 elapsed, loss: 1.4840602e-06\n",
      "step: 59930 train: 0.08347058296203613 elapsed, loss: 1.1534419e-06\n",
      "step: 59940 train: 0.07955455780029297 elapsed, loss: 1.1841757e-06\n",
      "step: 59950 train: 0.08369612693786621 elapsed, loss: 1.0402862e-06\n",
      "step: 59960 train: 0.08267688751220703 elapsed, loss: 1.084989e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 59970 train: 0.08056998252868652 elapsed, loss: 1.4519303e-06\n",
      "step: 59980 train: 0.08609914779663086 elapsed, loss: 4.7976637e-06\n",
      "step: 59990 train: 0.07950854301452637 elapsed, loss: 1.9766953e-06\n",
      "step: 60000 train: 0.08188486099243164 elapsed, loss: 1.6130489e-06\n",
      "step: 60010 train: 0.08355712890625 elapsed, loss: 1.0016367e-06\n",
      "step: 60020 train: 0.08416366577148438 elapsed, loss: 1.2256194e-06\n",
      "step: 60030 train: 0.0832967758178711 elapsed, loss: 1.4025704e-06\n",
      "step: 60040 train: 0.07790970802307129 elapsed, loss: 9.713685e-07\n",
      "step: 60050 train: 0.0826728343963623 elapsed, loss: 9.350473e-07\n",
      "step: 60060 train: 0.08755350112915039 elapsed, loss: 1.0523935e-06\n",
      "step: 60070 train: 0.07877635955810547 elapsed, loss: 1.5790556e-06\n",
      "step: 60080 train: 0.07708263397216797 elapsed, loss: 1.978592e-06\n",
      "step: 60090 train: 0.08394408226013184 elapsed, loss: 1.319683e-06\n",
      "step: 60100 train: 0.0811765193939209 elapsed, loss: 1.4710224e-06\n",
      "step: 60110 train: 0.0800178050994873 elapsed, loss: 1.3392407e-06\n",
      "step: 60120 train: 0.07921624183654785 elapsed, loss: 2.863809e-06\n",
      "step: 60130 train: 0.08287739753723145 elapsed, loss: 1.0924404e-06\n",
      "step: 60140 train: 0.0810701847076416 elapsed, loss: 1.8123516e-06\n",
      "step: 60150 train: 0.0869450569152832 elapsed, loss: 0.035467546\n",
      "step: 60160 train: 0.08285641670227051 elapsed, loss: 0.00016582658\n",
      "step: 60170 train: 0.08596444129943848 elapsed, loss: 3.072002e-05\n",
      "step: 60180 train: 0.08861255645751953 elapsed, loss: 2.3228613e-05\n",
      "step: 60190 train: 0.0764005184173584 elapsed, loss: 2.0302396e-05\n",
      "step: 60200 train: 0.08165836334228516 elapsed, loss: 3.19854e-05\n",
      "step: 60210 train: 0.08476495742797852 elapsed, loss: 1.7917744e-05\n",
      "step: 60220 train: 0.08404207229614258 elapsed, loss: 1.0258406e-05\n",
      "step: 60230 train: 0.08036613464355469 elapsed, loss: 1.120364e-05\n",
      "step: 60240 train: 0.08327937126159668 elapsed, loss: 5.7476263e-06\n",
      "step: 60250 train: 0.08330798149108887 elapsed, loss: 5.6242266e-06\n",
      "step: 60260 train: 0.08651208877563477 elapsed, loss: 2.9085131e-06\n",
      "step: 60270 train: 0.08904147148132324 elapsed, loss: 2.0656687e-06\n",
      "step: 60280 train: 0.0844583511352539 elapsed, loss: 2.8167788e-06\n",
      "step: 60290 train: 0.08262133598327637 elapsed, loss: 1.5739333e-06\n",
      "step: 60300 train: 0.07936787605285645 elapsed, loss: 2.2766133e-06\n",
      "step: 60310 train: 0.08520221710205078 elapsed, loss: 1.7248065e-06\n",
      "step: 60320 train: 0.07982826232910156 elapsed, loss: 1.7238755e-06\n",
      "step: 60330 train: 0.08639693260192871 elapsed, loss: 9.597272e-07\n",
      "step: 60340 train: 0.07788395881652832 elapsed, loss: 1.4584497e-06\n",
      "step: 60350 train: 0.08349227905273438 elapsed, loss: 1.2516964e-06\n",
      "step: 60360 train: 0.08150219917297363 elapsed, loss: 1.3019878e-06\n",
      "step: 60370 train: 0.08358287811279297 elapsed, loss: 1.2386575e-06\n",
      "step: 60380 train: 0.08133053779602051 elapsed, loss: 1.799702e-06\n",
      "step: 60390 train: 0.07874298095703125 elapsed, loss: 0.026340287\n",
      "step: 60400 train: 0.08247137069702148 elapsed, loss: 0.0003265455\n",
      "step: 60410 train: 0.0809776782989502 elapsed, loss: 4.2729713e-05\n",
      "step: 60420 train: 0.0777432918548584 elapsed, loss: 2.661356e-05\n",
      "step: 60430 train: 0.0799560546875 elapsed, loss: 3.8588394e-05\n",
      "step: 60440 train: 0.07600831985473633 elapsed, loss: 2.0488602e-05\n",
      "step: 60450 train: 0.08414769172668457 elapsed, loss: 8.8947945e-06\n",
      "step: 60460 train: 0.08272027969360352 elapsed, loss: 6.8833597e-06\n",
      "step: 60470 train: 0.08462667465209961 elapsed, loss: 4.5746337e-06\n",
      "step: 60480 train: 0.08280181884765625 elapsed, loss: 5.326187e-06\n",
      "step: 60490 train: 0.08222770690917969 elapsed, loss: 3.4323782e-06\n",
      "step: 60500 train: 0.08270406723022461 elapsed, loss: 4.5341103e-06\n",
      "step: 60510 train: 0.0831913948059082 elapsed, loss: 2.367883e-06\n",
      "step: 60520 train: 0.0811774730682373 elapsed, loss: 2.8088589e-06\n",
      "step: 60530 train: 0.0819087028503418 elapsed, loss: 1.9241083e-06\n",
      "step: 60540 train: 0.08261919021606445 elapsed, loss: 1.0961655e-06\n",
      "step: 60550 train: 0.07878804206848145 elapsed, loss: 1.1562358e-06\n",
      "step: 60560 train: 0.08005762100219727 elapsed, loss: 1.131556e-06\n",
      "step: 60570 train: 0.0864875316619873 elapsed, loss: 8.7032026e-07\n",
      "step: 60580 train: 0.0828557014465332 elapsed, loss: 1.0351642e-06\n",
      "step: 60590 train: 0.08059453964233398 elapsed, loss: 1.309904e-06\n",
      "step: 60600 train: 0.08038997650146484 elapsed, loss: 8.8848105e-07\n",
      "step: 60610 train: 0.08296322822570801 elapsed, loss: 9.341159e-07\n",
      "step: 60620 train: 0.08316564559936523 elapsed, loss: 8.8149613e-07\n",
      "step: 60630 train: 0.08085131645202637 elapsed, loss: 1.1641504e-06\n",
      "step: 60640 train: 0.08136153221130371 elapsed, loss: 1.0845243e-06\n",
      "step: 60650 train: 0.0849616527557373 elapsed, loss: 8.9174074e-07\n",
      "step: 60660 train: 0.07599425315856934 elapsed, loss: 1.3625233e-06\n",
      "step: 60670 train: 0.07896542549133301 elapsed, loss: 8.931378e-07\n",
      "step: 60680 train: 0.0852351188659668 elapsed, loss: 9.3132195e-07\n",
      "step: 60690 train: 0.08279156684875488 elapsed, loss: 8.6007583e-07\n",
      "step: 60700 train: 0.08134102821350098 elapsed, loss: 9.564676e-07\n",
      "step: 60710 train: 0.07771873474121094 elapsed, loss: 1.3918603e-06\n",
      "step: 60720 train: 0.08233428001403809 elapsed, loss: 1.1338836e-06\n",
      "step: 60730 train: 0.08333349227905273 elapsed, loss: 1.0724164e-06\n",
      "step: 60740 train: 0.07552218437194824 elapsed, loss: 2.1811525e-06\n",
      "step: 60750 train: 0.0847170352935791 elapsed, loss: 1.3709055e-06\n",
      "step: 60760 train: 0.0860741138458252 elapsed, loss: 9.802162e-07\n",
      "step: 60770 train: 0.07536578178405762 elapsed, loss: 1.8971016e-06\n",
      "step: 60780 train: 0.08079314231872559 elapsed, loss: 9.159552e-07\n",
      "step: 60790 train: 0.08433794975280762 elapsed, loss: 1.641454e-06\n",
      "step: 60800 train: 0.08130812644958496 elapsed, loss: 2.0167763e-06\n",
      "step: 60810 train: 0.0870051383972168 elapsed, loss: 0.002175865\n",
      "step: 60820 train: 0.08206915855407715 elapsed, loss: 2.8348748e-05\n",
      "step: 60830 train: 0.08718347549438477 elapsed, loss: 9.864399e-06\n",
      "step: 60840 train: 0.08506917953491211 elapsed, loss: 6.109437e-06\n",
      "step: 60850 train: 0.08160185813903809 elapsed, loss: 8.165296e-06\n",
      "step: 60860 train: 0.08307695388793945 elapsed, loss: 3.730397e-06\n",
      "step: 60870 train: 0.08539867401123047 elapsed, loss: 3.1604272e-06\n",
      "step: 60880 train: 0.07909059524536133 elapsed, loss: 4.393005e-06\n",
      "step: 60890 train: 0.08331441879272461 elapsed, loss: 3.2842977e-06\n",
      "step: 60900 train: 0.08494138717651367 elapsed, loss: 2.1960545e-06\n",
      "step: 60910 train: 0.09042143821716309 elapsed, loss: 1.8118856e-06\n",
      "step: 60920 train: 0.08373332023620605 elapsed, loss: 1.7387747e-06\n",
      "step: 60930 train: 0.08084988594055176 elapsed, loss: 1.5972158e-06\n",
      "step: 60940 train: 0.07996916770935059 elapsed, loss: 1.4677627e-06\n",
      "step: 60950 train: 0.08304691314697266 elapsed, loss: 1.3513478e-06\n",
      "step: 60960 train: 0.08122515678405762 elapsed, loss: 1.1273651e-06\n",
      "step: 60970 train: 0.07606840133666992 elapsed, loss: 1.2652006e-06\n",
      "step: 60980 train: 0.07723689079284668 elapsed, loss: 1.468693e-06\n",
      "step: 60990 train: 0.08445024490356445 elapsed, loss: 1.166015e-06\n",
      "step: 61000 train: 0.08532929420471191 elapsed, loss: 1.2167718e-06\n",
      "step: 61010 train: 0.08765077590942383 elapsed, loss: 1.1017535e-06\n",
      "step: 61020 train: 0.08042764663696289 elapsed, loss: 9.825444e-07\n",
      "step: 61030 train: 0.08480334281921387 elapsed, loss: 1.3564702e-06\n",
      "step: 61040 train: 0.07696938514709473 elapsed, loss: 4.389307e-06\n",
      "step: 61050 train: 0.07956862449645996 elapsed, loss: 1.7508844e-06\n",
      "step: 61060 train: 0.08096432685852051 elapsed, loss: 2.1979165e-06\n",
      "step: 61070 train: 0.07928991317749023 elapsed, loss: 1.847741e-06\n",
      "step: 61080 train: 0.08334994316101074 elapsed, loss: 1.0915091e-06\n",
      "step: 61090 train: 0.08231520652770996 elapsed, loss: 1.3988451e-06\n",
      "step: 61100 train: 0.08507299423217773 elapsed, loss: 9.4808576e-07\n",
      "step: 61110 train: 0.08567166328430176 elapsed, loss: 1.2689259e-06\n",
      "step: 61120 train: 0.08371758460998535 elapsed, loss: 1.13435e-06\n",
      "step: 61130 train: 0.08209729194641113 elapsed, loss: 1.3965135e-06\n",
      "step: 61140 train: 0.07770228385925293 elapsed, loss: 1.9059491e-06\n",
      "step: 61150 train: 0.0785369873046875 elapsed, loss: 1.3685773e-06\n",
      "step: 61160 train: 0.07789349555969238 elapsed, loss: 1.3005908e-06\n",
      "step: 61170 train: 0.08928751945495605 elapsed, loss: 1.013278e-06\n",
      "step: 61180 train: 0.07686734199523926 elapsed, loss: 4.0810382e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 61190 train: 0.0801398754119873 elapsed, loss: 1.9171218e-06\n",
      "step: 61200 train: 0.080108642578125 elapsed, loss: 1.5390092e-06\n",
      "step: 61210 train: 0.07928299903869629 elapsed, loss: 1.5189855e-06\n",
      "step: 61220 train: 0.08321404457092285 elapsed, loss: 1.4835954e-06\n",
      "step: 61230 train: 0.08107471466064453 elapsed, loss: 1.4244565e-06\n",
      "step: 61240 train: 0.08132147789001465 elapsed, loss: 2.094076e-06\n",
      "step: 61250 train: 0.08312320709228516 elapsed, loss: 1.9194529e-06\n",
      "step: 61260 train: 0.08443236351013184 elapsed, loss: 1.4202656e-06\n",
      "step: 61270 train: 0.07700419425964355 elapsed, loss: 2.5653235e-06\n",
      "step: 61280 train: 0.0830693244934082 elapsed, loss: 1.695005e-06\n",
      "step: 61290 train: 0.09079575538635254 elapsed, loss: 1.6093236e-06\n",
      "step: 61300 train: 0.08287310600280762 elapsed, loss: 2.0116536e-06\n",
      "step: 61310 train: 0.07742953300476074 elapsed, loss: 3.0477431e-06\n",
      "step: 61320 train: 0.08330464363098145 elapsed, loss: 1.6079267e-06\n",
      "step: 61330 train: 0.08287191390991211 elapsed, loss: 1.778358e-06\n",
      "step: 61340 train: 0.0774233341217041 elapsed, loss: 2.2482072e-06\n",
      "step: 61350 train: 0.08088850975036621 elapsed, loss: 2.6537987e-06\n",
      "step: 61360 train: 0.07838058471679688 elapsed, loss: 2.0582202e-06\n",
      "step: 61370 train: 0.0865321159362793 elapsed, loss: 8.739467e-05\n",
      "step: 61380 train: 0.07922840118408203 elapsed, loss: 5.0611554e-05\n",
      "step: 61390 train: 0.07931137084960938 elapsed, loss: 2.8378705e-05\n",
      "step: 61400 train: 0.07863569259643555 elapsed, loss: 1.3736704e-05\n",
      "step: 61410 train: 0.0809628963470459 elapsed, loss: 1.6941547e-05\n",
      "step: 61420 train: 0.08025145530700684 elapsed, loss: 1.1228367e-05\n",
      "step: 61430 train: 0.08079028129577637 elapsed, loss: 7.627018e-06\n",
      "step: 61440 train: 0.08180022239685059 elapsed, loss: 5.665211e-06\n",
      "step: 61450 train: 0.07787728309631348 elapsed, loss: 1.0080514e-05\n",
      "step: 61460 train: 0.08029603958129883 elapsed, loss: 4.1997837e-06\n",
      "step: 61470 train: 0.07871079444885254 elapsed, loss: 5.5250484e-06\n",
      "step: 61480 train: 0.08079385757446289 elapsed, loss: 3.646584e-06\n",
      "step: 61490 train: 0.08219122886657715 elapsed, loss: 1.929232e-06\n",
      "step: 61500 train: 0.08157038688659668 elapsed, loss: 2.114562e-06\n",
      "step: 61510 train: 0.08361434936523438 elapsed, loss: 2.2547163e-06\n",
      "step: 61520 train: 0.07783675193786621 elapsed, loss: 2.278011e-06\n",
      "step: 61530 train: 0.08403444290161133 elapsed, loss: 1.5865062e-06\n",
      "step: 61540 train: 0.0768883228302002 elapsed, loss: 1.7490217e-06\n",
      "step: 61550 train: 0.08145833015441895 elapsed, loss: 1.1860384e-06\n",
      "step: 61560 train: 0.0808255672454834 elapsed, loss: 1.4090895e-06\n",
      "step: 61570 train: 0.08722901344299316 elapsed, loss: 1.189763e-06\n",
      "step: 61580 train: 0.08240699768066406 elapsed, loss: 1.7345862e-06\n",
      "step: 61590 train: 0.07809257507324219 elapsed, loss: 1.7881373e-06\n",
      "step: 61600 train: 0.08302736282348633 elapsed, loss: 1.2954685e-06\n",
      "step: 61610 train: 0.0844719409942627 elapsed, loss: 1.5920943e-06\n",
      "step: 61620 train: 0.08353114128112793 elapsed, loss: 1.7513486e-06\n",
      "step: 61630 train: 0.07887077331542969 elapsed, loss: 1.7532126e-06\n",
      "step: 61640 train: 0.0813760757446289 elapsed, loss: 1.6009419e-06\n",
      "step: 61650 train: 0.08597421646118164 elapsed, loss: 1.2502994e-06\n",
      "step: 61660 train: 0.0834815502166748 elapsed, loss: 1.5250391e-06\n",
      "step: 61670 train: 0.08079195022583008 elapsed, loss: 1.8491387e-06\n",
      "step: 61680 train: 0.08055257797241211 elapsed, loss: 4.3320197e-06\n",
      "step: 61690 train: 0.07819151878356934 elapsed, loss: 3.3369188e-06\n",
      "step: 61700 train: 0.08142709732055664 elapsed, loss: 1.8863885e-06\n",
      "step: 61710 train: 0.08534049987792969 elapsed, loss: 1.3429657e-06\n",
      "step: 61720 train: 0.08411383628845215 elapsed, loss: 1.2260851e-06\n",
      "step: 61730 train: 0.08156633377075195 elapsed, loss: 2.927599e-06\n",
      "step: 61740 train: 0.07522034645080566 elapsed, loss: 0.004163361\n",
      "step: 61750 train: 0.08196592330932617 elapsed, loss: 9.81482e-05\n",
      "step: 61760 train: 0.07391023635864258 elapsed, loss: 5.777002e-05\n",
      "step: 61770 train: 0.08180952072143555 elapsed, loss: 3.7144073e-05\n",
      "step: 61780 train: 0.08542585372924805 elapsed, loss: 1.9012019e-05\n",
      "step: 61790 train: 0.08818292617797852 elapsed, loss: 1.4115366e-05\n",
      "step: 61800 train: 0.0802004337310791 elapsed, loss: 1.2196818e-05\n",
      "step: 61810 train: 0.07861661911010742 elapsed, loss: 1.4065574e-05\n",
      "step: 61820 train: 0.08618402481079102 elapsed, loss: 6.662118e-06\n",
      "step: 61830 train: 0.08883094787597656 elapsed, loss: 4.7105896e-06\n",
      "step: 61840 train: 0.0872340202331543 elapsed, loss: 5.4007032e-06\n",
      "step: 61850 train: 0.082366943359375 elapsed, loss: 3.395589e-06\n",
      "step: 61860 train: 0.08371901512145996 elapsed, loss: 3.2088626e-06\n",
      "step: 61870 train: 0.08968997001647949 elapsed, loss: 1.8901158e-06\n",
      "step: 61880 train: 0.08271050453186035 elapsed, loss: 1.0266673e-05\n",
      "step: 61890 train: 0.07800722122192383 elapsed, loss: 6.7166584e-06\n",
      "step: 61900 train: 0.07670116424560547 elapsed, loss: 4.522483e-06\n",
      "step: 61910 train: 0.07993459701538086 elapsed, loss: 2.7352876e-06\n",
      "step: 61920 train: 0.08258199691772461 elapsed, loss: 1.6516985e-06\n",
      "step: 61930 train: 0.08272051811218262 elapsed, loss: 1.6177055e-06\n",
      "step: 61940 train: 0.08595776557922363 elapsed, loss: 1.3164233e-06\n",
      "step: 61950 train: 0.08031415939331055 elapsed, loss: 3.1129373e-06\n",
      "step: 61960 train: 0.08237075805664062 elapsed, loss: 1.4691591e-06\n",
      "step: 61970 train: 0.08284378051757812 elapsed, loss: 1.5157209e-06\n",
      "step: 61980 train: 0.09716343879699707 elapsed, loss: 8.5541865e-07\n",
      "step: 61990 train: 0.08449244499206543 elapsed, loss: 1.1697402e-06\n",
      "step: 62000 train: 0.08460402488708496 elapsed, loss: 1.5282982e-06\n",
      "step: 62010 train: 0.08413457870483398 elapsed, loss: 1.6386597e-06\n",
      "step: 62020 train: 0.08038806915283203 elapsed, loss: 1.3709055e-06\n",
      "step: 62030 train: 0.08148741722106934 elapsed, loss: 1.308507e-06\n",
      "step: 62040 train: 0.08160591125488281 elapsed, loss: 1.1576317e-06\n",
      "step: 62050 train: 0.08639121055603027 elapsed, loss: 1.0821957e-06\n",
      "step: 62060 train: 0.08843779563903809 elapsed, loss: 1.0761422e-06\n",
      "step: 62070 train: 0.08681917190551758 elapsed, loss: 1.1380753e-06\n",
      "step: 62080 train: 0.08230876922607422 elapsed, loss: 2.6826633e-06\n",
      "step: 62090 train: 0.08445382118225098 elapsed, loss: 8.787023e-07\n",
      "step: 62100 train: 0.08095455169677734 elapsed, loss: 1.2931403e-06\n",
      "step: 62110 train: 0.08392167091369629 elapsed, loss: 9.83476e-07\n",
      "step: 62120 train: 0.08094954490661621 elapsed, loss: 1.1948856e-06\n",
      "step: 62130 train: 0.08035492897033691 elapsed, loss: 1.514329e-06\n",
      "step: 62140 train: 0.08179688453674316 elapsed, loss: 1.2405205e-06\n",
      "step: 62150 train: 0.0873408317565918 elapsed, loss: 2.3930236e-06\n",
      "step: 62160 train: 0.07645511627197266 elapsed, loss: 1.7406397e-06\n",
      "step: 62170 train: 0.08493542671203613 elapsed, loss: 1.4579837e-06\n",
      "step: 62180 train: 0.07923054695129395 elapsed, loss: 1.4225936e-06\n",
      "step: 62190 train: 0.08434581756591797 elapsed, loss: 2.0335394e-06\n",
      "step: 62200 train: 0.08725571632385254 elapsed, loss: 1.2926746e-06\n",
      "step: 62210 train: 0.08633971214294434 elapsed, loss: 1.4901145e-06\n",
      "step: 62220 train: 0.08534455299377441 elapsed, loss: 1.2377266e-06\n",
      "step: 62230 train: 0.08620190620422363 elapsed, loss: 1.1469228e-06\n",
      "step: 62240 train: 0.08282232284545898 elapsed, loss: 0.019701984\n",
      "step: 62250 train: 0.08259463310241699 elapsed, loss: 0.00019487088\n",
      "step: 62260 train: 0.07985138893127441 elapsed, loss: 5.4193035e-05\n",
      "step: 62270 train: 0.08590269088745117 elapsed, loss: 2.7563268e-05\n",
      "step: 62280 train: 0.08513784408569336 elapsed, loss: 1.5346053e-05\n",
      "step: 62290 train: 0.08571338653564453 elapsed, loss: 1.609852e-05\n",
      "step: 62300 train: 0.08171558380126953 elapsed, loss: 1.7346545e-05\n",
      "step: 62310 train: 0.0821540355682373 elapsed, loss: 8.927119e-06\n",
      "step: 62320 train: 0.07917070388793945 elapsed, loss: 1.0774959e-05\n",
      "step: 62330 train: 0.08854889869689941 elapsed, loss: 5.8932583e-06\n",
      "step: 62340 train: 0.08736681938171387 elapsed, loss: 3.3043234e-06\n",
      "step: 62350 train: 0.07600641250610352 elapsed, loss: 6.3120037e-06\n",
      "step: 62360 train: 0.09120583534240723 elapsed, loss: 2.3283023e-06\n",
      "step: 62370 train: 0.07851958274841309 elapsed, loss: 2.2104903e-06\n",
      "step: 62380 train: 0.07880640029907227 elapsed, loss: 2.862414e-06\n",
      "step: 62390 train: 0.08283185958862305 elapsed, loss: 1.8710239e-06\n",
      "step: 62400 train: 0.0867314338684082 elapsed, loss: 1.7844118e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 62410 train: 0.07964158058166504 elapsed, loss: 2.448442e-06\n",
      "step: 62420 train: 0.09089517593383789 elapsed, loss: 1.0146753e-06\n",
      "step: 62430 train: 0.07569360733032227 elapsed, loss: 1.9674162e-06\n",
      "step: 62440 train: 0.0810384750366211 elapsed, loss: 1.3052472e-06\n",
      "step: 62450 train: 0.07687091827392578 elapsed, loss: 1.3695083e-06\n",
      "step: 62460 train: 0.08176732063293457 elapsed, loss: 1.3201484e-06\n",
      "step: 62470 train: 0.08118891716003418 elapsed, loss: 1.2628724e-06\n",
      "step: 62480 train: 0.08541679382324219 elapsed, loss: 1.3178176e-06\n",
      "step: 62490 train: 0.07925295829772949 elapsed, loss: 2.150886e-06\n",
      "step: 62500 train: 0.08544254302978516 elapsed, loss: 1.6363314e-06\n",
      "step: 62510 train: 0.07812356948852539 elapsed, loss: 1.681035e-06\n",
      "step: 62520 train: 0.08553290367126465 elapsed, loss: 1.0873183e-06\n",
      "step: 62530 train: 0.08495497703552246 elapsed, loss: 1.4640377e-06\n",
      "step: 62540 train: 0.08394575119018555 elapsed, loss: 1.7234103e-06\n",
      "step: 62550 train: 0.08181476593017578 elapsed, loss: 1.6698594e-06\n",
      "step: 62560 train: 0.07965707778930664 elapsed, loss: 1.6251558e-06\n",
      "step: 62570 train: 0.0836188793182373 elapsed, loss: 1.3322557e-06\n",
      "step: 62580 train: 0.07988214492797852 elapsed, loss: 1.905018e-06\n",
      "step: 62590 train: 0.08466529846191406 elapsed, loss: 5.635865e-06\n",
      "step: 62600 train: 0.07694268226623535 elapsed, loss: 5.6178305e-06\n",
      "step: 62610 train: 0.0847022533416748 elapsed, loss: 2.248209e-06\n",
      "step: 62620 train: 0.08405613899230957 elapsed, loss: 2.9774296e-06\n",
      "step: 62630 train: 0.08934760093688965 elapsed, loss: 1.3634547e-06\n",
      "step: 62640 train: 0.08203125 elapsed, loss: 1.2833614e-06\n",
      "step: 62650 train: 0.08382511138916016 elapsed, loss: 1.3690429e-06\n",
      "step: 62660 train: 0.08480644226074219 elapsed, loss: 1.6181713e-06\n",
      "step: 62670 train: 0.08263397216796875 elapsed, loss: 1.6633384e-06\n",
      "step: 62680 train: 0.08136463165283203 elapsed, loss: 2.0638076e-06\n",
      "step: 62690 train: 0.08310604095458984 elapsed, loss: 1.4868544e-06\n",
      "step: 62700 train: 0.08461308479309082 elapsed, loss: 1.4496022e-06\n",
      "step: 62710 train: 0.08204340934753418 elapsed, loss: 2.116426e-06\n",
      "step: 62720 train: 0.08192706108093262 elapsed, loss: 1.5022218e-06\n",
      "step: 62730 train: 0.07769227027893066 elapsed, loss: 2.9904695e-06\n",
      "step: 62740 train: 0.07744264602661133 elapsed, loss: 2.6915163e-06\n",
      "step: 62750 train: 0.07979536056518555 elapsed, loss: 2.1466913e-06\n",
      "step: 62760 train: 0.08273196220397949 elapsed, loss: 1.8691621e-06\n",
      "step: 62770 train: 0.08304715156555176 elapsed, loss: 1.5674141e-06\n",
      "step: 62780 train: 0.078765869140625 elapsed, loss: 1.52923e-06\n",
      "step: 62790 train: 0.07664966583251953 elapsed, loss: 1.75135e-06\n",
      "step: 62800 train: 0.0816354751586914 elapsed, loss: 3.050073e-06\n",
      "step: 62810 train: 0.08188986778259277 elapsed, loss: 1.9902338e-06\n",
      "step: 62820 train: 0.07815361022949219 elapsed, loss: 2.474971e-06\n",
      "step: 62830 train: 0.08654141426086426 elapsed, loss: 1.8766127e-06\n",
      "step: 62840 train: 0.09111762046813965 elapsed, loss: 1.8239903e-06\n",
      "step: 62850 train: 0.08454394340515137 elapsed, loss: 2.1234127e-06\n",
      "step: 62860 train: 0.08379721641540527 elapsed, loss: 1.6712556e-06\n",
      "step: 62870 train: 0.07267475128173828 elapsed, loss: 3.1511227e-06\n",
      "step: 62880 train: 0.08204436302185059 elapsed, loss: 2.8479772e-06\n",
      "step: 62890 train: 0.0744483470916748 elapsed, loss: 2.7199226e-06\n",
      "step: 62900 train: 0.08287787437438965 elapsed, loss: 1.7872057e-06\n",
      "step: 62910 train: 0.07997536659240723 elapsed, loss: 2.6673008e-06\n",
      "step: 62920 train: 0.07453274726867676 elapsed, loss: 2.598851e-06\n",
      "step: 62930 train: 0.07280349731445312 elapsed, loss: 4.1136386e-06\n",
      "step: 62940 train: 0.07344961166381836 elapsed, loss: 3.4789443e-06\n",
      "step: 62950 train: 0.07785153388977051 elapsed, loss: 2.002341e-06\n",
      "step: 62960 train: 0.08401894569396973 elapsed, loss: 2.0116454e-06\n",
      "step: 62970 train: 0.08180427551269531 elapsed, loss: 2.4968713e-06\n",
      "step: 62980 train: 0.08347082138061523 elapsed, loss: 2.070793e-06\n",
      "step: 62990 train: 0.08141875267028809 elapsed, loss: 1.7667167e-06\n",
      "step: 63000 train: 0.07758378982543945 elapsed, loss: 0.0020560664\n",
      "step: 63010 train: 0.07960867881774902 elapsed, loss: 5.09325e-05\n",
      "step: 63020 train: 0.08371138572692871 elapsed, loss: 5.2995965e-05\n",
      "step: 63030 train: 0.08823537826538086 elapsed, loss: 2.3277033e-05\n",
      "step: 63040 train: 0.0815131664276123 elapsed, loss: 3.086651e-05\n",
      "step: 63050 train: 0.07682967185974121 elapsed, loss: 2.4022569e-05\n",
      "step: 63060 train: 0.07947301864624023 elapsed, loss: 1.1872314e-05\n",
      "step: 63070 train: 0.07775592803955078 elapsed, loss: 1.0401841e-05\n",
      "step: 63080 train: 0.07990193367004395 elapsed, loss: 7.743431e-06\n",
      "step: 63090 train: 0.08642458915710449 elapsed, loss: 5.9533877e-06\n",
      "step: 63100 train: 0.08559489250183105 elapsed, loss: 8.590599e-06\n",
      "step: 63110 train: 0.07911205291748047 elapsed, loss: 4.878707e-06\n",
      "step: 63120 train: 0.08739089965820312 elapsed, loss: 2.4093267e-06\n",
      "step: 63130 train: 0.08064150810241699 elapsed, loss: 1.9390104e-06\n",
      "step: 63140 train: 0.08248615264892578 elapsed, loss: 2.362758e-06\n",
      "step: 63150 train: 0.08298397064208984 elapsed, loss: 1.7578693e-06\n",
      "step: 63160 train: 0.0844264030456543 elapsed, loss: 1.8933756e-06\n",
      "step: 63170 train: 0.085296630859375 elapsed, loss: 1.3178201e-06\n",
      "step: 63180 train: 0.09055185317993164 elapsed, loss: 1.1967485e-06\n",
      "step: 63190 train: 0.08154630661010742 elapsed, loss: 1.6977976e-06\n",
      "step: 63200 train: 0.07840085029602051 elapsed, loss: 2.772074e-06\n",
      "step: 63210 train: 0.08130049705505371 elapsed, loss: 1.4058301e-06\n",
      "step: 63220 train: 0.08112645149230957 elapsed, loss: 1.5096718e-06\n",
      "step: 63230 train: 0.08750748634338379 elapsed, loss: 1.1208458e-06\n",
      "step: 63240 train: 0.08685922622680664 elapsed, loss: 1.3406366e-06\n",
      "step: 63250 train: 0.0839238166809082 elapsed, loss: 1.1208458e-06\n",
      "step: 63260 train: 0.08193373680114746 elapsed, loss: 2.9546109e-06\n",
      "step: 63270 train: 0.08167099952697754 elapsed, loss: 1.9385457e-06\n",
      "step: 63280 train: 0.08535385131835938 elapsed, loss: 1.6330723e-06\n",
      "step: 63290 train: 0.09026002883911133 elapsed, loss: 1.9115373e-06\n",
      "step: 63300 train: 0.07671403884887695 elapsed, loss: 2.0768464e-06\n",
      "step: 63310 train: 0.07970309257507324 elapsed, loss: 1.5441299e-06\n",
      "step: 63320 train: 0.08269309997558594 elapsed, loss: 1.5455271e-06\n",
      "step: 63330 train: 0.07700395584106445 elapsed, loss: 1.8081606e-06\n",
      "step: 63340 train: 0.07540655136108398 elapsed, loss: 2.386044e-06\n",
      "step: 63350 train: 0.08292126655578613 elapsed, loss: 1.5236421e-06\n",
      "step: 63360 train: 0.07968473434448242 elapsed, loss: 3.8482035e-06\n",
      "step: 63370 train: 0.08481478691101074 elapsed, loss: 1.532024e-06\n",
      "step: 63380 train: 0.07947373390197754 elapsed, loss: 1.9357515e-06\n",
      "step: 63390 train: 0.0822904109954834 elapsed, loss: 1.7457614e-06\n",
      "step: 63400 train: 0.08299660682678223 elapsed, loss: 1.7667169e-06\n",
      "step: 63410 train: 0.08170485496520996 elapsed, loss: 1.517123e-06\n",
      "step: 63420 train: 0.08183979988098145 elapsed, loss: 1.6521637e-06\n",
      "step: 63430 train: 0.07789325714111328 elapsed, loss: 0.00057048607\n",
      "step: 63440 train: 0.0885779857635498 elapsed, loss: 3.965154e-05\n",
      "step: 63450 train: 0.08146238327026367 elapsed, loss: 2.2349765e-05\n",
      "step: 63460 train: 0.07758736610412598 elapsed, loss: 2.3182616e-05\n",
      "step: 63470 train: 0.08438301086425781 elapsed, loss: 1.2889668e-05\n",
      "step: 63480 train: 0.08113265037536621 elapsed, loss: 1.7164635e-05\n",
      "step: 63490 train: 0.08395528793334961 elapsed, loss: 6.938295e-06\n",
      "step: 63500 train: 0.08182382583618164 elapsed, loss: 5.8007117e-06\n",
      "step: 63510 train: 0.08524131774902344 elapsed, loss: 5.7667226e-06\n",
      "step: 63520 train: 0.07831835746765137 elapsed, loss: 4.9820874e-06\n",
      "step: 63530 train: 0.08771896362304688 elapsed, loss: 2.7352885e-06\n",
      "step: 63540 train: 0.08492326736450195 elapsed, loss: 2.682669e-06\n",
      "step: 63550 train: 0.07652044296264648 elapsed, loss: 2.9047856e-06\n",
      "step: 63560 train: 0.07607889175415039 elapsed, loss: 2.1955898e-06\n",
      "step: 63570 train: 0.08173537254333496 elapsed, loss: 0.0009821391\n",
      "step: 63580 train: 0.08189630508422852 elapsed, loss: 3.3554144e-05\n",
      "step: 63590 train: 0.08014941215515137 elapsed, loss: 1.7015635e-05\n",
      "step: 63600 train: 0.07990050315856934 elapsed, loss: 1.239469e-05\n",
      "step: 63610 train: 0.0773618221282959 elapsed, loss: 2.0650412e-05\n",
      "step: 63620 train: 0.07790970802307129 elapsed, loss: 1.0937799e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 63630 train: 0.0810842514038086 elapsed, loss: 5.1865127e-06\n",
      "step: 63640 train: 0.07663702964782715 elapsed, loss: 7.535712e-06\n",
      "step: 63650 train: 0.09036922454833984 elapsed, loss: 5.303193e-06\n",
      "step: 63660 train: 0.08009123802185059 elapsed, loss: 4.0712534e-06\n",
      "step: 63670 train: 0.08246064186096191 elapsed, loss: 1.936682e-06\n",
      "step: 63680 train: 0.08238983154296875 elapsed, loss: 1.977195e-06\n",
      "step: 63690 train: 0.0818171501159668 elapsed, loss: 1.2628723e-06\n",
      "step: 63700 train: 0.07799100875854492 elapsed, loss: 2.2537965e-06\n",
      "step: 63710 train: 0.07829475402832031 elapsed, loss: 1.3196827e-06\n",
      "step: 63720 train: 0.08497428894042969 elapsed, loss: 1.0402857e-06\n",
      "step: 63730 train: 0.08054876327514648 elapsed, loss: 1.4505333e-06\n",
      "step: 63740 train: 0.08853960037231445 elapsed, loss: 8.7683964e-07\n",
      "step: 63750 train: 0.08158040046691895 elapsed, loss: 1.1310902e-06\n",
      "step: 63760 train: 0.08276653289794922 elapsed, loss: 1.1385409e-06\n",
      "step: 63770 train: 0.0788583755493164 elapsed, loss: 1.028645e-06\n",
      "step: 63780 train: 0.0775303840637207 elapsed, loss: 1.3229426e-06\n",
      "step: 63790 train: 0.08173751831054688 elapsed, loss: 1.3695085e-06\n",
      "step: 63800 train: 0.08250975608825684 elapsed, loss: 9.1409254e-07\n",
      "step: 63810 train: 0.08255290985107422 elapsed, loss: 1.6991953e-06\n",
      "step: 63820 train: 0.08535313606262207 elapsed, loss: 9.983771e-07\n",
      "step: 63830 train: 0.08402752876281738 elapsed, loss: 1.5948863e-06\n",
      "step: 63840 train: 0.08261561393737793 elapsed, loss: 1.1650836e-06\n",
      "step: 63850 train: 0.08252739906311035 elapsed, loss: 1.3369122e-06\n",
      "step: 63860 train: 0.08535170555114746 elapsed, loss: 1.2209628e-06\n",
      "step: 63870 train: 0.08403849601745605 elapsed, loss: 1.1157235e-06\n",
      "step: 63880 train: 0.08104944229125977 elapsed, loss: 1.0770736e-06\n",
      "step: 63890 train: 0.0761258602142334 elapsed, loss: 1.2922089e-06\n",
      "step: 63900 train: 0.0802619457244873 elapsed, loss: 1.2502994e-06\n",
      "step: 63910 train: 0.0870513916015625 elapsed, loss: 1.3792876e-06\n",
      "step: 63920 train: 0.08002424240112305 elapsed, loss: 0.009993497\n",
      "step: 63930 train: 0.08114457130432129 elapsed, loss: 0.00020109858\n",
      "step: 63940 train: 0.08858275413513184 elapsed, loss: 2.9573966e-05\n",
      "step: 63950 train: 0.08076596260070801 elapsed, loss: 6.496063e-05\n",
      "step: 63960 train: 0.08810687065124512 elapsed, loss: 1.6630187e-05\n",
      "step: 63970 train: 0.08427262306213379 elapsed, loss: 1.044308e-05\n",
      "step: 63980 train: 0.08017325401306152 elapsed, loss: 9.986897e-06\n",
      "step: 63990 train: 0.08212494850158691 elapsed, loss: 5.3443514e-06\n",
      "step: 64000 train: 0.08046627044677734 elapsed, loss: 4.305016e-06\n",
      "step: 64010 train: 0.07684755325317383 elapsed, loss: 5.3150147e-06\n",
      "step: 64020 train: 0.07593727111816406 elapsed, loss: 7.30756e-06\n",
      "step: 64030 train: 0.08577537536621094 elapsed, loss: 4.372999e-06\n",
      "step: 64040 train: 0.08072948455810547 elapsed, loss: 2.8642767e-06\n",
      "step: 64050 train: 0.08616399765014648 elapsed, loss: 1.3448283e-06\n",
      "step: 64060 train: 0.08228302001953125 elapsed, loss: 2.6007021e-06\n",
      "step: 64070 train: 0.09157490730285645 elapsed, loss: 5.1301577e-06\n",
      "step: 64080 train: 0.08045029640197754 elapsed, loss: 2.9983862e-06\n",
      "step: 64090 train: 0.08066701889038086 elapsed, loss: 3.8538e-06\n",
      "step: 64100 train: 0.08347749710083008 elapsed, loss: 1.4179361e-06\n",
      "step: 64110 train: 0.08692431449890137 elapsed, loss: 1.5282964e-06\n",
      "step: 64120 train: 0.07705831527709961 elapsed, loss: 1.4174714e-06\n",
      "step: 64130 train: 0.0783224105834961 elapsed, loss: 1.4160637e-06\n",
      "step: 64140 train: 0.07824516296386719 elapsed, loss: 1.3080414e-06\n",
      "step: 64150 train: 0.07878661155700684 elapsed, loss: 1.0663634e-06\n",
      "step: 64160 train: 0.0816805362701416 elapsed, loss: 1.0095528e-06\n",
      "step: 64170 train: 0.07989692687988281 elapsed, loss: 1.4011734e-06\n",
      "step: 64180 train: 0.08275914192199707 elapsed, loss: 8.7497705e-07\n",
      "step: 64190 train: 0.0828239917755127 elapsed, loss: 1.5054801e-06\n",
      "step: 64200 train: 0.08683252334594727 elapsed, loss: 9.317872e-07\n",
      "step: 64210 train: 0.08163833618164062 elapsed, loss: 1.1930233e-06\n",
      "step: 64220 train: 0.08119750022888184 elapsed, loss: 1.1110669e-06\n",
      "step: 64230 train: 0.08954119682312012 elapsed, loss: 8.204947e-07\n",
      "step: 64240 train: 0.07460355758666992 elapsed, loss: 1.411418e-06\n",
      "step: 64250 train: 0.08391594886779785 elapsed, loss: 1.0300421e-06\n",
      "step: 64260 train: 0.08138799667358398 elapsed, loss: 1.4109523e-06\n",
      "step: 64270 train: 0.08416008949279785 elapsed, loss: 1.12038e-06\n",
      "step: 64280 train: 0.07999897003173828 elapsed, loss: 1.0300421e-06\n",
      "step: 64290 train: 0.07948017120361328 elapsed, loss: 1.5282985e-06\n",
      "step: 64300 train: 0.08069062232971191 elapsed, loss: 1.2461082e-06\n",
      "step: 64310 train: 0.08124303817749023 elapsed, loss: 1.3122323e-06\n",
      "step: 64320 train: 0.07831931114196777 elapsed, loss: 0.0002680492\n",
      "step: 64330 train: 0.07509541511535645 elapsed, loss: 8.755093e-05\n",
      "step: 64340 train: 0.08703207969665527 elapsed, loss: 2.389061e-05\n",
      "step: 64350 train: 0.07780313491821289 elapsed, loss: 2.4070236e-05\n",
      "step: 64360 train: 0.07549047470092773 elapsed, loss: 2.4234581e-05\n",
      "step: 64370 train: 0.0799250602722168 elapsed, loss: 9.575759e-06\n",
      "step: 64380 train: 0.07183980941772461 elapsed, loss: 1.3927764e-05\n",
      "step: 64390 train: 0.08212947845458984 elapsed, loss: 6.929724e-06\n",
      "step: 64400 train: 0.08142924308776855 elapsed, loss: 4.651466e-06\n",
      "step: 64410 train: 0.07984542846679688 elapsed, loss: 3.924115e-06\n",
      "step: 64420 train: 0.08045244216918945 elapsed, loss: 3.5962908e-06\n",
      "step: 64430 train: 0.08669328689575195 elapsed, loss: 3.816081e-06\n",
      "step: 64440 train: 0.08575320243835449 elapsed, loss: 2.6016442e-06\n",
      "step: 64450 train: 0.09236311912536621 elapsed, loss: 1.6060637e-06\n",
      "step: 64460 train: 0.07917428016662598 elapsed, loss: 2.1778937e-06\n",
      "step: 64470 train: 0.07633638381958008 elapsed, loss: 2.3446007e-06\n",
      "step: 64480 train: 0.08389735221862793 elapsed, loss: 1.6172398e-06\n",
      "step: 64490 train: 0.08224368095397949 elapsed, loss: 2.0554253e-06\n",
      "step: 64500 train: 0.08568716049194336 elapsed, loss: 1.2069929e-06\n",
      "step: 64510 train: 0.07442283630371094 elapsed, loss: 1.8374967e-06\n",
      "step: 64520 train: 0.07976007461547852 elapsed, loss: 1.8137484e-06\n",
      "step: 64530 train: 0.0831298828125 elapsed, loss: 1.1431977e-06\n",
      "step: 64540 train: 0.08060693740844727 elapsed, loss: 1.348554e-06\n",
      "step: 64550 train: 0.07938599586486816 elapsed, loss: 1.4351664e-06\n",
      "step: 64560 train: 0.08563709259033203 elapsed, loss: 1.2177031e-06\n",
      "step: 64570 train: 0.08282208442687988 elapsed, loss: 1.0924404e-06\n",
      "step: 64580 train: 0.0864555835723877 elapsed, loss: 1.0812646e-06\n",
      "step: 64590 train: 0.08434557914733887 elapsed, loss: 1.3210797e-06\n",
      "step: 64600 train: 0.07851147651672363 elapsed, loss: 1.5120004e-06\n",
      "step: 64610 train: 0.07784032821655273 elapsed, loss: 1.2693915e-06\n",
      "step: 64620 train: 0.08428478240966797 elapsed, loss: 1.0868526e-06\n",
      "step: 64630 train: 0.07706522941589355 elapsed, loss: 1.7899998e-06\n",
      "step: 64640 train: 0.07769441604614258 elapsed, loss: 2.0693942e-06\n",
      "step: 64650 train: 0.07962751388549805 elapsed, loss: 1.7150286e-06\n",
      "step: 64660 train: 0.07796406745910645 elapsed, loss: 1.5222453e-06\n",
      "step: 64670 train: 0.07635164260864258 elapsed, loss: 1.792328e-06\n",
      "step: 64680 train: 0.08045792579650879 elapsed, loss: 0.019616451\n",
      "step: 64690 train: 0.08455157279968262 elapsed, loss: 9.081759e-05\n",
      "step: 64700 train: 0.07854104042053223 elapsed, loss: 7.701153e-05\n",
      "step: 64710 train: 0.07945680618286133 elapsed, loss: 7.222663e-05\n",
      "step: 64720 train: 0.0791926383972168 elapsed, loss: 4.916812e-05\n",
      "step: 64730 train: 0.0857083797454834 elapsed, loss: 1.8615132e-05\n",
      "step: 64740 train: 0.079742431640625 elapsed, loss: 1.1856386e-05\n",
      "step: 64750 train: 0.08235287666320801 elapsed, loss: 8.4740095e-06\n",
      "step: 64760 train: 0.08127665519714355 elapsed, loss: 6.6840216e-06\n",
      "step: 64770 train: 0.08015871047973633 elapsed, loss: 7.028182e-06\n",
      "step: 64780 train: 0.0814826488494873 elapsed, loss: 4.6123573e-06\n",
      "step: 64790 train: 0.08528637886047363 elapsed, loss: 3.9995493e-06\n",
      "step: 64800 train: 0.08094525337219238 elapsed, loss: 5.22374e-06\n",
      "step: 64810 train: 0.07878255844116211 elapsed, loss: 4.5862735e-06\n",
      "step: 64820 train: 0.07733964920043945 elapsed, loss: 2.2924453e-06\n",
      "step: 64830 train: 0.0725407600402832 elapsed, loss: 3.8198086e-06\n",
      "step: 64840 train: 0.08030080795288086 elapsed, loss: 1.335515e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64850 train: 0.07843255996704102 elapsed, loss: 1.508741e-06\n",
      "step: 64860 train: 0.08007502555847168 elapsed, loss: 1.3629892e-06\n",
      "step: 64870 train: 0.07726526260375977 elapsed, loss: 1.540406e-06\n",
      "step: 64880 train: 0.08088397979736328 elapsed, loss: 1.3308587e-06\n",
      "step: 64890 train: 0.08428001403808594 elapsed, loss: 1.2107181e-06\n",
      "step: 64900 train: 0.08251762390136719 elapsed, loss: 1.1492509e-06\n",
      "step: 64910 train: 0.08069181442260742 elapsed, loss: 2.1890696e-06\n",
      "step: 64920 train: 0.08136630058288574 elapsed, loss: 1.3108353e-06\n",
      "step: 64930 train: 0.08152532577514648 elapsed, loss: 2.4177084e-06\n",
      "step: 64940 train: 0.08056282997131348 elapsed, loss: 1.228879e-06\n",
      "step: 64950 train: 0.07829546928405762 elapsed, loss: 1.6777756e-06\n",
      "step: 64960 train: 0.08127522468566895 elapsed, loss: 1.3038502e-06\n",
      "step: 64970 train: 0.0778200626373291 elapsed, loss: 1.1385409e-06\n",
      "step: 64980 train: 0.07842302322387695 elapsed, loss: 1.2554216e-06\n",
      "step: 64990 train: 0.08388018608093262 elapsed, loss: 1.4463402e-06\n",
      "step: 65000 train: 0.07900261878967285 elapsed, loss: 1.245643e-06\n",
      "step: 65010 train: 0.07539057731628418 elapsed, loss: 1.7862744e-06\n",
      "step: 65020 train: 0.08534073829650879 elapsed, loss: 2.0046555e-06\n",
      "step: 65030 train: 0.07906699180603027 elapsed, loss: 0.003556334\n",
      "step: 65040 train: 0.0795133113861084 elapsed, loss: 0.00011680115\n",
      "step: 65050 train: 0.08065080642700195 elapsed, loss: 3.6764643e-05\n",
      "step: 65060 train: 0.08194994926452637 elapsed, loss: 2.868469e-05\n",
      "step: 65070 train: 0.07467174530029297 elapsed, loss: 3.650409e-05\n",
      "step: 65080 train: 0.08065176010131836 elapsed, loss: 2.0827865e-05\n",
      "step: 65090 train: 0.08217787742614746 elapsed, loss: 9.683553e-06\n",
      "step: 65100 train: 0.08063244819641113 elapsed, loss: 6.6393495e-06\n",
      "step: 65110 train: 0.07791018486022949 elapsed, loss: 9.9342615e-06\n",
      "step: 65120 train: 0.07978177070617676 elapsed, loss: 6.261247e-06\n",
      "step: 65130 train: 0.08796095848083496 elapsed, loss: 3.7327247e-06\n",
      "step: 65140 train: 0.08932113647460938 elapsed, loss: 2.2179408e-06\n",
      "step: 65150 train: 0.08023858070373535 elapsed, loss: 3.0174774e-06\n",
      "step: 65160 train: 0.08868074417114258 elapsed, loss: 1.6228275e-06\n",
      "step: 65170 train: 0.07689833641052246 elapsed, loss: 2.4405267e-06\n",
      "step: 65180 train: 0.08270144462585449 elapsed, loss: 1.9534464e-06\n",
      "step: 65190 train: 0.08630013465881348 elapsed, loss: 1.0170036e-06\n",
      "step: 65200 train: 0.07620811462402344 elapsed, loss: 1.7457612e-06\n",
      "step: 65210 train: 0.07714581489562988 elapsed, loss: 1.4202654e-06\n",
      "step: 65220 train: 0.08539915084838867 elapsed, loss: 1.0672937e-06\n",
      "step: 65230 train: 0.08676910400390625 elapsed, loss: 1.3438971e-06\n",
      "step: 65240 train: 0.08184695243835449 elapsed, loss: 1.2619408e-06\n",
      "step: 65250 train: 0.0783684253692627 elapsed, loss: 1.1739312e-06\n",
      "step: 65260 train: 0.08354806900024414 elapsed, loss: 2.1313276e-06\n",
      "step: 65270 train: 0.0813291072845459 elapsed, loss: 1.1981455e-06\n",
      "step: 65280 train: 0.085418701171875 elapsed, loss: 1.2903463e-06\n",
      "step: 65290 train: 0.08609724044799805 elapsed, loss: 0.0069341445\n",
      "step: 65300 train: 0.08454179763793945 elapsed, loss: 5.388722e-05\n",
      "step: 65310 train: 0.08372807502746582 elapsed, loss: 2.2517628e-05\n",
      "step: 65320 train: 0.08649516105651855 elapsed, loss: 1.2990786e-05\n",
      "step: 65330 train: 0.08356165885925293 elapsed, loss: 1.2128858e-05\n",
      "step: 65340 train: 0.07518267631530762 elapsed, loss: 1.5562942e-05\n",
      "step: 65350 train: 0.08549761772155762 elapsed, loss: 8.715207e-06\n",
      "step: 65360 train: 0.08502197265625 elapsed, loss: 3.9902357e-06\n",
      "step: 65370 train: 0.08307766914367676 elapsed, loss: 5.551764e-06\n",
      "step: 65380 train: 0.07583069801330566 elapsed, loss: 6.7748624e-06\n",
      "step: 65390 train: 0.07593059539794922 elapsed, loss: 3.5213207e-06\n",
      "step: 65400 train: 0.0784764289855957 elapsed, loss: 4.038651e-06\n",
      "step: 65410 train: 0.08710694313049316 elapsed, loss: 2.139709e-06\n",
      "step: 65420 train: 0.08027505874633789 elapsed, loss: 1.751815e-06\n",
      "step: 65430 train: 0.07795834541320801 elapsed, loss: 1.5048832e-05\n",
      "step: 65440 train: 0.07910609245300293 elapsed, loss: 3.181372e-06\n",
      "step: 65450 train: 0.07931351661682129 elapsed, loss: 1.8360963e-06\n",
      "step: 65460 train: 0.08131742477416992 elapsed, loss: 2.108976e-06\n",
      "step: 65470 train: 0.08443927764892578 elapsed, loss: 1.1278305e-06\n",
      "step: 65480 train: 0.08049726486206055 elapsed, loss: 1.2102525e-06\n",
      "step: 65490 train: 0.08112645149230957 elapsed, loss: 1.2251537e-06\n",
      "step: 65500 train: 0.08476829528808594 elapsed, loss: 9.578639e-07\n",
      "step: 65510 train: 0.07897543907165527 elapsed, loss: 1.3541411e-06\n",
      "step: 65520 train: 0.07817506790161133 elapsed, loss: 1.7895328e-06\n",
      "step: 65530 train: 0.07675004005432129 elapsed, loss: 1.1823125e-06\n",
      "step: 65540 train: 0.07685089111328125 elapsed, loss: 1.2246879e-06\n",
      "step: 65550 train: 0.08182406425476074 elapsed, loss: 1.902689e-06\n",
      "step: 65560 train: 0.07697892189025879 elapsed, loss: 1.2177032e-06\n",
      "step: 65570 train: 0.07782912254333496 elapsed, loss: 1.5706738e-06\n",
      "step: 65580 train: 0.07750248908996582 elapsed, loss: 1.5185199e-06\n",
      "step: 65590 train: 0.08501291275024414 elapsed, loss: 1.0281792e-06\n",
      "step: 65600 train: 0.08055877685546875 elapsed, loss: 1.2717187e-06\n",
      "step: 65610 train: 0.08586311340332031 elapsed, loss: 1.3303927e-06\n",
      "step: 65620 train: 0.07765650749206543 elapsed, loss: 5.0645076e-06\n",
      "step: 65630 train: 0.08259272575378418 elapsed, loss: 1.6270187e-06\n",
      "step: 65640 train: 0.07302045822143555 elapsed, loss: 1.9185215e-06\n",
      "step: 65650 train: 0.08027315139770508 elapsed, loss: 1.6102549e-06\n",
      "step: 65660 train: 0.08261561393737793 elapsed, loss: 1.0295764e-06\n",
      "step: 65670 train: 0.08116984367370605 elapsed, loss: 1.1664806e-06\n",
      "step: 65680 train: 0.07629251480102539 elapsed, loss: 3.956706e-06\n",
      "step: 65690 train: 0.08498239517211914 elapsed, loss: 0.00011122678\n",
      "step: 65700 train: 0.08514666557312012 elapsed, loss: 2.077271e-05\n",
      "step: 65710 train: 0.08104825019836426 elapsed, loss: 1.5265214e-05\n",
      "step: 65720 train: 0.08154416084289551 elapsed, loss: 1.43795505e-05\n",
      "step: 65730 train: 0.08084392547607422 elapsed, loss: 1.0397176e-05\n",
      "step: 65740 train: 0.08258891105651855 elapsed, loss: 5.3816193e-06\n",
      "step: 65750 train: 0.0747368335723877 elapsed, loss: 8.348752e-06\n",
      "step: 65760 train: 0.07964324951171875 elapsed, loss: 4.50479e-06\n",
      "step: 65770 train: 0.07781195640563965 elapsed, loss: 4.033074e-06\n",
      "step: 65780 train: 0.08264374732971191 elapsed, loss: 3.1916288e-06\n",
      "step: 65790 train: 0.07275104522705078 elapsed, loss: 3.0980366e-06\n",
      "step: 65800 train: 0.07815814018249512 elapsed, loss: 2.9047885e-06\n",
      "step: 65810 train: 0.0851435661315918 elapsed, loss: 1.6218963e-06\n",
      "step: 65820 train: 0.0830533504486084 elapsed, loss: 1.3299269e-06\n",
      "step: 65830 train: 0.07780742645263672 elapsed, loss: 1.4733507e-06\n",
      "step: 65840 train: 0.0818178653717041 elapsed, loss: 3.1315615e-06\n",
      "step: 65850 train: 0.08940410614013672 elapsed, loss: 1.8477409e-06\n",
      "step: 65860 train: 0.08230948448181152 elapsed, loss: 1.8295807e-06\n",
      "step: 65870 train: 0.08426666259765625 elapsed, loss: 1.0337671e-06\n",
      "step: 65880 train: 0.0784604549407959 elapsed, loss: 1.1455255e-06\n",
      "step: 65890 train: 0.08347511291503906 elapsed, loss: 1.3899969e-06\n",
      "step: 65900 train: 0.08613348007202148 elapsed, loss: 1.833306e-06\n",
      "step: 65910 train: 0.0770714282989502 elapsed, loss: 1.3262019e-06\n",
      "step: 65920 train: 0.07622194290161133 elapsed, loss: 2.1164274e-06\n",
      "step: 65930 train: 0.0771481990814209 elapsed, loss: 1.5399401e-06\n",
      "step: 65940 train: 0.08003997802734375 elapsed, loss: 1.3462255e-06\n",
      "step: 65950 train: 0.08034253120422363 elapsed, loss: 1.3890663e-06\n",
      "step: 65960 train: 0.08499026298522949 elapsed, loss: 1.4197994e-06\n",
      "step: 65970 train: 0.08853650093078613 elapsed, loss: 1.1394723e-06\n",
      "step: 65980 train: 0.07995080947875977 elapsed, loss: 1.9161935e-06\n",
      "step: 65990 train: 0.07959342002868652 elapsed, loss: 2.1415708e-06\n",
      "step: 66000 train: 0.08405327796936035 elapsed, loss: 1.8659026e-06\n",
      "step: 66010 train: 0.08176803588867188 elapsed, loss: 1.3466911e-06\n",
      "step: 66020 train: 0.08393549919128418 elapsed, loss: 4.460555e-06\n",
      "step: 66030 train: 0.07533884048461914 elapsed, loss: 2.0423875e-06\n",
      "step: 66040 train: 0.08398079872131348 elapsed, loss: 1.4165403e-06\n",
      "step: 66050 train: 0.08330082893371582 elapsed, loss: 1.6912801e-06\n",
      "step: 66060 train: 0.08604598045349121 elapsed, loss: 1.8714877e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 66070 train: 0.08451485633850098 elapsed, loss: 1.5585665e-06\n",
      "step: 66080 train: 0.0855412483215332 elapsed, loss: 1.6982644e-06\n",
      "step: 66090 train: 0.07870078086853027 elapsed, loss: 1.6805695e-06\n",
      "step: 66100 train: 0.07401728630065918 elapsed, loss: 2.463342e-06\n",
      "step: 66110 train: 0.07846260070800781 elapsed, loss: 1.5767275e-06\n",
      "step: 66120 train: 0.07996582984924316 elapsed, loss: 2.1969836e-06\n",
      "step: 66130 train: 0.08923554420471191 elapsed, loss: 1.5213138e-06\n",
      "step: 66140 train: 0.08687591552734375 elapsed, loss: 1.8305116e-06\n",
      "step: 66150 train: 0.08473420143127441 elapsed, loss: 1.2684603e-06\n",
      "step: 66160 train: 0.08037662506103516 elapsed, loss: 1.6433169e-06\n",
      "step: 66170 train: 0.08073019981384277 elapsed, loss: 2.043319e-06\n",
      "step: 66180 train: 0.08219647407531738 elapsed, loss: 2.1476262e-06\n",
      "step: 66190 train: 0.08946514129638672 elapsed, loss: 1.3802188e-06\n",
      "step: 66200 train: 0.0827186107635498 elapsed, loss: 1.6256216e-06\n",
      "step: 66210 train: 0.08353900909423828 elapsed, loss: 2.6840662e-06\n",
      "step: 66220 train: 0.08306884765625 elapsed, loss: 1.7089751e-06\n",
      "step: 66230 train: 0.07979965209960938 elapsed, loss: 2.0270204e-06\n",
      "step: 66240 train: 0.08291196823120117 elapsed, loss: 1.7504169e-06\n",
      "step: 66250 train: 0.07718086242675781 elapsed, loss: 2.522948e-06\n",
      "step: 66260 train: 0.08205819129943848 elapsed, loss: 1.5189851e-06\n",
      "step: 66270 train: 0.0794522762298584 elapsed, loss: 7.2242496e-05\n",
      "step: 66280 train: 0.08606815338134766 elapsed, loss: 0.017485714\n",
      "step: 66290 train: 0.07839584350585938 elapsed, loss: 0.00015342294\n",
      "step: 66300 train: 0.08034586906433105 elapsed, loss: 0.000107701446\n",
      "step: 66310 train: 0.08179879188537598 elapsed, loss: 6.98519e-05\n",
      "step: 66320 train: 0.08306264877319336 elapsed, loss: 3.528734e-05\n",
      "step: 66330 train: 0.08939051628112793 elapsed, loss: 1.5660815e-05\n",
      "step: 66340 train: 0.08117842674255371 elapsed, loss: 2.2818203e-05\n",
      "step: 66350 train: 0.0791313648223877 elapsed, loss: 1.7119066e-05\n",
      "step: 66360 train: 0.07926130294799805 elapsed, loss: 1.1760586e-05\n",
      "step: 66370 train: 0.08133244514465332 elapsed, loss: 1.06653115e-05\n",
      "step: 66380 train: 0.08472681045532227 elapsed, loss: 4.160195e-06\n",
      "step: 66390 train: 0.0824275016784668 elapsed, loss: 4.217467e-06\n",
      "step: 66400 train: 0.08264541625976562 elapsed, loss: 3.0277201e-06\n",
      "step: 66410 train: 0.08268547058105469 elapsed, loss: 2.183014e-06\n",
      "step: 66420 train: 0.08355903625488281 elapsed, loss: 2.0633413e-06\n",
      "step: 66430 train: 0.07895851135253906 elapsed, loss: 1.6908139e-06\n",
      "step: 66440 train: 0.07741665840148926 elapsed, loss: 1.7108375e-06\n",
      "step: 66450 train: 0.08618903160095215 elapsed, loss: 1.5152597e-06\n",
      "step: 66460 train: 0.08127593994140625 elapsed, loss: 2.2086278e-06\n",
      "step: 66470 train: 0.08369588851928711 elapsed, loss: 1.1776563e-06\n",
      "step: 66480 train: 0.08390974998474121 elapsed, loss: 1.1627551e-06\n",
      "step: 66490 train: 0.08214926719665527 elapsed, loss: 1.9203808e-06\n",
      "step: 66500 train: 0.08337211608886719 elapsed, loss: 2.4148992e-06\n",
      "step: 66510 train: 0.08630704879760742 elapsed, loss: 1.1851066e-06\n",
      "step: 66520 train: 0.0821535587310791 elapsed, loss: 8.254109e-05\n",
      "step: 66530 train: 0.07967567443847656 elapsed, loss: 2.613196e-05\n",
      "step: 66540 train: 0.07944154739379883 elapsed, loss: 6.399073e-06\n",
      "step: 66550 train: 0.08240604400634766 elapsed, loss: 4.9480927e-06\n",
      "step: 66560 train: 0.08976888656616211 elapsed, loss: 2.7026883e-06\n",
      "step: 66570 train: 0.0877695083618164 elapsed, loss: 2.5615964e-06\n",
      "step: 66580 train: 0.08378744125366211 elapsed, loss: 2.182085e-06\n",
      "step: 66590 train: 0.07744598388671875 elapsed, loss: 1.7299294e-06\n",
      "step: 66600 train: 0.08240270614624023 elapsed, loss: 1.7308607e-06\n",
      "step: 66610 train: 0.08198332786560059 elapsed, loss: 1.6107202e-06\n",
      "step: 66620 train: 0.08075928688049316 elapsed, loss: 1.7597317e-06\n",
      "step: 66630 train: 0.08055830001831055 elapsed, loss: 1.5133936e-06\n",
      "step: 66640 train: 0.08201742172241211 elapsed, loss: 1.7615941e-06\n",
      "step: 66650 train: 0.08212447166442871 elapsed, loss: 1.2465742e-06\n",
      "step: 66660 train: 0.08106350898742676 elapsed, loss: 1.3257351e-06\n",
      "step: 66670 train: 0.08421993255615234 elapsed, loss: 1.3369122e-06\n",
      "step: 66680 train: 0.08206772804260254 elapsed, loss: 1.4426166e-06\n",
      "step: 66690 train: 0.08408164978027344 elapsed, loss: 1.3718357e-06\n",
      "step: 66700 train: 0.08117341995239258 elapsed, loss: 1.311301e-06\n",
      "step: 66710 train: 0.07991838455200195 elapsed, loss: 1.0947683e-06\n",
      "step: 66720 train: 0.07455563545227051 elapsed, loss: 1.5548414e-06\n",
      "step: 66730 train: 0.08086919784545898 elapsed, loss: 1.4947714e-06\n",
      "step: 66740 train: 0.07877802848815918 elapsed, loss: 1.3136287e-06\n",
      "step: 66750 train: 0.08064794540405273 elapsed, loss: 1.1930233e-06\n",
      "step: 66760 train: 0.0834202766418457 elapsed, loss: 1.4957025e-06\n",
      "step: 66770 train: 0.08130526542663574 elapsed, loss: 9.2619973e-07\n",
      "step: 66780 train: 0.08472323417663574 elapsed, loss: 1.3206142e-06\n",
      "step: 66790 train: 0.08201479911804199 elapsed, loss: 1.0468059e-06\n",
      "step: 66800 train: 0.08400535583496094 elapsed, loss: 1.3839442e-06\n",
      "step: 66810 train: 0.08122420310974121 elapsed, loss: 1.5981475e-06\n",
      "step: 66820 train: 0.0774531364440918 elapsed, loss: 1.4812671e-06\n",
      "step: 66830 train: 0.08299756050109863 elapsed, loss: 1.5981476e-06\n",
      "step: 66840 train: 0.0775763988494873 elapsed, loss: 1.4379607e-06\n",
      "step: 66850 train: 0.08454680442810059 elapsed, loss: 1.3448284e-06\n",
      "step: 66860 train: 0.08153724670410156 elapsed, loss: 1.9702088e-06\n",
      "step: 66870 train: 0.08044004440307617 elapsed, loss: 5.093375e-06\n",
      "step: 66880 train: 0.07562828063964844 elapsed, loss: 2.184392e-06\n",
      "step: 66890 train: 0.07621240615844727 elapsed, loss: 1.8067637e-06\n",
      "step: 66900 train: 0.08200979232788086 elapsed, loss: 1.6423853e-06\n",
      "step: 66910 train: 0.08746147155761719 elapsed, loss: 1.2153749e-06\n",
      "step: 66920 train: 0.08037257194519043 elapsed, loss: 2.1438996e-06\n",
      "step: 66930 train: 0.08208870887756348 elapsed, loss: 3.198154e-06\n",
      "step: 66940 train: 0.0837256908416748 elapsed, loss: 1.8277178e-06\n",
      "step: 66950 train: 0.08159399032592773 elapsed, loss: 1.7518157e-06\n",
      "step: 66960 train: 0.08625626564025879 elapsed, loss: 1.2349328e-06\n",
      "step: 66970 train: 0.08345293998718262 elapsed, loss: 1.5990784e-06\n",
      "step: 66980 train: 0.08375382423400879 elapsed, loss: 1.922711e-06\n",
      "step: 66990 train: 0.0793917179107666 elapsed, loss: 3.0563242e-05\n",
      "step: 67000 train: 0.08115983009338379 elapsed, loss: 8.331317e-06\n",
      "step: 67010 train: 0.08520174026489258 elapsed, loss: 2.45729e-06\n",
      "step: 67020 train: 0.0791318416595459 elapsed, loss: 3.1818563e-06\n",
      "step: 67030 train: 0.08319282531738281 elapsed, loss: 1.7648538e-06\n",
      "step: 67040 train: 0.08562564849853516 elapsed, loss: 1.4225936e-06\n",
      "step: 67050 train: 0.08824539184570312 elapsed, loss: 1.3490196e-06\n",
      "step: 67060 train: 0.09315276145935059 elapsed, loss: 1.1892978e-06\n",
      "step: 67070 train: 0.07880568504333496 elapsed, loss: 1.682898e-06\n",
      "step: 67080 train: 0.08439826965332031 elapsed, loss: 0.005238508\n",
      "step: 67090 train: 0.08212018013000488 elapsed, loss: 0.0007997738\n",
      "step: 67100 train: 0.07926321029663086 elapsed, loss: 3.858562e-05\n",
      "step: 67110 train: 0.08650040626525879 elapsed, loss: 2.4232872e-05\n",
      "step: 67120 train: 0.08050298690795898 elapsed, loss: 5.6255016e-05\n",
      "step: 67130 train: 0.08471465110778809 elapsed, loss: 2.7099297e-05\n",
      "step: 67140 train: 0.07734322547912598 elapsed, loss: 1.2739867e-05\n",
      "step: 67150 train: 0.0819387435913086 elapsed, loss: 8.563824e-06\n",
      "step: 67160 train: 0.08407926559448242 elapsed, loss: 5.810957e-06\n",
      "step: 67170 train: 0.08022141456604004 elapsed, loss: 8.793481e-06\n",
      "step: 67180 train: 0.07921218872070312 elapsed, loss: 5.727122e-06\n",
      "step: 67190 train: 0.07697701454162598 elapsed, loss: 4.251462e-06\n",
      "step: 67200 train: 0.0747675895690918 elapsed, loss: 4.1909375e-06\n",
      "step: 67210 train: 0.0805976390838623 elapsed, loss: 2.7236463e-06\n",
      "step: 67220 train: 0.08401823043823242 elapsed, loss: 1.7355169e-06\n",
      "step: 67230 train: 0.08028078079223633 elapsed, loss: 2.2039708e-06\n",
      "step: 67240 train: 0.0817267894744873 elapsed, loss: 1.8542607e-06\n",
      "step: 67250 train: 0.0862267017364502 elapsed, loss: 1.2661319e-06\n",
      "step: 67260 train: 0.08890891075134277 elapsed, loss: 1.1888318e-06\n",
      "step: 67270 train: 0.08228778839111328 elapsed, loss: 1.6698593e-06\n",
      "step: 67280 train: 0.07821488380432129 elapsed, loss: 1.3480882e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 67290 train: 0.07970380783081055 elapsed, loss: 1.6493701e-06\n",
      "step: 67300 train: 0.08145737648010254 elapsed, loss: 1.1087387e-06\n",
      "step: 67310 train: 0.08173489570617676 elapsed, loss: 3.0002352e-06\n",
      "step: 67320 train: 0.08092999458312988 elapsed, loss: 1.3299218e-06\n",
      "step: 67330 train: 0.08119845390319824 elapsed, loss: 1.8822005e-06\n",
      "step: 67340 train: 0.08072137832641602 elapsed, loss: 1.3792874e-06\n",
      "step: 67350 train: 0.08452081680297852 elapsed, loss: 1.4794045e-06\n",
      "step: 67360 train: 0.0816793441772461 elapsed, loss: 2.8265586e-06\n",
      "step: 67370 train: 0.08182764053344727 elapsed, loss: 1.5581006e-06\n",
      "step: 67380 train: 0.0794532299041748 elapsed, loss: 1.3397062e-06\n",
      "step: 67390 train: 0.07777786254882812 elapsed, loss: 1.7201439e-06\n",
      "step: 67400 train: 0.08488845825195312 elapsed, loss: 1.354607e-06\n",
      "step: 67410 train: 0.08677792549133301 elapsed, loss: 1.9492556e-06\n",
      "step: 67420 train: 0.08629107475280762 elapsed, loss: 1.7732361e-06\n",
      "step: 67430 train: 0.08568120002746582 elapsed, loss: 1.4421514e-06\n",
      "step: 67440 train: 0.08364057540893555 elapsed, loss: 1.3816158e-06\n",
      "step: 67450 train: 0.08103013038635254 elapsed, loss: 2.323177e-06\n",
      "step: 67460 train: 0.08091425895690918 elapsed, loss: 1.9478584e-06\n",
      "step: 67470 train: 0.08005666732788086 elapsed, loss: 1.4319071e-06\n",
      "step: 67480 train: 0.07594776153564453 elapsed, loss: 2.089419e-06\n",
      "step: 67490 train: 0.08650565147399902 elapsed, loss: 1.3667131e-06\n",
      "step: 67500 train: 0.07904386520385742 elapsed, loss: 2.1965202e-06\n",
      "step: 67510 train: 0.08634257316589355 elapsed, loss: 1.362989e-06\n",
      "step: 67520 train: 0.08310580253601074 elapsed, loss: 2.8512377e-06\n",
      "step: 67530 train: 0.07565188407897949 elapsed, loss: 0.00051991636\n",
      "step: 67540 train: 0.0797877311706543 elapsed, loss: 3.757822e-05\n",
      "step: 67550 train: 0.0854794979095459 elapsed, loss: 2.54133e-05\n",
      "step: 67560 train: 0.08074188232421875 elapsed, loss: 2.8822013e-05\n",
      "step: 67570 train: 0.07828021049499512 elapsed, loss: 1.50247215e-05\n",
      "step: 67580 train: 0.07524800300598145 elapsed, loss: 1.8501429e-05\n",
      "step: 67590 train: 0.08905863761901855 elapsed, loss: 7.516518e-06\n",
      "step: 67600 train: 0.0839071273803711 elapsed, loss: 5.42585e-06\n",
      "step: 67610 train: 0.08058404922485352 elapsed, loss: 6.429353e-06\n",
      "step: 67620 train: 0.08482861518859863 elapsed, loss: 3.773707e-06\n",
      "step: 67630 train: 0.08166980743408203 elapsed, loss: 3.371374e-06\n",
      "step: 67640 train: 0.07429814338684082 elapsed, loss: 3.8943144e-06\n",
      "step: 67650 train: 0.07939457893371582 elapsed, loss: 2.6146822e-06\n",
      "step: 67660 train: 0.08933568000793457 elapsed, loss: 1.7816175e-06\n",
      "step: 67670 train: 0.08938789367675781 elapsed, loss: 2.1080455e-06\n",
      "step: 67680 train: 0.07649016380310059 elapsed, loss: 1.8053663e-06\n",
      "step: 67690 train: 0.0852501392364502 elapsed, loss: 1.1655492e-06\n",
      "step: 67700 train: 0.08747673034667969 elapsed, loss: 1.1702058e-06\n",
      "step: 67710 train: 0.08075737953186035 elapsed, loss: 2.2905829e-06\n",
      "step: 67720 train: 0.08133387565612793 elapsed, loss: 1.5832452e-06\n",
      "step: 67730 train: 0.07607841491699219 elapsed, loss: 1.4249221e-06\n",
      "step: 67740 train: 0.08315110206604004 elapsed, loss: 1.6814988e-06\n",
      "step: 67750 train: 0.07918763160705566 elapsed, loss: 1.5157258e-06\n",
      "step: 67760 train: 0.08167862892150879 elapsed, loss: 1.4589151e-06\n",
      "step: 67770 train: 0.0827627182006836 elapsed, loss: 1.5348139e-06\n",
      "step: 67780 train: 0.0772700309753418 elapsed, loss: 8.248146e-06\n",
      "step: 67790 train: 0.08133387565612793 elapsed, loss: 5.2499927e-06\n",
      "step: 67800 train: 0.08538699150085449 elapsed, loss: 1.8374968e-06\n",
      "step: 67810 train: 0.07918429374694824 elapsed, loss: 2.3888338e-06\n",
      "step: 67820 train: 0.07485008239746094 elapsed, loss: 2.636104e-06\n",
      "step: 67830 train: 0.0858774185180664 elapsed, loss: 1.3215453e-06\n",
      "step: 67840 train: 0.08670902252197266 elapsed, loss: 1.2102525e-06\n",
      "step: 67850 train: 0.08149385452270508 elapsed, loss: 1.6028036e-06\n",
      "step: 67860 train: 0.08716273307800293 elapsed, loss: 1.5902301e-06\n",
      "step: 67870 train: 0.08258271217346191 elapsed, loss: 1.4174716e-06\n",
      "step: 67880 train: 0.08085775375366211 elapsed, loss: 1.8686919e-06\n",
      "step: 67890 train: 0.08185482025146484 elapsed, loss: 1.440289e-06\n",
      "step: 67900 train: 0.0880887508392334 elapsed, loss: 1.177656e-06\n",
      "step: 67910 train: 0.08295440673828125 elapsed, loss: 1.7601976e-06\n",
      "step: 67920 train: 0.0802154541015625 elapsed, loss: 1.8808038e-06\n",
      "step: 67930 train: 0.08201289176940918 elapsed, loss: 1.891048e-06\n",
      "step: 67940 train: 0.08292818069458008 elapsed, loss: 1.3131637e-06\n",
      "step: 67950 train: 0.07822299003601074 elapsed, loss: 1.9292315e-06\n",
      "step: 67960 train: 0.08592462539672852 elapsed, loss: 0.004335027\n",
      "step: 67970 train: 0.0850217342376709 elapsed, loss: 0.00012871978\n",
      "step: 67980 train: 0.0766444206237793 elapsed, loss: 2.8888768e-05\n",
      "step: 67990 train: 0.08008146286010742 elapsed, loss: 2.3647146e-05\n",
      "step: 68000 train: 0.08265447616577148 elapsed, loss: 1.4626574e-05\n",
      "step: 68010 train: 0.08333849906921387 elapsed, loss: 1.6098871e-05\n",
      "step: 68020 train: 0.08031415939331055 elapsed, loss: 1.14066115e-05\n",
      "step: 68030 train: 0.07857537269592285 elapsed, loss: 1.2633194e-05\n",
      "step: 68040 train: 0.07721304893493652 elapsed, loss: 7.000237e-06\n",
      "step: 68050 train: 0.08517193794250488 elapsed, loss: 5.106877e-06\n",
      "step: 68060 train: 0.0888679027557373 elapsed, loss: 3.912472e-06\n",
      "step: 68070 train: 0.08440470695495605 elapsed, loss: 6.686386e-06\n",
      "step: 68080 train: 0.08832120895385742 elapsed, loss: 2.7594997e-06\n",
      "step: 68090 train: 0.0853121280670166 elapsed, loss: 1.8225959e-06\n",
      "step: 68100 train: 0.08140683174133301 elapsed, loss: 2.504787e-06\n",
      "step: 68110 train: 0.07701683044433594 elapsed, loss: 1.5855749e-06\n",
      "step: 68120 train: 0.07559514045715332 elapsed, loss: 1.9804547e-06\n",
      "step: 68130 train: 0.08109664916992188 elapsed, loss: 1.4882512e-06\n",
      "step: 68140 train: 0.07868838310241699 elapsed, loss: 1.4705568e-06\n",
      "step: 68150 train: 0.08032965660095215 elapsed, loss: 1.4826641e-06\n",
      "step: 68160 train: 0.07887983322143555 elapsed, loss: 1.3280649e-06\n",
      "step: 68170 train: 0.07423853874206543 elapsed, loss: 1.6693936e-06\n",
      "step: 68180 train: 0.07935976982116699 elapsed, loss: 1.1562361e-06\n",
      "step: 68190 train: 0.08000779151916504 elapsed, loss: 1.1879007e-06\n",
      "step: 68200 train: 0.08460497856140137 elapsed, loss: 1.088249e-06\n",
      "step: 68210 train: 0.08012223243713379 elapsed, loss: 1.4323728e-06\n",
      "step: 68220 train: 0.08124804496765137 elapsed, loss: 1.6447137e-06\n",
      "step: 68230 train: 0.07486772537231445 elapsed, loss: 1.3592639e-06\n",
      "step: 68240 train: 0.08071732521057129 elapsed, loss: 2.4409924e-06\n",
      "step: 68250 train: 0.08038568496704102 elapsed, loss: 1.8998956e-06\n",
      "step: 68260 train: 0.08748197555541992 elapsed, loss: 1.1054792e-06\n",
      "step: 68270 train: 0.07816958427429199 elapsed, loss: 2.4503047e-06\n",
      "step: 68280 train: 0.07831597328186035 elapsed, loss: 2.9276052e-06\n",
      "step: 68290 train: 0.08534741401672363 elapsed, loss: 1.1469227e-06\n",
      "step: 68300 train: 0.0843815803527832 elapsed, loss: 1.312698e-06\n",
      "step: 68310 train: 0.08753514289855957 elapsed, loss: 1.7378445e-06\n",
      "step: 68320 train: 0.08220958709716797 elapsed, loss: 2.006532e-06\n",
      "step: 68330 train: 0.08166050910949707 elapsed, loss: 1.7201498e-06\n",
      "step: 68340 train: 0.08100581169128418 elapsed, loss: 0.00012309357\n",
      "step: 68350 train: 0.08046674728393555 elapsed, loss: 6.833271e-05\n",
      "step: 68360 train: 0.08304810523986816 elapsed, loss: 1.8506247e-05\n",
      "step: 68370 train: 0.08330178260803223 elapsed, loss: 1.2890153e-05\n",
      "step: 68380 train: 0.08459591865539551 elapsed, loss: 7.086366e-06\n",
      "step: 68390 train: 0.07944583892822266 elapsed, loss: 9.599033e-06\n",
      "step: 68400 train: 0.08207964897155762 elapsed, loss: 4.3096793e-06\n",
      "step: 68410 train: 0.07895278930664062 elapsed, loss: 3.6372694e-06\n",
      "step: 68420 train: 0.08052635192871094 elapsed, loss: 3.5995458e-06\n",
      "step: 68430 train: 0.08572721481323242 elapsed, loss: 2.373005e-06\n",
      "step: 68440 train: 0.08748555183410645 elapsed, loss: 1.8891843e-06\n",
      "step: 68450 train: 0.08064818382263184 elapsed, loss: 2.4535643e-06\n",
      "step: 68460 train: 0.07930970191955566 elapsed, loss: 1.4733506e-06\n",
      "step: 68470 train: 0.07906508445739746 elapsed, loss: 1.7411006e-06\n",
      "step: 68480 train: 0.07724809646606445 elapsed, loss: 1.556238e-06\n",
      "step: 68490 train: 0.07948756217956543 elapsed, loss: 7.0557944e-05\n",
      "step: 68500 train: 0.08256840705871582 elapsed, loss: 3.134887e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 68510 train: 0.08739304542541504 elapsed, loss: 1.6392709e-05\n",
      "step: 68520 train: 0.0862884521484375 elapsed, loss: 2.1866937e-05\n",
      "step: 68530 train: 0.0808103084564209 elapsed, loss: 1.1543576e-05\n",
      "step: 68540 train: 0.08267402648925781 elapsed, loss: 2.9386174e-05\n",
      "step: 68550 train: 0.07723641395568848 elapsed, loss: 0.00011373805\n",
      "step: 68560 train: 0.07597851753234863 elapsed, loss: 4.9471688e-05\n",
      "step: 68570 train: 0.08439874649047852 elapsed, loss: 1.9125868e-05\n",
      "step: 68580 train: 0.08438301086425781 elapsed, loss: 1.174516e-05\n",
      "step: 68590 train: 0.08183836936950684 elapsed, loss: 1.2081267e-05\n",
      "step: 68600 train: 0.0818624496459961 elapsed, loss: 7.2414437e-06\n",
      "step: 68610 train: 0.08086037635803223 elapsed, loss: 6.100123e-06\n",
      "step: 68620 train: 0.07979416847229004 elapsed, loss: 6.0624006e-06\n",
      "step: 68630 train: 0.07735657691955566 elapsed, loss: 6.5038366e-06\n",
      "step: 68640 train: 0.08153486251831055 elapsed, loss: 4.1182916e-06\n",
      "step: 68650 train: 0.07997322082519531 elapsed, loss: 3.978128e-06\n",
      "step: 68660 train: 0.08366918563842773 elapsed, loss: 2.5713725e-06\n",
      "step: 68670 train: 0.08244442939758301 elapsed, loss: 2.0866241e-06\n",
      "step: 68680 train: 0.07863521575927734 elapsed, loss: 2.264041e-06\n",
      "step: 68690 train: 0.07962489128112793 elapsed, loss: 1.9981496e-06\n",
      "step: 68700 train: 0.08804678916931152 elapsed, loss: 1.24378e-06\n",
      "step: 68710 train: 0.0825660228729248 elapsed, loss: 1.40676e-06\n",
      "step: 68720 train: 0.08565115928649902 elapsed, loss: 1.132953e-06\n",
      "step: 68730 train: 0.07751822471618652 elapsed, loss: 1.3699738e-06\n",
      "step: 68740 train: 0.0743408203125 elapsed, loss: 1.4323725e-06\n",
      "step: 68750 train: 0.08140778541564941 elapsed, loss: 1.0444775e-06\n",
      "step: 68760 train: 0.07661914825439453 elapsed, loss: 8.5215964e-07\n",
      "step: 68770 train: 0.08069396018981934 elapsed, loss: 1.1003567e-06\n",
      "step: 68780 train: 0.08826994895935059 elapsed, loss: 0.0016420118\n",
      "step: 68790 train: 0.08141565322875977 elapsed, loss: 4.5564484e-05\n",
      "step: 68800 train: 0.08268404006958008 elapsed, loss: 2.6878472e-05\n",
      "step: 68810 train: 0.07963824272155762 elapsed, loss: 1.1603121e-05\n",
      "step: 68820 train: 0.09094071388244629 elapsed, loss: 7.6409215e-06\n",
      "step: 68830 train: 0.07564949989318848 elapsed, loss: 8.56435e-06\n",
      "step: 68840 train: 0.08289146423339844 elapsed, loss: 5.9618055e-06\n",
      "step: 68850 train: 0.08397626876831055 elapsed, loss: 3.217243e-06\n",
      "step: 68860 train: 0.08395242691040039 elapsed, loss: 3.119914e-06\n",
      "step: 68870 train: 0.07978487014770508 elapsed, loss: 3.2838338e-06\n",
      "step: 68880 train: 0.08055353164672852 elapsed, loss: 2.107111e-06\n",
      "step: 68890 train: 0.08664751052856445 elapsed, loss: 1.7932559e-06\n",
      "step: 68900 train: 0.0851438045501709 elapsed, loss: 1.4901142e-06\n",
      "step: 68910 train: 0.08117818832397461 elapsed, loss: 1.1194486e-06\n",
      "step: 68920 train: 0.08307576179504395 elapsed, loss: 1.6950045e-06\n",
      "step: 68930 train: 0.07935190200805664 elapsed, loss: 1.6917454e-06\n",
      "step: 68940 train: 0.08372759819030762 elapsed, loss: 1.2498336e-06\n",
      "step: 68950 train: 0.08361363410949707 elapsed, loss: 1.3611267e-06\n",
      "step: 68960 train: 0.0751030445098877 elapsed, loss: 1.759266e-06\n",
      "step: 68970 train: 0.0818026065826416 elapsed, loss: 9.629863e-07\n",
      "step: 68980 train: 0.07891368865966797 elapsed, loss: 1.3462254e-06\n",
      "step: 68990 train: 0.08417129516601562 elapsed, loss: 8.610072e-07\n",
      "step: 69000 train: 0.07951688766479492 elapsed, loss: 8.5495356e-07\n",
      "step: 69010 train: 0.07879900932312012 elapsed, loss: 1.4170059e-06\n",
      "step: 69020 train: 0.08352327346801758 elapsed, loss: 8.773053e-07\n",
      "step: 69030 train: 0.08309221267700195 elapsed, loss: 1.0277138e-06\n",
      "step: 69040 train: 0.0837409496307373 elapsed, loss: 9.755597e-07\n",
      "step: 69050 train: 0.0880424976348877 elapsed, loss: 8.4564033e-07\n",
      "step: 69060 train: 0.08047676086425781 elapsed, loss: 9.629867e-07\n",
      "step: 69070 train: 0.08686614036560059 elapsed, loss: 8.926721e-07\n",
      "step: 69080 train: 0.08546137809753418 elapsed, loss: 8.218916e-07\n",
      "step: 69090 train: 0.08162975311279297 elapsed, loss: 9.993083e-07\n",
      "step: 69100 train: 0.07583808898925781 elapsed, loss: 3.264312e-05\n",
      "step: 69110 train: 0.08062505722045898 elapsed, loss: 2.5323034e-05\n",
      "step: 69120 train: 0.08034539222717285 elapsed, loss: 1.9944462e-05\n",
      "step: 69130 train: 0.0824594497680664 elapsed, loss: 1.15184375e-05\n",
      "step: 69140 train: 0.07988739013671875 elapsed, loss: 8.613145e-06\n",
      "step: 69150 train: 0.07292628288269043 elapsed, loss: 1.1031333e-05\n",
      "step: 69160 train: 0.0827023983001709 elapsed, loss: 6.3943125e-06\n",
      "step: 69170 train: 0.07598066329956055 elapsed, loss: 5.361577e-06\n",
      "step: 69180 train: 0.08203887939453125 elapsed, loss: 3.3709032e-06\n",
      "step: 69190 train: 0.08114790916442871 elapsed, loss: 3.434703e-06\n",
      "step: 69200 train: 0.0770268440246582 elapsed, loss: 3.5958228e-06\n",
      "step: 69210 train: 0.07551026344299316 elapsed, loss: 2.4610144e-06\n",
      "step: 69220 train: 0.0803229808807373 elapsed, loss: 2.0046682e-06\n",
      "step: 69230 train: 0.07662153244018555 elapsed, loss: 2.4395877e-06\n",
      "step: 69240 train: 0.07964301109313965 elapsed, loss: 1.6437806e-06\n",
      "step: 69250 train: 0.08393263816833496 elapsed, loss: 1.0207287e-06\n",
      "step: 69260 train: 0.07943344116210938 elapsed, loss: 2.593726e-06\n",
      "step: 69270 train: 0.07835149765014648 elapsed, loss: 1.6572848e-06\n",
      "step: 69280 train: 0.08104419708251953 elapsed, loss: 1.6535612e-06\n",
      "step: 69290 train: 0.081787109375 elapsed, loss: 1.0328288e-06\n",
      "step: 69300 train: 0.083099365234375 elapsed, loss: 9.411008e-07\n",
      "step: 69310 train: 0.08558845520019531 elapsed, loss: 1.0094867e-05\n",
      "step: 69320 train: 0.08770203590393066 elapsed, loss: 2.6589087e-06\n",
      "step: 69330 train: 0.08454012870788574 elapsed, loss: 1.5199166e-06\n",
      "step: 69340 train: 0.0812222957611084 elapsed, loss: 1.2335349e-06\n",
      "step: 69350 train: 0.07752299308776855 elapsed, loss: 1.7206162e-06\n",
      "step: 69360 train: 0.0813751220703125 elapsed, loss: 1.2312073e-06\n",
      "step: 69370 train: 0.08476519584655762 elapsed, loss: 1.0570502e-06\n",
      "step: 69380 train: 0.08014631271362305 elapsed, loss: 9.252683e-07\n",
      "step: 69390 train: 0.08326435089111328 elapsed, loss: 9.206118e-07\n",
      "step: 69400 train: 0.07962441444396973 elapsed, loss: 2.1094404e-06\n",
      "step: 69410 train: 0.07779622077941895 elapsed, loss: 1.3941885e-06\n",
      "step: 69420 train: 0.0787808895111084 elapsed, loss: 1.0821959e-06\n",
      "step: 69430 train: 0.0761878490447998 elapsed, loss: 1.4849925e-06\n",
      "step: 69440 train: 0.08008694648742676 elapsed, loss: 9.788193e-07\n",
      "step: 69450 train: 0.08061385154724121 elapsed, loss: 1.1469228e-06\n",
      "step: 69460 train: 0.08502554893493652 elapsed, loss: 1.3583326e-06\n",
      "step: 69470 train: 0.08678269386291504 elapsed, loss: 1.0062931e-06\n",
      "step: 69480 train: 0.08036017417907715 elapsed, loss: 1.50129e-06\n",
      "step: 69490 train: 0.0855720043182373 elapsed, loss: 1.0184006e-06\n",
      "step: 69500 train: 0.07961750030517578 elapsed, loss: 1.4435485e-06\n",
      "step: 69510 train: 0.08412456512451172 elapsed, loss: 1.2051295e-06\n",
      "step: 69520 train: 0.07846760749816895 elapsed, loss: 1.2926746e-06\n",
      "step: 69530 train: 0.08568501472473145 elapsed, loss: 1.2926744e-06\n",
      "step: 69540 train: 0.08414101600646973 elapsed, loss: 1.392326e-06\n",
      "step: 69550 train: 0.07901573181152344 elapsed, loss: 1.5129312e-06\n",
      "step: 69560 train: 0.0785524845123291 elapsed, loss: 1.3755615e-06\n",
      "step: 69570 train: 0.07745027542114258 elapsed, loss: 1.4384264e-06\n",
      "step: 69580 train: 0.07644891738891602 elapsed, loss: 1.4598468e-06\n",
      "step: 69590 train: 0.08527612686157227 elapsed, loss: 1.0924404e-06\n",
      "step: 69600 train: 0.08572840690612793 elapsed, loss: 1.4668317e-06\n",
      "step: 69610 train: 0.08649992942810059 elapsed, loss: 1.0225915e-06\n",
      "step: 69620 train: 0.0846250057220459 elapsed, loss: 1.5408712e-06\n",
      "step: 69630 train: 0.08804774284362793 elapsed, loss: 1.2433143e-06\n",
      "step: 69640 train: 0.08550429344177246 elapsed, loss: 1.1352813e-06\n",
      "step: 69650 train: 0.08206701278686523 elapsed, loss: 0.00058756117\n",
      "step: 69660 train: 0.08152556419372559 elapsed, loss: 0.016413735\n",
      "step: 69670 train: 0.08121657371520996 elapsed, loss: 0.00014656172\n",
      "step: 69680 train: 0.07694363594055176 elapsed, loss: 7.8179044e-05\n",
      "step: 69690 train: 0.08224368095397949 elapsed, loss: 3.157075e-05\n",
      "step: 69700 train: 0.08607745170593262 elapsed, loss: 5.0293187e-05\n",
      "step: 69710 train: 0.088653564453125 elapsed, loss: 2.4642904e-05\n",
      "step: 69720 train: 0.08403205871582031 elapsed, loss: 2.5644844e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 69730 train: 0.0761725902557373 elapsed, loss: 1.7324583e-05\n",
      "step: 69740 train: 0.0825498104095459 elapsed, loss: 7.8430385e-06\n",
      "step: 69750 train: 0.08774304389953613 elapsed, loss: 8.031454e-06\n",
      "step: 69760 train: 0.08560562133789062 elapsed, loss: 7.961267e-06\n",
      "step: 69770 train: 0.0830087661743164 elapsed, loss: 4.691982e-06\n",
      "step: 69780 train: 0.08456778526306152 elapsed, loss: 3.0174767e-06\n",
      "step: 69790 train: 0.07998776435852051 elapsed, loss: 3.13855e-06\n",
      "step: 69800 train: 0.07992124557495117 elapsed, loss: 1.9050175e-06\n",
      "step: 69810 train: 0.08786296844482422 elapsed, loss: 2.4107205e-06\n",
      "step: 69820 train: 0.08056139945983887 elapsed, loss: 2.1383125e-06\n",
      "step: 69830 train: 0.08237361907958984 elapsed, loss: 1.971141e-06\n",
      "step: 69840 train: 0.07946991920471191 elapsed, loss: 1.2684602e-06\n",
      "step: 69850 train: 0.08052444458007812 elapsed, loss: 1.2493679e-06\n",
      "step: 69860 train: 0.08101344108581543 elapsed, loss: 1.1930232e-06\n",
      "step: 69870 train: 0.07527780532836914 elapsed, loss: 1.7210822e-06\n",
      "step: 69880 train: 0.07676458358764648 elapsed, loss: 2.2611516e-06\n",
      "step: 69890 train: 0.08133196830749512 elapsed, loss: 1.8638624e-05\n",
      "step: 69900 train: 0.07624053955078125 elapsed, loss: 3.079876e-06\n",
      "step: 69910 train: 0.08066439628601074 elapsed, loss: 3.1356951e-06\n",
      "step: 69920 train: 0.08279204368591309 elapsed, loss: 1.5054814e-06\n",
      "step: 69930 train: 0.08036470413208008 elapsed, loss: 1.4854508e-06\n",
      "step: 69940 train: 0.08024144172668457 elapsed, loss: 1.104082e-06\n",
      "step: 69950 train: 0.08335351943969727 elapsed, loss: 1.1953514e-06\n",
      "step: 69960 train: 0.08271241188049316 elapsed, loss: 1.6372568e-06\n",
      "step: 69970 train: 0.07882428169250488 elapsed, loss: 2.230048e-06\n",
      "step: 69980 train: 0.08202743530273438 elapsed, loss: 1.5585666e-06\n",
      "step: 69990 train: 0.08876299858093262 elapsed, loss: 1.0016366e-06\n",
      "step: 70000 train: 0.0897207260131836 elapsed, loss: 1.5883658e-06\n",
      "step: 70010 train: 0.08158516883850098 elapsed, loss: 1.5804528e-06\n",
      "step: 70020 train: 0.08394360542297363 elapsed, loss: 1.2861551e-06\n",
      "step: 70030 train: 0.0859992504119873 elapsed, loss: 1.3438973e-06\n",
      "step: 70040 train: 0.0788118839263916 elapsed, loss: 1.4379607e-06\n",
      "step: 70050 train: 0.08126020431518555 elapsed, loss: 3.5630765e-05\n",
      "step: 70060 train: 0.07561445236206055 elapsed, loss: 7.455164e-06\n",
      "step: 70070 train: 0.07651543617248535 elapsed, loss: 4.4279313e-06\n",
      "step: 70080 train: 0.07707738876342773 elapsed, loss: 3.1483287e-06\n",
      "step: 70090 train: 0.0817863941192627 elapsed, loss: 2.1764963e-06\n",
      "step: 70100 train: 0.08214187622070312 elapsed, loss: 1.3499507e-06\n",
      "step: 70110 train: 0.07950568199157715 elapsed, loss: 1.4845266e-06\n",
      "step: 70120 train: 0.0855097770690918 elapsed, loss: 1.1185175e-06\n",
      "step: 70130 train: 0.08011245727539062 elapsed, loss: 1.4482052e-06\n",
      "step: 70140 train: 0.08004260063171387 elapsed, loss: 1.2530934e-06\n",
      "step: 70150 train: 0.08417057991027832 elapsed, loss: 1.4356324e-06\n",
      "step: 70160 train: 0.08170509338378906 elapsed, loss: 1.5916287e-06\n",
      "step: 70170 train: 0.07515597343444824 elapsed, loss: 1.946461e-06\n",
      "step: 70180 train: 0.07909440994262695 elapsed, loss: 2.0004775e-06\n",
      "step: 70190 train: 0.07971835136413574 elapsed, loss: 1.2912776e-06\n",
      "step: 70200 train: 0.08619403839111328 elapsed, loss: 1.2018709e-06\n",
      "step: 70210 train: 0.0767369270324707 elapsed, loss: 2.1243438e-06\n",
      "step: 70220 train: 0.08447122573852539 elapsed, loss: 1.2977966e-06\n",
      "step: 70230 train: 0.08294153213500977 elapsed, loss: 1.3336528e-06\n",
      "step: 70240 train: 0.09168052673339844 elapsed, loss: 1.1930233e-06\n",
      "step: 70250 train: 0.08550667762756348 elapsed, loss: 2.0847624e-06\n",
      "step: 70260 train: 0.0809926986694336 elapsed, loss: 1.7615946e-06\n",
      "step: 70270 train: 0.08861708641052246 elapsed, loss: 1.0100187e-06\n",
      "step: 70280 train: 0.08708763122558594 elapsed, loss: 1.2931403e-06\n",
      "step: 70290 train: 0.08336091041564941 elapsed, loss: 1.6260873e-06\n",
      "step: 70300 train: 0.07746458053588867 elapsed, loss: 1.8044352e-06\n",
      "step: 70310 train: 0.08474922180175781 elapsed, loss: 1.744831e-06\n",
      "step: 70320 train: 0.07511734962463379 elapsed, loss: 2.6696307e-06\n",
      "step: 70330 train: 0.08311820030212402 elapsed, loss: 1.8156113e-06\n",
      "step: 70340 train: 0.08114886283874512 elapsed, loss: 2.0661346e-06\n",
      "step: 70350 train: 0.08072996139526367 elapsed, loss: 2.4344733e-06\n",
      "step: 70360 train: 0.07887506484985352 elapsed, loss: 9.1178554e-05\n",
      "step: 70370 train: 0.08414077758789062 elapsed, loss: 0.0003332488\n",
      "step: 70380 train: 0.07889032363891602 elapsed, loss: 3.7345006e-05\n",
      "step: 70390 train: 0.08228778839111328 elapsed, loss: 2.4220033e-05\n",
      "step: 70400 train: 0.08661890029907227 elapsed, loss: 1.2056663e-05\n",
      "step: 70410 train: 0.08223080635070801 elapsed, loss: 6.489414e-06\n",
      "step: 70420 train: 0.08459186553955078 elapsed, loss: 7.5529765e-06\n",
      "step: 70430 train: 0.08277416229248047 elapsed, loss: 5.959501e-06\n",
      "step: 70440 train: 0.07687950134277344 elapsed, loss: 6.7655487e-06\n",
      "step: 70450 train: 0.07809209823608398 elapsed, loss: 4.7269114e-06\n",
      "step: 70460 train: 0.07865762710571289 elapsed, loss: 3.585581e-06\n",
      "step: 70470 train: 0.07983922958374023 elapsed, loss: 3.694081e-06\n",
      "step: 70480 train: 0.07920384407043457 elapsed, loss: 2.5350541e-06\n",
      "step: 70490 train: 0.08361291885375977 elapsed, loss: 1.5543753e-06\n",
      "step: 70500 train: 0.08050012588500977 elapsed, loss: 2.0586858e-06\n",
      "step: 70510 train: 0.0884544849395752 elapsed, loss: 1.4007078e-06\n",
      "step: 70520 train: 0.08779525756835938 elapsed, loss: 1.0374927e-06\n",
      "step: 70530 train: 0.07851076126098633 elapsed, loss: 1.7415709e-06\n",
      "step: 70540 train: 0.0818171501159668 elapsed, loss: 1.3080414e-06\n",
      "step: 70550 train: 0.09037089347839355 elapsed, loss: 1.1841723e-06\n",
      "step: 70560 train: 0.08315634727478027 elapsed, loss: 1.0128126e-06\n",
      "step: 70570 train: 0.08147931098937988 elapsed, loss: 1.2493681e-06\n",
      "step: 70580 train: 0.08762574195861816 elapsed, loss: 2.2575223e-06\n",
      "step: 70590 train: 0.0809028148651123 elapsed, loss: 1.3513472e-06\n",
      "step: 70600 train: 0.08397865295410156 elapsed, loss: 1.192092e-06\n",
      "step: 70610 train: 0.07811784744262695 elapsed, loss: 1.5455283e-06\n",
      "step: 70620 train: 0.08032989501953125 elapsed, loss: 1.2139781e-06\n",
      "step: 70630 train: 0.08096480369567871 elapsed, loss: 1.5539098e-06\n",
      "step: 70640 train: 0.0756375789642334 elapsed, loss: 1.5865063e-06\n",
      "step: 70650 train: 0.08426713943481445 elapsed, loss: 1.0207285e-06\n",
      "step: 70660 train: 0.08211231231689453 elapsed, loss: 1.1781215e-06\n",
      "step: 70670 train: 0.07896947860717773 elapsed, loss: 1.3927917e-06\n",
      "step: 70680 train: 0.08237147331237793 elapsed, loss: 1.08499e-06\n",
      "step: 70690 train: 0.07946896553039551 elapsed, loss: 1.8686959e-06\n",
      "step: 70700 train: 0.08364224433898926 elapsed, loss: 1.6549582e-06\n",
      "step: 70710 train: 0.07779312133789062 elapsed, loss: 1.5399404e-06\n",
      "step: 70720 train: 0.08217430114746094 elapsed, loss: 1.5506503e-06\n",
      "step: 70730 train: 0.08139586448669434 elapsed, loss: 1.4505335e-06\n",
      "step: 70740 train: 0.08559727668762207 elapsed, loss: 1.8663678e-06\n",
      "step: 70750 train: 0.07771563529968262 elapsed, loss: 1.618637e-06\n",
      "step: 70760 train: 0.07835555076599121 elapsed, loss: 1.3294616e-06\n",
      "step: 70770 train: 0.08088898658752441 elapsed, loss: 1.8975666e-06\n",
      "step: 70780 train: 0.08558320999145508 elapsed, loss: 1.2991939e-06\n",
      "step: 70790 train: 0.08401203155517578 elapsed, loss: 3.8887233e-06\n",
      "step: 70800 train: 0.08486247062683105 elapsed, loss: 1.8714898e-06\n",
      "step: 70810 train: 0.0828094482421875 elapsed, loss: 1.4458769e-06\n",
      "step: 70820 train: 0.0776216983795166 elapsed, loss: 2.4079309e-06\n",
      "step: 70830 train: 0.08102083206176758 elapsed, loss: 2.1182896e-06\n",
      "step: 70840 train: 0.0792090892791748 elapsed, loss: 1.0391908e-05\n",
      "step: 70850 train: 0.07751154899597168 elapsed, loss: 6.022351e-06\n",
      "step: 70860 train: 0.07730937004089355 elapsed, loss: 3.2801104e-06\n",
      "step: 70870 train: 0.08337640762329102 elapsed, loss: 2.0600828e-06\n",
      "step: 70880 train: 0.08203959465026855 elapsed, loss: 2.5108416e-06\n",
      "step: 70890 train: 0.08144235610961914 elapsed, loss: 1.7727702e-06\n",
      "step: 70900 train: 0.08101272583007812 elapsed, loss: 1.8659026e-06\n",
      "step: 70910 train: 0.08642768859863281 elapsed, loss: 1.5604294e-06\n",
      "step: 70920 train: 0.08534479141235352 elapsed, loss: 1.6894172e-06\n",
      "step: 70930 train: 0.08073735237121582 elapsed, loss: 2.485695e-06\n",
      "step: 70940 train: 0.08526492118835449 elapsed, loss: 1.3667146e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 70950 train: 0.08091259002685547 elapsed, loss: 1.9245756e-06\n",
      "step: 70960 train: 0.08325552940368652 elapsed, loss: 1.3094377e-06\n",
      "step: 70970 train: 0.07312273979187012 elapsed, loss: 0.0002986179\n",
      "step: 70980 train: 0.08303403854370117 elapsed, loss: 9.795536e-05\n",
      "step: 70990 train: 0.0836646556854248 elapsed, loss: 3.6099605e-05\n",
      "step: 71000 train: 0.0906672477722168 elapsed, loss: 3.3771717e-05\n",
      "step: 71010 train: 0.08033347129821777 elapsed, loss: 2.2997945e-05\n",
      "step: 71020 train: 0.08121347427368164 elapsed, loss: 1.8713305e-05\n",
      "step: 71030 train: 0.07947421073913574 elapsed, loss: 4.7471076e-05\n",
      "step: 71040 train: 0.08747529983520508 elapsed, loss: 6.547156e-06\n",
      "step: 71050 train: 0.08085942268371582 elapsed, loss: 6.5187414e-06\n",
      "step: 71060 train: 0.08356189727783203 elapsed, loss: 4.8386664e-06\n",
      "step: 71070 train: 0.08449745178222656 elapsed, loss: 2.7771976e-06\n",
      "step: 71080 train: 0.0823049545288086 elapsed, loss: 2.8018776e-06\n",
      "step: 71090 train: 0.08563923835754395 elapsed, loss: 3.0877895e-06\n",
      "step: 71100 train: 0.08886885643005371 elapsed, loss: 2.3916323e-06\n",
      "step: 71110 train: 0.08249449729919434 elapsed, loss: 2.0456466e-06\n",
      "step: 71120 train: 0.08057856559753418 elapsed, loss: 1.3648519e-06\n",
      "step: 71130 train: 0.08278822898864746 elapsed, loss: 1.3262022e-06\n",
      "step: 71140 train: 0.08272838592529297 elapsed, loss: 9.397037e-07\n",
      "step: 71150 train: 0.0804293155670166 elapsed, loss: 1.4686943e-06\n",
      "step: 71160 train: 0.08344578742980957 elapsed, loss: 1.016072e-06\n",
      "step: 71170 train: 0.0901339054107666 elapsed, loss: 1.0705542e-06\n",
      "step: 71180 train: 0.08252978324890137 elapsed, loss: 1.2903462e-06\n",
      "step: 71190 train: 0.07712745666503906 elapsed, loss: 1.5408718e-06\n",
      "step: 71200 train: 0.07792091369628906 elapsed, loss: 1.9846457e-06\n",
      "step: 71210 train: 0.0773017406463623 elapsed, loss: 1.2172377e-06\n",
      "step: 71220 train: 0.07643556594848633 elapsed, loss: 1.486389e-06\n",
      "step: 71230 train: 0.08780670166015625 elapsed, loss: 1.1860382e-06\n",
      "step: 71240 train: 0.07663679122924805 elapsed, loss: 1.7192197e-06\n",
      "step: 71250 train: 0.07867622375488281 elapsed, loss: 1.3704321e-06\n",
      "step: 71260 train: 0.08882999420166016 elapsed, loss: 1.1008223e-06\n",
      "step: 71270 train: 0.08228063583374023 elapsed, loss: 1.6093231e-06\n",
      "step: 71280 train: 0.08283424377441406 elapsed, loss: 1.0360957e-06\n",
      "step: 71290 train: 0.08017849922180176 elapsed, loss: 1.145526e-06\n",
      "step: 71300 train: 0.081878662109375 elapsed, loss: 1.1376096e-06\n",
      "step: 71310 train: 0.08045101165771484 elapsed, loss: 1.2684603e-06\n",
      "step: 71320 train: 0.08206701278686523 elapsed, loss: 1.2726514e-06\n",
      "step: 71330 train: 0.08463215827941895 elapsed, loss: 2.0647394e-06\n",
      "step: 71340 train: 0.07946491241455078 elapsed, loss: 1.8635735e-06\n",
      "step: 71350 train: 0.0880577564239502 elapsed, loss: 1.2204973e-06\n",
      "step: 71360 train: 0.07962465286254883 elapsed, loss: 1.7187538e-06\n",
      "step: 71370 train: 0.0814056396484375 elapsed, loss: 1.8174726e-06\n",
      "step: 71380 train: 0.08088326454162598 elapsed, loss: 2.095473e-06\n",
      "step: 71390 train: 0.08328986167907715 elapsed, loss: 0.02411245\n",
      "step: 71400 train: 0.0823369026184082 elapsed, loss: 0.00015226236\n",
      "step: 71410 train: 0.08361959457397461 elapsed, loss: 0.0009864739\n",
      "step: 71420 train: 0.08730721473693848 elapsed, loss: 2.9067949e-05\n",
      "step: 71430 train: 0.07821059226989746 elapsed, loss: 4.6163776e-05\n",
      "step: 71440 train: 0.08335185050964355 elapsed, loss: 3.6941536e-05\n",
      "step: 71450 train: 0.08493566513061523 elapsed, loss: 1.8243572e-05\n",
      "step: 71460 train: 0.07596445083618164 elapsed, loss: 2.5613354e-05\n",
      "step: 71470 train: 0.07615041732788086 elapsed, loss: 2.056853e-05\n",
      "step: 71480 train: 0.08236169815063477 elapsed, loss: 8.040974e-06\n",
      "step: 71490 train: 0.08243632316589355 elapsed, loss: 6.5080226e-06\n",
      "step: 71500 train: 0.0821692943572998 elapsed, loss: 6.031681e-06\n",
      "step: 71510 train: 0.08270907402038574 elapsed, loss: 3.3080482e-06\n",
      "step: 71520 train: 0.07672286033630371 elapsed, loss: 4.5699853e-06\n",
      "step: 71530 train: 0.07852959632873535 elapsed, loss: 3.7699663e-06\n",
      "step: 71540 train: 0.07718563079833984 elapsed, loss: 3.6125903e-06\n",
      "step: 71550 train: 0.08538603782653809 elapsed, loss: 1.8021067e-06\n",
      "step: 71560 train: 0.08383607864379883 elapsed, loss: 2.0056e-06\n",
      "step: 71570 train: 0.08059430122375488 elapsed, loss: 1.7825492e-06\n",
      "step: 71580 train: 0.08364343643188477 elapsed, loss: 9.726309e-05\n",
      "step: 71590 train: 0.0841822624206543 elapsed, loss: 7.04202e-06\n",
      "step: 71600 train: 0.0761570930480957 elapsed, loss: 6.1796145e-06\n",
      "step: 71610 train: 0.08131027221679688 elapsed, loss: 2.776731e-06\n",
      "step: 71620 train: 0.07580733299255371 elapsed, loss: 3.5376186e-06\n",
      "step: 71630 train: 0.0899815559387207 elapsed, loss: 1.7136306e-06\n",
      "step: 71640 train: 0.07677507400512695 elapsed, loss: 2.0139798e-06\n",
      "step: 71650 train: 0.08551716804504395 elapsed, loss: 1.5371367e-06\n",
      "step: 71660 train: 0.0837545394897461 elapsed, loss: 1.2931403e-06\n",
      "step: 71670 train: 0.08737683296203613 elapsed, loss: 9.676434e-07\n",
      "step: 71680 train: 0.07576942443847656 elapsed, loss: 1.2745136e-06\n",
      "step: 71690 train: 0.08832502365112305 elapsed, loss: 8.4843424e-07\n",
      "step: 71700 train: 0.08285164833068848 elapsed, loss: 1.0309733e-06\n",
      "step: 71710 train: 0.08385038375854492 elapsed, loss: 1.3234078e-06\n",
      "step: 71720 train: 0.08463239669799805 elapsed, loss: 8.796336e-07\n",
      "step: 71730 train: 0.07785749435424805 elapsed, loss: 1.5031524e-06\n",
      "step: 71740 train: 0.08089828491210938 elapsed, loss: 1.1459904e-06\n",
      "step: 71750 train: 0.0769033432006836 elapsed, loss: 1.3816139e-06\n",
      "step: 71760 train: 0.082275390625 elapsed, loss: 1.2060616e-06\n",
      "step: 71770 train: 0.07456684112548828 elapsed, loss: 1.4426171e-06\n",
      "step: 71780 train: 0.0813138484954834 elapsed, loss: 1.213978e-06\n",
      "step: 71790 train: 0.07725286483764648 elapsed, loss: 1.4780073e-06\n",
      "step: 71800 train: 0.0866396427154541 elapsed, loss: 1.1110669e-06\n",
      "step: 71810 train: 0.08370018005371094 elapsed, loss: 1.1757938e-06\n",
      "step: 71820 train: 0.08442449569702148 elapsed, loss: 1.2386579e-06\n",
      "step: 71830 train: 0.08381342887878418 elapsed, loss: 1.1548391e-06\n",
      "step: 71840 train: 0.0836482048034668 elapsed, loss: 0.00024915958\n",
      "step: 71850 train: 0.08463597297668457 elapsed, loss: 5.486658e-05\n",
      "step: 71860 train: 0.07750391960144043 elapsed, loss: 3.1833995e-05\n",
      "step: 71870 train: 0.093780517578125 elapsed, loss: 1.7022478e-05\n",
      "step: 71880 train: 0.07831835746765137 elapsed, loss: 2.2291448e-05\n",
      "step: 71890 train: 0.08223986625671387 elapsed, loss: 1.3188251e-05\n",
      "step: 71900 train: 0.08135414123535156 elapsed, loss: 9.393709e-06\n",
      "step: 71910 train: 0.079986572265625 elapsed, loss: 7.322007e-06\n",
      "step: 71920 train: 0.07657527923583984 elapsed, loss: 6.8754457e-06\n",
      "step: 71930 train: 0.08429837226867676 elapsed, loss: 4.4102394e-06\n",
      "step: 71940 train: 0.0777897834777832 elapsed, loss: 4.270095e-06\n",
      "step: 71950 train: 0.0850362777709961 elapsed, loss: 2.8666022e-06\n",
      "step: 71960 train: 0.08740067481994629 elapsed, loss: 1.9734694e-06\n",
      "step: 71970 train: 0.07924175262451172 elapsed, loss: 2.4079304e-06\n",
      "step: 71980 train: 0.08695363998413086 elapsed, loss: 1.7844106e-06\n",
      "step: 71990 train: 0.08168387413024902 elapsed, loss: 1.4854579e-06\n",
      "step: 72000 train: 0.08397841453552246 elapsed, loss: 1.7830148e-06\n",
      "step: 72010 train: 0.08418440818786621 elapsed, loss: 2.2016416e-06\n",
      "step: 72020 train: 0.07942748069763184 elapsed, loss: 1.1334187e-06\n",
      "step: 72030 train: 0.08530521392822266 elapsed, loss: 1.2232912e-06\n",
      "step: 72040 train: 0.08164215087890625 elapsed, loss: 1.5008247e-06\n",
      "step: 72050 train: 0.08133697509765625 elapsed, loss: 1.0686917e-06\n",
      "step: 72060 train: 0.08161211013793945 elapsed, loss: 1.1641523e-06\n",
      "step: 72070 train: 0.07673764228820801 elapsed, loss: 1.9664847e-06\n",
      "step: 72080 train: 0.0867457389831543 elapsed, loss: 1.0039647e-06\n",
      "step: 72090 train: 0.07727193832397461 elapsed, loss: 1.4337697e-06\n",
      "step: 72100 train: 0.07566356658935547 elapsed, loss: 1.4845268e-06\n",
      "step: 72110 train: 0.0794515609741211 elapsed, loss: 1.4146774e-06\n",
      "step: 72120 train: 0.0814659595489502 elapsed, loss: 1.0733486e-06\n",
      "step: 72130 train: 0.08211612701416016 elapsed, loss: 1.3071101e-06\n",
      "step: 72140 train: 0.08306145668029785 elapsed, loss: 1.4342354e-06\n",
      "step: 72150 train: 0.07943010330200195 elapsed, loss: 1.7778923e-06\n",
      "step: 72160 train: 0.08182549476623535 elapsed, loss: 0.015033921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 72170 train: 0.0835728645324707 elapsed, loss: 3.5184476e-05\n",
      "step: 72180 train: 0.07747411727905273 elapsed, loss: 1.8224531e-05\n",
      "step: 72190 train: 0.0839993953704834 elapsed, loss: 1.850967e-05\n",
      "step: 72200 train: 0.0845334529876709 elapsed, loss: 7.167384e-06\n",
      "step: 72210 train: 0.08372044563293457 elapsed, loss: 6.2086156e-06\n",
      "step: 72220 train: 0.08046078681945801 elapsed, loss: 7.038797e-06\n",
      "step: 72230 train: 0.07785296440124512 elapsed, loss: 6.370208e-06\n",
      "step: 72240 train: 0.08276176452636719 elapsed, loss: 3.50828e-06\n",
      "step: 72250 train: 0.08570528030395508 elapsed, loss: 4.1304e-06\n",
      "step: 72260 train: 0.08105134963989258 elapsed, loss: 3.0849978e-06\n",
      "step: 72270 train: 0.07867884635925293 elapsed, loss: 3.199551e-06\n",
      "step: 72280 train: 0.08496856689453125 elapsed, loss: 2.2645058e-06\n",
      "step: 72290 train: 0.07880377769470215 elapsed, loss: 2.1639246e-06\n",
      "step: 72300 train: 0.08773159980773926 elapsed, loss: 1.3844066e-06\n",
      "step: 72310 train: 0.07833313941955566 elapsed, loss: 1.5171195e-06\n",
      "step: 72320 train: 0.0850067138671875 elapsed, loss: 1.056119e-06\n",
      "step: 72330 train: 0.08026909828186035 elapsed, loss: 1.3285304e-06\n",
      "step: 72340 train: 0.07859635353088379 elapsed, loss: 1.5301614e-06\n",
      "step: 72350 train: 0.08400869369506836 elapsed, loss: 1.205596e-06\n",
      "step: 72360 train: 0.07913351058959961 elapsed, loss: 3.1306308e-06\n",
      "step: 72370 train: 0.09306120872497559 elapsed, loss: 1.0090873e-06\n",
      "step: 72380 train: 0.07649493217468262 elapsed, loss: 2.407462e-06\n",
      "step: 72390 train: 0.08654999732971191 elapsed, loss: 1.125502e-06\n",
      "step: 72400 train: 0.08075571060180664 elapsed, loss: 1.8640351e-06\n",
      "step: 72410 train: 0.08013701438903809 elapsed, loss: 1.1958172e-06\n",
      "step: 72420 train: 0.08223628997802734 elapsed, loss: 1.2973308e-06\n",
      "step: 72430 train: 0.08222579956054688 elapsed, loss: 1.3322558e-06\n",
      "step: 72440 train: 0.07833170890808105 elapsed, loss: 1.4039674e-06\n",
      "step: 72450 train: 0.07974910736083984 elapsed, loss: 1.5324897e-06\n",
      "step: 72460 train: 0.07842206954956055 elapsed, loss: 1.8542609e-06\n",
      "step: 72470 train: 0.07719588279724121 elapsed, loss: 2.144367e-06\n",
      "step: 72480 train: 0.07766914367675781 elapsed, loss: 1.6712565e-06\n",
      "step: 72490 train: 0.08443784713745117 elapsed, loss: 1.5473909e-06\n",
      "step: 72500 train: 0.09019613265991211 elapsed, loss: 1.0887154e-06\n",
      "step: 72510 train: 0.09069561958312988 elapsed, loss: 1.2121152e-06\n",
      "step: 72520 train: 0.07970690727233887 elapsed, loss: 1.5394745e-06\n",
      "step: 72530 train: 0.08064413070678711 elapsed, loss: 3.3173562e-06\n",
      "step: 72540 train: 0.07613229751586914 elapsed, loss: 1.885926e-06\n",
      "step: 72550 train: 0.0783989429473877 elapsed, loss: 1.9455304e-06\n",
      "step: 72560 train: 0.08158755302429199 elapsed, loss: 1.5748649e-06\n",
      "step: 72570 train: 0.08180832862854004 elapsed, loss: 1.5096724e-06\n",
      "step: 72580 train: 0.08007240295410156 elapsed, loss: 2.0042035e-06\n",
      "step: 72590 train: 0.08351778984069824 elapsed, loss: 1.6405229e-06\n",
      "step: 72600 train: 0.08907556533813477 elapsed, loss: 1.3029188e-06\n",
      "step: 72610 train: 0.07619762420654297 elapsed, loss: 1.965088e-06\n",
      "step: 72620 train: 0.07741832733154297 elapsed, loss: 3.980655e-06\n",
      "step: 72630 train: 0.07971596717834473 elapsed, loss: 1.8086264e-06\n",
      "step: 72640 train: 0.07787418365478516 elapsed, loss: 1.9683478e-06\n",
      "step: 72650 train: 0.09056591987609863 elapsed, loss: 1.4728851e-06\n",
      "step: 72660 train: 0.08656883239746094 elapsed, loss: 1.5185199e-06\n",
      "step: 72670 train: 0.0859375 elapsed, loss: 1.9064148e-06\n",
      "step: 72680 train: 0.0870823860168457 elapsed, loss: 1.3383093e-06\n",
      "step: 72690 train: 0.08257079124450684 elapsed, loss: 1.8356343e-06\n",
      "step: 72700 train: 0.08360791206359863 elapsed, loss: 1.4859238e-06\n",
      "step: 72710 train: 0.08045721054077148 elapsed, loss: 1.5729931e-06\n",
      "step: 72720 train: 0.07789754867553711 elapsed, loss: 2.868002e-06\n",
      "step: 72730 train: 0.08479928970336914 elapsed, loss: 1.9413392e-06\n",
      "step: 72740 train: 0.08066916465759277 elapsed, loss: 1.6051326e-06\n",
      "step: 72750 train: 0.07819008827209473 elapsed, loss: 2.6230646e-06\n",
      "step: 72760 train: 0.08437275886535645 elapsed, loss: 1.5357494e-06\n",
      "step: 72770 train: 0.07909321784973145 elapsed, loss: 2.6104926e-06\n",
      "step: 72780 train: 0.0815744400024414 elapsed, loss: 2.3413418e-06\n",
      "step: 72790 train: 0.08621430397033691 elapsed, loss: 3.4812733e-06\n",
      "step: 72800 train: 0.08615922927856445 elapsed, loss: 2.333424e-06\n",
      "step: 72810 train: 0.07607436180114746 elapsed, loss: 2.4121218e-06\n",
      "step: 72820 train: 0.09595251083374023 elapsed, loss: 1.2447117e-06\n",
      "step: 72830 train: 0.0913543701171875 elapsed, loss: 1.4905802e-06\n",
      "step: 72840 train: 0.08585166931152344 elapsed, loss: 2.2482095e-06\n",
      "step: 72850 train: 0.0883781909942627 elapsed, loss: 2.302226e-06\n",
      "step: 72860 train: 0.07488536834716797 elapsed, loss: 2.7087403e-06\n",
      "step: 72870 train: 0.0829310417175293 elapsed, loss: 1.4915117e-06\n",
      "step: 72880 train: 0.07987236976623535 elapsed, loss: 1.991631e-06\n",
      "step: 72890 train: 0.07998085021972656 elapsed, loss: 3.5548487e-06\n",
      "step: 72900 train: 0.08304095268249512 elapsed, loss: 2.4596188e-06\n",
      "step: 72910 train: 0.08741545677185059 elapsed, loss: 1.6670648e-06\n",
      "step: 72920 train: 0.08256053924560547 elapsed, loss: 2.9196908e-06\n",
      "step: 72930 train: 0.08794784545898438 elapsed, loss: 0.005752568\n",
      "step: 72940 train: 0.08160114288330078 elapsed, loss: 0.00010556448\n",
      "step: 72950 train: 0.07961773872375488 elapsed, loss: 4.0005605e-05\n",
      "step: 72960 train: 0.08655047416687012 elapsed, loss: 2.592814e-05\n",
      "step: 72970 train: 0.08266401290893555 elapsed, loss: 0.00036447332\n",
      "step: 72980 train: 0.07854652404785156 elapsed, loss: 8.984219e-05\n",
      "step: 72990 train: 0.0801231861114502 elapsed, loss: 3.7293365e-05\n",
      "step: 73000 train: 0.08028960227966309 elapsed, loss: 4.0864703e-05\n",
      "step: 73010 train: 0.07719540596008301 elapsed, loss: 1.9279187e-05\n",
      "step: 73020 train: 0.07935118675231934 elapsed, loss: 1.7955143e-05\n",
      "step: 73030 train: 0.0810995101928711 elapsed, loss: 9.6771355e-06\n",
      "step: 73040 train: 0.07947444915771484 elapsed, loss: 1.09308e-05\n",
      "step: 73050 train: 0.08845925331115723 elapsed, loss: 5.4170096e-06\n",
      "step: 73060 train: 0.07871174812316895 elapsed, loss: 5.4584425e-06\n",
      "step: 73070 train: 0.08443140983581543 elapsed, loss: 3.2204894e-06\n",
      "step: 73080 train: 0.08170652389526367 elapsed, loss: 3.4621821e-06\n",
      "step: 73090 train: 0.08124470710754395 elapsed, loss: 1.6745159e-06\n",
      "step: 73100 train: 0.08499407768249512 elapsed, loss: 1.8314429e-06\n",
      "step: 73110 train: 0.07840919494628906 elapsed, loss: 2.0018751e-06\n",
      "step: 73120 train: 0.08104133605957031 elapsed, loss: 1.8062873e-06\n",
      "step: 73130 train: 0.08812856674194336 elapsed, loss: 1.179519e-06\n",
      "step: 73140 train: 0.08115267753601074 elapsed, loss: 1.3229426e-06\n",
      "step: 73150 train: 0.08569908142089844 elapsed, loss: 1.099891e-06\n",
      "step: 73160 train: 0.07787322998046875 elapsed, loss: 1.5734678e-06\n",
      "step: 73170 train: 0.0801992416381836 elapsed, loss: 1.9948895e-06\n",
      "step: 73180 train: 0.08632349967956543 elapsed, loss: 1.2395838e-06\n",
      "step: 73190 train: 0.08491373062133789 elapsed, loss: 1.1278306e-06\n",
      "step: 73200 train: 0.08226704597473145 elapsed, loss: 1.3560044e-06\n",
      "step: 73210 train: 0.07466912269592285 elapsed, loss: 1.7345862e-06\n",
      "step: 73220 train: 0.07775473594665527 elapsed, loss: 1.8458786e-06\n",
      "step: 73230 train: 0.07597231864929199 elapsed, loss: 4.3326672e-06\n",
      "step: 73240 train: 0.08148765563964844 elapsed, loss: 0.0003214142\n",
      "step: 73250 train: 0.08481907844543457 elapsed, loss: 3.1719726e-05\n",
      "step: 73260 train: 0.08147835731506348 elapsed, loss: 3.525136e-05\n",
      "step: 73270 train: 0.08623003959655762 elapsed, loss: 1.4694535e-05\n",
      "step: 73280 train: 0.0806879997253418 elapsed, loss: 1.93343e-05\n",
      "step: 73290 train: 0.08165836334228516 elapsed, loss: 1.1942064e-05\n",
      "step: 73300 train: 0.0826570987701416 elapsed, loss: 1.3105224e-05\n",
      "step: 73310 train: 0.08227038383483887 elapsed, loss: 6.871712e-06\n",
      "step: 73320 train: 0.07892465591430664 elapsed, loss: 7.2214198e-06\n",
      "step: 73330 train: 0.08212399482727051 elapsed, loss: 3.888258e-06\n",
      "step: 73340 train: 0.08033871650695801 elapsed, loss: 2.4278585e-05\n",
      "step: 73350 train: 0.0844419002532959 elapsed, loss: 6.7956207e-06\n",
      "step: 73360 train: 0.08153128623962402 elapsed, loss: 3.7290033e-06\n",
      "step: 73370 train: 0.08571720123291016 elapsed, loss: 2.6379598e-06\n",
      "step: 73380 train: 0.0747537612915039 elapsed, loss: 2.6915152e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 73390 train: 0.08697128295898438 elapsed, loss: 1.3834783e-06\n",
      "step: 73400 train: 0.07947921752929688 elapsed, loss: 1.4887158e-06\n",
      "step: 73410 train: 0.0798349380493164 elapsed, loss: 1.6926768e-06\n",
      "step: 73420 train: 0.08593130111694336 elapsed, loss: 1.3518134e-06\n",
      "step: 73430 train: 0.08484864234924316 elapsed, loss: 1.1473884e-06\n",
      "step: 73440 train: 0.08648109436035156 elapsed, loss: 9.2480275e-07\n",
      "step: 73450 train: 0.0785531997680664 elapsed, loss: 1.0938375e-06\n",
      "step: 73460 train: 0.08092474937438965 elapsed, loss: 1.1343499e-06\n",
      "step: 73470 train: 0.08272004127502441 elapsed, loss: 8.3912096e-07\n",
      "step: 73480 train: 0.0770263671875 elapsed, loss: 1.3057131e-06\n",
      "step: 73490 train: 0.07888031005859375 elapsed, loss: 1.2251538e-06\n",
      "step: 73500 train: 0.07696390151977539 elapsed, loss: 1.3052475e-06\n",
      "step: 73510 train: 0.07331967353820801 elapsed, loss: 1.5157259e-06\n",
      "step: 73520 train: 0.07866120338439941 elapsed, loss: 1.194886e-06\n",
      "step: 73530 train: 0.08387041091918945 elapsed, loss: 1.3192173e-06\n",
      "step: 73540 train: 0.08070015907287598 elapsed, loss: 1.3466913e-06\n",
      "step: 73550 train: 0.08515810966491699 elapsed, loss: 8.926721e-07\n",
      "step: 73560 train: 0.08369898796081543 elapsed, loss: 9.27131e-07\n",
      "step: 73570 train: 0.07747507095336914 elapsed, loss: 2.081502e-06\n",
      "step: 73580 train: 0.08331584930419922 elapsed, loss: 1.2214286e-06\n",
      "step: 73590 train: 0.09018325805664062 elapsed, loss: 2.1881383e-06\n",
      "step: 73600 train: 0.07907485961914062 elapsed, loss: 1.6693937e-06\n",
      "step: 73610 train: 0.08196187019348145 elapsed, loss: 1.0505312e-06\n",
      "step: 73620 train: 0.08307933807373047 elapsed, loss: 2.0874932e-06\n",
      "step: 73630 train: 0.0845639705657959 elapsed, loss: 5.333496e-05\n",
      "step: 73640 train: 0.08034729957580566 elapsed, loss: 2.7756654e-05\n",
      "step: 73650 train: 0.07898139953613281 elapsed, loss: 1.4480326e-05\n",
      "step: 73660 train: 0.07940340042114258 elapsed, loss: 1.1205419e-05\n",
      "step: 73670 train: 0.07846593856811523 elapsed, loss: 2.0597598e-05\n",
      "step: 73680 train: 0.07784318923950195 elapsed, loss: 1.8578467e-05\n",
      "step: 73690 train: 0.08451271057128906 elapsed, loss: 5.118962e-06\n",
      "step: 73700 train: 0.0810699462890625 elapsed, loss: 4.349207e-06\n",
      "step: 73710 train: 0.07727456092834473 elapsed, loss: 3.2074665e-06\n",
      "step: 73720 train: 0.08956146240234375 elapsed, loss: 2.330624e-06\n",
      "step: 73730 train: 0.07958078384399414 elapsed, loss: 3.110606e-06\n",
      "step: 73740 train: 0.07538557052612305 elapsed, loss: 4.5681027e-06\n",
      "step: 73750 train: 0.08239388465881348 elapsed, loss: 2.5601998e-06\n",
      "step: 73760 train: 0.08190035820007324 elapsed, loss: 1.2577495e-06\n",
      "step: 73770 train: 0.07874608039855957 elapsed, loss: 1.3434314e-06\n",
      "step: 73780 train: 0.08224225044250488 elapsed, loss: 1.2861553e-06\n",
      "step: 73790 train: 0.07744383811950684 elapsed, loss: 1.3280649e-06\n",
      "step: 73800 train: 0.08253073692321777 elapsed, loss: 9.681091e-07\n",
      "step: 73810 train: 0.08388137817382812 elapsed, loss: 1.5837102e-06\n",
      "step: 73820 train: 0.08305525779724121 elapsed, loss: 1.2591471e-06\n",
      "step: 73830 train: 0.08081245422363281 elapsed, loss: 1.1827788e-06\n",
      "step: 73840 train: 0.08709406852722168 elapsed, loss: 1.202802e-06\n",
      "step: 73850 train: 0.07658624649047852 elapsed, loss: 1.7308608e-06\n",
      "step: 73860 train: 0.08119344711303711 elapsed, loss: 1.2107184e-06\n",
      "step: 73870 train: 0.08669090270996094 elapsed, loss: 9.322501e-06\n",
      "step: 73880 train: 0.08577561378479004 elapsed, loss: 2.370677e-06\n",
      "step: 73890 train: 0.07973623275756836 elapsed, loss: 2.4912833e-06\n",
      "step: 73900 train: 0.08180952072143555 elapsed, loss: 1.5776588e-06\n",
      "step: 73910 train: 0.07805705070495605 elapsed, loss: 1.8137484e-06\n",
      "step: 73920 train: 0.08104634284973145 elapsed, loss: 1.1976799e-06\n",
      "step: 73930 train: 0.08523416519165039 elapsed, loss: 1.2046648e-06\n",
      "step: 73940 train: 0.08252596855163574 elapsed, loss: 1.5431991e-06\n",
      "step: 73950 train: 0.08468961715698242 elapsed, loss: 1.0351642e-06\n",
      "step: 73960 train: 0.07726526260375977 elapsed, loss: 1.695005e-06\n",
      "step: 73970 train: 0.08741331100463867 elapsed, loss: 3.1574677e-05\n",
      "step: 73980 train: 0.08611845970153809 elapsed, loss: 0.0002034214\n",
      "step: 73990 train: 0.07838225364685059 elapsed, loss: 9.591467e-05\n",
      "step: 74000 train: 0.08306002616882324 elapsed, loss: 2.702824e-05\n",
      "step: 74010 train: 0.08186459541320801 elapsed, loss: 2.471884e-05\n",
      "step: 74020 train: 0.08422732353210449 elapsed, loss: 6.529701e-05\n",
      "step: 74030 train: 0.08729958534240723 elapsed, loss: 1.216475e-05\n",
      "step: 74040 train: 0.08021044731140137 elapsed, loss: 1.3170733e-05\n",
      "step: 74050 train: 0.08114790916442871 elapsed, loss: 7.397396e-06\n",
      "step: 74060 train: 0.08049392700195312 elapsed, loss: 7.2489056e-06\n",
      "step: 74070 train: 0.08502078056335449 elapsed, loss: 4.074522e-06\n",
      "step: 74080 train: 0.08234095573425293 elapsed, loss: 4.8600864e-06\n",
      "step: 74090 train: 0.08870887756347656 elapsed, loss: 2.3627608e-06\n",
      "step: 74100 train: 0.07516288757324219 elapsed, loss: 3.5213197e-06\n",
      "step: 74110 train: 0.0823516845703125 elapsed, loss: 2.090816e-06\n",
      "step: 74120 train: 0.08084750175476074 elapsed, loss: 1.7592656e-06\n",
      "step: 74130 train: 0.08148741722106934 elapsed, loss: 1.9287606e-06\n",
      "step: 74140 train: 0.07628202438354492 elapsed, loss: 1.654958e-06\n",
      "step: 74150 train: 0.07726526260375977 elapsed, loss: 1.8458791e-06\n",
      "step: 74160 train: 0.07553434371948242 elapsed, loss: 1.6828973e-06\n",
      "step: 74170 train: 0.08055257797241211 elapsed, loss: 1.2498338e-06\n",
      "step: 74180 train: 0.08759260177612305 elapsed, loss: 1.2842925e-06\n",
      "step: 74190 train: 0.08320403099060059 elapsed, loss: 1.2023365e-06\n",
      "step: 74200 train: 0.08260250091552734 elapsed, loss: 1.0286451e-06\n",
      "step: 74210 train: 0.0856330394744873 elapsed, loss: 1.1916256e-06\n",
      "step: 74220 train: 0.08129572868347168 elapsed, loss: 1.116189e-06\n",
      "step: 74230 train: 0.0864408016204834 elapsed, loss: 1.3350497e-06\n",
      "step: 74240 train: 0.08367323875427246 elapsed, loss: 1.4696255e-06\n",
      "step: 74250 train: 0.08167695999145508 elapsed, loss: 1.2419175e-06\n",
      "step: 74260 train: 0.07662224769592285 elapsed, loss: 1.7792897e-06\n",
      "step: 74270 train: 0.08223986625671387 elapsed, loss: 2.1876722e-06\n",
      "step: 74280 train: 0.0821237564086914 elapsed, loss: 1.1627553e-06\n",
      "step: 74290 train: 0.09005212783813477 elapsed, loss: 1.3392406e-06\n",
      "step: 74300 train: 0.08505129814147949 elapsed, loss: 1.5464597e-06\n",
      "step: 74310 train: 0.08582186698913574 elapsed, loss: 1.1874351e-06\n",
      "step: 74320 train: 0.0829305648803711 elapsed, loss: 1.4137458e-06\n",
      "step: 74330 train: 0.07858967781066895 elapsed, loss: 2.819101e-06\n",
      "step: 74340 train: 0.08157825469970703 elapsed, loss: 1.7983771e-06\n",
      "step: 74350 train: 0.08578705787658691 elapsed, loss: 2.8740526e-06\n",
      "step: 74360 train: 0.08156013488769531 elapsed, loss: 1.2773079e-06\n",
      "step: 74370 train: 0.08601117134094238 elapsed, loss: 1.4803342e-06\n",
      "step: 74380 train: 0.07448434829711914 elapsed, loss: 2.197918e-06\n",
      "step: 74390 train: 0.08008074760437012 elapsed, loss: 2.0149132e-06\n",
      "step: 74400 train: 0.08507132530212402 elapsed, loss: 1.5399396e-06\n",
      "step: 74410 train: 0.08596634864807129 elapsed, loss: 1.7280671e-06\n",
      "step: 74420 train: 0.07879042625427246 elapsed, loss: 1.3620581e-06\n",
      "step: 74430 train: 0.07906770706176758 elapsed, loss: 1.4989621e-06\n",
      "step: 74440 train: 0.0804758071899414 elapsed, loss: 1.892445e-06\n",
      "step: 74450 train: 0.07688522338867188 elapsed, loss: 1.7578693e-06\n",
      "step: 74460 train: 0.08393335342407227 elapsed, loss: 1.6200338e-06\n",
      "step: 74470 train: 0.0777750015258789 elapsed, loss: 1.942271e-06\n",
      "step: 74480 train: 0.0769803524017334 elapsed, loss: 2.6975692e-06\n",
      "step: 74490 train: 0.07592964172363281 elapsed, loss: 2.1401731e-06\n",
      "step: 74500 train: 0.08530116081237793 elapsed, loss: 2.0265475e-06\n",
      "step: 74510 train: 0.07925224304199219 elapsed, loss: 1.5762618e-06\n",
      "step: 74520 train: 0.08429145812988281 elapsed, loss: 2.030746e-06\n",
      "step: 74530 train: 0.09189414978027344 elapsed, loss: 1.2633379e-06\n",
      "step: 74540 train: 0.09608078002929688 elapsed, loss: 0.014349463\n",
      "step: 74550 train: 0.07729339599609375 elapsed, loss: 9.828882e-05\n",
      "step: 74560 train: 0.07736873626708984 elapsed, loss: 3.7035777e-05\n",
      "step: 74570 train: 0.08490538597106934 elapsed, loss: 1.9314775e-05\n",
      "step: 74580 train: 0.08007168769836426 elapsed, loss: 4.113904e-05\n",
      "step: 74590 train: 0.08042168617248535 elapsed, loss: 1.4438489e-05\n",
      "step: 74600 train: 0.08776378631591797 elapsed, loss: 6.27149e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 74610 train: 0.07825613021850586 elapsed, loss: 7.672188e-06\n",
      "step: 74620 train: 0.07739996910095215 elapsed, loss: 8.615567e-06\n",
      "step: 74630 train: 0.07631850242614746 elapsed, loss: 7.0039628e-06\n",
      "step: 74640 train: 0.07634282112121582 elapsed, loss: 4.1448393e-06\n",
      "step: 74650 train: 0.08083224296569824 elapsed, loss: 2.6398286e-06\n",
      "step: 74660 train: 0.08343291282653809 elapsed, loss: 3.0365234e-06\n",
      "step: 74670 train: 0.08108949661254883 elapsed, loss: 1.9585686e-06\n",
      "step: 74680 train: 0.08297944068908691 elapsed, loss: 1.4589152e-06\n",
      "step: 74690 train: 0.07734394073486328 elapsed, loss: 1.5445967e-06\n",
      "step: 74700 train: 0.0804746150970459 elapsed, loss: 2.1783603e-06\n",
      "step: 74710 train: 0.08080482482910156 elapsed, loss: 1.3415689e-06\n",
      "step: 74720 train: 0.0846865177154541 elapsed, loss: 1.2782391e-06\n",
      "step: 74730 train: 0.0804755687713623 elapsed, loss: 1.2963997e-06\n",
      "step: 74740 train: 0.07989645004272461 elapsed, loss: 1.6470422e-06\n",
      "step: 74750 train: 0.08901095390319824 elapsed, loss: 1.3085071e-06\n",
      "step: 74760 train: 0.09102487564086914 elapsed, loss: 1.6437821e-06\n",
      "step: 74770 train: 0.08385372161865234 elapsed, loss: 2.9299315e-06\n",
      "step: 74780 train: 0.0810537338256836 elapsed, loss: 1.5543758e-06\n",
      "step: 74790 train: 0.07854199409484863 elapsed, loss: 2.0568223e-06\n",
      "step: 74800 train: 0.07971882820129395 elapsed, loss: 1.5515818e-06\n",
      "step: 74810 train: 0.0842437744140625 elapsed, loss: 1.1511138e-06\n",
      "step: 74820 train: 0.08010125160217285 elapsed, loss: 1.4011734e-06\n",
      "step: 74830 train: 0.08487987518310547 elapsed, loss: 1.4738165e-06\n",
      "step: 74840 train: 0.07800531387329102 elapsed, loss: 2.0307464e-06\n",
      "step: 74850 train: 0.08407044410705566 elapsed, loss: 1.2149093e-06\n",
      "step: 74860 train: 0.07864904403686523 elapsed, loss: 1.312698e-06\n",
      "step: 74870 train: 0.0885155200958252 elapsed, loss: 2.7081223e-06\n",
      "step: 74880 train: 0.08132147789001465 elapsed, loss: 2.0591515e-06\n",
      "step: 74890 train: 0.08382320404052734 elapsed, loss: 1.2936059e-06\n",
      "step: 74900 train: 0.08075475692749023 elapsed, loss: 1.3317901e-06\n",
      "step: 74910 train: 0.0795450210571289 elapsed, loss: 2.1131673e-06\n",
      "step: 74920 train: 0.07781267166137695 elapsed, loss: 1.9865083e-06\n",
      "step: 74930 train: 0.07623457908630371 elapsed, loss: 2.2677673e-06\n",
      "step: 74940 train: 0.08730125427246094 elapsed, loss: 1.6237515e-06\n",
      "step: 74950 train: 0.07989168167114258 elapsed, loss: 2.2859278e-06\n",
      "step: 74960 train: 0.08486628532409668 elapsed, loss: 1.8360995e-06\n",
      "step: 74970 train: 0.0778200626373291 elapsed, loss: 1.6801032e-06\n",
      "step: 74980 train: 0.08093142509460449 elapsed, loss: 1.2735826e-06\n",
      "step: 74990 train: 0.08101534843444824 elapsed, loss: 1.5734677e-06\n",
      "step: 75000 train: 0.07918238639831543 elapsed, loss: 1.9983454e-05\n",
      "step: 75010 train: 0.0821843147277832 elapsed, loss: 0.0004882912\n",
      "step: 75020 train: 0.0848386287689209 elapsed, loss: 6.7113164e-05\n",
      "step: 75030 train: 0.07830667495727539 elapsed, loss: 4.3187993e-05\n",
      "step: 75040 train: 0.08030462265014648 elapsed, loss: 2.5456935e-05\n",
      "step: 75050 train: 0.08207464218139648 elapsed, loss: 2.4369194e-05\n",
      "step: 75060 train: 0.08047795295715332 elapsed, loss: 1.9287176e-05\n",
      "step: 75070 train: 0.08290266990661621 elapsed, loss: 1.1920759e-05\n",
      "step: 75080 train: 0.08156228065490723 elapsed, loss: 1.022907e-05\n",
      "step: 75090 train: 0.08308982849121094 elapsed, loss: 6.791164e-06\n",
      "step: 75100 train: 0.07584953308105469 elapsed, loss: 8.213666e-06\n",
      "step: 75110 train: 0.08274269104003906 elapsed, loss: 4.413523e-06\n",
      "step: 75120 train: 0.07655882835388184 elapsed, loss: 4.366488e-06\n",
      "step: 75130 train: 0.07909369468688965 elapsed, loss: 3.2684686e-06\n",
      "step: 75140 train: 0.07993125915527344 elapsed, loss: 1.9883705e-06\n",
      "step: 75150 train: 0.08647656440734863 elapsed, loss: 1.7355169e-06\n",
      "step: 75160 train: 0.07773971557617188 elapsed, loss: 2.2621791e-06\n",
      "step: 75170 train: 0.07468032836914062 elapsed, loss: 1.8025727e-06\n",
      "step: 75180 train: 0.07732582092285156 elapsed, loss: 1.646575e-06\n",
      "step: 75190 train: 0.07926511764526367 elapsed, loss: 1.2428485e-06\n",
      "step: 75200 train: 0.08494997024536133 elapsed, loss: 1.2181689e-06\n",
      "step: 75210 train: 0.08043742179870605 elapsed, loss: 1.6801039e-06\n",
      "step: 75220 train: 0.08275008201599121 elapsed, loss: 1.3750965e-06\n",
      "step: 75230 train: 0.07990169525146484 elapsed, loss: 1.6503018e-06\n",
      "step: 75240 train: 0.0831599235534668 elapsed, loss: 1.3476209e-06\n",
      "step: 75250 train: 0.08380317687988281 elapsed, loss: 1.256353e-06\n",
      "step: 75260 train: 0.08243441581726074 elapsed, loss: 1.7606633e-06\n",
      "step: 75270 train: 0.07663798332214355 elapsed, loss: 3.4924453e-06\n",
      "step: 75280 train: 0.08475923538208008 elapsed, loss: 1.4412203e-06\n",
      "step: 75290 train: 0.08023595809936523 elapsed, loss: 1.6107207e-06\n",
      "step: 75300 train: 0.08182692527770996 elapsed, loss: 1.3695087e-06\n",
      "step: 75310 train: 0.08484721183776855 elapsed, loss: 1.1501825e-06\n",
      "step: 75320 train: 0.07774209976196289 elapsed, loss: 1.7774267e-06\n",
      "step: 75330 train: 0.08974337577819824 elapsed, loss: 1.4943049e-06\n",
      "step: 75340 train: 0.0933682918548584 elapsed, loss: 1.2582157e-06\n",
      "step: 75350 train: 0.08415603637695312 elapsed, loss: 1.3913942e-06\n",
      "step: 75360 train: 0.07631564140319824 elapsed, loss: 2.07219e-06\n",
      "step: 75370 train: 0.07691121101379395 elapsed, loss: 2.9536832e-06\n",
      "step: 75380 train: 0.07887434959411621 elapsed, loss: 1.9106053e-06\n",
      "step: 75390 train: 0.0904233455657959 elapsed, loss: 1.375096e-06\n",
      "step: 75400 train: 0.08347821235656738 elapsed, loss: 1.4863895e-06\n",
      "step: 75410 train: 0.0823526382446289 elapsed, loss: 1.7229447e-06\n",
      "step: 75420 train: 0.08357405662536621 elapsed, loss: 2.4479602e-06\n",
      "step: 75430 train: 0.07619929313659668 elapsed, loss: 1.8849947e-06\n",
      "step: 75440 train: 0.08162403106689453 elapsed, loss: 1.7108374e-06\n",
      "step: 75450 train: 0.0817711353302002 elapsed, loss: 1.7033872e-06\n",
      "step: 75460 train: 0.08072948455810547 elapsed, loss: 2.0889538e-06\n",
      "step: 75470 train: 0.08256125450134277 elapsed, loss: 1.8891853e-06\n",
      "step: 75480 train: 0.08548736572265625 elapsed, loss: 1.7816178e-06\n",
      "step: 75490 train: 0.08435916900634766 elapsed, loss: 1.7820831e-06\n",
      "step: 75500 train: 0.07909631729125977 elapsed, loss: 2.363227e-06\n",
      "step: 75510 train: 0.08206868171691895 elapsed, loss: 1.1564905e-05\n",
      "step: 75520 train: 0.0856161117553711 elapsed, loss: 0.00024351974\n",
      "step: 75530 train: 0.08500337600708008 elapsed, loss: 3.7242145e-05\n",
      "step: 75540 train: 0.08375978469848633 elapsed, loss: 1.6760532e-05\n",
      "step: 75550 train: 0.08121609687805176 elapsed, loss: 1.7238723e-05\n",
      "step: 75560 train: 0.08590245246887207 elapsed, loss: 9.477892e-06\n",
      "step: 75570 train: 0.08168673515319824 elapsed, loss: 1.0295174e-05\n",
      "step: 75580 train: 0.07657337188720703 elapsed, loss: 1.16781985e-05\n",
      "step: 75590 train: 0.08343267440795898 elapsed, loss: 4.9532164e-06\n",
      "step: 75600 train: 0.08268523216247559 elapsed, loss: 7.3474266e-06\n",
      "step: 75610 train: 0.0827023983001709 elapsed, loss: 4.178687e-06\n",
      "step: 75620 train: 0.07401013374328613 elapsed, loss: 4.4437875e-06\n",
      "step: 75630 train: 0.08059263229370117 elapsed, loss: 2.934125e-06\n",
      "step: 75640 train: 0.08421158790588379 elapsed, loss: 2.6454168e-06\n",
      "step: 75650 train: 0.07999491691589355 elapsed, loss: 2.4298163e-06\n",
      "step: 75660 train: 0.08156347274780273 elapsed, loss: 1.758335e-06\n",
      "step: 75670 train: 0.0819087028503418 elapsed, loss: 1.3625236e-06\n",
      "step: 75680 train: 0.0799856185913086 elapsed, loss: 1.6707907e-06\n",
      "step: 75690 train: 0.0784904956817627 elapsed, loss: 1.9175905e-06\n",
      "step: 75700 train: 0.08130073547363281 elapsed, loss: 1.9450645e-06\n",
      "step: 75710 train: 0.08113431930541992 elapsed, loss: 1.4170057e-06\n",
      "step: 75720 train: 0.07908105850219727 elapsed, loss: 2.0666002e-06\n",
      "step: 75730 train: 0.07864522933959961 elapsed, loss: 1.4984964e-06\n",
      "step: 75740 train: 0.07561826705932617 elapsed, loss: 1.946462e-06\n",
      "step: 75750 train: 0.08000397682189941 elapsed, loss: 1.9832482e-06\n",
      "step: 75760 train: 0.08440804481506348 elapsed, loss: 1.4407547e-06\n",
      "step: 75770 train: 0.07892084121704102 elapsed, loss: 2.0414564e-06\n",
      "step: 75780 train: 0.08178567886352539 elapsed, loss: 1.4463419e-06\n",
      "step: 75790 train: 0.07897090911865234 elapsed, loss: 1.9040867e-06\n",
      "step: 75800 train: 0.08106637001037598 elapsed, loss: 1.4393574e-06\n",
      "step: 75810 train: 0.08415889739990234 elapsed, loss: 1.5441314e-06\n",
      "step: 75820 train: 0.08360576629638672 elapsed, loss: 1.7788236e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 75830 train: 0.08205413818359375 elapsed, loss: 1.4924431e-06\n",
      "step: 75840 train: 0.07982540130615234 elapsed, loss: 1.7187535e-06\n",
      "step: 75850 train: 0.07754087448120117 elapsed, loss: 1.9534468e-06\n",
      "step: 75860 train: 0.08270502090454102 elapsed, loss: 1.1739312e-06\n",
      "step: 75870 train: 0.0822455883026123 elapsed, loss: 1.5520461e-06\n",
      "step: 75880 train: 0.08549690246582031 elapsed, loss: 0.0034452714\n",
      "step: 75890 train: 0.08214163780212402 elapsed, loss: 0.00036666373\n",
      "step: 75900 train: 0.07590961456298828 elapsed, loss: 9.7875716e-05\n",
      "step: 75910 train: 0.07895994186401367 elapsed, loss: 4.6095214e-05\n",
      "step: 75920 train: 0.08731293678283691 elapsed, loss: 1.8637618e-05\n",
      "step: 75930 train: 0.07242989540100098 elapsed, loss: 4.4182525e-05\n",
      "step: 75940 train: 0.08247113227844238 elapsed, loss: 1.1479784e-05\n",
      "step: 75950 train: 0.08148002624511719 elapsed, loss: 1.32883415e-05\n",
      "step: 75960 train: 0.07685160636901855 elapsed, loss: 1.1459773e-05\n",
      "step: 75970 train: 0.07963037490844727 elapsed, loss: 1.1419699e-05\n",
      "step: 75980 train: 0.07801103591918945 elapsed, loss: 6.142971e-06\n",
      "step: 75990 train: 0.07818984985351562 elapsed, loss: 5.340649e-06\n",
      "step: 76000 train: 0.08120846748352051 elapsed, loss: 2.2919794e-06\n",
      "step: 76010 train: 0.08407974243164062 elapsed, loss: 2.6542637e-06\n",
      "step: 76020 train: 0.08544516563415527 elapsed, loss: 2.1881392e-06\n",
      "step: 76030 train: 0.08351588249206543 elapsed, loss: 1.6195681e-06\n",
      "step: 76040 train: 0.08115291595458984 elapsed, loss: 1.5697423e-06\n",
      "step: 76050 train: 0.08741593360900879 elapsed, loss: 1.442616e-06\n",
      "step: 76060 train: 0.08173513412475586 elapsed, loss: 1.6437825e-06\n",
      "step: 76070 train: 0.08259916305541992 elapsed, loss: 1.3057131e-06\n",
      "step: 76080 train: 0.08314752578735352 elapsed, loss: 1.2097868e-06\n",
      "step: 76090 train: 0.08127450942993164 elapsed, loss: 1.7825491e-06\n",
      "step: 76100 train: 0.09030032157897949 elapsed, loss: 1.0998912e-06\n",
      "step: 76110 train: 0.08289384841918945 elapsed, loss: 1.9231786e-06\n",
      "step: 76120 train: 0.07503700256347656 elapsed, loss: 1.7262043e-06\n",
      "step: 76130 train: 0.07910275459289551 elapsed, loss: 1.4039676e-06\n",
      "step: 76140 train: 0.07883358001708984 elapsed, loss: 1.290812e-06\n",
      "step: 76150 train: 0.08588814735412598 elapsed, loss: 1.3979127e-06\n",
      "step: 76160 train: 0.07907915115356445 elapsed, loss: 0.0011896279\n",
      "step: 76170 train: 0.0863332748413086 elapsed, loss: 0.0006616035\n",
      "step: 76180 train: 0.07424712181091309 elapsed, loss: 0.00017109656\n",
      "step: 76190 train: 0.08061718940734863 elapsed, loss: 2.7887309e-05\n",
      "step: 76200 train: 0.08724594116210938 elapsed, loss: 1.4433758e-05\n",
      "step: 76210 train: 0.0767974853515625 elapsed, loss: 2.7764958e-05\n",
      "step: 76220 train: 0.08697223663330078 elapsed, loss: 1.0632712e-05\n",
      "step: 76230 train: 0.07807111740112305 elapsed, loss: 1.1037393e-05\n",
      "step: 76240 train: 0.08136749267578125 elapsed, loss: 3.0851446e-05\n",
      "step: 76250 train: 0.07934808731079102 elapsed, loss: 6.567181e-06\n",
      "step: 76260 train: 0.08684325218200684 elapsed, loss: 3.6386555e-06\n",
      "step: 76270 train: 0.07942485809326172 elapsed, loss: 3.0584524e-06\n",
      "step: 76280 train: 0.08460521697998047 elapsed, loss: 1.986974e-06\n",
      "step: 76290 train: 0.08692717552185059 elapsed, loss: 1.6302783e-06\n",
      "step: 76300 train: 0.07526278495788574 elapsed, loss: 4.7105505e-06\n",
      "step: 76310 train: 0.08205413818359375 elapsed, loss: 1.7392426e-06\n",
      "step: 76320 train: 0.07780718803405762 elapsed, loss: 1.5874375e-06\n",
      "step: 76330 train: 0.08033943176269531 elapsed, loss: 1.9175689e-06\n",
      "step: 76340 train: 0.07729458808898926 elapsed, loss: 1.7881367e-06\n",
      "step: 76350 train: 0.08066129684448242 elapsed, loss: 1.3057131e-06\n",
      "step: 76360 train: 0.08237767219543457 elapsed, loss: 1.4798683e-06\n",
      "step: 76370 train: 0.08044958114624023 elapsed, loss: 1.5092053e-06\n",
      "step: 76380 train: 0.07945799827575684 elapsed, loss: 1.1520444e-06\n",
      "step: 76390 train: 0.08066892623901367 elapsed, loss: 1.0337674e-06\n",
      "step: 76400 train: 0.0862119197845459 elapsed, loss: 9.522765e-07\n",
      "step: 76410 train: 0.08749723434448242 elapsed, loss: 1.037492e-06\n",
      "step: 76420 train: 0.08624982833862305 elapsed, loss: 1.0924405e-06\n",
      "step: 76430 train: 0.08297991752624512 elapsed, loss: 1.2032663e-06\n",
      "step: 76440 train: 0.07609987258911133 elapsed, loss: 1.4598456e-06\n",
      "step: 76450 train: 0.07954192161560059 elapsed, loss: 1.7126943e-06\n",
      "step: 76460 train: 0.07705974578857422 elapsed, loss: 1.5734677e-06\n",
      "step: 76470 train: 0.08265924453735352 elapsed, loss: 1.0812648e-06\n",
      "step: 76480 train: 0.07419657707214355 elapsed, loss: 1.969744e-06\n",
      "step: 76490 train: 0.07741880416870117 elapsed, loss: 1.656821e-06\n",
      "step: 76500 train: 0.08166861534118652 elapsed, loss: 1.211648e-06\n",
      "step: 76510 train: 0.0765523910522461 elapsed, loss: 1.6121177e-06\n",
      "step: 76520 train: 0.08762574195861816 elapsed, loss: 2.1653163e-06\n",
      "step: 76530 train: 0.08091878890991211 elapsed, loss: 1.413746e-06\n",
      "step: 76540 train: 0.07641720771789551 elapsed, loss: 1.8533296e-06\n",
      "step: 76550 train: 0.08411288261413574 elapsed, loss: 1.8426194e-06\n",
      "step: 76560 train: 0.08187413215637207 elapsed, loss: 1.44448e-06\n",
      "step: 76570 train: 0.07936882972717285 elapsed, loss: 1.4095556e-06\n",
      "step: 76580 train: 0.08484649658203125 elapsed, loss: 1.6544926e-06\n",
      "step: 76590 train: 0.07569527626037598 elapsed, loss: 2.0810376e-06\n",
      "step: 76600 train: 0.08045697212219238 elapsed, loss: 1.5185201e-06\n",
      "step: 76610 train: 0.08417868614196777 elapsed, loss: 1.4132804e-06\n",
      "step: 76620 train: 0.08464813232421875 elapsed, loss: 1.2596126e-06\n",
      "step: 76630 train: 0.0857553482055664 elapsed, loss: 1.8430846e-06\n",
      "step: 76640 train: 0.08130049705505371 elapsed, loss: 1.5818498e-06\n",
      "step: 76650 train: 0.08044195175170898 elapsed, loss: 1.7047837e-06\n",
      "step: 76660 train: 0.07812333106994629 elapsed, loss: 1.8649603e-06\n",
      "step: 76670 train: 0.08509159088134766 elapsed, loss: 2.1895364e-06\n",
      "step: 76680 train: 0.08645391464233398 elapsed, loss: 1.3513477e-06\n",
      "step: 76690 train: 0.08579540252685547 elapsed, loss: 1.3844096e-06\n",
      "step: 76700 train: 0.08693313598632812 elapsed, loss: 1.3033847e-06\n",
      "step: 76710 train: 0.0840609073638916 elapsed, loss: 1.7993125e-06\n",
      "step: 76720 train: 0.07969188690185547 elapsed, loss: 1.5166572e-06\n",
      "step: 76730 train: 0.0756833553314209 elapsed, loss: 2.2454146e-06\n",
      "step: 76740 train: 0.07440519332885742 elapsed, loss: 0.0007128042\n",
      "step: 76750 train: 0.08400988578796387 elapsed, loss: 7.6559365e-05\n",
      "step: 76760 train: 0.08053350448608398 elapsed, loss: 5.2103635e-05\n",
      "step: 76770 train: 0.09075140953063965 elapsed, loss: 2.1338197e-05\n",
      "step: 76780 train: 0.07768535614013672 elapsed, loss: 2.5610188e-05\n",
      "step: 76790 train: 0.08033394813537598 elapsed, loss: 1.506423e-05\n",
      "step: 76800 train: 0.0790402889251709 elapsed, loss: 1.1200858e-05\n",
      "step: 76810 train: 0.09098649024963379 elapsed, loss: 7.751759e-06\n",
      "step: 76820 train: 0.08120036125183105 elapsed, loss: 1.0935784e-05\n",
      "step: 76830 train: 0.07769250869750977 elapsed, loss: 6.6058324e-06\n",
      "step: 76840 train: 0.07312846183776855 elapsed, loss: 5.5543815e-06\n",
      "step: 76850 train: 0.07255220413208008 elapsed, loss: 3.9152687e-06\n",
      "step: 76860 train: 0.07673835754394531 elapsed, loss: 2.4670685e-06\n",
      "step: 76870 train: 0.08419466018676758 elapsed, loss: 2.285459e-06\n",
      "step: 76880 train: 0.08275818824768066 elapsed, loss: 1.5539097e-06\n",
      "step: 76890 train: 0.08473730087280273 elapsed, loss: 1.2614737e-06\n",
      "step: 76900 train: 0.07549500465393066 elapsed, loss: 1.6572867e-06\n",
      "step: 76910 train: 0.07203888893127441 elapsed, loss: 2.440525e-06\n",
      "step: 76920 train: 0.08039450645446777 elapsed, loss: 1.6232925e-06\n",
      "step: 76930 train: 0.07833027839660645 elapsed, loss: 1.804901e-06\n",
      "step: 76940 train: 0.08063292503356934 elapsed, loss: 1.0747456e-06\n",
      "step: 76950 train: 0.08118581771850586 elapsed, loss: 1.2028022e-06\n",
      "step: 76960 train: 0.08681941032409668 elapsed, loss: 1.2298103e-06\n",
      "step: 76970 train: 0.08647489547729492 elapsed, loss: 1.0333017e-06\n",
      "step: 76980 train: 0.07882952690124512 elapsed, loss: 1.1762594e-06\n",
      "step: 76990 train: 0.08513045310974121 elapsed, loss: 1.8663675e-06\n",
      "step: 77000 train: 0.07803940773010254 elapsed, loss: 1.6456453e-06\n",
      "step: 77010 train: 0.07773065567016602 elapsed, loss: 1.8659025e-06\n",
      "step: 77020 train: 0.08363056182861328 elapsed, loss: 1.2204972e-06\n",
      "step: 77030 train: 0.078338623046875 elapsed, loss: 1.7108378e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 77040 train: 0.07839536666870117 elapsed, loss: 3.5203789e-06\n",
      "step: 77050 train: 0.0783388614654541 elapsed, loss: 1.4696252e-06\n",
      "step: 77060 train: 0.0850679874420166 elapsed, loss: 1.1990767e-06\n",
      "step: 77070 train: 0.07966065406799316 elapsed, loss: 1.4291131e-06\n",
      "step: 77080 train: 0.08547830581665039 elapsed, loss: 1.2828957e-06\n",
      "step: 77090 train: 0.08431363105773926 elapsed, loss: 1.7662503e-06\n",
      "step: 77100 train: 0.0846257209777832 elapsed, loss: 1.5264361e-06\n",
      "step: 77110 train: 0.08253884315490723 elapsed, loss: 1.4551899e-06\n",
      "step: 77120 train: 0.08574056625366211 elapsed, loss: 1.6838289e-06\n",
      "step: 77130 train: 0.08388185501098633 elapsed, loss: 1.379753e-06\n",
      "step: 77140 train: 0.07437324523925781 elapsed, loss: 2.7841838e-06\n",
      "step: 77150 train: 0.08665728569030762 elapsed, loss: 1.3192168e-06\n",
      "step: 77160 train: 0.07689332962036133 elapsed, loss: 1.8929096e-06\n",
      "step: 77170 train: 0.08763742446899414 elapsed, loss: 1.7681136e-06\n",
      "step: 77180 train: 0.08743143081665039 elapsed, loss: 1.338774e-06\n",
      "step: 77190 train: 0.07989668846130371 elapsed, loss: 1.945996e-06\n",
      "step: 77200 train: 0.07796454429626465 elapsed, loss: 1.9939594e-06\n",
      "step: 77210 train: 0.0785362720489502 elapsed, loss: 2.7697479e-06\n",
      "step: 77220 train: 0.08127665519714355 elapsed, loss: 1.2973312e-06\n",
      "step: 77230 train: 0.08022952079772949 elapsed, loss: 0.032972936\n",
      "step: 77240 train: 0.08647513389587402 elapsed, loss: 0.0001207659\n",
      "step: 77250 train: 0.08635139465332031 elapsed, loss: 0.00029174847\n",
      "step: 77260 train: 0.09360218048095703 elapsed, loss: 1.43941015e-05\n",
      "step: 77270 train: 0.08075070381164551 elapsed, loss: 1.2127968e-05\n",
      "step: 77280 train: 0.0799567699432373 elapsed, loss: 0.0040886207\n",
      "step: 77290 train: 0.08236098289489746 elapsed, loss: 9.899969e-05\n",
      "step: 77300 train: 0.08291792869567871 elapsed, loss: 3.8573042e-05\n",
      "step: 77310 train: 0.08267521858215332 elapsed, loss: 7.1501025e-05\n",
      "step: 77320 train: 0.07898545265197754 elapsed, loss: 2.9494033e-05\n",
      "step: 77330 train: 0.07890200614929199 elapsed, loss: 2.031296e-05\n",
      "step: 77340 train: 0.08135318756103516 elapsed, loss: 1.0638774e-05\n",
      "step: 77350 train: 0.08121728897094727 elapsed, loss: 1.123247e-05\n",
      "step: 77360 train: 0.08140063285827637 elapsed, loss: 7.0996266e-06\n",
      "step: 77370 train: 0.08342814445495605 elapsed, loss: 5.126892e-06\n",
      "step: 77380 train: 0.08364200592041016 elapsed, loss: 4.0987234e-06\n",
      "step: 77390 train: 0.07803893089294434 elapsed, loss: 4.047516e-06\n",
      "step: 77400 train: 0.07758831977844238 elapsed, loss: 2.177894e-06\n",
      "step: 77410 train: 0.08498430252075195 elapsed, loss: 1.8323743e-06\n",
      "step: 77420 train: 0.08473062515258789 elapsed, loss: 1.7271354e-06\n",
      "step: 77430 train: 0.0817253589630127 elapsed, loss: 1.7499528e-06\n",
      "step: 77440 train: 0.08211684226989746 elapsed, loss: 1.4775417e-06\n",
      "step: 77450 train: 0.08183693885803223 elapsed, loss: 1.2968653e-06\n",
      "step: 77460 train: 0.08183097839355469 elapsed, loss: 1.4272503e-06\n",
      "step: 77470 train: 0.07998085021972656 elapsed, loss: 1.1464572e-06\n",
      "step: 77480 train: 0.08324933052062988 elapsed, loss: 1.2367939e-06\n",
      "step: 77490 train: 0.0798337459564209 elapsed, loss: 1.206527e-06\n",
      "step: 77500 train: 0.07751274108886719 elapsed, loss: 1.2363296e-06\n",
      "step: 77510 train: 0.07962584495544434 elapsed, loss: 1.2777732e-06\n",
      "step: 77520 train: 0.08570194244384766 elapsed, loss: 1.0351642e-06\n",
      "step: 77530 train: 0.07891273498535156 elapsed, loss: 1.5622917e-06\n",
      "step: 77540 train: 0.08086299896240234 elapsed, loss: 1.4002419e-06\n",
      "step: 77550 train: 0.0819847583770752 elapsed, loss: 1.0286451e-06\n",
      "step: 77560 train: 0.07900786399841309 elapsed, loss: 1.7192194e-06\n",
      "step: 77570 train: 0.08846592903137207 elapsed, loss: 1.3089726e-06\n",
      "step: 77580 train: 0.0819091796875 elapsed, loss: 1.7536784e-06\n",
      "step: 77590 train: 0.08043718338012695 elapsed, loss: 1.6386603e-06\n",
      "step: 77600 train: 0.07435965538024902 elapsed, loss: 1.9972185e-06\n",
      "step: 77610 train: 0.08619976043701172 elapsed, loss: 1.5785902e-06\n",
      "step: 77620 train: 0.08036375045776367 elapsed, loss: 1.6768444e-06\n",
      "step: 77630 train: 0.08746457099914551 elapsed, loss: 1.0989598e-06\n",
      "step: 77640 train: 0.08849930763244629 elapsed, loss: 1.2968651e-06\n",
      "step: 77650 train: 0.08236527442932129 elapsed, loss: 1.5906971e-06\n",
      "step: 77660 train: 0.07940554618835449 elapsed, loss: 1.6274844e-06\n",
      "step: 77670 train: 0.08468270301818848 elapsed, loss: 1.59256e-06\n",
      "step: 77680 train: 0.08592677116394043 elapsed, loss: 1.2870867e-06\n",
      "step: 77690 train: 0.08203792572021484 elapsed, loss: 1.7443652e-06\n",
      "step: 77700 train: 0.08077001571655273 elapsed, loss: 1.4444796e-06\n",
      "step: 77710 train: 0.08597087860107422 elapsed, loss: 2.1373817e-06\n",
      "step: 77720 train: 0.08507633209228516 elapsed, loss: 1.5697426e-06\n",
      "step: 77730 train: 0.08277511596679688 elapsed, loss: 1.7194645e-05\n",
      "step: 77740 train: 0.07871198654174805 elapsed, loss: 3.0325726e-05\n",
      "step: 77750 train: 0.08252763748168945 elapsed, loss: 1.4010596e-05\n",
      "step: 77760 train: 0.08643722534179688 elapsed, loss: 7.203257e-06\n",
      "step: 77770 train: 0.08680105209350586 elapsed, loss: 6.8461077e-06\n",
      "step: 77780 train: 0.0820000171661377 elapsed, loss: 3.76812e-06\n",
      "step: 77790 train: 0.07781314849853516 elapsed, loss: 4.0465848e-06\n",
      "step: 77800 train: 0.08091926574707031 elapsed, loss: 3.6666052e-06\n",
      "step: 77810 train: 0.08099079132080078 elapsed, loss: 3.2116368e-06\n",
      "step: 77820 train: 0.07679247856140137 elapsed, loss: 2.8023442e-06\n",
      "step: 77830 train: 0.08289003372192383 elapsed, loss: 1.6740503e-06\n",
      "step: 77840 train: 0.08109402656555176 elapsed, loss: 2.174635e-06\n",
      "step: 77850 train: 0.08895111083984375 elapsed, loss: 1.4775418e-06\n",
      "step: 77860 train: 0.08342671394348145 elapsed, loss: 1.6367976e-06\n",
      "step: 77870 train: 0.08257555961608887 elapsed, loss: 5.8812475e-06\n",
      "step: 77880 train: 0.08516383171081543 elapsed, loss: 1.4742818e-06\n",
      "step: 77890 train: 0.07855868339538574 elapsed, loss: 1.7140974e-06\n",
      "step: 77900 train: 0.08455085754394531 elapsed, loss: 1.189298e-06\n",
      "step: 77910 train: 0.07983231544494629 elapsed, loss: 1.736449e-06\n",
      "step: 77920 train: 0.08997344970703125 elapsed, loss: 1.0570502e-06\n",
      "step: 77930 train: 0.08386397361755371 elapsed, loss: 1.2377268e-06\n",
      "step: 77940 train: 0.0852210521697998 elapsed, loss: 1.879406e-06\n",
      "step: 77950 train: 0.08436059951782227 elapsed, loss: 3.4407503e-06\n",
      "step: 77960 train: 0.08692502975463867 elapsed, loss: 1.601873e-06\n",
      "step: 77970 train: 0.08000421524047852 elapsed, loss: 1.7895343e-06\n",
      "step: 77980 train: 0.08029651641845703 elapsed, loss: 1.4053641e-06\n",
      "step: 77990 train: 0.08661246299743652 elapsed, loss: 1.2009393e-06\n",
      "step: 78000 train: 0.0777425765991211 elapsed, loss: 1.6833635e-06\n",
      "step: 78010 train: 0.08089876174926758 elapsed, loss: 1.7825489e-06\n",
      "step: 78020 train: 0.08435773849487305 elapsed, loss: 1.6642715e-06\n",
      "step: 78030 train: 0.09099507331848145 elapsed, loss: 1.217703e-06\n",
      "step: 78040 train: 0.07578825950622559 elapsed, loss: 1.7294641e-06\n",
      "step: 78050 train: 0.07791471481323242 elapsed, loss: 1.7955872e-06\n",
      "step: 78060 train: 0.0828096866607666 elapsed, loss: 1.9571717e-06\n",
      "step: 78070 train: 0.08247947692871094 elapsed, loss: 1.4198e-06\n",
      "step: 78080 train: 0.07771110534667969 elapsed, loss: 2.2621782e-06\n",
      "step: 78090 train: 0.08458280563354492 elapsed, loss: 1.5702083e-06\n",
      "step: 78100 train: 0.07621979713439941 elapsed, loss: 3.6591566e-06\n",
      "step: 78110 train: 0.08070898056030273 elapsed, loss: 1.6791729e-06\n",
      "step: 78120 train: 0.07954287528991699 elapsed, loss: 3.132962e-06\n",
      "step: 78130 train: 0.08031392097473145 elapsed, loss: 2.0870903e-06\n",
      "step: 78140 train: 0.08140850067138672 elapsed, loss: 2.2714876e-06\n",
      "step: 78150 train: 0.0812690258026123 elapsed, loss: 1.96367e-06\n",
      "step: 78160 train: 0.07935380935668945 elapsed, loss: 2.103854e-06\n",
      "step: 78170 train: 0.08108711242675781 elapsed, loss: 1.7611267e-06\n",
      "step: 78180 train: 0.08162117004394531 elapsed, loss: 1.8095576e-06\n",
      "step: 78190 train: 0.07716703414916992 elapsed, loss: 1.9217819e-06\n",
      "step: 78200 train: 0.0817420482635498 elapsed, loss: 2.158331e-06\n",
      "step: 78210 train: 0.08177828788757324 elapsed, loss: 1.7215477e-06\n",
      "step: 78220 train: 0.09053730964660645 elapsed, loss: 1.7555411e-06\n",
      "step: 78230 train: 0.07626843452453613 elapsed, loss: 2.1620624e-06\n",
      "step: 78240 train: 0.0811147689819336 elapsed, loss: 1.972072e-06\n",
      "step: 78250 train: 0.08186173439025879 elapsed, loss: 1.7154937e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 78260 train: 0.07724666595458984 elapsed, loss: 4.024204e-06\n",
      "step: 78270 train: 0.0838775634765625 elapsed, loss: 0.00016743506\n",
      "step: 78280 train: 0.08155488967895508 elapsed, loss: 2.7695412e-05\n",
      "step: 78290 train: 0.08701229095458984 elapsed, loss: 1.2225324e-05\n",
      "step: 78300 train: 0.08089780807495117 elapsed, loss: 1.212218e-05\n",
      "step: 78310 train: 0.08451437950134277 elapsed, loss: 1.2052975e-05\n",
      "step: 78320 train: 0.07749271392822266 elapsed, loss: 1.1680439e-05\n",
      "step: 78330 train: 0.07609343528747559 elapsed, loss: 7.539455e-06\n",
      "step: 78340 train: 0.08308792114257812 elapsed, loss: 1.0281119e-05\n",
      "step: 78350 train: 0.08302855491638184 elapsed, loss: 3.4249297e-06\n",
      "step: 78360 train: 0.08061861991882324 elapsed, loss: 6.571799e-06\n",
      "step: 78370 train: 0.08293724060058594 elapsed, loss: 2.1806882e-06\n",
      "step: 78380 train: 0.07884883880615234 elapsed, loss: 2.5923316e-06\n",
      "step: 78390 train: 0.0769948959350586 elapsed, loss: 1.7890683e-06\n",
      "step: 78400 train: 0.07469773292541504 elapsed, loss: 8.108497e-06\n",
      "step: 78410 train: 0.08039975166320801 elapsed, loss: 2.0908137e-06\n",
      "step: 78420 train: 0.08004021644592285 elapsed, loss: 2.6272564e-06\n",
      "step: 78430 train: 0.07771134376525879 elapsed, loss: 1.4500674e-06\n",
      "step: 78440 train: 0.0855410099029541 elapsed, loss: 1.1185175e-06\n",
      "step: 78450 train: 0.08804059028625488 elapsed, loss: 1.1716028e-06\n",
      "step: 78460 train: 0.08391666412353516 elapsed, loss: 1.3033839e-06\n",
      "step: 78470 train: 0.08155155181884766 elapsed, loss: 1.4677629e-06\n",
      "step: 78480 train: 0.08389425277709961 elapsed, loss: 1.0402865e-06\n",
      "step: 78490 train: 0.07974004745483398 elapsed, loss: 1.5650858e-06\n",
      "step: 78500 train: 0.08324384689331055 elapsed, loss: 1.13435e-06\n",
      "step: 78510 train: 0.08152055740356445 elapsed, loss: 1.1008225e-06\n",
      "step: 78520 train: 0.0822598934173584 elapsed, loss: 1.2801016e-06\n",
      "step: 78530 train: 0.08366990089416504 elapsed, loss: 1.3848753e-06\n",
      "step: 78540 train: 0.07760310173034668 elapsed, loss: 2.2556571e-06\n",
      "step: 78550 train: 0.08066225051879883 elapsed, loss: 0.0003014679\n",
      "step: 78560 train: 0.08456206321716309 elapsed, loss: 3.9653896e-05\n",
      "step: 78570 train: 0.0790863037109375 elapsed, loss: 3.0894593e-05\n",
      "step: 78580 train: 0.08223366737365723 elapsed, loss: 4.3654934e-05\n",
      "step: 78590 train: 0.08115124702453613 elapsed, loss: 1.3270154e-05\n",
      "step: 78600 train: 0.08352804183959961 elapsed, loss: 9.395541e-06\n",
      "step: 78610 train: 0.08600044250488281 elapsed, loss: 1.0588963e-05\n",
      "step: 78620 train: 0.08173894882202148 elapsed, loss: 6.9364355e-06\n",
      "step: 78630 train: 0.08778166770935059 elapsed, loss: 3.9944025e-06\n",
      "step: 78640 train: 0.0805671215057373 elapsed, loss: 4.163927e-06\n",
      "step: 78650 train: 0.08426094055175781 elapsed, loss: 3.2022654e-06\n",
      "step: 78660 train: 0.07605743408203125 elapsed, loss: 3.2032756e-06\n",
      "step: 78670 train: 0.07882332801818848 elapsed, loss: 2.7338908e-06\n",
      "step: 78680 train: 0.08150458335876465 elapsed, loss: 1.2917432e-06\n",
      "step: 78690 train: 0.08553528785705566 elapsed, loss: 1.2195658e-06\n",
      "step: 78700 train: 0.07493853569030762 elapsed, loss: 1.7601974e-06\n",
      "step: 78710 train: 0.0787351131439209 elapsed, loss: 1.4351663e-06\n",
      "step: 78720 train: 0.07778716087341309 elapsed, loss: 1.410021e-06\n",
      "step: 78730 train: 0.08620715141296387 elapsed, loss: 1.4202652e-06\n",
      "step: 78740 train: 0.07771873474121094 elapsed, loss: 1.7164256e-06\n",
      "step: 78750 train: 0.07882499694824219 elapsed, loss: 1.5683418e-06\n",
      "step: 78760 train: 0.07975912094116211 elapsed, loss: 1.2163063e-06\n",
      "step: 78770 train: 0.07633638381958008 elapsed, loss: 1.437495e-06\n",
      "step: 78780 train: 0.08119654655456543 elapsed, loss: 1.5366783e-06\n",
      "step: 78790 train: 0.0794675350189209 elapsed, loss: 1.0542562e-06\n",
      "step: 78800 train: 0.07809138298034668 elapsed, loss: 1.5730022e-06\n",
      "step: 78810 train: 0.08304476737976074 elapsed, loss: 9.0198535e-07\n",
      "step: 78820 train: 0.07991838455200195 elapsed, loss: 1.8142141e-06\n",
      "step: 78830 train: 0.08228421211242676 elapsed, loss: 9.466887e-07\n",
      "step: 78840 train: 0.07939362525939941 elapsed, loss: 1.2046647e-06\n",
      "step: 78850 train: 0.08283734321594238 elapsed, loss: 1.0621727e-06\n",
      "step: 78860 train: 0.08150482177734375 elapsed, loss: 1.0379581e-06\n",
      "step: 78870 train: 0.08013534545898438 elapsed, loss: 2.2663703e-06\n",
      "step: 78880 train: 0.08714842796325684 elapsed, loss: 1.3695086e-06\n",
      "step: 78890 train: 0.08730340003967285 elapsed, loss: 1.4794032e-06\n",
      "step: 78900 train: 0.08686423301696777 elapsed, loss: 1.0505312e-06\n",
      "step: 78910 train: 0.07955694198608398 elapsed, loss: 1.4067616e-06\n",
      "step: 78920 train: 0.0798640251159668 elapsed, loss: 1.6577525e-06\n",
      "step: 78930 train: 0.0828409194946289 elapsed, loss: 1.4393577e-06\n",
      "step: 78940 train: 0.08153700828552246 elapsed, loss: 1.6852264e-06\n",
      "step: 78950 train: 0.08062458038330078 elapsed, loss: 1.4472738e-06\n",
      "step: 78960 train: 0.08270144462585449 elapsed, loss: 1.6144459e-06\n",
      "step: 78970 train: 0.08295822143554688 elapsed, loss: 1.2251538e-06\n",
      "step: 78980 train: 0.08794951438903809 elapsed, loss: 1.5641546e-06\n",
      "step: 78990 train: 0.0748145580291748 elapsed, loss: 2.332028e-06\n",
      "step: 79000 train: 0.08320069313049316 elapsed, loss: 1.7671823e-06\n",
      "step: 79010 train: 0.0832052230834961 elapsed, loss: 1.4817329e-06\n",
      "step: 79020 train: 0.08042383193969727 elapsed, loss: 1.3466904e-06\n",
      "step: 79030 train: 0.07985663414001465 elapsed, loss: 2.3157286e-06\n",
      "step: 79040 train: 0.08068656921386719 elapsed, loss: 1.3816159e-06\n",
      "step: 79050 train: 0.08276534080505371 elapsed, loss: 1.7648542e-06\n",
      "step: 79060 train: 0.0764322280883789 elapsed, loss: 1.9529812e-06\n",
      "step: 79070 train: 0.08157587051391602 elapsed, loss: 2.1522824e-06\n",
      "step: 79080 train: 0.08618450164794922 elapsed, loss: 2.2458512e-06\n",
      "step: 79090 train: 0.07842493057250977 elapsed, loss: 2.344601e-06\n",
      "step: 79100 train: 0.08002638816833496 elapsed, loss: 1.9026899e-06\n",
      "step: 79110 train: 0.08196043968200684 elapsed, loss: 2.0069945e-06\n",
      "step: 79120 train: 0.09033012390136719 elapsed, loss: 2.37999e-06\n",
      "step: 79130 train: 0.08189916610717773 elapsed, loss: 3.006302e-06\n",
      "step: 79140 train: 0.08154702186584473 elapsed, loss: 2.1499552e-06\n",
      "step: 79150 train: 0.07407379150390625 elapsed, loss: 2.801879e-06\n",
      "step: 79160 train: 0.07978105545043945 elapsed, loss: 1.591163e-06\n",
      "step: 79170 train: 0.08504319190979004 elapsed, loss: 1.3615925e-06\n",
      "step: 79180 train: 0.08555269241333008 elapsed, loss: 2.0028062e-06\n",
      "step: 79190 train: 0.08318924903869629 elapsed, loss: 1.6074612e-06\n",
      "step: 79200 train: 0.08653926849365234 elapsed, loss: 1.3434317e-06\n",
      "step: 79210 train: 0.08071684837341309 elapsed, loss: 2.0088603e-06\n",
      "step: 79220 train: 0.08097076416015625 elapsed, loss: 1.9115344e-06\n",
      "step: 79230 train: 0.08043193817138672 elapsed, loss: 2.1383137e-06\n",
      "step: 79240 train: 0.08794689178466797 elapsed, loss: 1.5520475e-06\n",
      "step: 79250 train: 0.08340668678283691 elapsed, loss: 1.8193366e-06\n",
      "step: 79260 train: 0.0862116813659668 elapsed, loss: 0.016530827\n",
      "step: 79270 train: 0.07745361328125 elapsed, loss: 0.0017248008\n",
      "step: 79280 train: 0.0878152847290039 elapsed, loss: 3.2678534e-05\n",
      "step: 79290 train: 0.08742022514343262 elapsed, loss: 4.6774683e-05\n",
      "step: 79300 train: 0.08713746070861816 elapsed, loss: 2.3015002e-05\n",
      "step: 79310 train: 0.07965445518493652 elapsed, loss: 1.620288e-05\n",
      "step: 79320 train: 0.08197546005249023 elapsed, loss: 2.046943e-05\n",
      "step: 79330 train: 0.07701516151428223 elapsed, loss: 8.895434e-06\n",
      "step: 79340 train: 0.07783365249633789 elapsed, loss: 9.612515e-06\n",
      "step: 79350 train: 0.07959222793579102 elapsed, loss: 1.0523381e-05\n",
      "step: 79360 train: 0.07988166809082031 elapsed, loss: 4.6700807e-06\n",
      "step: 79370 train: 0.076873779296875 elapsed, loss: 4.5741735e-06\n",
      "step: 79380 train: 0.08148813247680664 elapsed, loss: 3.307583e-06\n",
      "step: 79390 train: 0.08088922500610352 elapsed, loss: 3.5297037e-06\n",
      "step: 79400 train: 0.07993197441101074 elapsed, loss: 2.4293506e-06\n",
      "step: 79410 train: 0.07886409759521484 elapsed, loss: 2.2524005e-06\n",
      "step: 79420 train: 0.08101081848144531 elapsed, loss: 1.6433168e-06\n",
      "step: 79430 train: 0.08478498458862305 elapsed, loss: 1.5711396e-06\n",
      "step: 79440 train: 0.07726120948791504 elapsed, loss: 1.8049009e-06\n",
      "step: 79450 train: 0.07804417610168457 elapsed, loss: 1.4780076e-06\n",
      "step: 79460 train: 0.07634520530700684 elapsed, loss: 1.5008246e-06\n",
      "step: 79470 train: 0.08184266090393066 elapsed, loss: 1.6288803e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 79480 train: 0.08162975311279297 elapsed, loss: 1.5012884e-06\n",
      "step: 79490 train: 0.08185386657714844 elapsed, loss: 1.3546075e-06\n",
      "step: 79500 train: 0.08038759231567383 elapsed, loss: 1.5539099e-06\n",
      "step: 79510 train: 0.08109593391418457 elapsed, loss: 1.6037354e-06\n",
      "step: 79520 train: 0.08123898506164551 elapsed, loss: 1.6572864e-06\n",
      "step: 79530 train: 0.07984733581542969 elapsed, loss: 3.2433138e-06\n",
      "step: 79540 train: 0.08091521263122559 elapsed, loss: 1.4011734e-06\n",
      "step: 79550 train: 0.08421754837036133 elapsed, loss: 1.9124682e-06\n",
      "step: 79560 train: 0.08307147026062012 elapsed, loss: 1.6242238e-06\n",
      "step: 79570 train: 0.07921338081359863 elapsed, loss: 1.6344693e-06\n",
      "step: 79580 train: 0.08379364013671875 elapsed, loss: 1.6149115e-06\n",
      "step: 79590 train: 0.07602071762084961 elapsed, loss: 0.00015506009\n",
      "step: 79600 train: 0.07802653312683105 elapsed, loss: 2.8361654e-05\n",
      "step: 79610 train: 0.08406686782836914 elapsed, loss: 8.527113e-06\n",
      "step: 79620 train: 0.07884931564331055 elapsed, loss: 7.4579752e-06\n",
      "step: 79630 train: 0.08661460876464844 elapsed, loss: 3.53948e-06\n",
      "step: 79640 train: 0.0781562328338623 elapsed, loss: 5.4556585e-06\n",
      "step: 79650 train: 0.08113908767700195 elapsed, loss: 3.8550843e-06\n",
      "step: 79660 train: 0.08427882194519043 elapsed, loss: 2.0540288e-06\n",
      "step: 79670 train: 0.08490705490112305 elapsed, loss: 2.2896522e-06\n",
      "step: 79680 train: 0.08702754974365234 elapsed, loss: 1.8347001e-06\n",
      "step: 79690 train: 0.08581328392028809 elapsed, loss: 1.7709065e-06\n",
      "step: 79700 train: 0.07850098609924316 elapsed, loss: 1.9483239e-06\n",
      "step: 79710 train: 0.08201384544372559 elapsed, loss: 1.6409886e-06\n",
      "step: 79720 train: 0.0817413330078125 elapsed, loss: 0.0008613556\n",
      "step: 79730 train: 0.0878598690032959 elapsed, loss: 0.00016366097\n",
      "step: 79740 train: 0.08411383628845215 elapsed, loss: 9.3251554e-05\n",
      "step: 79750 train: 0.0811927318572998 elapsed, loss: 0.00016500302\n",
      "step: 79760 train: 0.08741235733032227 elapsed, loss: 1.152619e-05\n",
      "step: 79770 train: 0.07589364051818848 elapsed, loss: 1.28795e-05\n",
      "step: 79780 train: 0.08066582679748535 elapsed, loss: 8.440439e-06\n",
      "step: 79790 train: 0.07773542404174805 elapsed, loss: 1.029001e-05\n",
      "step: 79800 train: 0.08720993995666504 elapsed, loss: 5.425394e-06\n",
      "step: 79810 train: 0.08411026000976562 elapsed, loss: 7.76382e-06\n",
      "step: 79820 train: 0.08404350280761719 elapsed, loss: 4.3483133e-06\n",
      "step: 79830 train: 0.09188103675842285 elapsed, loss: 2.4656715e-06\n",
      "step: 79840 train: 0.07746529579162598 elapsed, loss: 2.744136e-06\n",
      "step: 79850 train: 0.0810689926147461 elapsed, loss: 1.9073454e-06\n",
      "step: 79860 train: 0.08455491065979004 elapsed, loss: 2.001409e-06\n",
      "step: 79870 train: 0.08229660987854004 elapsed, loss: 2.3264388e-06\n",
      "step: 79880 train: 0.08143353462219238 elapsed, loss: 1.5967506e-06\n",
      "step: 79890 train: 0.08340263366699219 elapsed, loss: 2.0619436e-06\n",
      "step: 79900 train: 0.08113765716552734 elapsed, loss: 1.3397062e-06\n",
      "step: 79910 train: 0.07785534858703613 elapsed, loss: 1.4156087e-06\n",
      "step: 79920 train: 0.07676005363464355 elapsed, loss: 1.3657833e-06\n",
      "step: 79930 train: 0.07576322555541992 elapsed, loss: 1.4593811e-06\n",
      "step: 79940 train: 0.08401799201965332 elapsed, loss: 1.2773073e-06\n",
      "step: 79950 train: 0.07625031471252441 elapsed, loss: 1.4221283e-06\n",
      "step: 79960 train: 0.07757282257080078 elapsed, loss: 1.9427362e-06\n",
      "step: 79970 train: 0.07826066017150879 elapsed, loss: 1.6880165e-06\n",
      "step: 79980 train: 0.0804903507232666 elapsed, loss: 1.4337695e-06\n",
      "step: 79990 train: 0.08551454544067383 elapsed, loss: 9.369099e-07\n",
      "step: 80000 train: 0.07674932479858398 elapsed, loss: 1.4440144e-06\n",
      "step: 80010 train: 0.07498288154602051 elapsed, loss: 1.5250391e-06\n",
      "step: 80020 train: 0.08047842979431152 elapsed, loss: 1.2246883e-06\n",
      "step: 80030 train: 0.08855509757995605 elapsed, loss: 1.2395892e-06\n",
      "step: 80040 train: 0.08671259880065918 elapsed, loss: 9.699718e-07\n",
      "step: 80050 train: 0.07918405532836914 elapsed, loss: 1.6335376e-06\n",
      "step: 80060 train: 0.07747936248779297 elapsed, loss: 2.1299295e-06\n",
      "step: 80070 train: 0.08600807189941406 elapsed, loss: 1.0626384e-06\n",
      "step: 80080 train: 0.07870030403137207 elapsed, loss: 1.4239909e-06\n",
      "step: 80090 train: 0.07734918594360352 elapsed, loss: 2.0181692e-06\n",
      "step: 80100 train: 0.08436799049377441 elapsed, loss: 2.8391269e-06\n",
      "step: 80110 train: 0.07988309860229492 elapsed, loss: 1.5501848e-06\n",
      "step: 80120 train: 0.08631253242492676 elapsed, loss: 1.2540247e-06\n",
      "step: 80130 train: 0.08212471008300781 elapsed, loss: 1.2768421e-06\n",
      "step: 80140 train: 0.08127379417419434 elapsed, loss: 2.4721905e-06\n",
      "step: 80150 train: 0.08327269554138184 elapsed, loss: 1.2991936e-06\n",
      "step: 80160 train: 0.0792381763458252 elapsed, loss: 1.9459958e-06\n",
      "step: 80170 train: 0.0838310718536377 elapsed, loss: 1.341569e-06\n",
      "step: 80180 train: 0.08243608474731445 elapsed, loss: 2.3608986e-06\n",
      "step: 80190 train: 0.08017849922180176 elapsed, loss: 1.6880203e-06\n",
      "step: 80200 train: 0.07903814315795898 elapsed, loss: 1.608858e-06\n",
      "step: 80210 train: 0.08214688301086426 elapsed, loss: 1.4142115e-06\n",
      "step: 80220 train: 0.07978582382202148 elapsed, loss: 2.6365688e-06\n",
      "step: 80230 train: 0.08329129219055176 elapsed, loss: 1.8849946e-06\n",
      "step: 80240 train: 0.07651019096374512 elapsed, loss: 4.68312e-06\n",
      "step: 80250 train: 0.0792701244354248 elapsed, loss: 1.9352856e-06\n",
      "step: 80260 train: 0.08568596839904785 elapsed, loss: 2.0693958e-06\n",
      "step: 80270 train: 0.08845281600952148 elapsed, loss: 1.49477e-06\n",
      "step: 80280 train: 0.08076906204223633 elapsed, loss: 1.3913946e-06\n",
      "step: 80290 train: 0.08208036422729492 elapsed, loss: 2.0167763e-06\n",
      "step: 80300 train: 0.08252978324890137 elapsed, loss: 1.4668316e-06\n",
      "step: 80310 train: 0.08085489273071289 elapsed, loss: 2.279657e-05\n",
      "step: 80320 train: 0.08080816268920898 elapsed, loss: 0.0003942395\n",
      "step: 80330 train: 0.08073663711547852 elapsed, loss: 0.00012383607\n",
      "step: 80340 train: 0.0824275016784668 elapsed, loss: 4.5468732e-05\n",
      "step: 80350 train: 0.0744633674621582 elapsed, loss: 4.4397573e-05\n",
      "step: 80360 train: 0.07855749130249023 elapsed, loss: 3.2066964e-05\n",
      "step: 80370 train: 0.07654786109924316 elapsed, loss: 2.1372678e-05\n",
      "step: 80380 train: 0.08642292022705078 elapsed, loss: 1.3691791e-05\n",
      "step: 80390 train: 0.08413243293762207 elapsed, loss: 8.376235e-06\n",
      "step: 80400 train: 0.0797271728515625 elapsed, loss: 8.841871e-06\n",
      "step: 80410 train: 0.08007264137268066 elapsed, loss: 4.5979186e-06\n",
      "step: 80420 train: 0.08691072463989258 elapsed, loss: 3.2209705e-06\n",
      "step: 80430 train: 0.08240723609924316 elapsed, loss: 2.8596196e-06\n",
      "step: 80440 train: 0.07751750946044922 elapsed, loss: 2.880574e-06\n",
      "step: 80450 train: 0.07777667045593262 elapsed, loss: 2.6016437e-06\n",
      "step: 80460 train: 0.07709765434265137 elapsed, loss: 2.291515e-06\n",
      "step: 80470 train: 0.07933759689331055 elapsed, loss: 1.4277161e-06\n",
      "step: 80480 train: 0.07571172714233398 elapsed, loss: 1.9194506e-06\n",
      "step: 80490 train: 0.08335089683532715 elapsed, loss: 1.1832435e-06\n",
      "step: 80500 train: 0.08280348777770996 elapsed, loss: 1.1981454e-06\n",
      "step: 80510 train: 0.07996845245361328 elapsed, loss: 1.3415688e-06\n",
      "step: 80520 train: 0.08186554908752441 elapsed, loss: 1.6386598e-06\n",
      "step: 80530 train: 0.0818333625793457 elapsed, loss: 1.0966314e-06\n",
      "step: 80540 train: 0.08486008644104004 elapsed, loss: 1.0500655e-06\n",
      "step: 80550 train: 0.08279156684875488 elapsed, loss: 1.2260848e-06\n",
      "step: 80560 train: 0.08437871932983398 elapsed, loss: 1.2307419e-06\n",
      "step: 80570 train: 0.08340597152709961 elapsed, loss: 1.3248051e-06\n",
      "step: 80580 train: 0.08031153678894043 elapsed, loss: 1.5404058e-06\n",
      "step: 80590 train: 0.0807032585144043 elapsed, loss: 1.0393553e-06\n",
      "step: 80600 train: 0.07323431968688965 elapsed, loss: 0.00027091685\n",
      "step: 80610 train: 0.08221077919006348 elapsed, loss: 0.0008643478\n",
      "step: 80620 train: 0.07614016532897949 elapsed, loss: 0.00013917175\n",
      "step: 80630 train: 0.07924461364746094 elapsed, loss: 4.9636e-05\n",
      "step: 80640 train: 0.07882022857666016 elapsed, loss: 6.4807675e-05\n",
      "step: 80650 train: 0.09366846084594727 elapsed, loss: 1.932979e-05\n",
      "step: 80660 train: 0.07953786849975586 elapsed, loss: 1.8645182e-05\n",
      "step: 80670 train: 0.08108949661254883 elapsed, loss: 9.350366e-06\n",
      "step: 80680 train: 0.08069443702697754 elapsed, loss: 9.69124e-06\n",
      "step: 80690 train: 0.07576560974121094 elapsed, loss: 1.16728315e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 80700 train: 0.07865667343139648 elapsed, loss: 8.868912e-06\n",
      "step: 80710 train: 0.08363103866577148 elapsed, loss: 3.986979e-06\n",
      "step: 80720 train: 0.09088659286499023 elapsed, loss: 3.5608996e-06\n",
      "step: 80730 train: 0.08568048477172852 elapsed, loss: 3.0086296e-06\n",
      "step: 80740 train: 0.08482027053833008 elapsed, loss: 2.5923314e-06\n",
      "step: 80750 train: 0.08547663688659668 elapsed, loss: 1.8156107e-06\n",
      "step: 80760 train: 0.08463215827941895 elapsed, loss: 1.4612438e-06\n",
      "step: 80770 train: 0.07863426208496094 elapsed, loss: 2.0563562e-06\n",
      "step: 80780 train: 0.08097147941589355 elapsed, loss: 1.1106013e-06\n",
      "step: 80790 train: 0.07674217224121094 elapsed, loss: 1.7029213e-06\n",
      "step: 80800 train: 0.08378195762634277 elapsed, loss: 1.6060635e-06\n",
      "step: 80810 train: 0.08018088340759277 elapsed, loss: 1.9157276e-06\n",
      "step: 80820 train: 0.07902693748474121 elapsed, loss: 9.587959e-07\n",
      "step: 80830 train: 0.07972288131713867 elapsed, loss: 1.2442458e-06\n",
      "step: 80840 train: 0.08128547668457031 elapsed, loss: 1.689417e-06\n",
      "step: 80850 train: 0.07514214515686035 elapsed, loss: 1.6535614e-06\n",
      "step: 80860 train: 0.07581615447998047 elapsed, loss: 1.4244565e-06\n",
      "step: 80870 train: 0.08261942863464355 elapsed, loss: 1.5320238e-06\n",
      "step: 80880 train: 0.08654236793518066 elapsed, loss: 1.1450602e-06\n",
      "step: 80890 train: 0.08207917213439941 elapsed, loss: 1.4859235e-06\n",
      "step: 80900 train: 0.08284378051757812 elapsed, loss: 1.3438971e-06\n",
      "step: 80910 train: 0.08575797080993652 elapsed, loss: 1.1380752e-06\n",
      "step: 80920 train: 0.0774085521697998 elapsed, loss: 1.3685774e-06\n",
      "step: 80930 train: 0.08337283134460449 elapsed, loss: 1.260544e-06\n",
      "step: 80940 train: 0.08284139633178711 elapsed, loss: 1.1138609e-06\n",
      "step: 80950 train: 0.0767202377319336 elapsed, loss: 1.5031517e-06\n",
      "step: 80960 train: 0.0796515941619873 elapsed, loss: 1.5986135e-06\n",
      "step: 80970 train: 0.07985401153564453 elapsed, loss: 2.1434348e-06\n",
      "step: 80980 train: 0.08252239227294922 elapsed, loss: 1.7690435e-06\n",
      "step: 80990 train: 0.07729220390319824 elapsed, loss: 1.8272525e-06\n",
      "step: 81000 train: 0.07905793190002441 elapsed, loss: 1.784412e-06\n",
      "step: 81010 train: 0.0827476978302002 elapsed, loss: 1.498962e-06\n",
      "step: 81020 train: 0.08038711547851562 elapsed, loss: 1.6866234e-06\n",
      "step: 81030 train: 0.07747197151184082 elapsed, loss: 1.436098e-06\n",
      "step: 81040 train: 0.08349132537841797 elapsed, loss: 3.382242e-05\n",
      "step: 81050 train: 0.08158588409423828 elapsed, loss: 3.6870897e-06\n",
      "step: 81060 train: 0.07754683494567871 elapsed, loss: 0.0004315783\n",
      "step: 81070 train: 0.08253908157348633 elapsed, loss: 0.00025392557\n",
      "step: 81080 train: 0.08277106285095215 elapsed, loss: 3.630905e-05\n",
      "step: 81090 train: 0.07717442512512207 elapsed, loss: 2.5202513e-05\n",
      "step: 81100 train: 0.08261585235595703 elapsed, loss: 1.6201377e-05\n",
      "step: 81110 train: 0.0805063247680664 elapsed, loss: 1.3224554e-05\n",
      "step: 81120 train: 0.08531808853149414 elapsed, loss: 8.844167e-06\n",
      "step: 81130 train: 0.0825042724609375 elapsed, loss: 9.338732e-06\n",
      "step: 81140 train: 0.08558821678161621 elapsed, loss: 4.382788e-06\n",
      "step: 81150 train: 0.07796120643615723 elapsed, loss: 6.1993037e-06\n",
      "step: 81160 train: 0.08316159248352051 elapsed, loss: 4.2770844e-06\n",
      "step: 81170 train: 0.08152627944946289 elapsed, loss: 3.4379607e-06\n",
      "step: 81180 train: 0.08494782447814941 elapsed, loss: 2.1741666e-06\n",
      "step: 81190 train: 0.08809208869934082 elapsed, loss: 1.8505353e-06\n",
      "step: 81200 train: 0.08384895324707031 elapsed, loss: 1.9138654e-06\n",
      "step: 81210 train: 0.07841753959655762 elapsed, loss: 1.8128167e-06\n",
      "step: 81220 train: 0.07390880584716797 elapsed, loss: 1.6293454e-06\n",
      "step: 81230 train: 0.07947754859924316 elapsed, loss: 1.1818472e-06\n",
      "step: 81240 train: 0.0755608081817627 elapsed, loss: 1.315492e-06\n",
      "step: 81250 train: 0.08347415924072266 elapsed, loss: 1.0388894e-06\n",
      "step: 81260 train: 0.08137202262878418 elapsed, loss: 1.0090278e-05\n",
      "step: 81270 train: 0.08246684074401855 elapsed, loss: 8.14979e-06\n",
      "step: 81280 train: 0.08510708808898926 elapsed, loss: 2.4270207e-06\n",
      "step: 81290 train: 0.0845954418182373 elapsed, loss: 3.0049002e-06\n",
      "step: 81300 train: 0.0876157283782959 elapsed, loss: 1.7275999e-06\n",
      "step: 81310 train: 0.08073854446411133 elapsed, loss: 1.3262021e-06\n",
      "step: 81320 train: 0.07746005058288574 elapsed, loss: 1.4123492e-06\n",
      "step: 81330 train: 0.07936334609985352 elapsed, loss: 1.6060635e-06\n",
      "step: 81340 train: 0.08111834526062012 elapsed, loss: 1.0295763e-06\n",
      "step: 81350 train: 0.07832169532775879 elapsed, loss: 1.5175885e-06\n",
      "step: 81360 train: 0.087646484375 elapsed, loss: 8.656638e-07\n",
      "step: 81370 train: 0.08774375915527344 elapsed, loss: 8.6892345e-07\n",
      "step: 81380 train: 0.07799863815307617 elapsed, loss: 1.5757907e-06\n",
      "step: 81390 train: 0.07381749153137207 elapsed, loss: 1.4821985e-06\n",
      "step: 81400 train: 0.08333325386047363 elapsed, loss: 9.755597e-07\n",
      "step: 81410 train: 0.07428669929504395 elapsed, loss: 1.3275991e-06\n",
      "step: 81420 train: 0.07549619674682617 elapsed, loss: 1.3136294e-06\n",
      "step: 81430 train: 0.08656072616577148 elapsed, loss: 7.767226e-07\n",
      "step: 81440 train: 0.08176279067993164 elapsed, loss: 9.643838e-07\n",
      "step: 81450 train: 0.08414173126220703 elapsed, loss: 9.885982e-07\n",
      "step: 81460 train: 0.07451987266540527 elapsed, loss: 1.770441e-06\n",
      "step: 81470 train: 0.07887387275695801 elapsed, loss: 1.325271e-06\n",
      "step: 81480 train: 0.07742142677307129 elapsed, loss: 1.1478542e-06\n",
      "step: 81490 train: 0.08563828468322754 elapsed, loss: 1.1301591e-06\n",
      "step: 81500 train: 0.08518385887145996 elapsed, loss: 1.1050134e-06\n",
      "step: 81510 train: 0.08626556396484375 elapsed, loss: 1.2884835e-06\n",
      "step: 81520 train: 0.08017325401306152 elapsed, loss: 1.4184029e-06\n",
      "step: 81530 train: 0.08326387405395508 elapsed, loss: 1.3397062e-06\n",
      "step: 81540 train: 0.08269476890563965 elapsed, loss: 1.2880181e-06\n",
      "step: 81550 train: 0.0772249698638916 elapsed, loss: 1.7881373e-06\n",
      "step: 81560 train: 0.07382059097290039 elapsed, loss: 1.6507674e-06\n",
      "step: 81570 train: 0.08592629432678223 elapsed, loss: 1.2465741e-06\n",
      "step: 81580 train: 0.07776975631713867 elapsed, loss: 1.1767252e-06\n",
      "step: 81590 train: 0.08149528503417969 elapsed, loss: 5.1384348e-05\n",
      "step: 81600 train: 0.07664656639099121 elapsed, loss: 1.2474858e-05\n",
      "step: 81610 train: 0.0799722671508789 elapsed, loss: 7.1543773e-06\n",
      "step: 81620 train: 0.0845956802368164 elapsed, loss: 3.6372687e-06\n",
      "step: 81630 train: 0.08036327362060547 elapsed, loss: 4.332491e-06\n",
      "step: 81640 train: 0.07879304885864258 elapsed, loss: 3.4426234e-06\n",
      "step: 81650 train: 0.0833272933959961 elapsed, loss: 2.7166616e-06\n",
      "step: 81660 train: 0.08472943305969238 elapsed, loss: 3.2198327e-06\n",
      "step: 81670 train: 0.08147406578063965 elapsed, loss: 2.0591508e-06\n",
      "step: 81680 train: 0.08301806449890137 elapsed, loss: 1.4710226e-06\n",
      "step: 81690 train: 0.07358765602111816 elapsed, loss: 2.2333084e-06\n",
      "step: 81700 train: 0.08196067810058594 elapsed, loss: 1.4663658e-06\n",
      "step: 81710 train: 0.07763195037841797 elapsed, loss: 1.4905804e-06\n",
      "step: 81720 train: 0.07967233657836914 elapsed, loss: 1.6908139e-06\n",
      "step: 81730 train: 0.07849359512329102 elapsed, loss: 1.4603118e-06\n",
      "step: 81740 train: 0.0861961841583252 elapsed, loss: 1.1511138e-06\n",
      "step: 81750 train: 0.0827021598815918 elapsed, loss: 1.4379607e-06\n",
      "step: 81760 train: 0.07297277450561523 elapsed, loss: 1.8733529e-06\n",
      "step: 81770 train: 0.08847713470458984 elapsed, loss: 1.22236e-06\n",
      "step: 81780 train: 0.08321571350097656 elapsed, loss: 1.4090891e-06\n",
      "step: 81790 train: 0.08425164222717285 elapsed, loss: 1.3303932e-06\n",
      "step: 81800 train: 0.08393120765686035 elapsed, loss: 1.1157235e-06\n",
      "step: 81810 train: 0.0856623649597168 elapsed, loss: 1.6367975e-06\n",
      "step: 81820 train: 0.07492446899414062 elapsed, loss: 1.7671825e-06\n",
      "step: 81830 train: 0.0835261344909668 elapsed, loss: 1.4984964e-06\n",
      "step: 81840 train: 0.0783534049987793 elapsed, loss: 1.3024533e-06\n",
      "step: 81850 train: 0.08333945274353027 elapsed, loss: 1.6977991e-06\n",
      "step: 81860 train: 0.0820157527923584 elapsed, loss: 0.000368595\n",
      "step: 81870 train: 0.0822141170501709 elapsed, loss: 0.0010437567\n",
      "step: 81880 train: 0.08143925666809082 elapsed, loss: 0.0005872302\n",
      "step: 81890 train: 0.0797109603881836 elapsed, loss: 8.711805e-05\n",
      "step: 81900 train: 0.08171391487121582 elapsed, loss: 4.6404326e-05\n",
      "step: 81910 train: 0.0830528736114502 elapsed, loss: 2.601679e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 81920 train: 0.07689452171325684 elapsed, loss: 2.090649e-05\n",
      "step: 81930 train: 0.08173060417175293 elapsed, loss: 1.6570122e-05\n",
      "step: 81940 train: 0.08134746551513672 elapsed, loss: 1.4301595e-05\n",
      "step: 81950 train: 0.08204412460327148 elapsed, loss: 9.948206e-06\n",
      "step: 81960 train: 0.07830214500427246 elapsed, loss: 1.0556872e-05\n",
      "step: 81970 train: 0.08261656761169434 elapsed, loss: 6.1694973e-06\n",
      "step: 81980 train: 0.07290339469909668 elapsed, loss: 7.4558426e-05\n",
      "step: 81990 train: 0.07931876182556152 elapsed, loss: 1.4476765e-05\n",
      "step: 82000 train: 0.08666181564331055 elapsed, loss: 3.4812617e-06\n",
      "step: 82010 train: 0.08285379409790039 elapsed, loss: 3.9822808e-06\n",
      "step: 82020 train: 0.0843966007232666 elapsed, loss: 2.7399394e-06\n",
      "step: 82030 train: 0.07941317558288574 elapsed, loss: 2.9629964e-06\n",
      "step: 82040 train: 0.08041644096374512 elapsed, loss: 2.2640413e-06\n",
      "step: 82050 train: 0.07556796073913574 elapsed, loss: 2.223529e-06\n",
      "step: 82060 train: 0.08177590370178223 elapsed, loss: 1.1576328e-06\n",
      "step: 82070 train: 0.07470440864562988 elapsed, loss: 1.6479735e-06\n",
      "step: 82080 train: 0.08140373229980469 elapsed, loss: 1.4160742e-06\n",
      "step: 82090 train: 0.08606576919555664 elapsed, loss: 9.965139e-07\n",
      "step: 82100 train: 0.08438277244567871 elapsed, loss: 9.634525e-07\n",
      "step: 82110 train: 0.09453892707824707 elapsed, loss: 1.0109496e-06\n",
      "step: 82120 train: 0.08661293983459473 elapsed, loss: 3.0789424e-06\n",
      "step: 82130 train: 0.08003115653991699 elapsed, loss: 1.2372603e-06\n",
      "step: 82140 train: 0.08293318748474121 elapsed, loss: 1.0062934e-06\n",
      "step: 82150 train: 0.07880234718322754 elapsed, loss: 1.073814e-06\n",
      "step: 82160 train: 0.08075499534606934 elapsed, loss: 1.2884835e-06\n",
      "step: 82170 train: 0.08525776863098145 elapsed, loss: 1.1674119e-06\n",
      "step: 82180 train: 0.07996153831481934 elapsed, loss: 1.7201497e-06\n",
      "step: 82190 train: 0.08479571342468262 elapsed, loss: 1.272183e-06\n",
      "step: 82200 train: 0.07880091667175293 elapsed, loss: 1.3355154e-06\n",
      "step: 82210 train: 0.0845034122467041 elapsed, loss: 8.926722e-07\n",
      "step: 82220 train: 0.08636093139648438 elapsed, loss: 9.988428e-07\n",
      "step: 82230 train: 0.08834338188171387 elapsed, loss: 1.075211e-06\n",
      "step: 82240 train: 0.08156180381774902 elapsed, loss: 1.2461087e-06\n",
      "step: 82250 train: 0.08553385734558105 elapsed, loss: 1.0998913e-06\n",
      "step: 82260 train: 0.07918906211853027 elapsed, loss: 1.2507652e-06\n",
      "step: 82270 train: 0.07600021362304688 elapsed, loss: 1.7127005e-06\n",
      "step: 82280 train: 0.08191919326782227 elapsed, loss: 1.2842927e-06\n",
      "step: 82290 train: 0.08449649810791016 elapsed, loss: 1.1599614e-06\n",
      "step: 82300 train: 0.08131098747253418 elapsed, loss: 1.8770768e-06\n",
      "step: 82310 train: 0.07684922218322754 elapsed, loss: 1.7797554e-06\n",
      "step: 82320 train: 0.08634066581726074 elapsed, loss: 3.1397718e-05\n",
      "step: 82330 train: 0.07826614379882812 elapsed, loss: 5.4505354e-06\n",
      "step: 82340 train: 0.08600521087646484 elapsed, loss: 3.251239e-06\n",
      "step: 82350 train: 0.08823227882385254 elapsed, loss: 1.8747498e-06\n",
      "step: 82360 train: 0.08627629280090332 elapsed, loss: 2.4381984e-06\n",
      "step: 82370 train: 0.08726334571838379 elapsed, loss: 1.5590322e-06\n",
      "step: 82380 train: 0.08319330215454102 elapsed, loss: 1.2712542e-06\n",
      "step: 82390 train: 0.08047032356262207 elapsed, loss: 1.5743992e-06\n",
      "step: 82400 train: 0.07803535461425781 elapsed, loss: 1.3122321e-06\n",
      "step: 82410 train: 0.07707905769348145 elapsed, loss: 1.6544926e-06\n",
      "step: 82420 train: 0.08806562423706055 elapsed, loss: 1.2502994e-06\n",
      "step: 82430 train: 0.07761192321777344 elapsed, loss: 1.3308586e-06\n",
      "step: 82440 train: 0.08698010444641113 elapsed, loss: 1.3029191e-06\n",
      "step: 82450 train: 0.07985854148864746 elapsed, loss: 1.1916262e-06\n",
      "step: 82460 train: 0.08505773544311523 elapsed, loss: 1.0351642e-06\n",
      "step: 82470 train: 0.08106827735900879 elapsed, loss: 1.6707909e-06\n",
      "step: 82480 train: 0.08850812911987305 elapsed, loss: 1.6107197e-06\n",
      "step: 82490 train: 0.08754372596740723 elapsed, loss: 1.4980309e-06\n",
      "step: 82500 train: 0.08200335502624512 elapsed, loss: 1.4794036e-06\n",
      "step: 82510 train: 0.07737874984741211 elapsed, loss: 1.5380779e-06\n",
      "step: 82520 train: 0.0823976993560791 elapsed, loss: 1.6405229e-06\n",
      "step: 82530 train: 0.081939697265625 elapsed, loss: 1.445877e-06\n",
      "step: 82540 train: 0.07935023307800293 elapsed, loss: 1.6372634e-06\n",
      "step: 82550 train: 0.08473396301269531 elapsed, loss: 1.6372635e-06\n",
      "step: 82560 train: 0.08157229423522949 elapsed, loss: 1.5334211e-06\n",
      "step: 82570 train: 0.08286023139953613 elapsed, loss: 1.6568212e-06\n",
      "step: 82580 train: 0.07769298553466797 elapsed, loss: 1.963225e-06\n",
      "step: 82590 train: 0.07811689376831055 elapsed, loss: 2.5285349e-06\n",
      "step: 82600 train: 0.07544922828674316 elapsed, loss: 2.4447177e-06\n",
      "step: 82610 train: 0.08749866485595703 elapsed, loss: 1.1459915e-06\n",
      "step: 82620 train: 0.07764601707458496 elapsed, loss: 2.0419218e-06\n",
      "step: 82630 train: 0.08668947219848633 elapsed, loss: 2.7981484e-06\n",
      "step: 82640 train: 0.08023858070373535 elapsed, loss: 1.9082777e-06\n",
      "step: 82650 train: 0.07890009880065918 elapsed, loss: 1.8179396e-06\n",
      "step: 82660 train: 0.07894206047058105 elapsed, loss: 2.552751e-06\n",
      "step: 82670 train: 0.07650470733642578 elapsed, loss: 1.8579851e-06\n",
      "step: 82680 train: 0.07704806327819824 elapsed, loss: 2.189066e-06\n",
      "step: 82690 train: 0.07918739318847656 elapsed, loss: 1.595354e-06\n",
      "step: 82700 train: 0.0781099796295166 elapsed, loss: 2.317439e-05\n",
      "step: 82710 train: 0.08102655410766602 elapsed, loss: 6.2794043e-06\n",
      "step: 82720 train: 0.08044791221618652 elapsed, loss: 2.7338917e-06\n",
      "step: 82730 train: 0.07845163345336914 elapsed, loss: 3.485001e-06\n",
      "step: 82740 train: 0.08523416519165039 elapsed, loss: 1.8426193e-06\n",
      "step: 82750 train: 0.07859683036804199 elapsed, loss: 2.1364497e-06\n",
      "step: 82760 train: 0.08127903938293457 elapsed, loss: 1.7341208e-06\n",
      "step: 82770 train: 0.08046817779541016 elapsed, loss: 1.752747e-06\n",
      "step: 82780 train: 0.0783991813659668 elapsed, loss: 2.066602e-06\n",
      "step: 82790 train: 0.08170557022094727 elapsed, loss: 1.5567041e-06\n",
      "step: 82800 train: 0.07923483848571777 elapsed, loss: 1.4342354e-06\n",
      "step: 82810 train: 0.08074736595153809 elapsed, loss: 1.8612445e-06\n",
      "step: 82820 train: 0.07298922538757324 elapsed, loss: 3.0840679e-06\n",
      "step: 82830 train: 0.07538723945617676 elapsed, loss: 2.0787088e-06\n",
      "step: 82840 train: 0.07951045036315918 elapsed, loss: 2.2342392e-06\n",
      "step: 82850 train: 0.08172917366027832 elapsed, loss: 1.5436656e-06\n",
      "step: 82860 train: 0.07926344871520996 elapsed, loss: 1.5562384e-06\n",
      "step: 82870 train: 0.07976555824279785 elapsed, loss: 1.3927918e-06\n",
      "step: 82880 train: 0.07848596572875977 elapsed, loss: 2.2696295e-06\n",
      "step: 82890 train: 0.08217763900756836 elapsed, loss: 3.4156164e-06\n",
      "step: 82900 train: 0.07817268371582031 elapsed, loss: 0.003090872\n",
      "step: 82910 train: 0.0845494270324707 elapsed, loss: 0.0005387398\n",
      "step: 82920 train: 0.08006787300109863 elapsed, loss: 5.6931185e-05\n",
      "step: 82930 train: 0.07468867301940918 elapsed, loss: 2.91414e-05\n",
      "step: 82940 train: 0.08066940307617188 elapsed, loss: 3.3316264e-05\n",
      "step: 82950 train: 0.08085131645202637 elapsed, loss: 1.4547649e-05\n",
      "step: 82960 train: 0.07884621620178223 elapsed, loss: 1.5303556e-05\n",
      "step: 82970 train: 0.08851933479309082 elapsed, loss: 6.1550754e-06\n",
      "step: 82980 train: 0.08050227165222168 elapsed, loss: 7.483583e-06\n",
      "step: 82990 train: 0.08449077606201172 elapsed, loss: 4.167189e-06\n",
      "step: 83000 train: 0.07854080200195312 elapsed, loss: 5.1022253e-06\n",
      "step: 83010 train: 0.07775402069091797 elapsed, loss: 4.0861632e-06\n",
      "step: 83020 train: 0.0756382942199707 elapsed, loss: 3.177644e-06\n",
      "step: 83030 train: 0.08777403831481934 elapsed, loss: 2.3236448e-06\n",
      "step: 83040 train: 0.07750558853149414 elapsed, loss: 2.3795253e-06\n",
      "step: 83050 train: 0.07707977294921875 elapsed, loss: 2.425626e-06\n",
      "step: 83060 train: 0.08318543434143066 elapsed, loss: 1.4943048e-06\n",
      "step: 83070 train: 0.08512735366821289 elapsed, loss: 1.5366804e-06\n",
      "step: 83080 train: 0.0787208080291748 elapsed, loss: 1.8561236e-06\n",
      "step: 83090 train: 0.08038616180419922 elapsed, loss: 1.6475078e-06\n",
      "step: 83100 train: 0.08582925796508789 elapsed, loss: 1.3359783e-06\n",
      "step: 83110 train: 0.07727408409118652 elapsed, loss: 1.8812691e-06\n",
      "step: 83120 train: 0.07734203338623047 elapsed, loss: 1.7471591e-06\n",
      "step: 83130 train: 0.07440638542175293 elapsed, loss: 1.9385452e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 83140 train: 0.07869768142700195 elapsed, loss: 1.8724216e-06\n",
      "step: 83150 train: 0.0821220874786377 elapsed, loss: 1.3983796e-06\n",
      "step: 83160 train: 0.08943533897399902 elapsed, loss: 1.3317899e-06\n",
      "step: 83170 train: 0.08104491233825684 elapsed, loss: 1.7574031e-06\n",
      "step: 83180 train: 0.07913804054260254 elapsed, loss: 2.493608e-06\n",
      "step: 83190 train: 0.07613539695739746 elapsed, loss: 4.815849e-06\n",
      "step: 83200 train: 0.08315229415893555 elapsed, loss: 2.0908153e-06\n",
      "step: 83210 train: 0.080841064453125 elapsed, loss: 1.9362171e-06\n",
      "step: 83220 train: 0.07892227172851562 elapsed, loss: 2.495009e-06\n",
      "step: 83230 train: 0.08698034286499023 elapsed, loss: 1.1189832e-06\n",
      "step: 83240 train: 0.07723331451416016 elapsed, loss: 2.101523e-06\n",
      "step: 83250 train: 0.08680939674377441 elapsed, loss: 1.289415e-06\n",
      "step: 83260 train: 0.07633399963378906 elapsed, loss: 1.7709078e-06\n",
      "step: 83270 train: 0.08554553985595703 elapsed, loss: 1.3527444e-06\n",
      "step: 83280 train: 0.08822083473205566 elapsed, loss: 1.9390068e-06\n",
      "step: 83290 train: 0.0759584903717041 elapsed, loss: 0.00034709825\n",
      "step: 83300 train: 0.08587169647216797 elapsed, loss: 9.949989e-05\n",
      "step: 83310 train: 0.08677196502685547 elapsed, loss: 5.030037e-05\n",
      "step: 83320 train: 0.07770848274230957 elapsed, loss: 2.3912899e-05\n",
      "step: 83330 train: 0.0803072452545166 elapsed, loss: 1.1098422e-05\n",
      "step: 83340 train: 0.08486080169677734 elapsed, loss: 1.8633458e-05\n",
      "step: 83350 train: 0.0841362476348877 elapsed, loss: 0.0007235379\n",
      "step: 83360 train: 0.08683395385742188 elapsed, loss: 8.759502e-05\n",
      "step: 83370 train: 0.08324623107910156 elapsed, loss: 3.478201e-05\n",
      "step: 83380 train: 0.07817625999450684 elapsed, loss: 2.4725523e-05\n",
      "step: 83390 train: 0.08602285385131836 elapsed, loss: 1.8776424e-05\n",
      "step: 83400 train: 0.086181640625 elapsed, loss: 1.1056056e-05\n",
      "step: 83410 train: 0.08307814598083496 elapsed, loss: 1.2613481e-05\n",
      "step: 83420 train: 0.08060431480407715 elapsed, loss: 1.38817195e-05\n",
      "step: 83430 train: 0.07559442520141602 elapsed, loss: 1.2039981e-05\n",
      "step: 83440 train: 0.08553194999694824 elapsed, loss: 4.2398297e-06\n",
      "step: 83450 train: 0.07955098152160645 elapsed, loss: 4.364153e-06\n",
      "step: 83460 train: 0.08771133422851562 elapsed, loss: 4.2635656e-06\n",
      "step: 83470 train: 0.07804059982299805 elapsed, loss: 3.2624134e-06\n",
      "step: 83480 train: 0.08513021469116211 elapsed, loss: 2.0409902e-06\n",
      "step: 83490 train: 0.07417869567871094 elapsed, loss: 2.5299335e-06\n",
      "step: 83500 train: 0.08192062377929688 elapsed, loss: 1.2954683e-06\n",
      "step: 83510 train: 0.08090686798095703 elapsed, loss: 1.4984962e-06\n",
      "step: 83520 train: 0.08256769180297852 elapsed, loss: 1.0263166e-06\n",
      "step: 83530 train: 0.08731412887573242 elapsed, loss: 8.987257e-07\n",
      "step: 83540 train: 0.07875633239746094 elapsed, loss: 1.0537907e-06\n",
      "step: 83550 train: 0.07653355598449707 elapsed, loss: 1.3075753e-06\n",
      "step: 83560 train: 0.08124470710754395 elapsed, loss: 9.969801e-07\n",
      "step: 83570 train: 0.07938456535339355 elapsed, loss: 9.727655e-07\n",
      "step: 83580 train: 0.08701634407043457 elapsed, loss: 8.0699044e-07\n",
      "step: 83590 train: 0.08433675765991211 elapsed, loss: 1.086387e-06\n",
      "step: 83600 train: 0.08305883407592773 elapsed, loss: 1.0924402e-06\n",
      "step: 83610 train: 0.07570147514343262 elapsed, loss: 1.6065296e-06\n",
      "step: 83620 train: 0.07911014556884766 elapsed, loss: 1.3085071e-06\n",
      "step: 83630 train: 0.07783889770507812 elapsed, loss: 1.1804505e-06\n",
      "step: 83640 train: 0.07683134078979492 elapsed, loss: 1.349485e-06\n",
      "step: 83650 train: 0.08514189720153809 elapsed, loss: 1.1306247e-06\n",
      "step: 83660 train: 0.07850527763366699 elapsed, loss: 1.1362127e-06\n",
      "step: 83670 train: 0.08313441276550293 elapsed, loss: 1.564619e-06\n",
      "step: 83680 train: 0.08101940155029297 elapsed, loss: 1.2675284e-06\n",
      "step: 83690 train: 0.08283448219299316 elapsed, loss: 1.2600783e-06\n",
      "step: 83700 train: 0.08366823196411133 elapsed, loss: 1.3369124e-06\n",
      "step: 83710 train: 0.08518671989440918 elapsed, loss: 3.7629873e-06\n",
      "step: 83720 train: 0.07920241355895996 elapsed, loss: 1.4933743e-06\n",
      "step: 83730 train: 0.07927060127258301 elapsed, loss: 1.1213116e-06\n",
      "step: 83740 train: 0.07593011856079102 elapsed, loss: 1.640523e-06\n",
      "step: 83750 train: 0.0796821117401123 elapsed, loss: 1.6214309e-06\n",
      "step: 83760 train: 0.07998442649841309 elapsed, loss: 1.3383092e-06\n",
      "step: 83770 train: 0.0790410041809082 elapsed, loss: 1.7085094e-06\n",
      "step: 83780 train: 0.08087611198425293 elapsed, loss: 1.2144436e-06\n",
      "step: 83790 train: 0.07874464988708496 elapsed, loss: 1.8486732e-06\n",
      "step: 83800 train: 0.07680296897888184 elapsed, loss: 1.5092066e-06\n",
      "step: 83810 train: 0.08237004280090332 elapsed, loss: 1.2582157e-06\n",
      "step: 83820 train: 0.08096551895141602 elapsed, loss: 1.2265507e-06\n",
      "step: 83830 train: 0.08133292198181152 elapsed, loss: 1.510138e-06\n",
      "step: 83840 train: 0.08445930480957031 elapsed, loss: 1.322011e-06\n",
      "step: 83850 train: 0.07753658294677734 elapsed, loss: 1.9087433e-06\n",
      "step: 83860 train: 0.08178400993347168 elapsed, loss: 1.665203e-06\n",
      "step: 83870 train: 0.08610415458679199 elapsed, loss: 1.3736994e-06\n",
      "step: 83880 train: 0.08158326148986816 elapsed, loss: 1.2130467e-06\n",
      "step: 83890 train: 0.07940840721130371 elapsed, loss: 2.1345884e-06\n",
      "step: 83900 train: 0.0852506160736084 elapsed, loss: 0.00048473102\n",
      "step: 83910 train: 0.08502697944641113 elapsed, loss: 0.0015394439\n",
      "step: 83920 train: 0.08309602737426758 elapsed, loss: 0.00017541408\n",
      "step: 83930 train: 0.08076739311218262 elapsed, loss: 1.5876642e-05\n",
      "step: 83940 train: 0.08339715003967285 elapsed, loss: 1.1134196e-05\n",
      "step: 83950 train: 0.08289170265197754 elapsed, loss: 6.524342e-06\n",
      "step: 83960 train: 0.07973265647888184 elapsed, loss: 7.3359433e-06\n",
      "step: 83970 train: 0.08066797256469727 elapsed, loss: 6.853559e-06\n",
      "step: 83980 train: 0.08107614517211914 elapsed, loss: 4.437254e-06\n",
      "step: 83990 train: 0.08082842826843262 elapsed, loss: 5.450076e-06\n",
      "step: 84000 train: 0.07640194892883301 elapsed, loss: 3.0677697e-06\n",
      "step: 84010 train: 0.08217358589172363 elapsed, loss: 2.379059e-06\n",
      "step: 84020 train: 0.07987308502197266 elapsed, loss: 2.1965207e-06\n",
      "step: 84030 train: 0.08343625068664551 elapsed, loss: 2.0232956e-06\n",
      "step: 84040 train: 0.08004164695739746 elapsed, loss: 2.0163106e-06\n",
      "step: 84050 train: 0.08077883720397949 elapsed, loss: 2.0069974e-06\n",
      "step: 84060 train: 0.08085775375366211 elapsed, loss: 1.2754451e-06\n",
      "step: 84070 train: 0.07707762718200684 elapsed, loss: 1.8086259e-06\n",
      "step: 84080 train: 0.0824885368347168 elapsed, loss: 1.3848753e-06\n",
      "step: 84090 train: 0.07966017723083496 elapsed, loss: 1.9632234e-06\n",
      "step: 84100 train: 0.07608461380004883 elapsed, loss: 2.3562422e-06\n",
      "step: 84110 train: 0.08095383644104004 elapsed, loss: 1.4845266e-06\n",
      "step: 84120 train: 0.08071327209472656 elapsed, loss: 1.3802187e-06\n",
      "step: 84130 train: 0.08210206031799316 elapsed, loss: 1.5818498e-06\n",
      "step: 84140 train: 0.08960914611816406 elapsed, loss: 1.4700895e-06\n",
      "step: 84150 train: 0.08144998550415039 elapsed, loss: 1.3709057e-06\n",
      "step: 84160 train: 0.07792186737060547 elapsed, loss: 1.895239e-06\n",
      "step: 84170 train: 0.0841667652130127 elapsed, loss: 1.1282964e-06\n",
      "step: 84180 train: 0.08335709571838379 elapsed, loss: 1.3723026e-06\n",
      "step: 84190 train: 0.08194160461425781 elapsed, loss: 1.6754476e-06\n",
      "step: 84200 train: 0.08347773551940918 elapsed, loss: 1.7732343e-06\n",
      "step: 84210 train: 0.0887913703918457 elapsed, loss: 1.1483198e-06\n",
      "step: 84220 train: 0.08580899238586426 elapsed, loss: 1.7406396e-06\n",
      "step: 84230 train: 0.0787973403930664 elapsed, loss: 7.3160234e-05\n",
      "step: 84240 train: 0.0786137580871582 elapsed, loss: 9.952961e-06\n",
      "step: 84250 train: 0.07811141014099121 elapsed, loss: 5.9324284e-06\n",
      "step: 84260 train: 0.08302021026611328 elapsed, loss: 4.2812735e-06\n",
      "step: 84270 train: 0.08055901527404785 elapsed, loss: 3.8449534e-06\n",
      "step: 84280 train: 0.08404254913330078 elapsed, loss: 2.7669525e-06\n",
      "step: 84290 train: 0.08331990242004395 elapsed, loss: 2.05822e-06\n",
      "step: 84300 train: 0.08416533470153809 elapsed, loss: 1.9194506e-06\n",
      "step: 84310 train: 0.07624244689941406 elapsed, loss: 1.9264382e-06\n",
      "step: 84320 train: 0.08172869682312012 elapsed, loss: 2.4703288e-06\n",
      "step: 84330 train: 0.08447909355163574 elapsed, loss: 1.5725365e-06\n",
      "step: 84340 train: 0.07604479789733887 elapsed, loss: 1.4910459e-06\n",
      "step: 84350 train: 0.07708144187927246 elapsed, loss: 0.025411021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 84360 train: 0.08187198638916016 elapsed, loss: 0.0029593208\n",
      "step: 84370 train: 0.07998275756835938 elapsed, loss: 4.833814e-05\n",
      "step: 84380 train: 0.0830376148223877 elapsed, loss: 4.0052786e-05\n",
      "step: 84390 train: 0.07918405532836914 elapsed, loss: 2.7169863e-05\n",
      "step: 84400 train: 0.0855705738067627 elapsed, loss: 2.0227742e-05\n",
      "step: 84410 train: 0.08572793006896973 elapsed, loss: 2.1655607e-05\n",
      "step: 84420 train: 0.0829775333404541 elapsed, loss: 9.415595e-06\n",
      "step: 84430 train: 0.08320283889770508 elapsed, loss: 6.9327134e-06\n",
      "step: 84440 train: 0.07654857635498047 elapsed, loss: 8.249112e-06\n",
      "step: 84450 train: 0.08179402351379395 elapsed, loss: 3.947863e-06\n",
      "step: 84460 train: 0.08446717262268066 elapsed, loss: 3.1203872e-06\n",
      "step: 84470 train: 0.08346700668334961 elapsed, loss: 2.898265e-06\n",
      "step: 84480 train: 0.0777275562286377 elapsed, loss: 2.589522e-06\n",
      "step: 84490 train: 0.08099079132080078 elapsed, loss: 2.1518176e-06\n",
      "step: 84500 train: 0.08600425720214844 elapsed, loss: 1.3792874e-06\n",
      "step: 84510 train: 0.08122611045837402 elapsed, loss: 1.3071099e-06\n",
      "step: 84520 train: 0.08434200286865234 elapsed, loss: 1.2000082e-06\n",
      "step: 84530 train: 0.0870060920715332 elapsed, loss: 1.0635692e-06\n",
      "step: 84540 train: 0.07735943794250488 elapsed, loss: 1.3061788e-06\n",
      "step: 84550 train: 0.08542847633361816 elapsed, loss: 1.2447103e-06\n",
      "step: 84560 train: 0.08569788932800293 elapsed, loss: 1.2633379e-06\n",
      "step: 84570 train: 0.08681058883666992 elapsed, loss: 9.760254e-07\n",
      "step: 84580 train: 0.08080816268920898 elapsed, loss: 0.00023355652\n",
      "step: 84590 train: 0.08960533142089844 elapsed, loss: 5.8458722e-06\n",
      "step: 84600 train: 0.07831001281738281 elapsed, loss: 3.8370363e-06\n",
      "step: 84610 train: 0.08059954643249512 elapsed, loss: 3.0356327e-06\n",
      "step: 84620 train: 0.08385086059570312 elapsed, loss: 2.009791e-06\n",
      "step: 84630 train: 0.08069849014282227 elapsed, loss: 2.102922e-06\n",
      "step: 84640 train: 0.07683968544006348 elapsed, loss: 2.4731162e-06\n",
      "step: 84650 train: 0.08434748649597168 elapsed, loss: 1.494305e-06\n",
      "step: 84660 train: 0.08856797218322754 elapsed, loss: 1.1227085e-06\n",
      "step: 84670 train: 0.09161114692687988 elapsed, loss: 1.0770735e-06\n",
      "step: 84680 train: 0.08100271224975586 elapsed, loss: 1.1823131e-06\n",
      "step: 84690 train: 0.08142590522766113 elapsed, loss: 1.1101357e-06\n",
      "step: 84700 train: 0.08310699462890625 elapsed, loss: 1.0812646e-06\n",
      "step: 84710 train: 0.08372902870178223 elapsed, loss: 1.2963994e-06\n",
      "step: 84720 train: 0.08375430107116699 elapsed, loss: 1.1054792e-06\n",
      "step: 84730 train: 0.07659673690795898 elapsed, loss: 1.4095555e-06\n",
      "step: 84740 train: 0.08287453651428223 elapsed, loss: 1.2526276e-06\n",
      "step: 84750 train: 0.0829782485961914 elapsed, loss: 1.2111841e-06\n",
      "step: 84760 train: 0.0792241096496582 elapsed, loss: 1.5641547e-06\n",
      "step: 84770 train: 0.08124899864196777 elapsed, loss: 9.560019e-07\n",
      "step: 84780 train: 0.08302855491638184 elapsed, loss: 1.3611268e-06\n",
      "step: 84790 train: 0.08022427558898926 elapsed, loss: 1.8146724e-06\n",
      "step: 84800 train: 0.08128786087036133 elapsed, loss: 1.1268994e-06\n",
      "step: 84810 train: 0.0833740234375 elapsed, loss: 1.3131635e-06\n",
      "step: 84820 train: 0.07898092269897461 elapsed, loss: 1.7848777e-06\n",
      "step: 84830 train: 0.0824282169342041 elapsed, loss: 9.7257805e-05\n",
      "step: 84840 train: 0.08370161056518555 elapsed, loss: 0.00016354382\n",
      "step: 84850 train: 0.08707952499389648 elapsed, loss: 0.000118761716\n",
      "step: 84860 train: 0.08192992210388184 elapsed, loss: 4.5361063e-05\n",
      "step: 84870 train: 0.08619880676269531 elapsed, loss: 2.7867616e-05\n",
      "step: 84880 train: 0.08807897567749023 elapsed, loss: 1.2720722e-05\n",
      "step: 84890 train: 0.08276748657226562 elapsed, loss: 1.2318417e-05\n",
      "step: 84900 train: 0.08254694938659668 elapsed, loss: 1.878752e-05\n",
      "step: 84910 train: 0.08465290069580078 elapsed, loss: 6.089391e-06\n",
      "step: 84920 train: 0.08223199844360352 elapsed, loss: 6.1583323e-06\n",
      "step: 84930 train: 0.07671833038330078 elapsed, loss: 6.324109e-06\n",
      "step: 84940 train: 0.07879519462585449 elapsed, loss: 2.9066512e-06\n",
      "step: 84950 train: 0.08241939544677734 elapsed, loss: 3.1157317e-06\n",
      "step: 84960 train: 0.0819711685180664 elapsed, loss: 2.7771625e-06\n",
      "step: 84970 train: 0.08097028732299805 elapsed, loss: 1.9133995e-06\n",
      "step: 84980 train: 0.07648944854736328 elapsed, loss: 1.8831317e-06\n",
      "step: 84990 train: 0.08243942260742188 elapsed, loss: 1.7234096e-06\n",
      "step: 85000 train: 0.08028316497802734 elapsed, loss: 1.5664825e-06\n",
      "step: 85010 train: 0.08454346656799316 elapsed, loss: 1.1026848e-06\n",
      "step: 85020 train: 0.0859832763671875 elapsed, loss: 1.0104843e-06\n",
      "step: 85030 train: 0.07906126976013184 elapsed, loss: 1.3797533e-06\n",
      "step: 85040 train: 0.07801032066345215 elapsed, loss: 1.2088557e-06\n",
      "step: 85050 train: 0.07567167282104492 elapsed, loss: 1.5590324e-06\n",
      "step: 85060 train: 0.08281922340393066 elapsed, loss: 1.2810331e-06\n",
      "step: 85070 train: 0.07603669166564941 elapsed, loss: 1.3387751e-06\n",
      "step: 85080 train: 0.08106255531311035 elapsed, loss: 1.6065292e-06\n",
      "step: 85090 train: 0.07969808578491211 elapsed, loss: 1.5175885e-06\n",
      "step: 85100 train: 0.07924962043762207 elapsed, loss: 1.5292302e-06\n",
      "step: 85110 train: 0.0794210433959961 elapsed, loss: 1.6237591e-06\n",
      "step: 85120 train: 0.08360910415649414 elapsed, loss: 1.0998912e-06\n",
      "step: 85130 train: 0.07846450805664062 elapsed, loss: 1.1827788e-06\n",
      "step: 85140 train: 0.07845377922058105 elapsed, loss: 1.559498e-06\n",
      "step: 85150 train: 0.08811259269714355 elapsed, loss: 1.0062934e-06\n",
      "step: 85160 train: 0.07602119445800781 elapsed, loss: 1.5976822e-06\n",
      "step: 85170 train: 0.07867956161499023 elapsed, loss: 1.4686887e-06\n",
      "step: 85180 train: 0.08167457580566406 elapsed, loss: 1.4454113e-06\n",
      "step: 85190 train: 0.08811640739440918 elapsed, loss: 1.1785878e-06\n",
      "step: 85200 train: 0.08875274658203125 elapsed, loss: 3.7443701e-06\n",
      "step: 85210 train: 0.08291745185852051 elapsed, loss: 3.2149173e-06\n",
      "step: 85220 train: 0.08091497421264648 elapsed, loss: 1.8821956e-06\n",
      "step: 85230 train: 0.08408570289611816 elapsed, loss: 1.7015244e-06\n",
      "step: 85240 train: 0.08255910873413086 elapsed, loss: 1.8179396e-06\n",
      "step: 85250 train: 0.08029055595397949 elapsed, loss: 2.3012944e-06\n",
      "step: 85260 train: 0.07712793350219727 elapsed, loss: 2.3068824e-06\n",
      "step: 85270 train: 0.08131980895996094 elapsed, loss: 1.1362126e-06\n",
      "step: 85280 train: 0.07584619522094727 elapsed, loss: 1.7974504e-06\n",
      "step: 85290 train: 0.08170628547668457 elapsed, loss: 1.7634574e-06\n",
      "step: 85300 train: 0.07846403121948242 elapsed, loss: 1.8980329e-06\n",
      "step: 85310 train: 0.0856325626373291 elapsed, loss: 2.0316766e-06\n",
      "step: 85320 train: 0.08072662353515625 elapsed, loss: 1.7997788e-06\n",
      "step: 85330 train: 0.07998466491699219 elapsed, loss: 1.7876716e-06\n",
      "step: 85340 train: 0.0872650146484375 elapsed, loss: 1.7229422e-06\n",
      "step: 85350 train: 0.08063173294067383 elapsed, loss: 2.0069974e-06\n",
      "step: 85360 train: 0.08039426803588867 elapsed, loss: 1.5948883e-06\n",
      "step: 85370 train: 0.08075451850891113 elapsed, loss: 1.9771946e-06\n",
      "step: 85380 train: 0.07943058013916016 elapsed, loss: 1.7187534e-06\n",
      "step: 85390 train: 0.07774090766906738 elapsed, loss: 1.974867e-06\n",
      "step: 85400 train: 0.08236980438232422 elapsed, loss: 1.8826664e-06\n",
      "step: 85410 train: 0.07895874977111816 elapsed, loss: 1.577193e-06\n",
      "step: 85420 train: 0.08484148979187012 elapsed, loss: 3.0123456e-06\n",
      "step: 85430 train: 0.08261990547180176 elapsed, loss: 2.559736e-06\n",
      "step: 85440 train: 0.08065462112426758 elapsed, loss: 1.8011751e-06\n",
      "step: 85450 train: 0.08294439315795898 elapsed, loss: 1.3448287e-06\n",
      "step: 85460 train: 0.07801055908203125 elapsed, loss: 2.0069974e-06\n",
      "step: 85470 train: 0.08118987083435059 elapsed, loss: 1.9562406e-06\n",
      "step: 85480 train: 0.08160948753356934 elapsed, loss: 2.4838323e-06\n",
      "step: 85490 train: 0.08439087867736816 elapsed, loss: 1.7983814e-06\n",
      "step: 85500 train: 0.08158159255981445 elapsed, loss: 1.9595e-06\n",
      "step: 85510 train: 0.07609248161315918 elapsed, loss: 3.0230658e-06\n",
      "step: 85520 train: 0.0800778865814209 elapsed, loss: 2.3874418e-06\n",
      "step: 85530 train: 0.07963347434997559 elapsed, loss: 2.183017e-06\n",
      "step: 85540 train: 0.08790254592895508 elapsed, loss: 0.0002866538\n",
      "step: 85550 train: 0.08338165283203125 elapsed, loss: 6.023432e-05\n",
      "step: 85560 train: 0.08086585998535156 elapsed, loss: 7.916629e-05\n",
      "step: 85570 train: 0.08736872673034668 elapsed, loss: 1.8000497e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 85580 train: 0.08089828491210938 elapsed, loss: 1.5763226e-05\n",
      "step: 85590 train: 0.08510780334472656 elapsed, loss: 9.997094e-06\n",
      "step: 85600 train: 0.08409810066223145 elapsed, loss: 9.074724e-06\n",
      "step: 85610 train: 0.08042120933532715 elapsed, loss: 9.710241e-06\n",
      "step: 85620 train: 0.08589792251586914 elapsed, loss: 4.1071185e-06\n",
      "step: 85630 train: 0.07789182662963867 elapsed, loss: 3.2689347e-06\n",
      "step: 85640 train: 0.07854008674621582 elapsed, loss: 3.716433e-06\n",
      "step: 85650 train: 0.08442568778991699 elapsed, loss: 1.7266699e-06\n",
      "step: 85660 train: 0.07568240165710449 elapsed, loss: 2.7953595e-06\n",
      "step: 85670 train: 0.08652043342590332 elapsed, loss: 1.8663679e-06\n",
      "step: 85680 train: 0.09061884880065918 elapsed, loss: 1.5185195e-06\n",
      "step: 85690 train: 0.08086299896240234 elapsed, loss: 1.3923259e-06\n",
      "step: 85700 train: 0.07843971252441406 elapsed, loss: 1.780221e-06\n",
      "step: 85710 train: 0.08535575866699219 elapsed, loss: 1.4388919e-06\n",
      "step: 85720 train: 0.07899332046508789 elapsed, loss: 1.7904655e-06\n",
      "step: 85730 train: 0.08200812339782715 elapsed, loss: 1.4817324e-06\n",
      "step: 85740 train: 0.08509325981140137 elapsed, loss: 9.811477e-07\n",
      "step: 85750 train: 0.08057451248168945 elapsed, loss: 1.5092064e-06\n",
      "step: 85760 train: 0.07902646064758301 elapsed, loss: 1.6544928e-06\n",
      "step: 85770 train: 0.07764720916748047 elapsed, loss: 1.9916306e-06\n",
      "step: 85780 train: 0.08420753479003906 elapsed, loss: 1.2065273e-06\n",
      "step: 85790 train: 0.08280420303344727 elapsed, loss: 1.9436675e-06\n",
      "step: 85800 train: 0.08088278770446777 elapsed, loss: 2.0987325e-06\n",
      "step: 85810 train: 0.08401918411254883 elapsed, loss: 2.468e-06\n",
      "step: 85820 train: 0.08371710777282715 elapsed, loss: 1.421197e-06\n",
      "step: 85830 train: 0.07843852043151855 elapsed, loss: 1.4929087e-06\n",
      "step: 85840 train: 0.07238483428955078 elapsed, loss: 3.7834836e-06\n",
      "step: 85850 train: 0.07835745811462402 elapsed, loss: 1.9683462e-06\n",
      "step: 85860 train: 0.07925105094909668 elapsed, loss: 1.6889517e-06\n",
      "step: 85870 train: 0.08398056030273438 elapsed, loss: 1.4896489e-06\n",
      "step: 85880 train: 0.0841512680053711 elapsed, loss: 2.1825513e-06\n",
      "step: 85890 train: 0.08274292945861816 elapsed, loss: 2.4321453e-06\n",
      "step: 85900 train: 0.08592343330383301 elapsed, loss: 1.3736995e-06\n",
      "step: 85910 train: 0.08490514755249023 elapsed, loss: 1.4645029e-06\n",
      "step: 85920 train: 0.08618283271789551 elapsed, loss: 1.2535593e-06\n",
      "step: 85930 train: 0.08297324180603027 elapsed, loss: 1.3960512e-06\n",
      "step: 85940 train: 0.07744121551513672 elapsed, loss: 2.1564742e-06\n",
      "step: 85950 train: 0.08261919021606445 elapsed, loss: 1.8486732e-06\n",
      "step: 85960 train: 0.08726644515991211 elapsed, loss: 2.0042035e-06\n",
      "step: 85970 train: 0.08278512954711914 elapsed, loss: 2.0982652e-06\n",
      "step: 85980 train: 0.08016657829284668 elapsed, loss: 1.9399424e-06\n",
      "step: 85990 train: 0.08058428764343262 elapsed, loss: 3.4919835e-06\n",
      "step: 86000 train: 0.07736015319824219 elapsed, loss: 2.0596167e-06\n",
      "step: 86010 train: 0.0884244441986084 elapsed, loss: 1.477076e-06\n",
      "step: 86020 train: 0.07514262199401855 elapsed, loss: 2.8395975e-06\n",
      "step: 86030 train: 0.08264660835266113 elapsed, loss: 0.0013853811\n",
      "step: 86040 train: 0.08434057235717773 elapsed, loss: 9.367083e-05\n",
      "step: 86050 train: 0.08640456199645996 elapsed, loss: 3.894363e-05\n",
      "step: 86060 train: 0.07688784599304199 elapsed, loss: 4.795391e-05\n",
      "step: 86070 train: 0.07399654388427734 elapsed, loss: 3.0304776e-05\n",
      "step: 86080 train: 0.08285331726074219 elapsed, loss: 1.3498284e-05\n",
      "step: 86090 train: 0.08064532279968262 elapsed, loss: 9.869597e-06\n",
      "step: 86100 train: 0.08099961280822754 elapsed, loss: 8.300239e-06\n",
      "step: 86110 train: 0.07739686965942383 elapsed, loss: 8.1723065e-06\n",
      "step: 86120 train: 0.07707738876342773 elapsed, loss: 7.2349217e-06\n",
      "step: 86130 train: 0.08269500732421875 elapsed, loss: 5.941253e-06\n",
      "step: 86140 train: 0.07799887657165527 elapsed, loss: 3.1697452e-06\n",
      "step: 86150 train: 0.08077192306518555 elapsed, loss: 3.5208486e-06\n",
      "step: 86160 train: 0.08407950401306152 elapsed, loss: 1.8882535e-06\n",
      "step: 86170 train: 0.08078694343566895 elapsed, loss: 2.043319e-06\n",
      "step: 86180 train: 0.08294057846069336 elapsed, loss: 1.3927918e-06\n",
      "step: 86190 train: 0.07673883438110352 elapsed, loss: 1.6400572e-06\n",
      "step: 86200 train: 0.07743525505065918 elapsed, loss: 1.2931401e-06\n",
      "step: 86210 train: 0.08281707763671875 elapsed, loss: 1.443083e-06\n",
      "step: 86220 train: 0.0865321159362793 elapsed, loss: 1.2936059e-06\n",
      "step: 86230 train: 0.08079719543457031 elapsed, loss: 1.4449454e-06\n",
      "step: 86240 train: 0.0790107250213623 elapsed, loss: 1.7196852e-06\n",
      "step: 86250 train: 0.07698297500610352 elapsed, loss: 1.3285302e-06\n",
      "step: 86260 train: 0.07950448989868164 elapsed, loss: 1.4682287e-06\n",
      "step: 86270 train: 0.07787966728210449 elapsed, loss: 0.000114441\n",
      "step: 86280 train: 0.07943439483642578 elapsed, loss: 0.0001764443\n",
      "step: 86290 train: 0.08550024032592773 elapsed, loss: 3.7604295e-05\n",
      "step: 86300 train: 0.0805349349975586 elapsed, loss: 2.4546747e-05\n",
      "step: 86310 train: 0.08072519302368164 elapsed, loss: 4.329643e-05\n",
      "step: 86320 train: 0.0860140323638916 elapsed, loss: 1.1598962e-05\n",
      "step: 86330 train: 0.07789921760559082 elapsed, loss: 1.3611974e-05\n",
      "step: 86340 train: 0.08678555488586426 elapsed, loss: 1.0247649e-05\n",
      "step: 86350 train: 0.08368277549743652 elapsed, loss: 6.696602e-06\n",
      "step: 86360 train: 0.08801507949829102 elapsed, loss: 3.6507718e-06\n",
      "step: 86370 train: 0.07602977752685547 elapsed, loss: 4.7203807e-06\n",
      "step: 86380 train: 0.08463859558105469 elapsed, loss: 2.9574055e-06\n",
      "step: 86390 train: 0.08372998237609863 elapsed, loss: 3.0193332e-06\n",
      "step: 86400 train: 0.07922625541687012 elapsed, loss: 2.019569e-06\n",
      "step: 86410 train: 0.08459687232971191 elapsed, loss: 1.4849913e-06\n",
      "step: 86420 train: 0.08323287963867188 elapsed, loss: 1.2936057e-06\n",
      "step: 86430 train: 0.09268331527709961 elapsed, loss: 1.2679945e-06\n",
      "step: 86440 train: 0.07837986946105957 elapsed, loss: 1.2558874e-06\n",
      "step: 86450 train: 0.0755913257598877 elapsed, loss: 2.2766144e-06\n",
      "step: 86460 train: 0.080810546875 elapsed, loss: 1.2931403e-06\n",
      "step: 86470 train: 0.08087778091430664 elapsed, loss: 1.2777734e-06\n",
      "step: 86480 train: 0.08358263969421387 elapsed, loss: 1.1292251e-06\n",
      "step: 86490 train: 0.08394646644592285 elapsed, loss: 1.0258511e-06\n",
      "step: 86500 train: 0.08242511749267578 elapsed, loss: 9.895294e-07\n",
      "step: 86510 train: 0.07946109771728516 elapsed, loss: 1.3299273e-06\n",
      "step: 86520 train: 0.08329916000366211 elapsed, loss: 1.0840587e-06\n",
      "step: 86530 train: 0.08298420906066895 elapsed, loss: 8.987258e-07\n",
      "step: 86540 train: 0.08185315132141113 elapsed, loss: 1.7280294e-06\n",
      "step: 86550 train: 0.07985091209411621 elapsed, loss: 1.6121171e-06\n",
      "step: 86560 train: 0.07691121101379395 elapsed, loss: 1.379753e-06\n",
      "step: 86570 train: 0.09044075012207031 elapsed, loss: 9.2573407e-07\n",
      "step: 86580 train: 0.08043193817138672 elapsed, loss: 1.1762595e-06\n",
      "step: 86590 train: 0.08520650863647461 elapsed, loss: 1.2605437e-06\n",
      "step: 86600 train: 0.0775461196899414 elapsed, loss: 1.3732338e-06\n",
      "step: 86610 train: 0.07759404182434082 elapsed, loss: 1.9129316e-06\n",
      "step: 86620 train: 0.07833003997802734 elapsed, loss: 2.0908103e-06\n",
      "step: 86630 train: 0.0786585807800293 elapsed, loss: 1.3736994e-06\n",
      "step: 86640 train: 0.083770751953125 elapsed, loss: 1.2391238e-06\n",
      "step: 86650 train: 0.07885909080505371 elapsed, loss: 1.1534419e-06\n",
      "step: 86660 train: 0.0800774097442627 elapsed, loss: 1.0100187e-06\n",
      "step: 86670 train: 0.07953810691833496 elapsed, loss: 0.13005953\n",
      "step: 86680 train: 0.07968616485595703 elapsed, loss: 9.370499e-05\n",
      "step: 86690 train: 0.0831153392791748 elapsed, loss: 2.7393173e-05\n",
      "step: 86700 train: 0.08411121368408203 elapsed, loss: 2.0878113e-05\n",
      "step: 86710 train: 0.0841066837310791 elapsed, loss: 2.008501e-05\n",
      "step: 86720 train: 0.07621240615844727 elapsed, loss: 1.2742101e-05\n",
      "step: 86730 train: 0.08150649070739746 elapsed, loss: 8.991834e-06\n",
      "step: 86740 train: 0.07917666435241699 elapsed, loss: 6.553682e-06\n",
      "step: 86750 train: 0.07775712013244629 elapsed, loss: 5.2405107e-06\n",
      "step: 86760 train: 0.08645105361938477 elapsed, loss: 3.4463505e-06\n",
      "step: 86770 train: 0.07508611679077148 elapsed, loss: 3.6456518e-06\n",
      "step: 86780 train: 0.0772550106048584 elapsed, loss: 3.0915185e-06\n",
      "step: 86790 train: 0.07687497138977051 elapsed, loss: 2.1755636e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 86800 train: 0.07379364967346191 elapsed, loss: 3.095709e-06\n",
      "step: 86810 train: 0.07724308967590332 elapsed, loss: 1.6381941e-06\n",
      "step: 86820 train: 0.07695794105529785 elapsed, loss: 2.0577545e-06\n",
      "step: 86830 train: 0.08464908599853516 elapsed, loss: 1.1422643e-06\n",
      "step: 86840 train: 0.07509636878967285 elapsed, loss: 2.405602e-06\n",
      "step: 86850 train: 0.08166933059692383 elapsed, loss: 1.1897637e-06\n",
      "step: 86860 train: 0.08626770973205566 elapsed, loss: 9.955832e-07\n",
      "step: 86870 train: 0.08219599723815918 elapsed, loss: 1.1431976e-06\n",
      "step: 86880 train: 0.08524465560913086 elapsed, loss: 1.005362e-06\n",
      "step: 86890 train: 0.0763094425201416 elapsed, loss: 1.6568165e-06\n",
      "step: 86900 train: 0.08092403411865234 elapsed, loss: 1.1003569e-06\n",
      "step: 86910 train: 0.08326292037963867 elapsed, loss: 1.2144436e-06\n",
      "step: 86920 train: 0.0840768814086914 elapsed, loss: 1.2665976e-06\n",
      "step: 86930 train: 0.09036564826965332 elapsed, loss: 1.0859212e-06\n",
      "step: 86940 train: 0.07824325561523438 elapsed, loss: 1.6177057e-06\n",
      "step: 86950 train: 0.08555340766906738 elapsed, loss: 0.00025265297\n",
      "step: 86960 train: 0.09249663352966309 elapsed, loss: 1.22912725e-05\n",
      "step: 86970 train: 0.08545732498168945 elapsed, loss: 0.00042492457\n",
      "step: 86980 train: 0.08111238479614258 elapsed, loss: 0.0007716688\n",
      "step: 86990 train: 0.07724261283874512 elapsed, loss: 6.752644e-05\n",
      "step: 87000 train: 0.08323812484741211 elapsed, loss: 4.1927593e-05\n",
      "step: 87010 train: 0.0743093490600586 elapsed, loss: 3.4857596e-05\n",
      "step: 87020 train: 0.07572722434997559 elapsed, loss: 1.6776627e-05\n",
      "step: 87030 train: 0.08133816719055176 elapsed, loss: 1.37819825e-05\n",
      "step: 87040 train: 0.07910871505737305 elapsed, loss: 1.1346139e-05\n",
      "step: 87050 train: 0.08226513862609863 elapsed, loss: 6.088489e-06\n",
      "step: 87060 train: 0.07477569580078125 elapsed, loss: 9.142719e-06\n",
      "step: 87070 train: 0.08064484596252441 elapsed, loss: 3.6838333e-06\n",
      "step: 87080 train: 0.08161401748657227 elapsed, loss: 2.3813695e-06\n",
      "step: 87090 train: 0.08677244186401367 elapsed, loss: 1.8794062e-06\n",
      "step: 87100 train: 0.08534359931945801 elapsed, loss: 1.6908135e-06\n",
      "step: 87110 train: 0.08331847190856934 elapsed, loss: 1.7331886e-06\n",
      "step: 87120 train: 0.07876706123352051 elapsed, loss: 1.3369119e-06\n",
      "step: 87130 train: 0.08342576026916504 elapsed, loss: 1.2228247e-06\n",
      "step: 87140 train: 0.08710718154907227 elapsed, loss: 9.946517e-07\n",
      "step: 87150 train: 0.08592057228088379 elapsed, loss: 1.0817301e-06\n",
      "step: 87160 train: 0.08077096939086914 elapsed, loss: 1.3927909e-06\n",
      "step: 87170 train: 0.07866334915161133 elapsed, loss: 1.2484369e-06\n",
      "step: 87180 train: 0.0812368392944336 elapsed, loss: 8.5402223e-07\n",
      "step: 87190 train: 0.08520364761352539 elapsed, loss: 1.4272503e-06\n",
      "step: 87200 train: 0.08167076110839844 elapsed, loss: 9.057105e-07\n",
      "step: 87210 train: 0.07588434219360352 elapsed, loss: 1.3569359e-06\n",
      "step: 87220 train: 0.07944893836975098 elapsed, loss: 1.1804505e-06\n",
      "step: 87230 train: 0.07702207565307617 elapsed, loss: 1.18371e-06\n",
      "step: 87240 train: 0.07870268821716309 elapsed, loss: 1.2055955e-06\n",
      "step: 87250 train: 0.07525372505187988 elapsed, loss: 1.3192157e-06\n",
      "step: 87260 train: 0.08120036125183105 elapsed, loss: 1.288018e-06\n",
      "step: 87270 train: 0.0836038589477539 elapsed, loss: 1.4048986e-06\n",
      "step: 87280 train: 0.07555317878723145 elapsed, loss: 1.2740478e-06\n",
      "step: 87290 train: 0.07602596282958984 elapsed, loss: 1.3341185e-06\n",
      "step: 87300 train: 0.0800626277923584 elapsed, loss: 1.4579841e-06\n",
      "step: 87310 train: 0.07469606399536133 elapsed, loss: 1.8696236e-06\n",
      "step: 87320 train: 0.08498263359069824 elapsed, loss: 1.1380752e-06\n",
      "step: 87330 train: 0.08420133590698242 elapsed, loss: 1.2870864e-06\n",
      "step: 87340 train: 0.0868079662322998 elapsed, loss: 1.0342329e-06\n",
      "step: 87350 train: 0.07709932327270508 elapsed, loss: 1.396517e-06\n",
      "step: 87360 train: 0.0798335075378418 elapsed, loss: 1.4919773e-06\n",
      "step: 87370 train: 0.07746601104736328 elapsed, loss: 1.511535e-06\n",
      "step: 87380 train: 0.08171296119689941 elapsed, loss: 1.3797531e-06\n",
      "step: 87390 train: 0.08081197738647461 elapsed, loss: 1.7662508e-06\n",
      "step: 87400 train: 0.08097672462463379 elapsed, loss: 1.0887153e-06\n",
      "step: 87410 train: 0.07663345336914062 elapsed, loss: 1.6246905e-06\n",
      "step: 87420 train: 0.07792806625366211 elapsed, loss: 1.5096723e-06\n",
      "step: 87430 train: 0.08327221870422363 elapsed, loss: 1.415609e-06\n",
      "step: 87440 train: 0.0840921401977539 elapsed, loss: 1.0728827e-06\n",
      "step: 87450 train: 0.08034729957580566 elapsed, loss: 1.6665999e-06\n",
      "step: 87460 train: 0.08215618133544922 elapsed, loss: 1.3783562e-06\n",
      "step: 87470 train: 0.07590532302856445 elapsed, loss: 1.3601955e-06\n",
      "step: 87480 train: 0.08189249038696289 elapsed, loss: 1.5832468e-06\n",
      "step: 87490 train: 0.07880830764770508 elapsed, loss: 4.6961604e-06\n",
      "step: 87500 train: 0.08315610885620117 elapsed, loss: 1.7234106e-06\n",
      "step: 87510 train: 0.07792520523071289 elapsed, loss: 1.7979157e-06\n",
      "step: 87520 train: 0.07990336418151855 elapsed, loss: 1.3122324e-06\n",
      "step: 87530 train: 0.08115148544311523 elapsed, loss: 2.1057165e-06\n",
      "step: 87540 train: 0.07519912719726562 elapsed, loss: 2.058686e-06\n",
      "step: 87550 train: 0.08116507530212402 elapsed, loss: 5.069607e-06\n",
      "step: 87560 train: 0.08112144470214844 elapsed, loss: 3.3101856e-05\n",
      "step: 87570 train: 0.08994245529174805 elapsed, loss: 4.7189824e-06\n",
      "step: 87580 train: 0.07868719100952148 elapsed, loss: 5.411425e-06\n",
      "step: 87590 train: 0.07941746711730957 elapsed, loss: 3.6270221e-06\n",
      "step: 87600 train: 0.08314752578735352 elapsed, loss: 2.300362e-06\n",
      "step: 87610 train: 0.08038854598999023 elapsed, loss: 2.295706e-06\n",
      "step: 87620 train: 0.0774221420288086 elapsed, loss: 2.9802259e-06\n",
      "step: 87630 train: 0.08217740058898926 elapsed, loss: 1.7187537e-06\n",
      "step: 87640 train: 0.08447623252868652 elapsed, loss: 1.4626403e-06\n",
      "step: 87650 train: 0.07384324073791504 elapsed, loss: 2.5546133e-06\n",
      "step: 87660 train: 0.07678914070129395 elapsed, loss: 1.3569359e-06\n",
      "step: 87670 train: 0.07890677452087402 elapsed, loss: 1.3131636e-06\n",
      "step: 87680 train: 0.08750319480895996 elapsed, loss: 1.0025681e-06\n",
      "step: 87690 train: 0.07274770736694336 elapsed, loss: 1.6316756e-06\n",
      "step: 87700 train: 0.08405113220214844 elapsed, loss: 1.4617087e-06\n",
      "step: 87710 train: 0.08661222457885742 elapsed, loss: 9.965142e-07\n",
      "step: 87720 train: 0.09083890914916992 elapsed, loss: 1.104082e-06\n",
      "step: 87730 train: 0.08043265342712402 elapsed, loss: 1.7946566e-06\n",
      "step: 87740 train: 0.08488821983337402 elapsed, loss: 1.0761426e-06\n",
      "step: 87750 train: 0.08112382888793945 elapsed, loss: 1.3345841e-06\n",
      "step: 87760 train: 0.07886004447937012 elapsed, loss: 1.7518149e-06\n",
      "step: 87770 train: 0.07639098167419434 elapsed, loss: 1.6372635e-06\n",
      "step: 87780 train: 0.07919073104858398 elapsed, loss: 1.9688127e-06\n",
      "step: 87790 train: 0.08247709274291992 elapsed, loss: 1.2102525e-06\n",
      "step: 87800 train: 0.08240509033203125 elapsed, loss: 3.9408797e-06\n",
      "step: 87810 train: 0.08068060874938965 elapsed, loss: 1.6340039e-06\n",
      "step: 87820 train: 0.07720327377319336 elapsed, loss: 1.907812e-06\n",
      "step: 87830 train: 0.08412003517150879 elapsed, loss: 1.9036207e-06\n",
      "step: 87840 train: 0.08083343505859375 elapsed, loss: 1.9348183e-06\n",
      "step: 87850 train: 0.07732939720153809 elapsed, loss: 2.2542615e-06\n",
      "step: 87860 train: 0.07750678062438965 elapsed, loss: 2.0414566e-06\n",
      "step: 87870 train: 0.08098649978637695 elapsed, loss: 1.4440136e-06\n",
      "step: 87880 train: 0.08490252494812012 elapsed, loss: 2.6957082e-06\n",
      "step: 87890 train: 0.08374547958374023 elapsed, loss: 1.4379608e-06\n",
      "step: 87900 train: 0.07692170143127441 elapsed, loss: 6.6440016e-06\n",
      "step: 87910 train: 0.08368539810180664 elapsed, loss: 2.325509e-06\n",
      "step: 87920 train: 0.07970380783081055 elapsed, loss: 3.1432048e-06\n",
      "step: 87930 train: 0.08104348182678223 elapsed, loss: 1.4062957e-06\n",
      "step: 87940 train: 0.07609724998474121 elapsed, loss: 2.239824e-06\n",
      "step: 87950 train: 0.08072876930236816 elapsed, loss: 1.6740497e-06\n",
      "step: 87960 train: 0.08380436897277832 elapsed, loss: 2.0894195e-06\n",
      "step: 87970 train: 0.07621526718139648 elapsed, loss: 2.054495e-06\n",
      "step: 87980 train: 0.07503318786621094 elapsed, loss: 2.9159653e-06\n",
      "step: 87990 train: 0.07668900489807129 elapsed, loss: 1.8067622e-06\n",
      "step: 88000 train: 0.0747518539428711 elapsed, loss: 2.9974517e-06\n",
      "step: 88010 train: 0.0795736312866211 elapsed, loss: 2.3823195e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 88020 train: 0.07630252838134766 elapsed, loss: 2.0805703e-06\n",
      "step: 88030 train: 0.08071446418762207 elapsed, loss: 2.1215478e-06\n",
      "step: 88040 train: 0.08413267135620117 elapsed, loss: 1.352279e-06\n",
      "step: 88050 train: 0.08094644546508789 elapsed, loss: 1.852397e-06\n",
      "step: 88060 train: 0.0800468921661377 elapsed, loss: 1.8645054e-06\n",
      "step: 88070 train: 0.08243989944458008 elapsed, loss: 1.4770762e-06\n",
      "step: 88080 train: 0.08206629753112793 elapsed, loss: 1.6936083e-06\n",
      "step: 88090 train: 0.08474421501159668 elapsed, loss: 2.0698608e-06\n",
      "step: 88100 train: 0.08080720901489258 elapsed, loss: 2.4987346e-06\n",
      "step: 88110 train: 0.08134055137634277 elapsed, loss: 2.4535657e-06\n",
      "step: 88120 train: 0.08513307571411133 elapsed, loss: 1.9930267e-06\n",
      "step: 88130 train: 0.0782918930053711 elapsed, loss: 2.3101416e-06\n",
      "step: 88140 train: 0.08055448532104492 elapsed, loss: 1.6922113e-06\n",
      "step: 88150 train: 0.07373785972595215 elapsed, loss: 1.644568e-05\n",
      "step: 88160 train: 0.07533788681030273 elapsed, loss: 0.00034871785\n",
      "step: 88170 train: 0.08628296852111816 elapsed, loss: 4.3779335e-05\n",
      "step: 88180 train: 0.09066319465637207 elapsed, loss: 3.6893885e-05\n",
      "step: 88190 train: 0.07525825500488281 elapsed, loss: 3.8253773e-05\n",
      "step: 88200 train: 0.08051419258117676 elapsed, loss: 1.30830385e-05\n",
      "step: 88210 train: 0.08002495765686035 elapsed, loss: 2.4782305e-05\n",
      "step: 88220 train: 0.08211398124694824 elapsed, loss: 9.783442e-06\n",
      "step: 88230 train: 0.07691311836242676 elapsed, loss: 1.3695035e-05\n",
      "step: 88240 train: 0.07858777046203613 elapsed, loss: 9.7462025e-06\n",
      "step: 88250 train: 0.07982277870178223 elapsed, loss: 6.259383e-06\n",
      "step: 88260 train: 0.0831444263458252 elapsed, loss: 4.2230668e-06\n",
      "step: 88270 train: 0.07910656929016113 elapsed, loss: 3.3927993e-06\n",
      "step: 88280 train: 0.07379889488220215 elapsed, loss: 3.9166657e-06\n",
      "step: 88290 train: 0.07784605026245117 elapsed, loss: 3.1804557e-06\n",
      "step: 88300 train: 0.08442425727844238 elapsed, loss: 2.3664868e-06\n",
      "step: 88310 train: 0.08720922470092773 elapsed, loss: 2.506185e-06\n",
      "step: 88320 train: 0.08932828903198242 elapsed, loss: 1.5245736e-06\n",
      "step: 88330 train: 0.07972168922424316 elapsed, loss: 1.7788241e-06\n",
      "step: 88340 train: 0.08419036865234375 elapsed, loss: 3.4458844e-06\n",
      "step: 88350 train: 0.08258366584777832 elapsed, loss: 1.8198017e-06\n",
      "step: 88360 train: 0.08178162574768066 elapsed, loss: 1.3955856e-06\n",
      "step: 88370 train: 0.08368659019470215 elapsed, loss: 1.3108353e-06\n",
      "step: 88380 train: 0.08349204063415527 elapsed, loss: 1.6358663e-06\n",
      "step: 88390 train: 0.07747745513916016 elapsed, loss: 1.969279e-06\n",
      "step: 88400 train: 0.08045268058776855 elapsed, loss: 1.5329547e-06\n",
      "step: 88410 train: 0.08491110801696777 elapsed, loss: 1.8426194e-06\n",
      "step: 88420 train: 0.08198022842407227 elapsed, loss: 1.6135145e-06\n",
      "step: 88430 train: 0.08046174049377441 elapsed, loss: 1.8905824e-06\n",
      "step: 88440 train: 0.0829019546508789 elapsed, loss: 1.6470422e-06\n",
      "step: 88450 train: 0.08520197868347168 elapsed, loss: 1.6731192e-06\n",
      "step: 88460 train: 0.08119702339172363 elapsed, loss: 1.5869721e-06\n",
      "step: 88470 train: 0.08041238784790039 elapsed, loss: 1.3257365e-06\n",
      "step: 88480 train: 0.09042668342590332 elapsed, loss: 1.9851109e-06\n",
      "step: 88490 train: 0.08195734024047852 elapsed, loss: 2.0624109e-06\n",
      "step: 88500 train: 0.08463692665100098 elapsed, loss: 1.8617111e-06\n",
      "step: 88510 train: 0.08201289176940918 elapsed, loss: 1.6340036e-06\n",
      "step: 88520 train: 0.0805201530456543 elapsed, loss: 0.0003043635\n",
      "step: 88530 train: 0.08785319328308105 elapsed, loss: 5.8347425e-05\n",
      "step: 88540 train: 0.08090853691101074 elapsed, loss: 1.7943443e-05\n",
      "step: 88550 train: 0.07889032363891602 elapsed, loss: 3.789273e-05\n",
      "step: 88560 train: 0.08437013626098633 elapsed, loss: 1.4474912e-05\n",
      "step: 88570 train: 0.08037900924682617 elapsed, loss: 1.1292639e-05\n",
      "step: 88580 train: 0.0801694393157959 elapsed, loss: 5.0388944e-06\n",
      "step: 88590 train: 0.0749819278717041 elapsed, loss: 7.1944237e-06\n",
      "step: 88600 train: 0.08055877685546875 elapsed, loss: 0.0003025391\n",
      "step: 88610 train: 0.0835866928100586 elapsed, loss: 5.275477e-05\n",
      "step: 88620 train: 0.08163046836853027 elapsed, loss: 3.434102e-05\n",
      "step: 88630 train: 0.08073925971984863 elapsed, loss: 1.6128586e-05\n",
      "step: 88640 train: 0.08379936218261719 elapsed, loss: 1.06150665e-05\n",
      "step: 88650 train: 0.08295297622680664 elapsed, loss: 7.5193793e-06\n",
      "step: 88660 train: 0.0808706283569336 elapsed, loss: 1.0309142e-05\n",
      "step: 88670 train: 0.07960319519042969 elapsed, loss: 7.2731e-06\n",
      "step: 88680 train: 0.0852060317993164 elapsed, loss: 4.811586e-06\n",
      "step: 88690 train: 0.08293795585632324 elapsed, loss: 3.9343586e-06\n",
      "step: 88700 train: 0.08530092239379883 elapsed, loss: 2.9303992e-06\n",
      "step: 88710 train: 0.08097696304321289 elapsed, loss: 2.614683e-06\n",
      "step: 88720 train: 0.07907247543334961 elapsed, loss: 2.7748688e-06\n",
      "step: 88730 train: 0.08093452453613281 elapsed, loss: 1.610255e-06\n",
      "step: 88740 train: 0.07971882820129395 elapsed, loss: 2.3539096e-06\n",
      "step: 88750 train: 0.07999372482299805 elapsed, loss: 1.6735845e-06\n",
      "step: 88760 train: 0.08063030242919922 elapsed, loss: 1.1273648e-06\n",
      "step: 88770 train: 0.08505964279174805 elapsed, loss: 1.0658979e-06\n",
      "step: 88780 train: 0.08458924293518066 elapsed, loss: 1.1827783e-06\n",
      "step: 88790 train: 0.07381367683410645 elapsed, loss: 1.3075758e-06\n",
      "step: 88800 train: 0.0833425521850586 elapsed, loss: 1.3103695e-06\n",
      "step: 88810 train: 0.07981538772583008 elapsed, loss: 1.6894171e-06\n",
      "step: 88820 train: 0.08312439918518066 elapsed, loss: 1.1711362e-06\n",
      "step: 88830 train: 0.07739686965942383 elapsed, loss: 1.293606e-06\n",
      "step: 88840 train: 0.08624434471130371 elapsed, loss: 1.1413349e-06\n",
      "step: 88850 train: 0.08420658111572266 elapsed, loss: 1.2875524e-06\n",
      "step: 88860 train: 0.08048200607299805 elapsed, loss: 1.3839426e-06\n",
      "step: 88870 train: 0.0804901123046875 elapsed, loss: 1.5953533e-06\n",
      "step: 88880 train: 0.07787156105041504 elapsed, loss: 1.4258536e-06\n",
      "step: 88890 train: 0.0814826488494873 elapsed, loss: 1.2945372e-06\n",
      "step: 88900 train: 0.08217024803161621 elapsed, loss: 1.4207312e-06\n",
      "step: 88910 train: 0.08358263969421387 elapsed, loss: 1.1222429e-06\n",
      "step: 88920 train: 0.076873779296875 elapsed, loss: 1.4281817e-06\n",
      "step: 88930 train: 0.08809614181518555 elapsed, loss: 1.6065123e-06\n",
      "step: 88940 train: 0.07847332954406738 elapsed, loss: 1.9189877e-06\n",
      "step: 88950 train: 0.08104276657104492 elapsed, loss: 1.2032679e-06\n",
      "step: 88960 train: 0.07957148551940918 elapsed, loss: 1.293606e-06\n",
      "step: 88970 train: 0.08109879493713379 elapsed, loss: 1.5534445e-06\n",
      "step: 88980 train: 0.07380938529968262 elapsed, loss: 2.2007084e-06\n",
      "step: 88990 train: 0.08057999610900879 elapsed, loss: 1.7979162e-06\n",
      "step: 89000 train: 0.08645009994506836 elapsed, loss: 1.9744011e-06\n",
      "step: 89010 train: 0.07996606826782227 elapsed, loss: 1.5045498e-06\n",
      "step: 89020 train: 0.08392000198364258 elapsed, loss: 1.5255046e-06\n",
      "step: 89030 train: 0.0795278549194336 elapsed, loss: 1.8766125e-06\n",
      "step: 89040 train: 0.08124852180480957 elapsed, loss: 1.4412204e-06\n",
      "step: 89050 train: 0.07878422737121582 elapsed, loss: 2.548094e-06\n",
      "step: 89060 train: 0.0796663761138916 elapsed, loss: 1.3932573e-06\n",
      "step: 89070 train: 0.08117818832397461 elapsed, loss: 2.0768462e-06\n",
      "step: 89080 train: 0.07972311973571777 elapsed, loss: 1.8994299e-06\n",
      "step: 89090 train: 0.08520364761352539 elapsed, loss: 1.917591e-06\n",
      "step: 89100 train: 0.08581256866455078 elapsed, loss: 1.7178223e-06\n",
      "step: 89110 train: 0.08588337898254395 elapsed, loss: 2.259385e-06\n",
      "step: 89120 train: 0.08896923065185547 elapsed, loss: 1.7476248e-06\n",
      "step: 89130 train: 0.08757925033569336 elapsed, loss: 0.00092010334\n",
      "step: 89140 train: 0.07957720756530762 elapsed, loss: 2.1572996e-05\n",
      "step: 89150 train: 0.08346247673034668 elapsed, loss: 1.4337896e-05\n",
      "step: 89160 train: 0.08248424530029297 elapsed, loss: 1.8459537e-05\n",
      "step: 89170 train: 0.08345174789428711 elapsed, loss: 1.1677198e-05\n",
      "step: 89180 train: 0.07993555068969727 elapsed, loss: 9.046293e-06\n",
      "step: 89190 train: 0.08389616012573242 elapsed, loss: 5.261013e-06\n",
      "step: 89200 train: 0.08463454246520996 elapsed, loss: 4.786045e-06\n",
      "step: 89210 train: 0.08058595657348633 elapsed, loss: 4.1210897e-06\n",
      "step: 89220 train: 0.08487701416015625 elapsed, loss: 3.0803344e-06\n",
      "step: 89230 train: 0.0785374641418457 elapsed, loss: 4.155542e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 89240 train: 0.08151531219482422 elapsed, loss: 2.4964058e-06\n",
      "step: 89250 train: 0.08613944053649902 elapsed, loss: 1.9892873e-06\n",
      "step: 89260 train: 0.08604145050048828 elapsed, loss: 1.5334205e-06\n",
      "step: 89270 train: 0.0803537368774414 elapsed, loss: 1.7154939e-06\n",
      "step: 89280 train: 0.0811457633972168 elapsed, loss: 1.940992e-05\n",
      "step: 89290 train: 0.08735299110412598 elapsed, loss: 6.119747e-05\n",
      "step: 89300 train: 0.08887600898742676 elapsed, loss: 0.0002949754\n",
      "step: 89310 train: 0.07859587669372559 elapsed, loss: 0.00016646174\n",
      "step: 89320 train: 0.08122873306274414 elapsed, loss: 5.2044517e-05\n",
      "step: 89330 train: 0.08373808860778809 elapsed, loss: 3.128335e-05\n",
      "step: 89340 train: 0.08608579635620117 elapsed, loss: 1.9370003e-05\n",
      "step: 89350 train: 0.07762861251831055 elapsed, loss: 2.7180364e-05\n",
      "step: 89360 train: 0.0772554874420166 elapsed, loss: 1.153051e-05\n",
      "step: 89370 train: 0.08044028282165527 elapsed, loss: 8.909852e-06\n",
      "step: 89380 train: 0.07379794120788574 elapsed, loss: 9.307094e-06\n",
      "step: 89390 train: 0.079315185546875 elapsed, loss: 6.6332414e-06\n",
      "step: 89400 train: 0.08339142799377441 elapsed, loss: 3.060763e-06\n",
      "step: 89410 train: 0.08505392074584961 elapsed, loss: 3.0021108e-06\n",
      "step: 89420 train: 0.07700586318969727 elapsed, loss: 2.8614816e-06\n",
      "step: 89430 train: 0.08513593673706055 elapsed, loss: 1.9483234e-06\n",
      "step: 89440 train: 0.07932209968566895 elapsed, loss: 2.1425042e-06\n",
      "step: 89450 train: 0.0768439769744873 elapsed, loss: 1.6591489e-06\n",
      "step: 89460 train: 0.07866144180297852 elapsed, loss: 1.2102527e-06\n",
      "step: 89470 train: 0.08117341995239258 elapsed, loss: 1.2768421e-06\n",
      "step: 89480 train: 0.08376073837280273 elapsed, loss: 9.499479e-07\n",
      "step: 89490 train: 0.0777592658996582 elapsed, loss: 2.1015258e-06\n",
      "step: 89500 train: 0.07520222663879395 elapsed, loss: 1.1352813e-06\n",
      "step: 89510 train: 0.08211278915405273 elapsed, loss: 1.1175863e-06\n",
      "step: 89520 train: 0.08077669143676758 elapsed, loss: 1.1953516e-06\n",
      "step: 89530 train: 0.08278560638427734 elapsed, loss: 8.6007583e-07\n",
      "step: 89540 train: 0.08299899101257324 elapsed, loss: 9.1502386e-07\n",
      "step: 89550 train: 0.0829014778137207 elapsed, loss: 9.071076e-07\n",
      "step: 89560 train: 0.07896232604980469 elapsed, loss: 1.1362126e-06\n",
      "step: 89570 train: 0.08911395072937012 elapsed, loss: 8.1909775e-07\n",
      "step: 89580 train: 0.07751035690307617 elapsed, loss: 1.1236398e-06\n",
      "step: 89590 train: 0.08773446083068848 elapsed, loss: 1.078005e-06\n",
      "step: 89600 train: 0.08222365379333496 elapsed, loss: 8.828933e-07\n",
      "step: 89610 train: 0.08083128929138184 elapsed, loss: 8.8149625e-07\n",
      "step: 89620 train: 0.07816672325134277 elapsed, loss: 1.4412202e-06\n",
      "step: 89630 train: 0.07999897003173828 elapsed, loss: 1.2461086e-06\n",
      "step: 89640 train: 0.07764911651611328 elapsed, loss: 1.3266678e-06\n",
      "step: 89650 train: 0.07471442222595215 elapsed, loss: 1.5860408e-06\n",
      "step: 89660 train: 0.08366560935974121 elapsed, loss: 1.1268995e-06\n",
      "step: 89670 train: 0.07464408874511719 elapsed, loss: 1.6735835e-06\n",
      "step: 89680 train: 0.07608318328857422 elapsed, loss: 1.7257378e-06\n",
      "step: 89690 train: 0.08069276809692383 elapsed, loss: 3.194998e-05\n",
      "step: 89700 train: 0.08355331420898438 elapsed, loss: 3.035173e-06\n",
      "step: 89710 train: 0.08483076095581055 elapsed, loss: 3.09105e-06\n",
      "step: 89720 train: 0.07626700401306152 elapsed, loss: 3.0179444e-06\n",
      "step: 89730 train: 0.08386945724487305 elapsed, loss: 2.9918672e-06\n",
      "step: 89740 train: 0.0858917236328125 elapsed, loss: 1.546925e-06\n",
      "step: 89750 train: 0.08023905754089355 elapsed, loss: 2.16439e-06\n",
      "step: 89760 train: 0.0845801830291748 elapsed, loss: 1.2502991e-06\n",
      "step: 89770 train: 0.07628631591796875 elapsed, loss: 1.6577524e-06\n",
      "step: 89780 train: 0.07975506782531738 elapsed, loss: 1.4011734e-06\n",
      "step: 89790 train: 0.07911252975463867 elapsed, loss: 1.3108348e-06\n",
      "step: 89800 train: 0.0863196849822998 elapsed, loss: 1.0374927e-06\n",
      "step: 89810 train: 0.07421731948852539 elapsed, loss: 3.3154993e-06\n",
      "step: 89820 train: 0.07639408111572266 elapsed, loss: 1.5767275e-06\n",
      "step: 89830 train: 0.08389663696289062 elapsed, loss: 1.2726512e-06\n",
      "step: 89840 train: 0.08009219169616699 elapsed, loss: 1.2051305e-06\n",
      "step: 89850 train: 0.08882951736450195 elapsed, loss: 1.0505312e-06\n",
      "step: 89860 train: 0.08607983589172363 elapsed, loss: 1.2307419e-06\n",
      "step: 89870 train: 0.08048224449157715 elapsed, loss: 1.3541419e-06\n",
      "step: 89880 train: 0.07838678359985352 elapsed, loss: 1.3373781e-06\n",
      "step: 89890 train: 0.08365797996520996 elapsed, loss: 1.785343e-06\n",
      "step: 89900 train: 0.08130097389221191 elapsed, loss: 1.2749767e-06\n",
      "step: 89910 train: 0.08023834228515625 elapsed, loss: 1.3201483e-06\n",
      "step: 89920 train: 0.07536149024963379 elapsed, loss: 1.6745162e-06\n",
      "step: 89930 train: 0.07888674736022949 elapsed, loss: 1.6852255e-06\n",
      "step: 89940 train: 0.0829777717590332 elapsed, loss: 1.5897659e-06\n",
      "step: 89950 train: 0.08402657508850098 elapsed, loss: 1.6684626e-06\n",
      "step: 89960 train: 0.08018612861633301 elapsed, loss: 2.0130503e-06\n",
      "step: 89970 train: 0.07719969749450684 elapsed, loss: 2.7641604e-06\n",
      "step: 89980 train: 0.07939958572387695 elapsed, loss: 1.7397081e-06\n",
      "step: 89990 train: 0.08011960983276367 elapsed, loss: 1.4868546e-06\n",
      "step: 90000 train: 0.08222222328186035 elapsed, loss: 1.6670646e-06\n",
      "step: 90010 train: 0.08608555793762207 elapsed, loss: 1.9283011e-06\n",
      "step: 90020 train: 0.08604884147644043 elapsed, loss: 2.4703131e-06\n",
      "step: 90030 train: 0.08000993728637695 elapsed, loss: 2.2468105e-06\n",
      "step: 90040 train: 0.07668113708496094 elapsed, loss: 1.7811524e-06\n",
      "step: 90050 train: 0.07901620864868164 elapsed, loss: 3.149256e-06\n",
      "step: 90060 train: 0.08149290084838867 elapsed, loss: 1.7229446e-06\n",
      "step: 90070 train: 0.09224987030029297 elapsed, loss: 1.218169e-06\n",
      "step: 90080 train: 0.08343958854675293 elapsed, loss: 1.6931426e-06\n",
      "step: 90090 train: 0.08269524574279785 elapsed, loss: 1.6298128e-06\n",
      "step: 90100 train: 0.07704401016235352 elapsed, loss: 2.4568249e-06\n",
      "step: 90110 train: 0.08214807510375977 elapsed, loss: 2.353914e-06\n",
      "step: 90120 train: 0.0778346061706543 elapsed, loss: 1.2725333e-05\n",
      "step: 90130 train: 0.07973623275756836 elapsed, loss: 5.6344725e-06\n",
      "step: 90140 train: 0.08073806762695312 elapsed, loss: 3.8491453e-06\n",
      "step: 90150 train: 0.08144927024841309 elapsed, loss: 3.573476e-06\n",
      "step: 90160 train: 0.08107972145080566 elapsed, loss: 2.2789427e-06\n",
      "step: 90170 train: 0.07939648628234863 elapsed, loss: 2.9127054e-06\n",
      "step: 90180 train: 0.07837963104248047 elapsed, loss: 2.1043206e-06\n",
      "step: 90190 train: 0.08289623260498047 elapsed, loss: 2.3329594e-06\n",
      "step: 90200 train: 0.08287715911865234 elapsed, loss: 1.7019881e-06\n",
      "step: 90210 train: 0.07496881484985352 elapsed, loss: 2.524344e-06\n",
      "step: 90220 train: 0.0836629867553711 elapsed, loss: 1.3653175e-06\n",
      "step: 90230 train: 0.08879256248474121 elapsed, loss: 1.3625236e-06\n",
      "step: 90240 train: 0.08036923408508301 elapsed, loss: 1.3629895e-06\n",
      "step: 90250 train: 0.07933807373046875 elapsed, loss: 2.1294663e-06\n",
      "step: 90260 train: 0.08439230918884277 elapsed, loss: 1.51852e-06\n",
      "step: 90270 train: 0.08519244194030762 elapsed, loss: 2.0959385e-06\n",
      "step: 90280 train: 0.08769559860229492 elapsed, loss: 1.4104778e-06\n",
      "step: 90290 train: 0.08376407623291016 elapsed, loss: 1.3806846e-06\n",
      "step: 90300 train: 0.08108806610107422 elapsed, loss: 1.9967479e-06\n",
      "step: 90310 train: 0.0866551399230957 elapsed, loss: 1.2465741e-06\n",
      "step: 90320 train: 0.09042596817016602 elapsed, loss: 1.414212e-06\n",
      "step: 90330 train: 0.07908773422241211 elapsed, loss: 1.774633e-06\n",
      "step: 90340 train: 0.08223247528076172 elapsed, loss: 2.512238e-06\n",
      "step: 90350 train: 0.07811903953552246 elapsed, loss: 2.4097935e-06\n",
      "step: 90360 train: 0.07680988311767578 elapsed, loss: 2.3925625e-06\n",
      "step: 90370 train: 0.08469390869140625 elapsed, loss: 1.9217819e-06\n",
      "step: 90380 train: 0.08391046524047852 elapsed, loss: 1.6107208e-06\n",
      "step: 90390 train: 0.07656741142272949 elapsed, loss: 1.952515e-06\n",
      "step: 90400 train: 0.08336925506591797 elapsed, loss: 1.7047838e-06\n",
      "step: 90410 train: 0.08482861518859863 elapsed, loss: 1.6554241e-06\n",
      "step: 90420 train: 0.07747864723205566 elapsed, loss: 1.8565888e-06\n",
      "step: 90430 train: 0.08095502853393555 elapsed, loss: 1.7806868e-06\n",
      "step: 90440 train: 0.0796511173248291 elapsed, loss: 1.7955878e-06\n",
      "step: 90450 train: 0.08321070671081543 elapsed, loss: 1.6838294e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 90460 train: 0.07917976379394531 elapsed, loss: 1.9604317e-06\n",
      "step: 90470 train: 0.08181953430175781 elapsed, loss: 2.8754466e-06\n",
      "step: 90480 train: 0.08422613143920898 elapsed, loss: 2.350653e-06\n",
      "step: 90490 train: 0.09050726890563965 elapsed, loss: 1.554841e-06\n",
      "step: 90500 train: 0.08926582336425781 elapsed, loss: 1.500825e-06\n",
      "step: 90510 train: 0.08036637306213379 elapsed, loss: 0.00033813016\n",
      "step: 90520 train: 0.08152961730957031 elapsed, loss: 0.005751951\n",
      "step: 90530 train: 0.0746924877166748 elapsed, loss: 0.00018594507\n",
      "step: 90540 train: 0.08393144607543945 elapsed, loss: 3.8532824e-05\n",
      "step: 90550 train: 0.08153223991394043 elapsed, loss: 3.542981e-05\n",
      "step: 90560 train: 0.0860745906829834 elapsed, loss: 0.001432025\n",
      "step: 90570 train: 0.07753968238830566 elapsed, loss: 7.013947e-05\n",
      "step: 90580 train: 0.07673788070678711 elapsed, loss: 3.8202863e-05\n",
      "step: 90590 train: 0.08049631118774414 elapsed, loss: 1.2590354e-05\n",
      "step: 90600 train: 0.09116840362548828 elapsed, loss: 1.1689799e-05\n",
      "step: 90610 train: 0.0836021900177002 elapsed, loss: 1.1905286e-05\n",
      "step: 90620 train: 0.07864546775817871 elapsed, loss: 7.703315e-06\n",
      "step: 90630 train: 0.08797359466552734 elapsed, loss: 5.456095e-06\n",
      "step: 90640 train: 0.07703757286071777 elapsed, loss: 5.957174e-06\n",
      "step: 90650 train: 0.07546377182006836 elapsed, loss: 8.532295e-06\n",
      "step: 90660 train: 0.07884073257446289 elapsed, loss: 3.059386e-06\n",
      "step: 90670 train: 0.08070659637451172 elapsed, loss: 2.2402885e-06\n",
      "step: 90680 train: 0.08421015739440918 elapsed, loss: 1.5068784e-06\n",
      "step: 90690 train: 0.07897424697875977 elapsed, loss: 2.16439e-06\n",
      "step: 90700 train: 0.08131837844848633 elapsed, loss: 1.314095e-06\n",
      "step: 90710 train: 0.08094668388366699 elapsed, loss: 1.6065292e-06\n",
      "step: 90720 train: 0.08369803428649902 elapsed, loss: 1.4277161e-06\n",
      "step: 90730 train: 0.08440303802490234 elapsed, loss: 1.8258554e-06\n",
      "step: 90740 train: 0.0814359188079834 elapsed, loss: 1.1972143e-06\n",
      "step: 90750 train: 0.08604145050048828 elapsed, loss: 1.1562358e-06\n",
      "step: 90760 train: 0.0785219669342041 elapsed, loss: 1.5925599e-06\n",
      "step: 90770 train: 0.0833742618560791 elapsed, loss: 1.0435458e-06\n",
      "step: 90780 train: 0.08327245712280273 elapsed, loss: 1.2540247e-06\n",
      "step: 90790 train: 0.08023357391357422 elapsed, loss: 1.4947711e-06\n",
      "step: 90800 train: 0.07766342163085938 elapsed, loss: 1.3415685e-06\n",
      "step: 90810 train: 0.0795583724975586 elapsed, loss: 1.4058302e-06\n",
      "step: 90820 train: 0.07843995094299316 elapsed, loss: 2.8880222e-06\n",
      "step: 90830 train: 0.07954573631286621 elapsed, loss: 1.4700912e-06\n",
      "step: 90840 train: 0.08425378799438477 elapsed, loss: 1.5138633e-06\n",
      "step: 90850 train: 0.07696676254272461 elapsed, loss: 1.5208484e-06\n",
      "step: 90860 train: 0.08533120155334473 elapsed, loss: 2.201642e-06\n",
      "step: 90870 train: 0.08246231079101562 elapsed, loss: 1.1846414e-06\n",
      "step: 90880 train: 0.08402037620544434 elapsed, loss: 1.4025704e-06\n",
      "step: 90890 train: 0.08496594429016113 elapsed, loss: 1.4612438e-06\n",
      "step: 90900 train: 0.08674407005310059 elapsed, loss: 1.3392404e-06\n",
      "step: 90910 train: 0.08109450340270996 elapsed, loss: 1.3872025e-06\n",
      "step: 90920 train: 0.07970166206359863 elapsed, loss: 2.1741694e-06\n",
      "step: 90930 train: 0.08126401901245117 elapsed, loss: 1.1734653e-06\n",
      "step: 90940 train: 0.08294272422790527 elapsed, loss: 2.3847739e-05\n",
      "step: 90950 train: 0.08528375625610352 elapsed, loss: 3.412822e-06\n",
      "step: 90960 train: 0.07922911643981934 elapsed, loss: 3.0142187e-06\n",
      "step: 90970 train: 0.07594084739685059 elapsed, loss: 2.2607824e-06\n",
      "step: 90980 train: 0.0844261646270752 elapsed, loss: 1.8854602e-06\n",
      "step: 90990 train: 0.08966946601867676 elapsed, loss: 1.2707883e-06\n",
      "step: 91000 train: 0.093963623046875 elapsed, loss: 1.1660148e-06\n",
      "step: 91010 train: 0.0763845443725586 elapsed, loss: 2.3269054e-06\n",
      "step: 91020 train: 0.08422207832336426 elapsed, loss: 1.4756793e-06\n",
      "step: 91030 train: 0.08646154403686523 elapsed, loss: 1.3695087e-06\n",
      "step: 91040 train: 0.08860325813293457 elapsed, loss: 1.2111841e-06\n",
      "step: 91050 train: 0.0809483528137207 elapsed, loss: 1.3932568e-06\n",
      "step: 91060 train: 0.0788736343383789 elapsed, loss: 4.7478534e-06\n",
      "step: 91070 train: 0.07890486717224121 elapsed, loss: 2.8473052e-05\n",
      "step: 91080 train: 0.08236956596374512 elapsed, loss: 0.00012114003\n",
      "step: 91090 train: 0.08595824241638184 elapsed, loss: 4.4974226e-05\n",
      "step: 91100 train: 0.08336353302001953 elapsed, loss: 2.13782e-05\n",
      "step: 91110 train: 0.07547879219055176 elapsed, loss: 2.6976415e-05\n",
      "step: 91120 train: 0.08112525939941406 elapsed, loss: 2.0799442e-05\n",
      "step: 91130 train: 0.08019614219665527 elapsed, loss: 1.534581e-05\n",
      "step: 91140 train: 0.07672715187072754 elapsed, loss: 8.741255e-06\n",
      "step: 91150 train: 0.07848596572875977 elapsed, loss: 6.739944e-06\n",
      "step: 91160 train: 0.08377265930175781 elapsed, loss: 4.192333e-06\n",
      "step: 91170 train: 0.08770751953125 elapsed, loss: 3.7085147e-06\n",
      "step: 91180 train: 0.08438324928283691 elapsed, loss: 3.5115409e-06\n",
      "step: 91190 train: 0.0855550765991211 elapsed, loss: 1.9459942e-06\n",
      "step: 91200 train: 0.08318066596984863 elapsed, loss: 1.7876714e-06\n",
      "step: 91210 train: 0.08498692512512207 elapsed, loss: 1.7336547e-06\n",
      "step: 91220 train: 0.0831289291381836 elapsed, loss: 1.0598444e-06\n",
      "step: 91230 train: 0.08180975914001465 elapsed, loss: 1.5958183e-06\n",
      "step: 91240 train: 0.07924580574035645 elapsed, loss: 1.5259703e-06\n",
      "step: 91250 train: 0.08587765693664551 elapsed, loss: 9.4808587e-07\n",
      "step: 91260 train: 0.07755565643310547 elapsed, loss: 1.4291131e-06\n",
      "step: 91270 train: 0.0780177116394043 elapsed, loss: 9.951175e-07\n",
      "step: 91280 train: 0.07899832725524902 elapsed, loss: 1.5473902e-06\n",
      "step: 91290 train: 0.08409476280212402 elapsed, loss: 1.1776565e-06\n",
      "step: 91300 train: 0.08448362350463867 elapsed, loss: 1.3285304e-06\n",
      "step: 91310 train: 0.08223867416381836 elapsed, loss: 1.1594957e-06\n",
      "step: 91320 train: 0.08254766464233398 elapsed, loss: 1.3364465e-06\n",
      "step: 91330 train: 0.08381938934326172 elapsed, loss: 1.0849901e-06\n",
      "step: 91340 train: 0.07758617401123047 elapsed, loss: 1.3099038e-06\n",
      "step: 91350 train: 0.08301377296447754 elapsed, loss: 1.0831272e-06\n",
      "step: 91360 train: 0.08058524131774902 elapsed, loss: 1.037027e-06\n",
      "step: 91370 train: 0.08722949028015137 elapsed, loss: 1.291277e-06\n",
      "step: 91380 train: 0.07662272453308105 elapsed, loss: 1.3480883e-06\n",
      "step: 91390 train: 0.08345508575439453 elapsed, loss: 1.4216627e-06\n",
      "step: 91400 train: 0.08212089538574219 elapsed, loss: 1.4994279e-06\n",
      "step: 91410 train: 0.0835564136505127 elapsed, loss: 1.1553047e-06\n",
      "step: 91420 train: 0.08768272399902344 elapsed, loss: 1.1171205e-06\n",
      "step: 91430 train: 0.08417487144470215 elapsed, loss: 1.1674119e-06\n",
      "step: 91440 train: 0.08256244659423828 elapsed, loss: 1.6256215e-06\n",
      "step: 91450 train: 0.07867908477783203 elapsed, loss: 1.6805697e-06\n",
      "step: 91460 train: 0.07949614524841309 elapsed, loss: 1.3159572e-06\n",
      "step: 91470 train: 0.08303713798522949 elapsed, loss: 1.4961681e-06\n",
      "step: 91480 train: 0.07749414443969727 elapsed, loss: 1.522711e-06\n",
      "step: 91490 train: 0.07340884208679199 elapsed, loss: 2.484764e-06\n",
      "step: 91500 train: 0.07543516159057617 elapsed, loss: 2.5821584e-05\n",
      "step: 91510 train: 0.08083772659301758 elapsed, loss: 8.637365e-06\n",
      "step: 91520 train: 0.082427978515625 elapsed, loss: 0.037029132\n",
      "step: 91530 train: 0.07524776458740234 elapsed, loss: 7.244047e-05\n",
      "step: 91540 train: 0.08122920989990234 elapsed, loss: 2.8409462e-05\n",
      "step: 91550 train: 0.08220911026000977 elapsed, loss: 1.5507609e-05\n",
      "step: 91560 train: 0.08269906044006348 elapsed, loss: 1.48361105e-05\n",
      "step: 91570 train: 0.07680964469909668 elapsed, loss: 8.725949e-06\n",
      "step: 91580 train: 0.08220100402832031 elapsed, loss: 9.063525e-06\n",
      "step: 91590 train: 0.07842493057250977 elapsed, loss: 5.7737043e-06\n",
      "step: 91600 train: 0.07767343521118164 elapsed, loss: 4.9550817e-06\n",
      "step: 91610 train: 0.07897305488586426 elapsed, loss: 5.810492e-06\n",
      "step: 91620 train: 0.07838559150695801 elapsed, loss: 3.3457668e-06\n",
      "step: 91630 train: 0.07819032669067383 elapsed, loss: 3.2335427e-06\n",
      "step: 91640 train: 0.08345365524291992 elapsed, loss: 2.4964052e-06\n",
      "step: 91650 train: 0.0821073055267334 elapsed, loss: 1.6316751e-06\n",
      "step: 91660 train: 0.0924375057220459 elapsed, loss: 1.4980283e-06\n",
      "step: 91670 train: 0.08092713356018066 elapsed, loss: 1.5390088e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 91680 train: 0.0919501781463623 elapsed, loss: 1.1562355e-06\n",
      "step: 91690 train: 0.0818781852722168 elapsed, loss: 1.1790535e-06\n",
      "step: 91700 train: 0.0812375545501709 elapsed, loss: 1.2367952e-06\n",
      "step: 91710 train: 0.08036923408508301 elapsed, loss: 1.6461108e-06\n",
      "step: 91720 train: 0.07961392402648926 elapsed, loss: 1.4118838e-06\n",
      "step: 91730 train: 0.08212471008300781 elapsed, loss: 1.5925598e-06\n",
      "step: 91740 train: 0.08047342300415039 elapsed, loss: 1.161824e-06\n",
      "step: 91750 train: 0.09198164939880371 elapsed, loss: 1.027248e-06\n",
      "step: 91760 train: 0.08608198165893555 elapsed, loss: 1.0216602e-06\n",
      "step: 91770 train: 0.07831311225891113 elapsed, loss: 1.368577e-06\n",
      "step: 91780 train: 0.07865118980407715 elapsed, loss: 1.3732338e-06\n",
      "step: 91790 train: 0.08706235885620117 elapsed, loss: 1.1469223e-06\n",
      "step: 91800 train: 0.08600831031799316 elapsed, loss: 1.334118e-06\n",
      "step: 91810 train: 0.08051729202270508 elapsed, loss: 1.0868526e-06\n",
      "step: 91820 train: 0.07416224479675293 elapsed, loss: 1.4496022e-06\n",
      "step: 91830 train: 0.0889291763305664 elapsed, loss: 1.4780073e-06\n",
      "step: 91840 train: 0.0886225700378418 elapsed, loss: 1.656821e-06\n",
      "step: 91850 train: 0.0837240219116211 elapsed, loss: 1.9152621e-06\n",
      "step: 91860 train: 0.08590459823608398 elapsed, loss: 1.2251538e-06\n",
      "step: 91870 train: 0.078582763671875 elapsed, loss: 1.6097895e-06\n",
      "step: 91880 train: 0.08645129203796387 elapsed, loss: 1.1152579e-06\n",
      "step: 91890 train: 0.07921791076660156 elapsed, loss: 2.0689301e-06\n",
      "step: 91900 train: 0.08001494407653809 elapsed, loss: 1.187901e-06\n",
      "step: 91910 train: 0.08615255355834961 elapsed, loss: 1.44448e-06\n",
      "step: 91920 train: 0.08269858360290527 elapsed, loss: 1.3997764e-06\n",
      "step: 91930 train: 0.07406997680664062 elapsed, loss: 1.7951222e-06\n",
      "step: 91940 train: 0.08352828025817871 elapsed, loss: 1.5106037e-06\n",
      "step: 91950 train: 0.08479905128479004 elapsed, loss: 1.2987282e-06\n",
      "step: 91960 train: 0.08107209205627441 elapsed, loss: 1.6991956e-06\n",
      "step: 91970 train: 0.08039307594299316 elapsed, loss: 1.6139802e-06\n",
      "step: 91980 train: 0.08061599731445312 elapsed, loss: 1.6274798e-06\n",
      "step: 91990 train: 0.07314443588256836 elapsed, loss: 2.4093274e-06\n",
      "step: 92000 train: 0.08186459541320801 elapsed, loss: 0.00034861936\n",
      "step: 92010 train: 0.08061671257019043 elapsed, loss: 6.6852066e-05\n",
      "step: 92020 train: 0.08419060707092285 elapsed, loss: 2.367705e-05\n",
      "step: 92030 train: 0.08265042304992676 elapsed, loss: 2.6734877e-05\n",
      "step: 92040 train: 0.08358883857727051 elapsed, loss: 1.7682585e-05\n",
      "step: 92050 train: 0.08192062377929688 elapsed, loss: 1.3490813e-05\n",
      "step: 92060 train: 0.09490203857421875 elapsed, loss: 6.337125e-06\n",
      "step: 92070 train: 0.08025145530700684 elapsed, loss: 0.01677009\n",
      "step: 92080 train: 0.08269453048706055 elapsed, loss: 0.00137958\n",
      "step: 92090 train: 0.0771174430847168 elapsed, loss: 7.0141265e-05\n",
      "step: 92100 train: 0.08128690719604492 elapsed, loss: 4.3541302e-05\n",
      "step: 92110 train: 0.0770714282989502 elapsed, loss: 3.9850987e-05\n",
      "step: 92120 train: 0.07688283920288086 elapsed, loss: 2.340006e-05\n",
      "step: 92130 train: 0.08470392227172852 elapsed, loss: 1.7308745e-05\n",
      "step: 92140 train: 0.0750572681427002 elapsed, loss: 1.2910283e-05\n",
      "step: 92150 train: 0.0789327621459961 elapsed, loss: 1.4284741e-05\n",
      "step: 92160 train: 0.08345508575439453 elapsed, loss: 7.6237366e-06\n",
      "step: 92170 train: 0.08000755310058594 elapsed, loss: 5.7620596e-06\n",
      "step: 92180 train: 0.09184527397155762 elapsed, loss: 3.4593854e-06\n",
      "step: 92190 train: 0.08008265495300293 elapsed, loss: 4.4237663e-06\n",
      "step: 92200 train: 0.08451986312866211 elapsed, loss: 2.4759138e-06\n",
      "step: 92210 train: 0.07602214813232422 elapsed, loss: 2.2398265e-06\n",
      "step: 92220 train: 0.08166670799255371 elapsed, loss: 1.5157256e-06\n",
      "step: 92230 train: 0.07851481437683105 elapsed, loss: 1.960431e-06\n",
      "step: 92240 train: 0.0788125991821289 elapsed, loss: 1.7276013e-06\n",
      "step: 92250 train: 0.08003520965576172 elapsed, loss: 1.4957021e-06\n",
      "step: 92260 train: 0.0818643569946289 elapsed, loss: 1.0323699e-06\n",
      "step: 92270 train: 0.08330321311950684 elapsed, loss: 1.102685e-06\n",
      "step: 92280 train: 0.08315420150756836 elapsed, loss: 1.7391941e-06\n",
      "step: 92290 train: 0.0765371322631836 elapsed, loss: 1.3033846e-06\n",
      "step: 92300 train: 0.07528066635131836 elapsed, loss: 1.2307416e-06\n",
      "step: 92310 train: 0.07807683944702148 elapsed, loss: 1.0812648e-06\n",
      "step: 92320 train: 0.07366561889648438 elapsed, loss: 1.5865062e-06\n",
      "step: 92330 train: 0.07944083213806152 elapsed, loss: 1.5529788e-06\n",
      "step: 92340 train: 0.0774531364440918 elapsed, loss: 1.4249222e-06\n",
      "step: 92350 train: 0.08474588394165039 elapsed, loss: 9.3923825e-07\n",
      "step: 92360 train: 0.0852346420288086 elapsed, loss: 9.899952e-07\n",
      "step: 92370 train: 0.07723522186279297 elapsed, loss: 1.2456429e-06\n",
      "step: 92380 train: 0.08352518081665039 elapsed, loss: 1.4593809e-06\n",
      "step: 92390 train: 0.08661770820617676 elapsed, loss: 1.1846414e-06\n",
      "step: 92400 train: 0.08069014549255371 elapsed, loss: 1.3257364e-06\n",
      "step: 92410 train: 0.08525538444519043 elapsed, loss: 1.0593786e-06\n",
      "step: 92420 train: 0.08833813667297363 elapsed, loss: 3.946928e-06\n",
      "step: 92430 train: 0.08944225311279297 elapsed, loss: 1.1487855e-06\n",
      "step: 92440 train: 0.08482933044433594 elapsed, loss: 1.0649667e-06\n",
      "step: 92450 train: 0.09191155433654785 elapsed, loss: 6.8469776e-06\n",
      "step: 92460 train: 0.08458471298217773 elapsed, loss: 2.2430863e-06\n",
      "step: 92470 train: 0.07449889183044434 elapsed, loss: 2.8875588e-06\n",
      "step: 92480 train: 0.08250927925109863 elapsed, loss: 2.1574022e-06\n",
      "step: 92490 train: 0.07929110527038574 elapsed, loss: 1.5599632e-06\n",
      "step: 92500 train: 0.08118510246276855 elapsed, loss: 1.9716067e-06\n",
      "step: 92510 train: 0.08751392364501953 elapsed, loss: 1.192092e-06\n",
      "step: 92520 train: 0.0841073989868164 elapsed, loss: 1.0766082e-06\n",
      "step: 92530 train: 0.08097457885742188 elapsed, loss: 1.1767252e-06\n",
      "step: 92540 train: 0.08370614051818848 elapsed, loss: 1.5632233e-06\n",
      "step: 92550 train: 0.07668185234069824 elapsed, loss: 1.4011736e-06\n",
      "step: 92560 train: 0.08101606369018555 elapsed, loss: 1.0780051e-06\n",
      "step: 92570 train: 0.07981467247009277 elapsed, loss: 1.9031544e-06\n",
      "step: 92580 train: 0.07904243469238281 elapsed, loss: 2.0628768e-06\n",
      "step: 92590 train: 0.07565975189208984 elapsed, loss: 3.4264376e-05\n",
      "step: 92600 train: 0.07855558395385742 elapsed, loss: 0.0003511515\n",
      "step: 92610 train: 0.08281588554382324 elapsed, loss: 6.147995e-05\n",
      "step: 92620 train: 0.08203339576721191 elapsed, loss: 4.4469398e-05\n",
      "step: 92630 train: 0.08305811882019043 elapsed, loss: 2.829292e-05\n",
      "step: 92640 train: 0.07626056671142578 elapsed, loss: 2.8548473e-05\n",
      "step: 92650 train: 0.08153033256530762 elapsed, loss: 2.1747062e-05\n",
      "step: 92660 train: 0.0787653923034668 elapsed, loss: 2.7275802e-05\n",
      "step: 92670 train: 0.08665204048156738 elapsed, loss: 7.952036e-06\n",
      "step: 92680 train: 0.08152484893798828 elapsed, loss: 8.561577e-06\n",
      "step: 92690 train: 0.07836556434631348 elapsed, loss: 7.1720683e-06\n",
      "step: 92700 train: 0.08175325393676758 elapsed, loss: 3.393265e-06\n",
      "step: 92710 train: 0.08452630043029785 elapsed, loss: 2.744128e-06\n",
      "step: 92720 train: 0.08288145065307617 elapsed, loss: 2.0470438e-06\n",
      "step: 92730 train: 0.07692289352416992 elapsed, loss: 2.6621751e-06\n",
      "step: 92740 train: 0.08194780349731445 elapsed, loss: 1.525504e-06\n",
      "step: 92750 train: 0.07868027687072754 elapsed, loss: 1.7499501e-06\n",
      "step: 92760 train: 0.0824732780456543 elapsed, loss: 1.1827784e-06\n",
      "step: 92770 train: 0.08594560623168945 elapsed, loss: 1.2549561e-06\n",
      "step: 92780 train: 0.08166384696960449 elapsed, loss: 9.578646e-07\n",
      "step: 92790 train: 0.07918953895568848 elapsed, loss: 1.272185e-06\n",
      "step: 92800 train: 0.07752466201782227 elapsed, loss: 9.932546e-07\n",
      "step: 92810 train: 0.0896754264831543 elapsed, loss: 8.926719e-07\n",
      "step: 92820 train: 0.0776524543762207 elapsed, loss: 1.1846414e-06\n",
      "step: 92830 train: 0.08264684677124023 elapsed, loss: 1.2381922e-06\n",
      "step: 92840 train: 0.07924628257751465 elapsed, loss: 1.697764e-06\n",
      "step: 92850 train: 0.08293795585632324 elapsed, loss: 0.00024152314\n",
      "step: 92860 train: 0.08314013481140137 elapsed, loss: 3.950177e-05\n",
      "step: 92870 train: 0.08986115455627441 elapsed, loss: 2.4717923e-05\n",
      "step: 92880 train: 0.0795438289642334 elapsed, loss: 1.3285535e-05\n",
      "step: 92890 train: 0.08184313774108887 elapsed, loss: 6.148548e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 92900 train: 0.08093142509460449 elapsed, loss: 7.220483e-06\n",
      "step: 92910 train: 0.08597707748413086 elapsed, loss: 5.5716077e-06\n",
      "step: 92920 train: 0.07944869995117188 elapsed, loss: 5.7895404e-06\n",
      "step: 92930 train: 0.08246541023254395 elapsed, loss: 3.8249227e-06\n",
      "step: 92940 train: 0.0907740592956543 elapsed, loss: 2.656591e-06\n",
      "step: 92950 train: 0.07808709144592285 elapsed, loss: 2.9960574e-06\n",
      "step: 92960 train: 0.08609843254089355 elapsed, loss: 8.4588255e-06\n",
      "step: 92970 train: 0.08688688278198242 elapsed, loss: 1.6917439e-06\n",
      "step: 92980 train: 0.07950615882873535 elapsed, loss: 1.6493698e-06\n",
      "step: 92990 train: 0.08352017402648926 elapsed, loss: 1.3010565e-06\n",
      "step: 93000 train: 0.08437514305114746 elapsed, loss: 1.2987281e-06\n",
      "step: 93010 train: 0.0884256362915039 elapsed, loss: 9.643838e-07\n",
      "step: 93020 train: 0.08699440956115723 elapsed, loss: 1.4468079e-06\n",
      "step: 93030 train: 0.08507370948791504 elapsed, loss: 7.739286e-07\n",
      "step: 93040 train: 0.07690858840942383 elapsed, loss: 1.8221303e-06\n",
      "step: 93050 train: 0.08365178108215332 elapsed, loss: 1.2936058e-06\n",
      "step: 93060 train: 0.08288240432739258 elapsed, loss: 0.002148071\n",
      "step: 93070 train: 0.07725000381469727 elapsed, loss: 2.9524252e-05\n",
      "step: 93080 train: 0.07348918914794922 elapsed, loss: 2.1658188e-05\n",
      "step: 93090 train: 0.07812762260437012 elapsed, loss: 2.197166e-05\n",
      "step: 93100 train: 0.08379197120666504 elapsed, loss: 5.664269e-06\n",
      "step: 93110 train: 0.08362913131713867 elapsed, loss: 4.0940795e-06\n",
      "step: 93120 train: 0.07755374908447266 elapsed, loss: 6.254191e-06\n",
      "step: 93130 train: 0.08554649353027344 elapsed, loss: 3.880805e-06\n",
      "step: 93140 train: 0.07950043678283691 elapsed, loss: 3.679176e-06\n",
      "step: 93150 train: 0.08051943778991699 elapsed, loss: 2.8633438e-06\n",
      "step: 93160 train: 0.08248376846313477 elapsed, loss: 1.9264378e-06\n",
      "step: 93170 train: 0.08090686798095703 elapsed, loss: 2.388358e-06\n",
      "step: 93180 train: 0.07782578468322754 elapsed, loss: 1.8053663e-06\n",
      "step: 93190 train: 0.08014345169067383 elapsed, loss: 1.563688e-06\n",
      "step: 93200 train: 0.08437728881835938 elapsed, loss: 1.378821e-06\n",
      "step: 93210 train: 0.08121895790100098 elapsed, loss: 2.0405118e-06\n",
      "step: 93220 train: 0.08459186553955078 elapsed, loss: 1.4360978e-06\n",
      "step: 93230 train: 0.08166074752807617 elapsed, loss: 1.3327215e-06\n",
      "step: 93240 train: 0.07837533950805664 elapsed, loss: 1.253559e-06\n",
      "step: 93250 train: 0.08301401138305664 elapsed, loss: 1.0784703e-06\n",
      "step: 93260 train: 0.07768440246582031 elapsed, loss: 1.0291108e-06\n",
      "step: 93270 train: 0.07810044288635254 elapsed, loss: 1.4943057e-06\n",
      "step: 93280 train: 0.07913994789123535 elapsed, loss: 1.2749795e-06\n",
      "step: 93290 train: 0.07769513130187988 elapsed, loss: 1.4859236e-06\n",
      "step: 93300 train: 0.08307361602783203 elapsed, loss: 9.411008e-07\n",
      "step: 93310 train: 0.08219695091247559 elapsed, loss: 1.8738185e-06\n",
      "step: 93320 train: 0.07870006561279297 elapsed, loss: 1.7550756e-06\n",
      "step: 93330 train: 0.08022165298461914 elapsed, loss: 1.1874351e-06\n",
      "step: 93340 train: 0.07839751243591309 elapsed, loss: 1.969278e-06\n",
      "step: 93350 train: 0.08010578155517578 elapsed, loss: 1.8272524e-06\n",
      "step: 93360 train: 0.08672332763671875 elapsed, loss: 1.405364e-06\n",
      "step: 93370 train: 0.08404421806335449 elapsed, loss: 1.1585644e-06\n",
      "step: 93380 train: 0.08471918106079102 elapsed, loss: 1.1259679e-06\n",
      "step: 93390 train: 0.07995939254760742 elapsed, loss: 1.2861553e-06\n",
      "step: 93400 train: 0.07775712013244629 elapsed, loss: 1.8049011e-06\n",
      "step: 93410 train: 0.07892251014709473 elapsed, loss: 1.3262013e-06\n",
      "step: 93420 train: 0.0791161060333252 elapsed, loss: 1.3657823e-06\n",
      "step: 93430 train: 0.07689142227172852 elapsed, loss: 1.9590345e-06\n",
      "step: 93440 train: 0.07932758331298828 elapsed, loss: 1.5799872e-06\n",
      "step: 93450 train: 0.07606387138366699 elapsed, loss: 2.0516986e-06\n",
      "step: 93460 train: 0.0835411548614502 elapsed, loss: 1.1748625e-06\n",
      "step: 93470 train: 0.08158230781555176 elapsed, loss: 1.1338843e-06\n",
      "step: 93480 train: 0.08076214790344238 elapsed, loss: 1.7136315e-06\n",
      "step: 93490 train: 0.07930564880371094 elapsed, loss: 1.8016415e-06\n",
      "step: 93500 train: 0.08954238891601562 elapsed, loss: 1.6223623e-06\n",
      "step: 93510 train: 0.08754396438598633 elapsed, loss: 1.7001274e-06\n",
      "step: 93520 train: 0.08520698547363281 elapsed, loss: 1.4840609e-06\n",
      "step: 93530 train: 0.0799858570098877 elapsed, loss: 2.082434e-06\n",
      "step: 93540 train: 0.07590937614440918 elapsed, loss: 1.8393594e-06\n",
      "step: 93550 train: 0.08267903327941895 elapsed, loss: 1.5953541e-06\n",
      "step: 93560 train: 0.08172011375427246 elapsed, loss: 1.6665997e-06\n",
      "step: 93570 train: 0.08237552642822266 elapsed, loss: 1.9078118e-06\n",
      "step: 93580 train: 0.08442234992980957 elapsed, loss: 1.6763788e-06\n",
      "step: 93590 train: 0.08223676681518555 elapsed, loss: 1.5217795e-06\n",
      "step: 93600 train: 0.08068394660949707 elapsed, loss: 6.8451527e-06\n",
      "step: 93610 train: 0.07987213134765625 elapsed, loss: 2.8768495e-06\n",
      "step: 93620 train: 0.08687233924865723 elapsed, loss: 2.020034e-06\n",
      "step: 93630 train: 0.07871294021606445 elapsed, loss: 2.3446005e-06\n",
      "step: 93640 train: 0.08103561401367188 elapsed, loss: 1.4337699e-06\n",
      "step: 93650 train: 0.0774078369140625 elapsed, loss: 1.7792898e-06\n",
      "step: 93660 train: 0.07969188690185547 elapsed, loss: 1.6819657e-06\n",
      "step: 93670 train: 0.08572530746459961 elapsed, loss: 1.4416856e-06\n",
      "step: 93680 train: 0.09293270111083984 elapsed, loss: 2.8409906e-06\n",
      "step: 93690 train: 0.08229279518127441 elapsed, loss: 1.9152626e-06\n",
      "step: 93700 train: 0.07536125183105469 elapsed, loss: 2.402342e-06\n",
      "step: 93710 train: 0.08318734169006348 elapsed, loss: 1.9436675e-06\n",
      "step: 93720 train: 0.08202147483825684 elapsed, loss: 1.7932595e-06\n",
      "step: 93730 train: 0.08501935005187988 elapsed, loss: 1.6055985e-06\n",
      "step: 93740 train: 0.08053874969482422 elapsed, loss: 2.5043225e-06\n",
      "step: 93750 train: 0.0826263427734375 elapsed, loss: 1.5455274e-06\n",
      "step: 93760 train: 0.08394312858581543 elapsed, loss: 1.6572867e-06\n",
      "step: 93770 train: 0.08395719528198242 elapsed, loss: 0.00048540306\n",
      "step: 93780 train: 0.08199286460876465 elapsed, loss: 0.00028586737\n",
      "step: 93790 train: 0.07889723777770996 elapsed, loss: 5.3205273e-05\n",
      "step: 93800 train: 0.08575606346130371 elapsed, loss: 3.0598752e-05\n",
      "step: 93810 train: 0.08403205871582031 elapsed, loss: 1.333728e-05\n",
      "step: 93820 train: 0.08017325401306152 elapsed, loss: 1.3497003e-05\n",
      "step: 93830 train: 0.08129620552062988 elapsed, loss: 1.27861695e-05\n",
      "step: 93840 train: 0.08106160163879395 elapsed, loss: 8.178507e-06\n",
      "step: 93850 train: 0.07958340644836426 elapsed, loss: 7.6051265e-06\n",
      "step: 93860 train: 0.08174824714660645 elapsed, loss: 4.200714e-06\n",
      "step: 93870 train: 0.08037161827087402 elapsed, loss: 5.3033928e-06\n",
      "step: 93880 train: 0.08279085159301758 elapsed, loss: 2.6728908e-06\n",
      "step: 93890 train: 0.07951879501342773 elapsed, loss: 2.5583386e-06\n",
      "step: 93900 train: 0.08762049674987793 elapsed, loss: 2.2174754e-06\n",
      "step: 93910 train: 0.08501648902893066 elapsed, loss: 2.3897687e-06\n",
      "step: 93920 train: 0.07855939865112305 elapsed, loss: 2.3450666e-06\n",
      "step: 93930 train: 0.07937955856323242 elapsed, loss: 1.7108375e-06\n",
      "step: 93940 train: 0.08000898361206055 elapsed, loss: 1.5902303e-06\n",
      "step: 93950 train: 0.08092355728149414 elapsed, loss: 1.2568187e-06\n",
      "step: 93960 train: 0.07661747932434082 elapsed, loss: 2.0326088e-06\n",
      "step: 93970 train: 0.0818185806274414 elapsed, loss: 2.0000127e-06\n",
      "step: 93980 train: 0.08791804313659668 elapsed, loss: 2.9769617e-06\n",
      "step: 93990 train: 0.08106803894042969 elapsed, loss: 1.6493697e-06\n",
      "step: 94000 train: 0.07687735557556152 elapsed, loss: 1.8612438e-06\n",
      "step: 94010 train: 0.07678723335266113 elapsed, loss: 2.1830106e-06\n",
      "step: 94020 train: 0.07584500312805176 elapsed, loss: 2.3683497e-06\n",
      "step: 94030 train: 0.08629918098449707 elapsed, loss: 1.6223622e-06\n",
      "step: 94040 train: 0.08042025566101074 elapsed, loss: 2.1103742e-06\n",
      "step: 94050 train: 0.08773446083068848 elapsed, loss: 1.9296974e-06\n",
      "step: 94060 train: 0.07969236373901367 elapsed, loss: 1.4724195e-06\n",
      "step: 94070 train: 0.07830524444580078 elapsed, loss: 2.2659042e-06\n",
      "step: 94080 train: 0.078460693359375 elapsed, loss: 2.4577562e-06\n",
      "step: 94090 train: 0.0863487720489502 elapsed, loss: 1.539009e-06\n",
      "step: 94100 train: 0.08202552795410156 elapsed, loss: 2.0954717e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 94110 train: 0.08639121055603027 elapsed, loss: 3.8048672e-06\n",
      "step: 94120 train: 0.07701730728149414 elapsed, loss: 1.917125e-06\n",
      "step: 94130 train: 0.08263373374938965 elapsed, loss: 1.406761e-06\n",
      "step: 94140 train: 0.08503961563110352 elapsed, loss: 1.4491366e-06\n",
      "step: 94150 train: 0.07749509811401367 elapsed, loss: 2.0726554e-06\n",
      "step: 94160 train: 0.08167004585266113 elapsed, loss: 1.6218966e-06\n",
      "step: 94170 train: 0.08156323432922363 elapsed, loss: 1.5399402e-06\n",
      "step: 94180 train: 0.08535647392272949 elapsed, loss: 1.6936076e-06\n",
      "step: 94190 train: 0.07911944389343262 elapsed, loss: 2.6007133e-06\n",
      "step: 94200 train: 0.08122658729553223 elapsed, loss: 2.2933787e-06\n",
      "step: 94210 train: 0.08816671371459961 elapsed, loss: 1.566483e-06\n",
      "step: 94220 train: 0.08193588256835938 elapsed, loss: 2.8032755e-06\n",
      "step: 94230 train: 0.09244418144226074 elapsed, loss: 1.4896491e-06\n",
      "step: 94240 train: 0.07900738716125488 elapsed, loss: 1.69128e-06\n",
      "step: 94250 train: 0.08163070678710938 elapsed, loss: 1.6642716e-06\n",
      "step: 94260 train: 0.07690072059631348 elapsed, loss: 1.881735e-06\n",
      "step: 94270 train: 0.0863962173461914 elapsed, loss: 2.8540271e-06\n",
      "step: 94280 train: 0.08259797096252441 elapsed, loss: 1.6917456e-06\n",
      "step: 94290 train: 0.0863947868347168 elapsed, loss: 2.0056007e-06\n",
      "step: 94300 train: 0.0782308578491211 elapsed, loss: 3.005372e-06\n",
      "step: 94310 train: 0.0786745548248291 elapsed, loss: 2.0619452e-06\n",
      "step: 94320 train: 0.08441162109375 elapsed, loss: 1.5906974e-06\n",
      "step: 94330 train: 0.08243775367736816 elapsed, loss: 2.114563e-06\n",
      "step: 94340 train: 0.08460426330566406 elapsed, loss: 2.1452986e-06\n",
      "step: 94350 train: 0.0789942741394043 elapsed, loss: 2.3799914e-06\n",
      "step: 94360 train: 0.07614660263061523 elapsed, loss: 2.2761492e-06\n",
      "step: 94370 train: 0.08184218406677246 elapsed, loss: 2.203506e-06\n",
      "step: 94380 train: 0.0779421329498291 elapsed, loss: 1.8062981e-06\n",
      "step: 94390 train: 0.08066987991333008 elapsed, loss: 2.7660221e-06\n",
      "step: 94400 train: 0.08281803131103516 elapsed, loss: 1.8677651e-06\n",
      "step: 94410 train: 0.08199667930603027 elapsed, loss: 1.785809e-06\n",
      "step: 94420 train: 0.08271479606628418 elapsed, loss: 2.1066492e-06\n",
      "step: 94430 train: 0.08155012130737305 elapsed, loss: 0.00029277516\n",
      "step: 94440 train: 0.07936787605285645 elapsed, loss: 6.631761e-05\n",
      "step: 94450 train: 0.07743310928344727 elapsed, loss: 0.0003763282\n",
      "step: 94460 train: 0.0821080207824707 elapsed, loss: 7.2123206e-05\n",
      "step: 94470 train: 0.07633519172668457 elapsed, loss: 3.9644103e-05\n",
      "step: 94480 train: 0.08298754692077637 elapsed, loss: 2.835404e-05\n",
      "step: 94490 train: 0.07953953742980957 elapsed, loss: 2.9270988e-05\n",
      "step: 94500 train: 0.08768510818481445 elapsed, loss: 1.1383864e-05\n",
      "step: 94510 train: 0.07833242416381836 elapsed, loss: 1.532374e-05\n",
      "step: 94520 train: 0.08200931549072266 elapsed, loss: 7.3457563e-06\n",
      "step: 94530 train: 0.07682681083679199 elapsed, loss: 9.093341e-06\n",
      "step: 94540 train: 0.07619738578796387 elapsed, loss: 5.332733e-06\n",
      "step: 94550 train: 0.08048009872436523 elapsed, loss: 4.938781e-06\n",
      "step: 94560 train: 0.08504343032836914 elapsed, loss: 2.571842e-06\n",
      "step: 94570 train: 0.08400988578796387 elapsed, loss: 2.8838344e-06\n",
      "step: 94580 train: 0.08007502555847168 elapsed, loss: 2.90198e-06\n",
      "step: 94590 train: 0.08456540107727051 elapsed, loss: 1.8332998e-06\n",
      "step: 94600 train: 0.08104681968688965 elapsed, loss: 1.4519304e-06\n",
      "step: 94610 train: 0.08347058296203613 elapsed, loss: 1.288018e-06\n",
      "step: 94620 train: 0.0800175666809082 elapsed, loss: 3.7965256e-06\n",
      "step: 94630 train: 0.08365869522094727 elapsed, loss: 2.1248093e-06\n",
      "step: 94640 train: 0.08176708221435547 elapsed, loss: 1.9003611e-06\n",
      "step: 94650 train: 0.08648490905761719 elapsed, loss: 1.6512331e-06\n",
      "step: 94660 train: 0.0747377872467041 elapsed, loss: 2.4437813e-06\n",
      "step: 94670 train: 0.07645392417907715 elapsed, loss: 1.8998956e-06\n",
      "step: 94680 train: 0.07861185073852539 elapsed, loss: 1.6493699e-06\n",
      "step: 94690 train: 0.08519220352172852 elapsed, loss: 1.070089e-06\n",
      "step: 94700 train: 0.0858614444732666 elapsed, loss: 1.3299275e-06\n",
      "step: 94710 train: 0.08280706405639648 elapsed, loss: 1.3080414e-06\n",
      "step: 94720 train: 0.0849301815032959 elapsed, loss: 1.2917432e-06\n",
      "step: 94730 train: 0.07920360565185547 elapsed, loss: 1.4393578e-06\n",
      "step: 94740 train: 0.07607603073120117 elapsed, loss: 1.4286476e-06\n",
      "step: 94750 train: 0.0859673023223877 elapsed, loss: 1.2577502e-06\n",
      "step: 94760 train: 0.07873272895812988 elapsed, loss: 1.3899977e-06\n",
      "step: 94770 train: 0.08380603790283203 elapsed, loss: 8.200672e-06\n",
      "step: 94780 train: 0.08349442481994629 elapsed, loss: 1.6051329e-06\n",
      "step: 94790 train: 0.08062624931335449 elapsed, loss: 1.4500661e-06\n",
      "step: 94800 train: 0.07886266708374023 elapsed, loss: 1.790931e-06\n",
      "step: 94810 train: 0.0929865837097168 elapsed, loss: 1.2735821e-06\n",
      "step: 94820 train: 0.0808572769165039 elapsed, loss: 2.0875568e-06\n",
      "step: 94830 train: 0.08373165130615234 elapsed, loss: 1.200008e-06\n",
      "step: 94840 train: 0.08664059638977051 elapsed, loss: 1.3485538e-06\n",
      "step: 94850 train: 0.08065629005432129 elapsed, loss: 1.264735e-06\n",
      "step: 94860 train: 0.08171510696411133 elapsed, loss: 2.2747515e-06\n",
      "step: 94870 train: 0.08172178268432617 elapsed, loss: 1.610255e-06\n",
      "step: 94880 train: 0.0769498348236084 elapsed, loss: 2.2812703e-06\n",
      "step: 94890 train: 0.08014655113220215 elapsed, loss: 1.8691621e-06\n",
      "step: 94900 train: 0.07799625396728516 elapsed, loss: 1.7965192e-06\n",
      "step: 94910 train: 0.0819704532623291 elapsed, loss: 1.4081584e-06\n",
      "step: 94920 train: 0.07962989807128906 elapsed, loss: 7.518944e-06\n",
      "step: 94930 train: 0.07747220993041992 elapsed, loss: 2.336219e-06\n",
      "step: 94940 train: 0.07862329483032227 elapsed, loss: 2.667769e-06\n",
      "step: 94950 train: 0.08423781394958496 elapsed, loss: 1.61724e-06\n",
      "step: 94960 train: 0.07992887496948242 elapsed, loss: 1.367646e-06\n",
      "step: 94970 train: 0.07898640632629395 elapsed, loss: 1.8761428e-06\n",
      "step: 94980 train: 0.08220791816711426 elapsed, loss: 1.4537932e-06\n",
      "step: 94990 train: 0.084197998046875 elapsed, loss: 1.4901148e-06\n",
      "step: 95000 train: 0.08473539352416992 elapsed, loss: 1.862642e-06\n",
      "step: 95010 train: 0.08433818817138672 elapsed, loss: 1.9236443e-06\n",
      "step: 95020 train: 0.08620810508728027 elapsed, loss: 1.4784733e-06\n",
      "step: 95030 train: 0.08053708076477051 elapsed, loss: 2.2458778e-06\n",
      "step: 95040 train: 0.07657623291015625 elapsed, loss: 2.1699784e-06\n",
      "step: 95050 train: 0.08037519454956055 elapsed, loss: 1.7592663e-06\n",
      "step: 95060 train: 0.08020162582397461 elapsed, loss: 1.9273689e-06\n",
      "step: 95070 train: 0.08937335014343262 elapsed, loss: 1.4198e-06\n",
      "step: 95080 train: 0.08631110191345215 elapsed, loss: 2.0386624e-06\n",
      "step: 95090 train: 0.07963395118713379 elapsed, loss: 1.8021071e-06\n",
      "step: 95100 train: 0.08174586296081543 elapsed, loss: 2.2388967e-06\n",
      "step: 95110 train: 0.08334469795227051 elapsed, loss: 1.5031533e-06\n",
      "step: 95120 train: 0.0814826488494873 elapsed, loss: 1.426785e-06\n",
      "step: 95130 train: 0.08427071571350098 elapsed, loss: 1.8058322e-06\n",
      "step: 95140 train: 0.07985043525695801 elapsed, loss: 2.310142e-06\n",
      "step: 95150 train: 0.0826876163482666 elapsed, loss: 0.00037902023\n",
      "step: 95160 train: 0.08262467384338379 elapsed, loss: 1.5559766e-05\n",
      "step: 95170 train: 0.07757091522216797 elapsed, loss: 1.4472895e-05\n",
      "step: 95180 train: 0.08349752426147461 elapsed, loss: 9.777294e-06\n",
      "step: 95190 train: 0.07306814193725586 elapsed, loss: 8.735738e-06\n",
      "step: 95200 train: 0.08902549743652344 elapsed, loss: 3.997224e-06\n",
      "step: 95210 train: 0.08077096939086914 elapsed, loss: 4.3129385e-06\n",
      "step: 95220 train: 0.07724881172180176 elapsed, loss: 3.0510046e-06\n",
      "step: 95230 train: 0.08617568016052246 elapsed, loss: 2.7888395e-06\n",
      "step: 95240 train: 0.08043909072875977 elapsed, loss: 2.3217829e-06\n",
      "step: 95250 train: 0.07598161697387695 elapsed, loss: 2.521086e-06\n",
      "step: 95260 train: 0.08481311798095703 elapsed, loss: 1.5324895e-06\n",
      "step: 95270 train: 0.08438897132873535 elapsed, loss: 1.5962853e-06\n",
      "step: 95280 train: 0.0849151611328125 elapsed, loss: 1.3052475e-06\n",
      "step: 95290 train: 0.08396387100219727 elapsed, loss: 1.4221283e-06\n",
      "step: 95300 train: 0.08159995079040527 elapsed, loss: 1.5087408e-06\n",
      "step: 95310 train: 0.08111739158630371 elapsed, loss: 1.4030361e-06\n",
      "step: 95320 train: 0.08385252952575684 elapsed, loss: 1.3587983e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 95330 train: 0.08022356033325195 elapsed, loss: 1.7029215e-06\n",
      "step: 95340 train: 0.08566451072692871 elapsed, loss: 1.1981456e-06\n",
      "step: 95350 train: 0.08676886558532715 elapsed, loss: 2.3492528e-06\n",
      "step: 95360 train: 0.08527183532714844 elapsed, loss: 1.3494849e-06\n",
      "step: 95370 train: 0.08237695693969727 elapsed, loss: 1.7122343e-06\n",
      "step: 95380 train: 0.07861924171447754 elapsed, loss: 1.98418e-06\n",
      "step: 95390 train: 0.08065557479858398 elapsed, loss: 1.7397088e-06\n",
      "step: 95400 train: 0.08496284484863281 elapsed, loss: 1.9837144e-06\n",
      "step: 95410 train: 0.07862067222595215 elapsed, loss: 1.6847607e-06\n",
      "step: 95420 train: 0.07932066917419434 elapsed, loss: 1.7387772e-06\n",
      "step: 95430 train: 0.08580207824707031 elapsed, loss: 1.4970992e-06\n",
      "step: 95440 train: 0.0853424072265625 elapsed, loss: 1.7578694e-06\n",
      "step: 95450 train: 0.07952022552490234 elapsed, loss: 1.5245737e-06\n",
      "step: 95460 train: 0.07729172706604004 elapsed, loss: 2.242156e-06\n",
      "step: 95470 train: 0.0872960090637207 elapsed, loss: 1.3397064e-06\n",
      "step: 95480 train: 0.0821220874786377 elapsed, loss: 1.6125827e-06\n",
      "step: 95490 train: 0.08418989181518555 elapsed, loss: 1.6135144e-06\n",
      "step: 95500 train: 0.08028793334960938 elapsed, loss: 2.4489086e-06\n",
      "step: 95510 train: 0.08435320854187012 elapsed, loss: 1.4887169e-06\n",
      "step: 95520 train: 0.08391261100769043 elapsed, loss: 1.6363321e-06\n",
      "step: 95530 train: 0.0824120044708252 elapsed, loss: 2.0288835e-06\n",
      "step: 95540 train: 0.08103752136230469 elapsed, loss: 2.4703295e-06\n",
      "step: 95550 train: 0.08938384056091309 elapsed, loss: 1.7113027e-06\n",
      "step: 95560 train: 0.08333611488342285 elapsed, loss: 0.0005102835\n",
      "step: 95570 train: 0.0847616195678711 elapsed, loss: 8.928145e-05\n",
      "step: 95580 train: 0.08278965950012207 elapsed, loss: 4.7698188e-05\n",
      "step: 95590 train: 0.08141636848449707 elapsed, loss: 2.8713966e-05\n",
      "step: 95600 train: 0.08507037162780762 elapsed, loss: 1.8249662e-05\n",
      "step: 95610 train: 0.07772016525268555 elapsed, loss: 1.7970648e-05\n",
      "step: 95620 train: 0.0795755386352539 elapsed, loss: 1.0723597e-05\n",
      "step: 95630 train: 0.08404707908630371 elapsed, loss: 7.1324907e-06\n",
      "step: 95640 train: 0.08032584190368652 elapsed, loss: 6.8055397e-06\n",
      "step: 95650 train: 0.08338403701782227 elapsed, loss: 6.0214343e-06\n",
      "step: 95660 train: 0.08323287963867188 elapsed, loss: 4.6770815e-06\n",
      "step: 95670 train: 0.08614993095397949 elapsed, loss: 2.917361e-06\n",
      "step: 95680 train: 0.07969093322753906 elapsed, loss: 3.856594e-06\n",
      "step: 95690 train: 0.07563543319702148 elapsed, loss: 2.9625298e-06\n",
      "step: 95700 train: 0.07895946502685547 elapsed, loss: 2.0931418e-06\n",
      "step: 95710 train: 0.0840611457824707 elapsed, loss: 1.8179394e-06\n",
      "step: 95720 train: 0.08131718635559082 elapsed, loss: 2.1411017e-06\n",
      "step: 95730 train: 0.08021283149719238 elapsed, loss: 1.4924428e-06\n",
      "step: 95740 train: 0.07652592658996582 elapsed, loss: 1.6102551e-06\n",
      "step: 95750 train: 0.07741975784301758 elapsed, loss: 1.3099038e-06\n",
      "step: 95760 train: 0.08139634132385254 elapsed, loss: 1.5767273e-06\n",
      "step: 95770 train: 0.08099722862243652 elapsed, loss: 1.6009418e-06\n",
      "step: 95780 train: 0.08460712432861328 elapsed, loss: 2.5266722e-06\n",
      "step: 95790 train: 0.08304429054260254 elapsed, loss: 1.6423853e-06\n",
      "step: 95800 train: 0.07992053031921387 elapsed, loss: 1.9851113e-06\n",
      "step: 95810 train: 0.07855868339538574 elapsed, loss: 1.268926e-06\n",
      "step: 95820 train: 0.08231210708618164 elapsed, loss: 7.13648e-06\n",
      "step: 95830 train: 0.08267354965209961 elapsed, loss: 5.085144e-05\n",
      "step: 95840 train: 0.08229732513427734 elapsed, loss: 3.737134e-05\n",
      "step: 95850 train: 0.0866236686706543 elapsed, loss: 1.438292e-05\n",
      "step: 95860 train: 0.0830237865447998 elapsed, loss: 1.5475918e-05\n",
      "step: 95870 train: 0.08840346336364746 elapsed, loss: 9.074841e-06\n",
      "step: 95880 train: 0.08208823204040527 elapsed, loss: 7.582778e-06\n",
      "step: 95890 train: 0.08017349243164062 elapsed, loss: 5.97859e-06\n",
      "step: 95900 train: 0.07869076728820801 elapsed, loss: 6.2179333e-06\n",
      "step: 95910 train: 0.08534622192382812 elapsed, loss: 5.7597344e-06\n",
      "step: 95920 train: 0.08257484436035156 elapsed, loss: 4.5806937e-06\n",
      "step: 95930 train: 0.08418416976928711 elapsed, loss: 2.956009e-06\n",
      "step: 95940 train: 0.08644390106201172 elapsed, loss: 2.8200357e-06\n",
      "step: 95950 train: 0.08072543144226074 elapsed, loss: 2.3147977e-06\n",
      "step: 95960 train: 0.07717442512512207 elapsed, loss: 2.2435524e-06\n",
      "step: 95970 train: 0.08394217491149902 elapsed, loss: 1.4230594e-06\n",
      "step: 95980 train: 0.0835273265838623 elapsed, loss: 1.2414515e-06\n",
      "step: 95990 train: 0.08196568489074707 elapsed, loss: 1.3252708e-06\n",
      "step: 96000 train: 0.07766222953796387 elapsed, loss: 1.4165392e-06\n",
      "step: 96010 train: 0.08407139778137207 elapsed, loss: 1.1422662e-06\n",
      "step: 96020 train: 0.08333683013916016 elapsed, loss: 1.3723024e-06\n",
      "step: 96030 train: 0.07860231399536133 elapsed, loss: 1.1245711e-06\n",
      "step: 96040 train: 0.07930493354797363 elapsed, loss: 1.6312098e-06\n",
      "step: 96050 train: 0.0778815746307373 elapsed, loss: 1.4645033e-06\n",
      "step: 96060 train: 0.07720518112182617 elapsed, loss: 1.45519e-06\n",
      "step: 96070 train: 0.08267855644226074 elapsed, loss: 1.168809e-06\n",
      "step: 96080 train: 0.08683657646179199 elapsed, loss: 1.2279478e-06\n",
      "step: 96090 train: 0.07855653762817383 elapsed, loss: 3.36253e-06\n",
      "step: 96100 train: 0.08487653732299805 elapsed, loss: 1.5529778e-06\n",
      "step: 96110 train: 0.08989071846008301 elapsed, loss: 1.5427345e-06\n",
      "step: 96120 train: 0.08214187622070312 elapsed, loss: 1.5320236e-06\n",
      "step: 96130 train: 0.07629895210266113 elapsed, loss: 1.8645048e-06\n",
      "step: 96140 train: 0.08298897743225098 elapsed, loss: 1.44448e-06\n",
      "step: 96150 train: 0.07426047325134277 elapsed, loss: 2.3459977e-06\n",
      "step: 96160 train: 0.08655929565429688 elapsed, loss: 1.3010562e-06\n",
      "step: 96170 train: 0.08181524276733398 elapsed, loss: 1.3955855e-06\n",
      "step: 96180 train: 0.08130574226379395 elapsed, loss: 1.3164234e-06\n",
      "step: 96190 train: 0.07919645309448242 elapsed, loss: 2.512703e-06\n",
      "step: 96200 train: 0.07799720764160156 elapsed, loss: 1.8901167e-06\n",
      "step: 96210 train: 0.0780794620513916 elapsed, loss: 1.6028046e-06\n",
      "step: 96220 train: 0.07971858978271484 elapsed, loss: 2.4773117e-06\n",
      "step: 96230 train: 0.08741545677185059 elapsed, loss: 1.3373781e-06\n",
      "step: 96240 train: 0.08143353462219238 elapsed, loss: 2.5024588e-06\n",
      "step: 96250 train: 0.08007454872131348 elapsed, loss: 1.6051326e-06\n",
      "step: 96260 train: 0.07883834838867188 elapsed, loss: 1.5185186e-06\n",
      "step: 96270 train: 0.08121252059936523 elapsed, loss: 1.72667e-06\n",
      "step: 96280 train: 0.07927322387695312 elapsed, loss: 1.5888348e-06\n",
      "step: 96290 train: 0.0834801197052002 elapsed, loss: 1.3322558e-06\n",
      "step: 96300 train: 0.08090686798095703 elapsed, loss: 2.5657869e-06\n",
      "step: 96310 train: 0.07813024520874023 elapsed, loss: 1.8258554e-06\n",
      "step: 96320 train: 0.08389019966125488 elapsed, loss: 1.5422684e-06\n",
      "step: 96330 train: 0.08025336265563965 elapsed, loss: 2.3688149e-06\n",
      "step: 96340 train: 0.08839726448059082 elapsed, loss: 1.2763767e-06\n",
      "step: 96350 train: 0.07891511917114258 elapsed, loss: 1.6591495e-06\n",
      "step: 96360 train: 0.08840274810791016 elapsed, loss: 1.6032702e-06\n",
      "step: 96370 train: 0.07607865333557129 elapsed, loss: 9.6724856e-05\n",
      "step: 96380 train: 0.08762836456298828 elapsed, loss: 0.0003889904\n",
      "step: 96390 train: 0.07841658592224121 elapsed, loss: 3.573403e-05\n",
      "step: 96400 train: 0.09261822700500488 elapsed, loss: 1.8669629e-05\n",
      "step: 96410 train: 0.08193492889404297 elapsed, loss: 1.5203279e-05\n",
      "step: 96420 train: 0.07761716842651367 elapsed, loss: 1.5774924e-05\n",
      "step: 96430 train: 0.08835959434509277 elapsed, loss: 8.122453e-06\n",
      "step: 96440 train: 0.08584022521972656 elapsed, loss: 8.8073875e-06\n",
      "step: 96450 train: 0.0805511474609375 elapsed, loss: 5.460311e-06\n",
      "step: 96460 train: 0.08054590225219727 elapsed, loss: 5.497081e-06\n",
      "step: 96470 train: 0.08143424987792969 elapsed, loss: 4.3557693e-06\n",
      "step: 96480 train: 0.0834205150604248 elapsed, loss: 2.8111883e-06\n",
      "step: 96490 train: 0.08619356155395508 elapsed, loss: 2.126672e-06\n",
      "step: 96500 train: 0.08358407020568848 elapsed, loss: 2.0330735e-06\n",
      "step: 96510 train: 0.08449363708496094 elapsed, loss: 1.6656686e-06\n",
      "step: 96520 train: 0.08100557327270508 elapsed, loss: 2.069396e-06\n",
      "step: 96530 train: 0.08025956153869629 elapsed, loss: 1.3248052e-06\n",
      "step: 96540 train: 0.08646321296691895 elapsed, loss: 1.6484292e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 96550 train: 0.07794189453125 elapsed, loss: 1.6218967e-06\n",
      "step: 96560 train: 0.08357501029968262 elapsed, loss: 1.5674142e-06\n",
      "step: 96570 train: 0.08184814453125 elapsed, loss: 1.5911628e-06\n",
      "step: 96580 train: 0.08951759338378906 elapsed, loss: 1.1641519e-06\n",
      "step: 96590 train: 0.07703542709350586 elapsed, loss: 2.4517024e-06\n",
      "step: 96600 train: 0.08768057823181152 elapsed, loss: 0.0011355049\n",
      "step: 96610 train: 0.07657885551452637 elapsed, loss: 4.4796107e-05\n",
      "step: 96620 train: 0.08440327644348145 elapsed, loss: 2.9363251e-05\n",
      "step: 96630 train: 0.07759475708007812 elapsed, loss: 2.050348e-05\n",
      "step: 96640 train: 0.08455014228820801 elapsed, loss: 1.2326306e-05\n",
      "step: 96650 train: 0.07877159118652344 elapsed, loss: 1.6608688e-05\n",
      "step: 96660 train: 0.0773463249206543 elapsed, loss: 1.0507554e-05\n",
      "step: 96670 train: 0.07962656021118164 elapsed, loss: 5.845405e-06\n",
      "step: 96680 train: 0.07703280448913574 elapsed, loss: 5.819804e-06\n",
      "step: 96690 train: 0.08219242095947266 elapsed, loss: 5.380221e-06\n",
      "step: 96700 train: 0.0854330062866211 elapsed, loss: 3.2787098e-06\n",
      "step: 96710 train: 0.07589411735534668 elapsed, loss: 3.6950087e-06\n",
      "step: 96720 train: 0.07740306854248047 elapsed, loss: 3.9632314e-06\n",
      "step: 96730 train: 0.0824432373046875 elapsed, loss: 1.4994278e-06\n",
      "step: 96740 train: 0.08302617073059082 elapsed, loss: 1.6484391e-06\n",
      "step: 96750 train: 0.0813910961151123 elapsed, loss: 1.7173567e-06\n",
      "step: 96760 train: 0.08140063285827637 elapsed, loss: 1.4896485e-06\n",
      "step: 96770 train: 0.07789778709411621 elapsed, loss: 1.4244565e-06\n",
      "step: 96780 train: 0.08065557479858398 elapsed, loss: 1.2307416e-06\n",
      "step: 96790 train: 0.08743572235107422 elapsed, loss: 9.825446e-07\n",
      "step: 96800 train: 0.08745861053466797 elapsed, loss: 2.9955854e-06\n",
      "step: 96810 train: 0.0772700309753418 elapsed, loss: 1.4645033e-06\n",
      "step: 96820 train: 0.07452082633972168 elapsed, loss: 1.6731192e-06\n",
      "step: 96830 train: 0.08165860176086426 elapsed, loss: 1.3397064e-06\n",
      "step: 96840 train: 0.08458590507507324 elapsed, loss: 1.5040845e-06\n",
      "step: 96850 train: 0.08438420295715332 elapsed, loss: 1.1399379e-06\n",
      "step: 96860 train: 0.08256864547729492 elapsed, loss: 1.5851093e-06\n",
      "step: 96870 train: 0.08194327354431152 elapsed, loss: 0.0030095899\n",
      "step: 96880 train: 0.08330106735229492 elapsed, loss: 6.8535264e-05\n",
      "step: 96890 train: 0.08114886283874512 elapsed, loss: 2.3607095e-05\n",
      "step: 96900 train: 0.08036184310913086 elapsed, loss: 1.3711939e-05\n",
      "step: 96910 train: 0.08470463752746582 elapsed, loss: 1.04050505e-05\n",
      "step: 96920 train: 0.08217334747314453 elapsed, loss: 6.7855685e-06\n",
      "step: 96930 train: 0.08073902130126953 elapsed, loss: 4.393955e-06\n",
      "step: 96940 train: 0.07738780975341797 elapsed, loss: 6.9066455e-06\n",
      "step: 96950 train: 0.082794189453125 elapsed, loss: 3.9581028e-06\n",
      "step: 96960 train: 0.08405637741088867 elapsed, loss: 3.1529794e-06\n",
      "step: 96970 train: 0.07875800132751465 elapsed, loss: 3.0342421e-06\n",
      "step: 96980 train: 0.08350896835327148 elapsed, loss: 2.1867274e-06\n",
      "step: 96990 train: 0.08288764953613281 elapsed, loss: 1.4048978e-06\n",
      "step: 97000 train: 0.07695651054382324 elapsed, loss: 2.0218986e-06\n",
      "step: 97010 train: 0.0790867805480957 elapsed, loss: 2.1816202e-06\n",
      "step: 97020 train: 0.07808446884155273 elapsed, loss: 1.3625225e-06\n",
      "step: 97030 train: 0.0882108211517334 elapsed, loss: 1.003965e-06\n",
      "step: 97040 train: 0.08066678047180176 elapsed, loss: 1.5711391e-06\n",
      "step: 97050 train: 0.08215475082397461 elapsed, loss: 1.6493702e-06\n",
      "step: 97060 train: 0.0795590877532959 elapsed, loss: 1.0281794e-06\n",
      "step: 97070 train: 0.07799792289733887 elapsed, loss: 1.5734677e-06\n",
      "step: 97080 train: 0.07378268241882324 elapsed, loss: 1.8510011e-06\n",
      "step: 97090 train: 0.08925652503967285 elapsed, loss: 1.2633379e-06\n",
      "step: 97100 train: 0.07994532585144043 elapsed, loss: 1.1310902e-06\n",
      "step: 97110 train: 0.08436322212219238 elapsed, loss: 1.7946564e-06\n",
      "step: 97120 train: 0.07973384857177734 elapsed, loss: 1.9110716e-06\n",
      "step: 97130 train: 0.08005762100219727 elapsed, loss: 1.3974482e-06\n",
      "step: 97140 train: 0.07536745071411133 elapsed, loss: 1.7285329e-06\n",
      "step: 97150 train: 0.08702278137207031 elapsed, loss: 1.2600783e-06\n",
      "step: 97160 train: 0.07790422439575195 elapsed, loss: 1.5906974e-06\n",
      "step: 97170 train: 0.08017802238464355 elapsed, loss: 1.0430805e-06\n",
      "step: 97180 train: 0.08623313903808594 elapsed, loss: 1.6503018e-06\n",
      "step: 97190 train: 0.08056378364562988 elapsed, loss: 1.6330721e-06\n",
      "step: 97200 train: 0.07889556884765625 elapsed, loss: 1.4281817e-06\n",
      "step: 97210 train: 0.08645319938659668 elapsed, loss: 1.1995426e-06\n",
      "step: 97220 train: 0.08527517318725586 elapsed, loss: 9.84873e-07\n",
      "step: 97230 train: 0.0857388973236084 elapsed, loss: 1.3173545e-06\n",
      "step: 97240 train: 0.08390998840332031 elapsed, loss: 1.3303923e-06\n",
      "step: 97250 train: 0.07964015007019043 elapsed, loss: 1.9525155e-06\n",
      "step: 97260 train: 0.08164358139038086 elapsed, loss: 1.4640377e-06\n",
      "step: 97270 train: 0.09161138534545898 elapsed, loss: 6.774554e-05\n",
      "step: 97280 train: 0.078857421875 elapsed, loss: 4.6818106e-05\n",
      "step: 97290 train: 0.07613444328308105 elapsed, loss: 2.0465657e-05\n",
      "step: 97300 train: 0.07656240463256836 elapsed, loss: 1.2554546e-05\n",
      "step: 97310 train: 0.08167624473571777 elapsed, loss: 1.1394593e-05\n",
      "step: 97320 train: 0.08266878128051758 elapsed, loss: 5.843527e-06\n",
      "step: 97330 train: 0.07795524597167969 elapsed, loss: 5.903155e-06\n",
      "step: 97340 train: 0.07959842681884766 elapsed, loss: 4.8510883e-06\n",
      "step: 97350 train: 0.0815882682800293 elapsed, loss: 3.7527536e-06\n",
      "step: 97360 train: 0.08022713661193848 elapsed, loss: 3.6093038e-06\n",
      "step: 97370 train: 0.08345985412597656 elapsed, loss: 2.328768e-06\n",
      "step: 97380 train: 0.07811808586120605 elapsed, loss: 2.5830186e-06\n",
      "step: 97390 train: 0.07684898376464844 elapsed, loss: 2.1983822e-06\n",
      "step: 97400 train: 0.08773374557495117 elapsed, loss: 1.5143289e-06\n",
      "step: 97410 train: 0.08089494705200195 elapsed, loss: 1.560895e-06\n",
      "step: 97420 train: 0.09026217460632324 elapsed, loss: 1.1739311e-06\n",
      "step: 97430 train: 0.08977532386779785 elapsed, loss: 5.1256966e-06\n",
      "step: 97440 train: 0.07783126831054688 elapsed, loss: 8.646082e-05\n",
      "step: 97450 train: 0.08676719665527344 elapsed, loss: 2.6793949e-05\n",
      "step: 97460 train: 0.07760882377624512 elapsed, loss: 2.6458167e-05\n",
      "step: 97470 train: 0.07762002944946289 elapsed, loss: 1.0874015e-05\n",
      "step: 97480 train: 0.0784311294555664 elapsed, loss: 9.297781e-06\n",
      "step: 97490 train: 0.08287310600280762 elapsed, loss: 6.965315e-06\n",
      "step: 97500 train: 0.0821526050567627 elapsed, loss: 4.6947816e-06\n",
      "step: 97510 train: 0.08386802673339844 elapsed, loss: 3.8659073e-06\n",
      "step: 97520 train: 0.08052659034729004 elapsed, loss: 2.985347e-06\n",
      "step: 97530 train: 0.07885909080505371 elapsed, loss: 3.3783544e-06\n",
      "step: 97540 train: 0.08495259284973145 elapsed, loss: 3.4233874e-06\n",
      "step: 97550 train: 0.08525419235229492 elapsed, loss: 1.5664828e-06\n",
      "step: 97560 train: 0.08079338073730469 elapsed, loss: 1.3876692e-06\n",
      "step: 97570 train: 0.08544301986694336 elapsed, loss: 1.4267846e-06\n",
      "step: 97580 train: 0.08171439170837402 elapsed, loss: 1.9068802e-06\n",
      "step: 97590 train: 0.08336520195007324 elapsed, loss: 1.3466897e-06\n",
      "step: 97600 train: 0.08097052574157715 elapsed, loss: 1.6223617e-06\n",
      "step: 97610 train: 0.08089041709899902 elapsed, loss: 2.333889e-06\n",
      "step: 97620 train: 0.08095049858093262 elapsed, loss: 1.1166549e-06\n",
      "step: 97630 train: 0.08197021484375 elapsed, loss: 1.0780052e-06\n",
      "step: 97640 train: 0.0841379165649414 elapsed, loss: 1.0142096e-06\n",
      "step: 97650 train: 0.0801384449005127 elapsed, loss: 1.0984941e-06\n",
      "step: 97660 train: 0.08486604690551758 elapsed, loss: 8.1770077e-07\n",
      "step: 97670 train: 0.08375287055969238 elapsed, loss: 1.0440119e-06\n",
      "step: 97680 train: 0.07819175720214844 elapsed, loss: 1.3927918e-06\n",
      "step: 97690 train: 0.07999134063720703 elapsed, loss: 1.1944203e-06\n",
      "step: 97700 train: 0.08281731605529785 elapsed, loss: 1.954842e-06\n",
      "step: 97710 train: 0.08675813674926758 elapsed, loss: 1.4295786e-06\n",
      "step: 97720 train: 0.0770416259765625 elapsed, loss: 1.7369146e-06\n",
      "step: 97730 train: 0.08321547508239746 elapsed, loss: 1.8915136e-06\n",
      "step: 97740 train: 0.07887840270996094 elapsed, loss: 1.2544903e-06\n",
      "step: 97750 train: 0.07522225379943848 elapsed, loss: 1.9851104e-06\n",
      "step: 97760 train: 0.09121155738830566 elapsed, loss: 1.2028019e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 97770 train: 0.08267068862915039 elapsed, loss: 2.2067597e-06\n",
      "step: 97780 train: 0.08316993713378906 elapsed, loss: 1.9688132e-06\n",
      "step: 97790 train: 0.07497167587280273 elapsed, loss: 1.7336544e-06\n",
      "step: 97800 train: 0.08147931098937988 elapsed, loss: 1.5995449e-06\n",
      "step: 97810 train: 0.08885788917541504 elapsed, loss: 1.0863869e-06\n",
      "step: 97820 train: 0.0783851146697998 elapsed, loss: 1.5767275e-06\n",
      "step: 97830 train: 0.08339285850524902 elapsed, loss: 1.5120006e-06\n",
      "step: 97840 train: 0.0760660171508789 elapsed, loss: 2.4409924e-06\n",
      "step: 97850 train: 0.07999062538146973 elapsed, loss: 1.3913937e-06\n",
      "step: 97860 train: 0.08107542991638184 elapsed, loss: 1.7448308e-06\n",
      "step: 97870 train: 0.07715797424316406 elapsed, loss: 1.8249241e-06\n",
      "step: 97880 train: 0.07949972152709961 elapsed, loss: 2.211888e-06\n",
      "step: 97890 train: 0.0760183334350586 elapsed, loss: 2.2049012e-06\n",
      "step: 97900 train: 0.08453369140625 elapsed, loss: 1.2950029e-06\n",
      "step: 97910 train: 0.07945871353149414 elapsed, loss: 1.945065e-06\n",
      "step: 97920 train: 0.07949972152709961 elapsed, loss: 1.267529e-06\n",
      "step: 97930 train: 0.0836794376373291 elapsed, loss: 2.9960543e-06\n",
      "step: 97940 train: 0.09084606170654297 elapsed, loss: 1.4477396e-06\n",
      "step: 97950 train: 0.08430361747741699 elapsed, loss: 1.2964e-06\n",
      "step: 97960 train: 0.08436441421508789 elapsed, loss: 1.7597318e-06\n",
      "step: 97970 train: 0.07878851890563965 elapsed, loss: 2.856359e-06\n",
      "step: 97980 train: 0.079864501953125 elapsed, loss: 1.7047835e-06\n",
      "step: 97990 train: 0.08147096633911133 elapsed, loss: 1.9716072e-06\n",
      "step: 98000 train: 0.08083987236022949 elapsed, loss: 2.0232956e-06\n",
      "step: 98010 train: 0.08493757247924805 elapsed, loss: 2.2142156e-06\n",
      "step: 98020 train: 0.0828094482421875 elapsed, loss: 0.00041779113\n",
      "step: 98030 train: 0.07653379440307617 elapsed, loss: 0.00015266627\n",
      "step: 98040 train: 0.08158612251281738 elapsed, loss: 3.4892386e-05\n",
      "step: 98050 train: 0.07733035087585449 elapsed, loss: 3.2304997e-05\n",
      "step: 98060 train: 0.08422636985778809 elapsed, loss: 1.3111778e-05\n",
      "step: 98070 train: 0.0798957347869873 elapsed, loss: 2.019868e-05\n",
      "step: 98080 train: 0.08340573310852051 elapsed, loss: 1.4389963e-05\n",
      "step: 98090 train: 0.08203721046447754 elapsed, loss: 7.3452775e-06\n",
      "step: 98100 train: 0.08520150184631348 elapsed, loss: 3.9255065e-06\n",
      "step: 98110 train: 0.0862877368927002 elapsed, loss: 3.8901217e-06\n",
      "step: 98120 train: 0.08284878730773926 elapsed, loss: 4.9806945e-06\n",
      "step: 98130 train: 0.08499526977539062 elapsed, loss: 2.3236453e-06\n",
      "step: 98140 train: 0.08125090599060059 elapsed, loss: 2.4642727e-06\n",
      "step: 98150 train: 0.08991765975952148 elapsed, loss: 1.644248e-06\n",
      "step: 98160 train: 0.08062171936035156 elapsed, loss: 2.2160784e-06\n",
      "step: 98170 train: 0.08141112327575684 elapsed, loss: 1.6433169e-06\n",
      "step: 98180 train: 0.08822989463806152 elapsed, loss: 1.238655e-06\n",
      "step: 98190 train: 0.08051037788391113 elapsed, loss: 1.6312094e-06\n",
      "step: 98200 train: 0.07967519760131836 elapsed, loss: 1.4710226e-06\n",
      "step: 98210 train: 0.07755851745605469 elapsed, loss: 2.1420387e-06\n",
      "step: 98220 train: 0.08102226257324219 elapsed, loss: 1.2698572e-06\n",
      "step: 98230 train: 0.08059501647949219 elapsed, loss: 1.2437804e-06\n",
      "step: 98240 train: 0.08469724655151367 elapsed, loss: 1.1669463e-06\n",
      "step: 98250 train: 0.08348512649536133 elapsed, loss: 1.4863894e-06\n",
      "step: 98260 train: 0.08180928230285645 elapsed, loss: 1.1757938e-06\n",
      "step: 98270 train: 0.08596944808959961 elapsed, loss: 1.0388896e-06\n",
      "step: 98280 train: 0.08392786979675293 elapsed, loss: 1.2610087e-06\n",
      "step: 98290 train: 0.08272361755371094 elapsed, loss: 1.6340036e-06\n",
      "step: 98300 train: 0.07895827293395996 elapsed, loss: 0.00024482002\n",
      "step: 98310 train: 0.08823585510253906 elapsed, loss: 5.1221094e-05\n",
      "step: 98320 train: 0.07998466491699219 elapsed, loss: 1.3107601e-05\n",
      "step: 98330 train: 0.0793924331665039 elapsed, loss: 1.1764263e-05\n",
      "step: 98340 train: 0.08108377456665039 elapsed, loss: 1.2604141e-05\n",
      "step: 98350 train: 0.0765371322631836 elapsed, loss: 8.989509e-06\n",
      "step: 98360 train: 0.07702207565307617 elapsed, loss: 5.5291684e-06\n",
      "step: 98370 train: 0.08508849143981934 elapsed, loss: 3.488249e-06\n",
      "step: 98380 train: 0.08057904243469238 elapsed, loss: 3.2307494e-06\n",
      "step: 98390 train: 0.08807706832885742 elapsed, loss: 2.2081613e-06\n",
      "step: 98400 train: 0.08854293823242188 elapsed, loss: 2.7185251e-06\n",
      "step: 98410 train: 0.08116841316223145 elapsed, loss: 2.3022226e-06\n",
      "step: 98420 train: 0.0791635513305664 elapsed, loss: 2.1378462e-06\n",
      "step: 98430 train: 0.07775592803955078 elapsed, loss: 2.1564738e-06\n",
      "step: 98440 train: 0.08029603958129883 elapsed, loss: 1.5315583e-06\n",
      "step: 98450 train: 0.0799863338470459 elapsed, loss: 1.2149087e-06\n",
      "step: 98460 train: 0.07965278625488281 elapsed, loss: 1.2367954e-06\n",
      "step: 98470 train: 0.08551836013793945 elapsed, loss: 1.1427318e-06\n",
      "step: 98480 train: 0.08496689796447754 elapsed, loss: 1.3997765e-06\n",
      "step: 98490 train: 0.08468890190124512 elapsed, loss: 1.0603098e-06\n",
      "step: 98500 train: 0.07891368865966797 elapsed, loss: 0.00040429158\n",
      "step: 98510 train: 0.08703851699829102 elapsed, loss: 1.1497854e-05\n",
      "step: 98520 train: 0.07972025871276855 elapsed, loss: 7.857507e-06\n",
      "step: 98530 train: 0.08420896530151367 elapsed, loss: 4.5946263e-06\n",
      "step: 98540 train: 0.07812070846557617 elapsed, loss: 3.1208542e-06\n",
      "step: 98550 train: 0.08596348762512207 elapsed, loss: 2.59419e-06\n",
      "step: 98560 train: 0.08394694328308105 elapsed, loss: 2.2826598e-06\n",
      "step: 98570 train: 0.08076238632202148 elapsed, loss: 3.4542545e-06\n",
      "step: 98580 train: 0.08559656143188477 elapsed, loss: 1.2349325e-06\n",
      "step: 98590 train: 0.08130669593811035 elapsed, loss: 1.5338862e-06\n",
      "step: 98600 train: 0.08671236038208008 elapsed, loss: 1.1003567e-06\n",
      "step: 98610 train: 0.08782768249511719 elapsed, loss: 1.0575159e-06\n",
      "step: 98620 train: 0.08472204208374023 elapsed, loss: 1.0943027e-06\n",
      "step: 98630 train: 0.07882809638977051 elapsed, loss: 1.3490196e-06\n",
      "step: 98640 train: 0.08531570434570312 elapsed, loss: 2.0428527e-06\n",
      "step: 98650 train: 0.08873963356018066 elapsed, loss: 1.1306247e-06\n",
      "step: 98660 train: 0.08353519439697266 elapsed, loss: 1.1185175e-06\n",
      "step: 98670 train: 0.07784819602966309 elapsed, loss: 1.696854e-06\n",
      "step: 98680 train: 0.08484625816345215 elapsed, loss: 1.064501e-06\n",
      "step: 98690 train: 0.07619905471801758 elapsed, loss: 1.5171229e-06\n",
      "step: 98700 train: 0.08813238143920898 elapsed, loss: 1.5008247e-06\n",
      "step: 98710 train: 0.0809931755065918 elapsed, loss: 1.4649692e-06\n",
      "step: 98720 train: 0.08264017105102539 elapsed, loss: 1.6186361e-06\n",
      "step: 98730 train: 0.07780098915100098 elapsed, loss: 1.7555408e-06\n",
      "step: 98740 train: 0.08187985420227051 elapsed, loss: 9.08039e-07\n",
      "step: 98750 train: 0.07741022109985352 elapsed, loss: 1.3336526e-06\n",
      "step: 98760 train: 0.08488059043884277 elapsed, loss: 1.5664828e-06\n",
      "step: 98770 train: 0.08639073371887207 elapsed, loss: 2.2128193e-06\n",
      "step: 98780 train: 0.08838653564453125 elapsed, loss: 1.2707884e-06\n",
      "step: 98790 train: 0.08777332305908203 elapsed, loss: 1.6917456e-06\n",
      "step: 98800 train: 0.08086705207824707 elapsed, loss: 1.7289983e-06\n",
      "step: 98810 train: 0.07947850227355957 elapsed, loss: 1.3979139e-06\n",
      "step: 98820 train: 0.08527326583862305 elapsed, loss: 1.0919749e-06\n",
      "step: 98830 train: 0.07920098304748535 elapsed, loss: 1.9711415e-06\n",
      "step: 98840 train: 0.08652472496032715 elapsed, loss: 0.000121851575\n",
      "step: 98850 train: 0.07966423034667969 elapsed, loss: 0.0029636777\n",
      "step: 98860 train: 0.08318710327148438 elapsed, loss: 0.00010308091\n",
      "step: 98870 train: 0.07770514488220215 elapsed, loss: 2.762454e-05\n",
      "step: 98880 train: 0.07797455787658691 elapsed, loss: 2.3207669e-05\n",
      "step: 98890 train: 0.08414196968078613 elapsed, loss: 1.286597e-05\n",
      "step: 98900 train: 0.08025860786437988 elapsed, loss: 9.519353e-06\n",
      "step: 98910 train: 0.08279871940612793 elapsed, loss: 1.0578691e-05\n",
      "step: 98920 train: 0.08518528938293457 elapsed, loss: 3.841223e-06\n",
      "step: 98930 train: 0.08157706260681152 elapsed, loss: 4.757171e-06\n",
      "step: 98940 train: 0.08326554298400879 elapsed, loss: 2.7506555e-06\n",
      "step: 98950 train: 0.07479643821716309 elapsed, loss: 3.6386666e-06\n",
      "step: 98960 train: 0.07863354682922363 elapsed, loss: 2.8689287e-06\n",
      "step: 98970 train: 0.07834506034851074 elapsed, loss: 2.321783e-06\n",
      "step: 98980 train: 0.07588911056518555 elapsed, loss: 2.2123527e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 98990 train: 0.07808685302734375 elapsed, loss: 1.8286497e-06\n",
      "step: 99000 train: 0.07984399795532227 elapsed, loss: 1.6554241e-06\n",
      "step: 99010 train: 0.08061838150024414 elapsed, loss: 1.6964021e-06\n",
      "step: 99020 train: 0.08137321472167969 elapsed, loss: 1.1608927e-06\n",
      "step: 99030 train: 0.07863163948059082 elapsed, loss: 0.0007341753\n",
      "step: 99040 train: 0.08275961875915527 elapsed, loss: 0.00014884415\n",
      "step: 99050 train: 0.07979273796081543 elapsed, loss: 5.7581496e-05\n",
      "step: 99060 train: 0.0815742015838623 elapsed, loss: 2.9607889e-05\n",
      "step: 99070 train: 0.08477425575256348 elapsed, loss: 1.27853455e-05\n",
      "step: 99080 train: 0.08179855346679688 elapsed, loss: 1.2164737e-05\n",
      "step: 99090 train: 0.08100605010986328 elapsed, loss: 9.326609e-06\n",
      "step: 99100 train: 0.08398842811584473 elapsed, loss: 5.4472775e-06\n",
      "step: 99110 train: 0.07647371292114258 elapsed, loss: 7.0179294e-06\n",
      "step: 99120 train: 0.08415865898132324 elapsed, loss: 3.2922158e-06\n",
      "step: 99130 train: 0.07802653312683105 elapsed, loss: 4.4828616e-06\n",
      "step: 99140 train: 0.08100247383117676 elapsed, loss: 2.6011785e-06\n",
      "step: 99150 train: 0.08021044731140137 elapsed, loss: 2.743205e-06\n",
      "step: 99160 train: 0.08436107635498047 elapsed, loss: 4.253153e-06\n",
      "step: 99170 train: 0.08398914337158203 elapsed, loss: 1.5743974e-06\n",
      "step: 99180 train: 0.08509111404418945 elapsed, loss: 1.3732333e-06\n",
      "step: 99190 train: 0.0794980525970459 elapsed, loss: 1.2596126e-06\n",
      "step: 99200 train: 0.08433723449707031 elapsed, loss: 1.3187516e-06\n",
      "step: 99210 train: 0.08501362800598145 elapsed, loss: 1.1208458e-06\n",
      "step: 99220 train: 0.07399702072143555 elapsed, loss: 1.4468081e-06\n",
      "step: 99230 train: 0.08381175994873047 elapsed, loss: 1.1492508e-06\n",
      "step: 99240 train: 0.08004570007324219 elapsed, loss: 1.5399394e-06\n",
      "step: 99250 train: 0.08249068260192871 elapsed, loss: 1.2456429e-06\n",
      "step: 99260 train: 0.08297467231750488 elapsed, loss: 6.6482057e-06\n",
      "step: 99270 train: 0.08460211753845215 elapsed, loss: 3.2395951e-06\n",
      "step: 99280 train: 0.07946538925170898 elapsed, loss: 3.2237642e-06\n",
      "step: 99290 train: 0.08416581153869629 elapsed, loss: 1.9031544e-06\n",
      "step: 99300 train: 0.08045721054077148 elapsed, loss: 1.6456449e-06\n",
      "step: 99310 train: 0.08268237113952637 elapsed, loss: 1.4808011e-06\n",
      "step: 99320 train: 0.07976531982421875 elapsed, loss: 1.5799872e-06\n",
      "step: 99330 train: 0.08177852630615234 elapsed, loss: 1.1133952e-06\n",
      "step: 99340 train: 0.08371710777282715 elapsed, loss: 1.0556535e-06\n",
      "step: 99350 train: 0.08480191230773926 elapsed, loss: 1.0854548e-06\n",
      "step: 99360 train: 0.0869452953338623 elapsed, loss: 1.15903e-06\n",
      "step: 99370 train: 0.07702922821044922 elapsed, loss: 1.0547221e-06\n",
      "step: 99380 train: 0.08224844932556152 elapsed, loss: 1.1087386e-06\n",
      "step: 99390 train: 0.07955718040466309 elapsed, loss: 1.179519e-06\n",
      "step: 99400 train: 0.07632946968078613 elapsed, loss: 1.5860408e-06\n",
      "step: 99410 train: 0.0854804515838623 elapsed, loss: 9.44826e-07\n",
      "step: 99420 train: 0.0844736099243164 elapsed, loss: 1.3625227e-06\n",
      "step: 99430 train: 0.08109331130981445 elapsed, loss: 1.3303932e-06\n",
      "step: 99440 train: 0.08153772354125977 elapsed, loss: 1.1664786e-06\n",
      "step: 99450 train: 0.0817422866821289 elapsed, loss: 1.0221258e-06\n",
      "step: 99460 train: 0.07379746437072754 elapsed, loss: 3.6544552e-06\n",
      "step: 99470 train: 0.08156228065490723 elapsed, loss: 1.8402842e-06\n",
      "step: 99480 train: 0.08297348022460938 elapsed, loss: 1.4393578e-06\n",
      "step: 99490 train: 0.08129286766052246 elapsed, loss: 1.2828958e-06\n",
      "step: 99500 train: 0.08224630355834961 elapsed, loss: 1.4714878e-06\n",
      "step: 99510 train: 0.075347900390625 elapsed, loss: 1.4235252e-06\n",
      "step: 99520 train: 0.08265042304992676 elapsed, loss: 1.2912776e-06\n",
      "step: 99530 train: 0.07958149909973145 elapsed, loss: 1.6544927e-06\n",
      "step: 99540 train: 0.08501791954040527 elapsed, loss: 1.2293447e-06\n",
      "step: 99550 train: 0.08185577392578125 elapsed, loss: 1.4486706e-06\n",
      "step: 99560 train: 0.08797836303710938 elapsed, loss: 1.3448283e-06\n",
      "step: 99570 train: 0.09095501899719238 elapsed, loss: 1.7620589e-06\n",
      "step: 99580 train: 0.08281850814819336 elapsed, loss: 2.415381e-06\n",
      "step: 99590 train: 0.08481264114379883 elapsed, loss: 1.2544901e-06\n",
      "step: 99600 train: 0.07797646522521973 elapsed, loss: 2.126179e-06\n",
      "step: 99610 train: 0.0805811882019043 elapsed, loss: 1.7904656e-06\n",
      "step: 99620 train: 0.0838315486907959 elapsed, loss: 1.6349345e-06\n",
      "step: 99630 train: 0.08405447006225586 elapsed, loss: 1.9441316e-06\n",
      "step: 99640 train: 0.08080911636352539 elapsed, loss: 2.7017595e-06\n",
      "step: 99650 train: 0.08147454261779785 elapsed, loss: 0.000168509\n",
      "step: 99660 train: 0.07931900024414062 elapsed, loss: 0.00013697385\n",
      "step: 99670 train: 0.08717179298400879 elapsed, loss: 0.0039385306\n",
      "step: 99680 train: 0.07787084579467773 elapsed, loss: 4.449115e-05\n",
      "step: 99690 train: 0.08302831649780273 elapsed, loss: 2.8473913e-05\n",
      "step: 99700 train: 0.07975602149963379 elapsed, loss: 2.5955564e-05\n",
      "step: 99710 train: 0.08040976524353027 elapsed, loss: 1.3407008e-05\n",
      "step: 99720 train: 0.07852530479431152 elapsed, loss: 1.6882164e-05\n",
      "step: 99730 train: 0.08180809020996094 elapsed, loss: 1.1605051e-05\n",
      "step: 99740 train: 0.08029985427856445 elapsed, loss: 6.0014036e-06\n",
      "step: 99750 train: 0.08341217041015625 elapsed, loss: 5.3234044e-06\n",
      "step: 99760 train: 0.07641291618347168 elapsed, loss: 4.4712615e-06\n",
      "step: 99770 train: 0.08238816261291504 elapsed, loss: 3.0915007e-06\n",
      "step: 99780 train: 0.07796025276184082 elapsed, loss: 2.9797498e-06\n",
      "step: 99790 train: 0.07871603965759277 elapsed, loss: 2.1350538e-06\n",
      "step: 99800 train: 0.07591032981872559 elapsed, loss: 2.4163123e-06\n",
      "step: 99810 train: 0.08233356475830078 elapsed, loss: 2.0875564e-06\n",
      "step: 99820 train: 0.07760930061340332 elapsed, loss: 2.2961683e-06\n",
      "step: 99830 train: 0.0783698558807373 elapsed, loss: 1.9501865e-06\n",
      "step: 99840 train: 0.07684111595153809 elapsed, loss: 1.789534e-06\n",
      "step: 99850 train: 0.07719874382019043 elapsed, loss: 1.6968669e-06\n",
      "step: 99860 train: 0.07854819297790527 elapsed, loss: 1.125037e-06\n",
      "step: 99870 train: 0.08038043975830078 elapsed, loss: 4.6277205e-06\n",
      "step: 99880 train: 0.07831764221191406 elapsed, loss: 1.6023384e-06\n",
      "step: 99890 train: 0.08283424377441406 elapsed, loss: 1.5720707e-06\n",
      "step: 99900 train: 0.0824739933013916 elapsed, loss: 1.3303932e-06\n",
      "step: 99910 train: 0.08155584335327148 elapsed, loss: 1.0621727e-06\n",
      "step: 99920 train: 0.07973742485046387 elapsed, loss: 1.5050158e-06\n",
      "step: 99930 train: 0.08381795883178711 elapsed, loss: 1.1757935e-06\n",
      "step: 99940 train: 0.08110165596008301 elapsed, loss: 1.6409882e-06\n",
      "step: 99950 train: 0.08432960510253906 elapsed, loss: 1.6381947e-06\n",
      "step: 99960 train: 0.08104228973388672 elapsed, loss: 1.544596e-06\n",
      "step: 99970 train: 0.07893943786621094 elapsed, loss: 1.0021024e-06\n",
      "step: 99980 train: 0.07352375984191895 elapsed, loss: 1.5012906e-06\n",
      "step: 99990 train: 0.0790255069732666 elapsed, loss: 1.4090898e-06\n",
      "step: 100000 train: 0.07996892929077148 elapsed, loss: 1.2894151e-06\n",
      "step: 100010 train: 0.0838613510131836 elapsed, loss: 1.2167718e-06\n",
      "step: 100020 train: 0.08378839492797852 elapsed, loss: 1.6777757e-06\n",
      "step: 100030 train: 0.08109927177429199 elapsed, loss: 2.1504206e-06\n",
      "step: 100040 train: 0.07891964912414551 elapsed, loss: 2.1345882e-06\n",
      "step: 100050 train: 0.0740349292755127 elapsed, loss: 2.0922134e-06\n",
      "step: 100060 train: 0.08006882667541504 elapsed, loss: 1.7667169e-06\n",
      "step: 100070 train: 0.08086991310119629 elapsed, loss: 1.2591472e-06\n",
      "step: 100080 train: 0.07609844207763672 elapsed, loss: 2.1508865e-06\n",
      "step: 100090 train: 0.08022451400756836 elapsed, loss: 1.8989645e-06\n",
      "step: 100100 train: 0.08395886421203613 elapsed, loss: 2.0768452e-06\n",
      "step: 100110 train: 0.0830068588256836 elapsed, loss: 1.2698571e-06\n",
      "step: 100120 train: 0.0858302116394043 elapsed, loss: 1.1408694e-06\n",
      "step: 100130 train: 0.08351349830627441 elapsed, loss: 1.3187516e-06\n",
      "step: 100140 train: 0.0821387767791748 elapsed, loss: 1.5487881e-06\n",
      "step: 100150 train: 0.08199548721313477 elapsed, loss: 2.5182915e-06\n",
      "step: 100160 train: 0.08226346969604492 elapsed, loss: 0.00012839076\n",
      "step: 100170 train: 0.0765526294708252 elapsed, loss: 4.915768e-05\n",
      "step: 100180 train: 0.08190345764160156 elapsed, loss: 1.6406791e-05\n",
      "step: 100190 train: 0.08271288871765137 elapsed, loss: 9.030926e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 100200 train: 0.08283090591430664 elapsed, loss: 1.6917013e-05\n",
      "step: 100210 train: 0.09022068977355957 elapsed, loss: 7.2879993e-06\n",
      "step: 100220 train: 0.08944249153137207 elapsed, loss: 4.4489034e-06\n",
      "step: 100230 train: 0.07710886001586914 elapsed, loss: 5.7154957e-06\n",
      "step: 100240 train: 0.07908987998962402 elapsed, loss: 3.510142e-06\n",
      "step: 100250 train: 0.07859277725219727 elapsed, loss: 3.9934985e-06\n",
      "step: 100260 train: 0.07477974891662598 elapsed, loss: 3.905956e-06\n",
      "step: 100270 train: 0.08427906036376953 elapsed, loss: 4.0897403e-06\n",
      "step: 100280 train: 0.07698488235473633 elapsed, loss: 2.522017e-06\n",
      "step: 100290 train: 0.0769186019897461 elapsed, loss: 2.5290008e-06\n",
      "step: 100300 train: 0.07844972610473633 elapsed, loss: 2.117359e-06\n",
      "step: 100310 train: 0.08088946342468262 elapsed, loss: 2.1127023e-06\n",
      "step: 100320 train: 0.08342838287353516 elapsed, loss: 1.2991939e-06\n",
      "step: 100330 train: 0.08007025718688965 elapsed, loss: 1.3820811e-06\n",
      "step: 100340 train: 0.0799708366394043 elapsed, loss: 1.8165426e-06\n",
      "step: 100350 train: 0.08124709129333496 elapsed, loss: 2.0596171e-06\n",
      "step: 100360 train: 0.07784581184387207 elapsed, loss: 2.2691643e-06\n",
      "step: 100370 train: 0.0872046947479248 elapsed, loss: 1.3736995e-06\n",
      "step: 100380 train: 0.08073687553405762 elapsed, loss: 1.467763e-06\n",
      "step: 100390 train: 0.08295726776123047 elapsed, loss: 1.5203823e-06\n",
      "step: 100400 train: 0.0770869255065918 elapsed, loss: 1.7154936e-06\n",
      "step: 100410 train: 0.08051514625549316 elapsed, loss: 1.6260872e-06\n",
      "step: 100420 train: 0.08330869674682617 elapsed, loss: 1.788137e-06\n",
      "step: 100430 train: 0.08285331726074219 elapsed, loss: 1.3448285e-06\n",
      "step: 100440 train: 0.08872222900390625 elapsed, loss: 1.4388916e-06\n",
      "step: 100450 train: 0.08026361465454102 elapsed, loss: 1.828646e-06\n",
      "step: 100460 train: 0.09096932411193848 elapsed, loss: 1.3634551e-06\n",
      "step: 100470 train: 0.0770730972290039 elapsed, loss: 2.027952e-06\n",
      "step: 100480 train: 0.0865182876586914 elapsed, loss: 1.4356324e-06\n",
      "step: 100490 train: 0.08101081848144531 elapsed, loss: 2.2966374e-06\n",
      "step: 100500 train: 0.0748298168182373 elapsed, loss: 1.9716074e-06\n",
      "step: 100510 train: 0.07972860336303711 elapsed, loss: 1.621431e-06\n",
      "step: 100520 train: 0.077484130859375 elapsed, loss: 5.4858274e-06\n",
      "step: 100530 train: 0.0784761905670166 elapsed, loss: 1.935286e-06\n",
      "step: 100540 train: 0.08117437362670898 elapsed, loss: 1.6195683e-06\n",
      "step: 100550 train: 0.07982301712036133 elapsed, loss: 2.2738209e-06\n",
      "step: 100560 train: 0.0824286937713623 elapsed, loss: 1.7420368e-06\n",
      "step: 100570 train: 0.0848543643951416 elapsed, loss: 1.7415712e-06\n",
      "step: 100580 train: 0.08658981323242188 elapsed, loss: 1.499893e-06\n",
      "step: 100590 train: 0.08143329620361328 elapsed, loss: 1.7243416e-06\n",
      "step: 100600 train: 0.0830068588256836 elapsed, loss: 1.26846e-06\n",
      "step: 100610 train: 0.07971072196960449 elapsed, loss: 2.0386624e-06\n",
      "step: 100620 train: 0.08319568634033203 elapsed, loss: 2.4596188e-06\n",
      "step: 100630 train: 0.08088064193725586 elapsed, loss: 2.0582202e-06\n",
      "step: 100640 train: 0.08540058135986328 elapsed, loss: 1.7480907e-06\n",
      "step: 100650 train: 0.08502602577209473 elapsed, loss: 1.4742823e-06\n",
      "step: 100660 train: 0.08592104911804199 elapsed, loss: 2.130863e-06\n",
      "step: 100670 train: 0.07948875427246094 elapsed, loss: 1.7252728e-06\n",
      "step: 100680 train: 0.08153152465820312 elapsed, loss: 1.5893002e-06\n",
      "step: 100690 train: 0.08166193962097168 elapsed, loss: 1.7289984e-06\n",
      "step: 100700 train: 0.08092427253723145 elapsed, loss: 4.3902364e-06\n",
      "step: 100710 train: 0.07858896255493164 elapsed, loss: 2.2938443e-06\n",
      "step: 100720 train: 0.07966446876525879 elapsed, loss: 0.00019878626\n",
      "step: 100730 train: 0.07921242713928223 elapsed, loss: 6.774934e-05\n",
      "step: 100740 train: 0.08454751968383789 elapsed, loss: 3.3905955e-05\n",
      "step: 100750 train: 0.08463811874389648 elapsed, loss: 2.9610554e-05\n",
      "step: 100760 train: 0.08703732490539551 elapsed, loss: 1.4290375e-05\n",
      "step: 100770 train: 0.0782780647277832 elapsed, loss: 1.3358062e-05\n",
      "step: 100780 train: 0.08064651489257812 elapsed, loss: 8.570772e-06\n",
      "step: 100790 train: 0.07879376411437988 elapsed, loss: 1.0894142e-05\n",
      "step: 100800 train: 0.08220243453979492 elapsed, loss: 5.111076e-06\n",
      "step: 100810 train: 0.07588839530944824 elapsed, loss: 5.877545e-06\n",
      "step: 100820 train: 0.07740330696105957 elapsed, loss: 4.997459e-06\n",
      "step: 100830 train: 0.08223891258239746 elapsed, loss: 3.5534522e-06\n",
      "step: 100840 train: 0.0820016860961914 elapsed, loss: 2.569513e-06\n",
      "step: 100850 train: 0.07895994186401367 elapsed, loss: 2.1988494e-06\n",
      "step: 100860 train: 0.07946610450744629 elapsed, loss: 2.1411067e-06\n",
      "step: 100870 train: 0.08193564414978027 elapsed, loss: 1.5073441e-06\n",
      "step: 100880 train: 0.08178353309631348 elapsed, loss: 1.3378428e-06\n",
      "step: 100890 train: 0.08498120307922363 elapsed, loss: 1.2102527e-06\n",
      "step: 100900 train: 0.08436012268066406 elapsed, loss: 1.8989633e-06\n",
      "step: 100910 train: 0.08287358283996582 elapsed, loss: 1.7518157e-06\n",
      "step: 100920 train: 0.08298802375793457 elapsed, loss: 1.7504187e-06\n",
      "step: 100930 train: 0.07749629020690918 elapsed, loss: 1.6922111e-06\n",
      "step: 100940 train: 0.0849754810333252 elapsed, loss: 1.2144433e-06\n",
      "step: 100950 train: 0.07979869842529297 elapsed, loss: 2.240759e-06\n",
      "step: 100960 train: 0.08032393455505371 elapsed, loss: 2.163925e-06\n",
      "step: 100970 train: 0.08222150802612305 elapsed, loss: 1.4174716e-06\n",
      "step: 100980 train: 0.07434487342834473 elapsed, loss: 2.985813e-06\n",
      "step: 100990 train: 0.08558344841003418 elapsed, loss: 1.246574e-06\n",
      "step: 101000 train: 0.08175253868103027 elapsed, loss: 1.4132809e-06\n",
      "step: 101010 train: 0.07451152801513672 elapsed, loss: 1.8477419e-06\n",
      "step: 101020 train: 0.07884454727172852 elapsed, loss: 4.466594e-06\n",
      "step: 101030 train: 0.08475184440612793 elapsed, loss: 2.1858098e-06\n",
      "step: 101040 train: 0.07403564453125 elapsed, loss: 2.7795259e-06\n",
      "step: 101050 train: 0.08105278015136719 elapsed, loss: 1.3341185e-06\n",
      "step: 101060 train: 0.0827329158782959 elapsed, loss: 1.3937229e-06\n",
      "step: 101070 train: 0.08449220657348633 elapsed, loss: 1.2572846e-06\n",
      "step: 101080 train: 0.08133697509765625 elapsed, loss: 2.2272548e-06\n",
      "step: 101090 train: 0.0842599868774414 elapsed, loss: 1.5399396e-06\n",
      "step: 101100 train: 0.08100247383117676 elapsed, loss: 6.387422e-06\n",
      "step: 101110 train: 0.08026576042175293 elapsed, loss: 2.5108168e-06\n",
      "step: 101120 train: 0.07929134368896484 elapsed, loss: 2.167185e-06\n",
      "step: 101130 train: 0.08928203582763672 elapsed, loss: 1.2647351e-06\n",
      "step: 101140 train: 0.07984542846679688 elapsed, loss: 1.6130491e-06\n",
      "step: 101150 train: 0.08753204345703125 elapsed, loss: 1.9939591e-06\n",
      "step: 101160 train: 0.08244013786315918 elapsed, loss: 2.0889538e-06\n",
      "step: 101170 train: 0.08409380912780762 elapsed, loss: 1.2745138e-06\n",
      "step: 101180 train: 0.08373379707336426 elapsed, loss: 1.8510012e-06\n",
      "step: 101190 train: 0.07679033279418945 elapsed, loss: 2.080105e-06\n",
      "step: 101200 train: 0.07993435859680176 elapsed, loss: 1.7671825e-06\n",
      "step: 101210 train: 0.08212971687316895 elapsed, loss: 2.013982e-06\n",
      "step: 101220 train: 0.07632946968078613 elapsed, loss: 2.2198024e-06\n",
      "step: 101230 train: 0.08079791069030762 elapsed, loss: 0.00011715423\n",
      "step: 101240 train: 0.08163118362426758 elapsed, loss: 1.1934203e-05\n",
      "step: 101250 train: 0.08824324607849121 elapsed, loss: 6.513078e-06\n",
      "step: 101260 train: 0.0802617073059082 elapsed, loss: 7.5539074e-06\n",
      "step: 101270 train: 0.07776618003845215 elapsed, loss: 5.898505e-06\n",
      "step: 101280 train: 0.0805356502532959 elapsed, loss: 3.6936149e-06\n",
      "step: 101290 train: 0.07857728004455566 elapsed, loss: 2.8638094e-06\n",
      "step: 101300 train: 0.08227920532226562 elapsed, loss: 3.2572927e-06\n",
      "step: 101310 train: 0.08436965942382812 elapsed, loss: 2.919689e-06\n",
      "step: 101320 train: 0.07977819442749023 elapsed, loss: 2.6244602e-06\n",
      "step: 101330 train: 0.07921576499938965 elapsed, loss: 1.7438997e-06\n",
      "step: 101340 train: 0.07985496520996094 elapsed, loss: 1.2945372e-06\n",
      "step: 101350 train: 0.0803074836730957 elapsed, loss: 1.3336526e-06\n",
      "step: 101360 train: 0.07736921310424805 elapsed, loss: 1.3634548e-06\n",
      "step: 101370 train: 0.0832679271697998 elapsed, loss: 1.4100211e-06\n",
      "step: 101380 train: 0.08123350143432617 elapsed, loss: 1.4901146e-06\n",
      "step: 101390 train: 0.07672762870788574 elapsed, loss: 1.4710226e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 101400 train: 0.08199405670166016 elapsed, loss: 1.5650859e-06\n",
      "step: 101410 train: 0.0818336009979248 elapsed, loss: 1.7625254e-06\n",
      "step: 101420 train: 0.07779121398925781 elapsed, loss: 1.9334234e-06\n",
      "step: 101430 train: 0.07939910888671875 elapsed, loss: 1.4188686e-06\n",
      "step: 101440 train: 0.07319855690002441 elapsed, loss: 3.1795175e-06\n",
      "step: 101450 train: 0.08150625228881836 elapsed, loss: 1.5809184e-06\n",
      "step: 101460 train: 0.07822370529174805 elapsed, loss: 2.1322603e-06\n",
      "step: 101470 train: 0.08034706115722656 elapsed, loss: 1.5092068e-06\n",
      "step: 101480 train: 0.07995843887329102 elapsed, loss: 1.7494874e-06\n",
      "step: 101490 train: 0.07871150970458984 elapsed, loss: 2.2305144e-06\n",
      "step: 101500 train: 0.08360624313354492 elapsed, loss: 9.117997e-06\n",
      "step: 101510 train: 0.08123421669006348 elapsed, loss: 4.3051805e-05\n",
      "step: 101520 train: 0.08011722564697266 elapsed, loss: 2.0577529e-05\n",
      "step: 101530 train: 0.08398127555847168 elapsed, loss: 1.2571407e-05\n",
      "step: 101540 train: 0.08686113357543945 elapsed, loss: 1.000912e-05\n",
      "step: 101550 train: 0.08112716674804688 elapsed, loss: 7.592551e-06\n",
      "step: 101560 train: 0.0789024829864502 elapsed, loss: 8.35482e-06\n",
      "step: 101570 train: 0.08890271186828613 elapsed, loss: 3.5879089e-06\n",
      "step: 101580 train: 0.07484745979309082 elapsed, loss: 5.719687e-06\n",
      "step: 101590 train: 0.07848739624023438 elapsed, loss: 3.367652e-06\n",
      "step: 101600 train: 0.08554458618164062 elapsed, loss: 9.448164e-05\n",
      "step: 101610 train: 0.08627057075500488 elapsed, loss: 5.8463397e-06\n",
      "step: 101620 train: 0.07749652862548828 elapsed, loss: 4.728294e-06\n",
      "step: 101630 train: 0.08456850051879883 elapsed, loss: 2.8828822e-06\n",
      "step: 101640 train: 0.08490467071533203 elapsed, loss: 1.7634562e-06\n",
      "step: 101650 train: 0.08420920372009277 elapsed, loss: 2.8227996e-06\n",
      "step: 101660 train: 0.07988715171813965 elapsed, loss: 1.7150273e-06\n",
      "step: 101670 train: 0.08721160888671875 elapsed, loss: 1.2307407e-06\n",
      "step: 101680 train: 0.08118653297424316 elapsed, loss: 1.8523907e-06\n",
      "step: 101690 train: 0.08742284774780273 elapsed, loss: 1.0128126e-06\n",
      "step: 101700 train: 0.08258819580078125 elapsed, loss: 1.2819643e-06\n",
      "step: 101710 train: 0.07721495628356934 elapsed, loss: 1.5040846e-06\n",
      "step: 101720 train: 0.0856478214263916 elapsed, loss: 1.2814987e-06\n",
      "step: 101730 train: 0.07586002349853516 elapsed, loss: 1.5133978e-06\n",
      "step: 101740 train: 0.08642053604125977 elapsed, loss: 1.0030336e-06\n",
      "step: 101750 train: 0.07792520523071289 elapsed, loss: 1.2991936e-06\n",
      "step: 101760 train: 0.08621478080749512 elapsed, loss: 1.15903e-06\n",
      "step: 101770 train: 0.0799102783203125 elapsed, loss: 1.7457621e-06\n",
      "step: 101780 train: 0.08107995986938477 elapsed, loss: 1.1804505e-06\n",
      "step: 101790 train: 0.08582448959350586 elapsed, loss: 1.1739312e-06\n",
      "step: 101800 train: 0.08133888244628906 elapsed, loss: 1.8458791e-06\n",
      "step: 101810 train: 0.08228659629821777 elapsed, loss: 1.3215454e-06\n",
      "step: 101820 train: 0.09257292747497559 elapsed, loss: 7.20178e-06\n",
      "step: 101830 train: 0.08367586135864258 elapsed, loss: 3.3150332e-06\n",
      "step: 101840 train: 0.08922123908996582 elapsed, loss: 2.262635e-06\n",
      "step: 101850 train: 0.0787045955657959 elapsed, loss: 2.3748685e-06\n",
      "step: 101860 train: 0.08678936958312988 elapsed, loss: 1.2377267e-06\n",
      "step: 101870 train: 0.07933926582336426 elapsed, loss: 1.3764936e-06\n",
      "step: 101880 train: 0.08435726165771484 elapsed, loss: 1.1827788e-06\n",
      "step: 101890 train: 0.0834505558013916 elapsed, loss: 1.3522791e-06\n",
      "step: 101900 train: 0.08495593070983887 elapsed, loss: 1.1706716e-06\n",
      "step: 101910 train: 0.0764169692993164 elapsed, loss: 1.7336552e-06\n",
      "step: 101920 train: 0.07419013977050781 elapsed, loss: 2.2370336e-06\n",
      "step: 101930 train: 0.07892084121704102 elapsed, loss: 1.6405229e-06\n",
      "step: 101940 train: 0.07960772514343262 elapsed, loss: 1.5310927e-06\n",
      "step: 101950 train: 0.08168435096740723 elapsed, loss: 7.671504e-06\n",
      "step: 101960 train: 0.08319616317749023 elapsed, loss: 0.0001451777\n",
      "step: 101970 train: 0.08127498626708984 elapsed, loss: 6.856989e-05\n",
      "step: 101980 train: 0.0850214958190918 elapsed, loss: 2.3990066e-05\n",
      "step: 101990 train: 0.07843351364135742 elapsed, loss: 2.679078e-05\n",
      "step: 102000 train: 0.08249926567077637 elapsed, loss: 1.2726221e-05\n",
      "step: 102010 train: 0.083587646484375 elapsed, loss: 9.263776e-06\n",
      "step: 102020 train: 0.07835626602172852 elapsed, loss: 1.0686237e-05\n",
      "step: 102030 train: 0.0826718807220459 elapsed, loss: 6.921532e-06\n",
      "step: 102040 train: 0.08415412902832031 elapsed, loss: 4.9075543e-06\n",
      "step: 102050 train: 0.08208227157592773 elapsed, loss: 3.4691675e-06\n",
      "step: 102060 train: 0.07592082023620605 elapsed, loss: 6.242521e-06\n",
      "step: 102070 train: 0.07890892028808594 elapsed, loss: 3.3429744e-06\n",
      "step: 102080 train: 0.07923460006713867 elapsed, loss: 2.865674e-06\n",
      "step: 102090 train: 0.07956814765930176 elapsed, loss: 2.1439012e-06\n",
      "step: 102100 train: 0.07995367050170898 elapsed, loss: 1.8128169e-06\n",
      "step: 102110 train: 0.07642197608947754 elapsed, loss: 1.9422707e-06\n",
      "step: 102120 train: 0.08005619049072266 elapsed, loss: 1.6209642e-06\n",
      "step: 102130 train: 0.08272600173950195 elapsed, loss: 1.5520466e-06\n",
      "step: 102140 train: 0.0807950496673584 elapsed, loss: 3.95389e-06\n",
      "step: 102150 train: 0.08006525039672852 elapsed, loss: 1.776961e-06\n",
      "step: 102160 train: 0.07667899131774902 elapsed, loss: 1.9217819e-06\n",
      "step: 102170 train: 0.07985830307006836 elapsed, loss: 1.61724e-06\n",
      "step: 102180 train: 0.07970929145812988 elapsed, loss: 2.0209666e-06\n",
      "step: 102190 train: 0.07979011535644531 elapsed, loss: 2.1276035e-06\n",
      "step: 102200 train: 0.07810831069946289 elapsed, loss: 1.9096747e-06\n",
      "step: 102210 train: 0.08870220184326172 elapsed, loss: 1.1753281e-06\n",
      "step: 102220 train: 0.07864212989807129 elapsed, loss: 2.2673016e-06\n",
      "step: 102230 train: 0.0777883529663086 elapsed, loss: 2.000944e-06\n",
      "step: 102240 train: 0.08630156517028809 elapsed, loss: 1.4025696e-06\n",
      "step: 102250 train: 0.08118915557861328 elapsed, loss: 1.5385431e-06\n",
      "step: 102260 train: 0.08170008659362793 elapsed, loss: 1.4402887e-06\n",
      "step: 102270 train: 0.08248686790466309 elapsed, loss: 1.2186347e-06\n",
      "step: 102280 train: 0.0780940055847168 elapsed, loss: 1.6768444e-06\n",
      "step: 102290 train: 0.07918715476989746 elapsed, loss: 1.9199192e-06\n",
      "step: 102300 train: 0.08373475074768066 elapsed, loss: 1.3899978e-06\n",
      "step: 102310 train: 0.08488082885742188 elapsed, loss: 6.5294655e-05\n",
      "step: 102320 train: 0.07826018333435059 elapsed, loss: 0.000114252834\n",
      "step: 102330 train: 0.08631348609924316 elapsed, loss: 5.6346486e-05\n",
      "step: 102340 train: 0.08292841911315918 elapsed, loss: 3.7801e-05\n",
      "step: 102350 train: 0.07669568061828613 elapsed, loss: 1.7310223e-05\n",
      "step: 102360 train: 0.07876253128051758 elapsed, loss: 2.7515527e-05\n",
      "step: 102370 train: 0.07949233055114746 elapsed, loss: 1.0607097e-05\n",
      "step: 102380 train: 0.08486247062683105 elapsed, loss: 3.2283388e-05\n",
      "step: 102390 train: 0.08303999900817871 elapsed, loss: 4.972558e-05\n",
      "step: 102400 train: 0.0817415714263916 elapsed, loss: 6.132724e-06\n",
      "step: 102410 train: 0.07936906814575195 elapsed, loss: 5.376497e-06\n",
      "step: 102420 train: 0.0774693489074707 elapsed, loss: 3.5064131e-06\n",
      "step: 102430 train: 0.07744717597961426 elapsed, loss: 9.59794e-06\n",
      "step: 102440 train: 0.0855264663696289 elapsed, loss: 2.2789422e-06\n",
      "step: 102450 train: 0.08295679092407227 elapsed, loss: 1.7136314e-06\n",
      "step: 102460 train: 0.07862472534179688 elapsed, loss: 1.4272505e-06\n",
      "step: 102470 train: 0.08677411079406738 elapsed, loss: 1.2442459e-06\n",
      "step: 102480 train: 0.08063650131225586 elapsed, loss: 1.5739334e-06\n",
      "step: 102490 train: 0.07886075973510742 elapsed, loss: 1.161824e-06\n",
      "step: 102500 train: 0.08088159561157227 elapsed, loss: 1.1795191e-06\n",
      "step: 102510 train: 0.08284163475036621 elapsed, loss: 1.2563531e-06\n",
      "step: 102520 train: 0.0901174545288086 elapsed, loss: 1.0537906e-06\n",
      "step: 102530 train: 0.08272218704223633 elapsed, loss: 1.6069953e-06\n",
      "step: 102540 train: 0.07909750938415527 elapsed, loss: 1.3941888e-06\n",
      "step: 102550 train: 0.08098745346069336 elapsed, loss: 2.7455328e-06\n",
      "step: 102560 train: 0.0831000804901123 elapsed, loss: 1.3443625e-06\n",
      "step: 102570 train: 0.08441519737243652 elapsed, loss: 1.1227085e-06\n",
      "step: 102580 train: 0.0781559944152832 elapsed, loss: 1.5338867e-06\n",
      "step: 102590 train: 0.08251452445983887 elapsed, loss: 1.6344693e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 102600 train: 0.08138823509216309 elapsed, loss: 1.8263212e-06\n",
      "step: 102610 train: 0.08257889747619629 elapsed, loss: 1.6707909e-06\n",
      "step: 102620 train: 0.07584571838378906 elapsed, loss: 1.8342364e-06\n",
      "step: 102630 train: 0.0867149829864502 elapsed, loss: 1.1557704e-06\n",
      "step: 102640 train: 0.08543181419372559 elapsed, loss: 1.4123469e-06\n",
      "step: 102650 train: 0.08426785469055176 elapsed, loss: 1.286621e-06\n",
      "step: 102660 train: 0.07902026176452637 elapsed, loss: 1.3099042e-06\n",
      "step: 102670 train: 0.0874934196472168 elapsed, loss: 1.2726512e-06\n",
      "step: 102680 train: 0.07707548141479492 elapsed, loss: 2.1164276e-06\n",
      "step: 102690 train: 0.08196330070495605 elapsed, loss: 2.052166e-06\n",
      "step: 102700 train: 0.07822966575622559 elapsed, loss: 1.5795215e-06\n",
      "step: 102710 train: 0.08396339416503906 elapsed, loss: 1.4151434e-06\n",
      "step: 102720 train: 0.07801985740661621 elapsed, loss: 1.5418027e-06\n",
      "step: 102730 train: 0.08511734008789062 elapsed, loss: 1.4030363e-06\n",
      "step: 102740 train: 0.07297706604003906 elapsed, loss: 2.4442502e-06\n",
      "step: 102750 train: 0.08227300643920898 elapsed, loss: 2.3026907e-06\n",
      "step: 102760 train: 0.08733606338500977 elapsed, loss: 1.0551878e-06\n",
      "step: 102770 train: 0.08040380477905273 elapsed, loss: 1.4347008e-06\n",
      "step: 102780 train: 0.08587074279785156 elapsed, loss: 1.2638036e-06\n",
      "step: 102790 train: 0.0794379711151123 elapsed, loss: 2.2207355e-06\n",
      "step: 102800 train: 0.0835421085357666 elapsed, loss: 1.5594981e-06\n",
      "step: 102810 train: 0.07941031455993652 elapsed, loss: 2.450769e-06\n",
      "step: 102820 train: 0.07994389533996582 elapsed, loss: 1.9804547e-06\n",
      "step: 102830 train: 0.07958340644836426 elapsed, loss: 3.837722e-06\n",
      "step: 102840 train: 0.07809710502624512 elapsed, loss: 8.338971e-06\n",
      "step: 102850 train: 0.08788108825683594 elapsed, loss: 2.6696305e-06\n",
      "step: 102860 train: 0.07776999473571777 elapsed, loss: 2.2766144e-06\n",
      "step: 102870 train: 0.08028650283813477 elapsed, loss: 1.9450647e-06\n",
      "step: 102880 train: 0.07460308074951172 elapsed, loss: 2.625859e-06\n",
      "step: 102890 train: 0.0820925235748291 elapsed, loss: 2.2733293e-06\n",
      "step: 102900 train: 0.0817568302154541 elapsed, loss: 2.0004782e-06\n",
      "step: 102910 train: 0.08527207374572754 elapsed, loss: 1.2521621e-06\n",
      "step: 102920 train: 0.08405017852783203 elapsed, loss: 1.2936059e-06\n",
      "step: 102930 train: 0.08344769477844238 elapsed, loss: 1.455187e-06\n",
      "step: 102940 train: 0.0859220027923584 elapsed, loss: 1.7462278e-06\n",
      "step: 102950 train: 0.08568882942199707 elapsed, loss: 2.0833659e-06\n",
      "step: 102960 train: 0.09037113189697266 elapsed, loss: 1.3052475e-06\n",
      "step: 102970 train: 0.07981538772583008 elapsed, loss: 2.2910494e-06\n",
      "step: 102980 train: 0.08224058151245117 elapsed, loss: 1.8682308e-06\n",
      "step: 102990 train: 0.08931279182434082 elapsed, loss: 1.3252707e-06\n",
      "step: 103000 train: 0.0808267593383789 elapsed, loss: 1.9590345e-06\n",
      "step: 103010 train: 0.07825112342834473 elapsed, loss: 3.0645087e-06\n",
      "step: 103020 train: 0.08337020874023438 elapsed, loss: 1.7550753e-06\n",
      "step: 103030 train: 0.0863184928894043 elapsed, loss: 1.2097871e-06\n",
      "step: 103040 train: 0.07861161231994629 elapsed, loss: 2.0363339e-06\n",
      "step: 103050 train: 0.08458352088928223 elapsed, loss: 1.7713737e-06\n",
      "step: 103060 train: 0.07978010177612305 elapsed, loss: 1.7993128e-06\n",
      "step: 103070 train: 0.07513117790222168 elapsed, loss: 2.228186e-06\n",
      "step: 103080 train: 0.07892417907714844 elapsed, loss: 2.3478606e-06\n",
      "step: 103090 train: 0.08648896217346191 elapsed, loss: 2.5462314e-06\n",
      "step: 103100 train: 0.08732843399047852 elapsed, loss: 1.6479722e-06\n",
      "step: 103110 train: 0.08469796180725098 elapsed, loss: 0.065777\n",
      "step: 103120 train: 0.08378100395202637 elapsed, loss: 8.9821355e-05\n",
      "step: 103130 train: 0.07703685760498047 elapsed, loss: 5.1761737e-05\n",
      "step: 103140 train: 0.08370304107666016 elapsed, loss: 1.8239594e-05\n",
      "step: 103150 train: 0.08512401580810547 elapsed, loss: 1.2030654e-05\n",
      "step: 103160 train: 0.08020925521850586 elapsed, loss: 1.1510549e-05\n",
      "step: 103170 train: 0.07973289489746094 elapsed, loss: 1.210381e-05\n",
      "step: 103180 train: 0.08016109466552734 elapsed, loss: 1.1199522e-05\n",
      "step: 103190 train: 0.07926654815673828 elapsed, loss: 8.618854e-06\n",
      "step: 103200 train: 0.08860325813293457 elapsed, loss: 3.829118e-06\n",
      "step: 103210 train: 0.08364009857177734 elapsed, loss: 3.0700976e-06\n",
      "step: 103220 train: 0.077301025390625 elapsed, loss: 3.1697457e-06\n",
      "step: 103230 train: 0.0794687271118164 elapsed, loss: 2.4139804e-06\n",
      "step: 103240 train: 0.08749008178710938 elapsed, loss: 2.6053692e-06\n",
      "step: 103250 train: 0.08208084106445312 elapsed, loss: 2.2388958e-06\n",
      "step: 103260 train: 0.08069729804992676 elapsed, loss: 1.7178209e-06\n",
      "step: 103270 train: 0.08448386192321777 elapsed, loss: 1.551116e-06\n",
      "step: 103280 train: 0.08451581001281738 elapsed, loss: 1.962294e-06\n",
      "step: 103290 train: 0.08452105522155762 elapsed, loss: 1.2586814e-06\n",
      "step: 103300 train: 0.09069609642028809 elapsed, loss: 1.5203826e-06\n",
      "step: 103310 train: 0.08042120933532715 elapsed, loss: 1.9133972e-06\n",
      "step: 103320 train: 0.08496904373168945 elapsed, loss: 1.2018709e-06\n",
      "step: 103330 train: 0.08283782005310059 elapsed, loss: 1.6782412e-06\n",
      "step: 103340 train: 0.082763671875 elapsed, loss: 1.5594981e-06\n",
      "step: 103350 train: 0.07808828353881836 elapsed, loss: 5.3308668e-06\n",
      "step: 103360 train: 0.07706165313720703 elapsed, loss: 2.473123e-06\n",
      "step: 103370 train: 0.08082294464111328 elapsed, loss: 2.3073467e-06\n",
      "step: 103380 train: 0.08175468444824219 elapsed, loss: 1.4542583e-06\n",
      "step: 103390 train: 0.08026432991027832 elapsed, loss: 1.4868529e-06\n",
      "step: 103400 train: 0.08280134201049805 elapsed, loss: 1.4114178e-06\n",
      "step: 103410 train: 0.08424782752990723 elapsed, loss: 1.5390086e-06\n",
      "step: 103420 train: 0.08071351051330566 elapsed, loss: 1.2596128e-06\n",
      "step: 103430 train: 0.08337688446044922 elapsed, loss: 1.7303951e-06\n",
      "step: 103440 train: 0.07893061637878418 elapsed, loss: 1.9660192e-06\n",
      "step: 103450 train: 0.08164644241333008 elapsed, loss: 2.072189e-06\n",
      "step: 103460 train: 0.08135604858398438 elapsed, loss: 1.5837124e-06\n",
      "step: 103470 train: 0.07720375061035156 elapsed, loss: 2.1462297e-06\n",
      "step: 103480 train: 0.08726096153259277 elapsed, loss: 1.3979138e-06\n",
      "step: 103490 train: 0.08182954788208008 elapsed, loss: 1.8249243e-06\n",
      "step: 103500 train: 0.08060455322265625 elapsed, loss: 1.4347013e-06\n",
      "step: 103510 train: 0.08451581001281738 elapsed, loss: 1.3587983e-06\n",
      "step: 103520 train: 0.0876307487487793 elapsed, loss: 1.3941885e-06\n",
      "step: 103530 train: 0.08130311965942383 elapsed, loss: 1.4100211e-06\n",
      "step: 103540 train: 0.07880592346191406 elapsed, loss: 2.4530991e-06\n",
      "step: 103550 train: 0.08719420433044434 elapsed, loss: 1.4845267e-06\n",
      "step: 103560 train: 0.0800626277923584 elapsed, loss: 1.6838294e-06\n",
      "step: 103570 train: 0.07960653305053711 elapsed, loss: 2.1234114e-06\n",
      "step: 103580 train: 0.08576750755310059 elapsed, loss: 2.6428925e-06\n",
      "step: 103590 train: 0.07697415351867676 elapsed, loss: 0.0008816851\n",
      "step: 103600 train: 0.0872800350189209 elapsed, loss: 2.6882539e-05\n",
      "step: 103610 train: 0.08055734634399414 elapsed, loss: 2.0325839e-05\n",
      "step: 103620 train: 0.08242034912109375 elapsed, loss: 1.3124005e-05\n",
      "step: 103630 train: 0.07448267936706543 elapsed, loss: 1.6538077e-05\n",
      "step: 103640 train: 0.07483696937561035 elapsed, loss: 1.0989493e-05\n",
      "step: 103650 train: 0.08294987678527832 elapsed, loss: 6.118751e-06\n",
      "step: 103660 train: 0.08788037300109863 elapsed, loss: 4.368354e-06\n",
      "step: 103670 train: 0.08262014389038086 elapsed, loss: 5.144606e-06\n",
      "step: 103680 train: 0.07604122161865234 elapsed, loss: 5.5376086e-06\n",
      "step: 103690 train: 0.08310675621032715 elapsed, loss: 2.4968713e-06\n",
      "step: 103700 train: 0.07698893547058105 elapsed, loss: 2.4847636e-06\n",
      "step: 103710 train: 0.08382391929626465 elapsed, loss: 2.1103733e-06\n",
      "step: 103720 train: 0.08104753494262695 elapsed, loss: 2.3520479e-06\n",
      "step: 103730 train: 0.08176922798156738 elapsed, loss: 1.4654338e-06\n",
      "step: 103740 train: 0.08189082145690918 elapsed, loss: 1.6498361e-06\n",
      "step: 103750 train: 0.07824349403381348 elapsed, loss: 2.064739e-06\n",
      "step: 103760 train: 0.08364105224609375 elapsed, loss: 1.7764958e-06\n",
      "step: 103770 train: 0.0861823558807373 elapsed, loss: 1.6405231e-06\n",
      "step: 103780 train: 0.07856297492980957 elapsed, loss: 1.6144451e-06\n",
      "step: 103790 train: 0.07863140106201172 elapsed, loss: 1.5413374e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 103800 train: 0.08569836616516113 elapsed, loss: 1.5837124e-06\n",
      "step: 103810 train: 0.07955765724182129 elapsed, loss: 2.9373855e-06\n",
      "step: 103820 train: 0.08164525032043457 elapsed, loss: 1.5455283e-06\n",
      "step: 103830 train: 0.07953929901123047 elapsed, loss: 1.2256196e-06\n",
      "step: 103840 train: 0.07729506492614746 elapsed, loss: 1.884525e-06\n",
      "step: 103850 train: 0.08434915542602539 elapsed, loss: 1.6530855e-06\n",
      "step: 103860 train: 0.08819770812988281 elapsed, loss: 1.3899976e-06\n",
      "step: 103870 train: 0.08154535293579102 elapsed, loss: 3.7182951e-06\n",
      "step: 103880 train: 0.08326005935668945 elapsed, loss: 1.581384e-06\n",
      "step: 103890 train: 0.08365941047668457 elapsed, loss: 1.4854581e-06\n",
      "step: 103900 train: 0.08036518096923828 elapsed, loss: 1.8007098e-06\n",
      "step: 103910 train: 0.08053755760192871 elapsed, loss: 1.6307442e-06\n",
      "step: 103920 train: 0.0769493579864502 elapsed, loss: 1.898962e-06\n",
      "step: 103930 train: 0.08411264419555664 elapsed, loss: 1.6693932e-06\n",
      "step: 103940 train: 0.08465933799743652 elapsed, loss: 0.00013105264\n",
      "step: 103950 train: 0.08283376693725586 elapsed, loss: 2.6293272e-05\n",
      "step: 103960 train: 0.08110761642456055 elapsed, loss: 1.4344485e-05\n",
      "step: 103970 train: 0.08309078216552734 elapsed, loss: 1.0793448e-05\n",
      "step: 103980 train: 0.0818018913269043 elapsed, loss: 1.0946624e-05\n",
      "step: 103990 train: 0.07613468170166016 elapsed, loss: 1.09303055e-05\n",
      "step: 104000 train: 0.0767672061920166 elapsed, loss: 7.5045523e-06\n",
      "step: 104010 train: 0.07626080513000488 elapsed, loss: 6.156478e-06\n",
      "step: 104020 train: 0.07902765274047852 elapsed, loss: 3.8514727e-06\n",
      "step: 104030 train: 0.0793914794921875 elapsed, loss: 3.070564e-06\n",
      "step: 104040 train: 0.07973957061767578 elapsed, loss: 2.9261903e-06\n",
      "step: 104050 train: 0.08652710914611816 elapsed, loss: 2.0004775e-06\n",
      "step: 104060 train: 0.08060765266418457 elapsed, loss: 2.0228288e-06\n",
      "step: 104070 train: 0.08311820030212402 elapsed, loss: 1.6526292e-06\n",
      "step: 104080 train: 0.08438897132873535 elapsed, loss: 1.4179371e-06\n",
      "step: 104090 train: 0.08404374122619629 elapsed, loss: 1.3560045e-06\n",
      "step: 104100 train: 0.0797727108001709 elapsed, loss: 1.892445e-06\n",
      "step: 104110 train: 0.07931923866271973 elapsed, loss: 1.9720649e-06\n",
      "step: 104120 train: 0.08018040657043457 elapsed, loss: 2.0852267e-06\n",
      "step: 104130 train: 0.07805991172790527 elapsed, loss: 1.5539101e-06\n",
      "step: 104140 train: 0.08036684989929199 elapsed, loss: 1.4002421e-06\n",
      "step: 104150 train: 0.08172821998596191 elapsed, loss: 1.3001253e-06\n",
      "step: 104160 train: 0.08428335189819336 elapsed, loss: 1.3443628e-06\n",
      "step: 104170 train: 0.08610343933105469 elapsed, loss: 1.1199145e-06\n",
      "step: 104180 train: 0.08094930648803711 elapsed, loss: 1.3215456e-06\n",
      "step: 104190 train: 0.07950115203857422 elapsed, loss: 1.4891832e-06\n",
      "step: 104200 train: 0.08253216743469238 elapsed, loss: 1.5860408e-06\n",
      "step: 104210 train: 0.07739853858947754 elapsed, loss: 1.7206166e-06\n",
      "step: 104220 train: 0.0859534740447998 elapsed, loss: 1.206993e-06\n",
      "step: 104230 train: 0.0823218822479248 elapsed, loss: 1.3643864e-06\n",
      "step: 104240 train: 0.07783746719360352 elapsed, loss: 1.7434338e-06\n",
      "step: 104250 train: 0.08392715454101562 elapsed, loss: 1.7997779e-06\n",
      "step: 104260 train: 0.08212065696716309 elapsed, loss: 1.542734e-06\n",
      "step: 104270 train: 0.08393979072570801 elapsed, loss: 1.6926766e-06\n",
      "step: 104280 train: 0.0801541805267334 elapsed, loss: 0.00015083472\n",
      "step: 104290 train: 0.08237504959106445 elapsed, loss: 3.4626835e-05\n",
      "step: 104300 train: 0.07680869102478027 elapsed, loss: 3.8988117e-05\n",
      "step: 104310 train: 0.0839545726776123 elapsed, loss: 1.8959607e-05\n",
      "step: 104320 train: 0.08248519897460938 elapsed, loss: 1.464155e-05\n",
      "step: 104330 train: 0.08218789100646973 elapsed, loss: 7.4174745e-06\n",
      "step: 104340 train: 0.08159208297729492 elapsed, loss: 6.4593456e-05\n",
      "step: 104350 train: 0.0843510627746582 elapsed, loss: 1.3395422e-05\n",
      "step: 104360 train: 0.08160209655761719 elapsed, loss: 5.642631e-06\n",
      "step: 104370 train: 0.08521485328674316 elapsed, loss: 2.7315634e-06\n",
      "step: 104380 train: 0.07711505889892578 elapsed, loss: 3.9194547e-06\n",
      "step: 104390 train: 0.07939362525939941 elapsed, loss: 2.759502e-06\n",
      "step: 104400 train: 0.08114385604858398 elapsed, loss: 2.4349379e-06\n",
      "step: 104410 train: 0.0874624252319336 elapsed, loss: 4.5019806e-06\n",
      "step: 104420 train: 0.07867264747619629 elapsed, loss: 2.2142158e-06\n",
      "step: 104430 train: 0.08541727066040039 elapsed, loss: 1.5436656e-06\n",
      "step: 104440 train: 0.08076286315917969 elapsed, loss: 1.5464594e-06\n",
      "step: 104450 train: 0.0795743465423584 elapsed, loss: 1.5813841e-06\n",
      "step: 104460 train: 0.07909750938415527 elapsed, loss: 2.0274865e-06\n",
      "step: 104470 train: 0.08351802825927734 elapsed, loss: 1.5944224e-06\n",
      "step: 104480 train: 0.08234333992004395 elapsed, loss: 1.7709076e-06\n",
      "step: 104490 train: 0.08236360549926758 elapsed, loss: 1.4007078e-06\n",
      "step: 104500 train: 0.08376860618591309 elapsed, loss: 1.3601953e-06\n",
      "step: 104510 train: 0.08950448036193848 elapsed, loss: 1.108273e-06\n",
      "step: 104520 train: 0.08085417747497559 elapsed, loss: 1.688486e-06\n",
      "step: 104530 train: 0.08400774002075195 elapsed, loss: 2.013982e-06\n",
      "step: 104540 train: 0.0823216438293457 elapsed, loss: 1.3336521e-06\n",
      "step: 104550 train: 0.07970428466796875 elapsed, loss: 1.347157e-06\n",
      "step: 104560 train: 0.08250045776367188 elapsed, loss: 2.8372633e-06\n",
      "step: 104570 train: 0.08213281631469727 elapsed, loss: 1.4700912e-06\n",
      "step: 104580 train: 0.08387303352355957 elapsed, loss: 1.6936083e-06\n",
      "step: 104590 train: 0.08748173713684082 elapsed, loss: 1.4742811e-06\n",
      "step: 104600 train: 0.07903504371643066 elapsed, loss: 2.3189896e-06\n",
      "step: 104610 train: 0.08250975608825684 elapsed, loss: 1.4528612e-06\n",
      "step: 104620 train: 0.0756528377532959 elapsed, loss: 2.1345884e-06\n",
      "step: 104630 train: 0.07948994636535645 elapsed, loss: 2.196056e-06\n",
      "step: 104640 train: 0.08702731132507324 elapsed, loss: 1.5478565e-06\n",
      "step: 104650 train: 0.0741431713104248 elapsed, loss: 2.7180602e-06\n",
      "step: 104660 train: 0.08166861534118652 elapsed, loss: 1.8822009e-06\n",
      "step: 104670 train: 0.08865880966186523 elapsed, loss: 1.3969824e-06\n",
      "step: 104680 train: 0.08448576927185059 elapsed, loss: 1.4919773e-06\n",
      "step: 104690 train: 0.0798180103302002 elapsed, loss: 1.6819665e-06\n",
      "step: 104700 train: 0.08240270614624023 elapsed, loss: 2.0535635e-06\n",
      "step: 104710 train: 0.08165812492370605 elapsed, loss: 2.5290024e-06\n",
      "step: 104720 train: 0.08150649070739746 elapsed, loss: 2.325975e-06\n",
      "step: 104730 train: 0.07990050315856934 elapsed, loss: 1.8947736e-06\n",
      "step: 104740 train: 0.0818636417388916 elapsed, loss: 2.2612476e-06\n",
      "step: 104750 train: 0.08675575256347656 elapsed, loss: 2.2291163e-06\n",
      "step: 104760 train: 0.08615946769714355 elapsed, loss: 2.151352e-06\n",
      "step: 104770 train: 0.08543229103088379 elapsed, loss: 1.8230619e-06\n",
      "step: 104780 train: 0.08050847053527832 elapsed, loss: 1.7597314e-06\n",
      "step: 104790 train: 0.08453607559204102 elapsed, loss: 1.8733531e-06\n",
      "step: 104800 train: 0.08120512962341309 elapsed, loss: 2.5387799e-06\n",
      "step: 104810 train: 0.08152294158935547 elapsed, loss: 1.8030385e-06\n",
      "step: 104820 train: 0.08589935302734375 elapsed, loss: 0.00010593773\n",
      "step: 104830 train: 0.07656121253967285 elapsed, loss: 0.0003936587\n",
      "step: 104840 train: 0.08028483390808105 elapsed, loss: 8.653628e-05\n",
      "step: 104850 train: 0.07919144630432129 elapsed, loss: 0.0004803959\n",
      "step: 104860 train: 0.08019113540649414 elapsed, loss: 0.00014826463\n",
      "step: 104870 train: 0.07844877243041992 elapsed, loss: 0.00011944119\n",
      "step: 104880 train: 0.07880187034606934 elapsed, loss: 4.6942936e-05\n",
      "step: 104890 train: 0.08359766006469727 elapsed, loss: 2.0325726e-05\n",
      "step: 104900 train: 0.08340620994567871 elapsed, loss: 1.7025475e-05\n",
      "step: 104910 train: 0.0790395736694336 elapsed, loss: 1.5642221e-05\n",
      "step: 104920 train: 0.08279895782470703 elapsed, loss: 7.1259506e-06\n",
      "step: 104930 train: 0.08246994018554688 elapsed, loss: 8.007411e-06\n",
      "step: 104940 train: 0.08560752868652344 elapsed, loss: 5.4682287e-06\n",
      "step: 104950 train: 0.0874490737915039 elapsed, loss: 3.5152675e-06\n",
      "step: 104960 train: 0.08115363121032715 elapsed, loss: 5.262843e-06\n",
      "step: 104970 train: 0.08627510070800781 elapsed, loss: 2.271026e-06\n",
      "step: 104980 train: 0.08062481880187988 elapsed, loss: 3.8374765e-06\n",
      "step: 104990 train: 0.08302164077758789 elapsed, loss: 2.3720743e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 105000 train: 0.07917571067810059 elapsed, loss: 2.0023406e-06\n",
      "step: 105010 train: 0.07953572273254395 elapsed, loss: 0.0064479206\n",
      "step: 105020 train: 0.07757067680358887 elapsed, loss: 6.882675e-05\n",
      "step: 105030 train: 0.07926297187805176 elapsed, loss: 3.926901e-05\n",
      "step: 105040 train: 0.08121824264526367 elapsed, loss: 2.240616e-05\n",
      "step: 105050 train: 0.0808558464050293 elapsed, loss: 1.5850099e-05\n",
      "step: 105060 train: 0.08410286903381348 elapsed, loss: 7.868188e-06\n",
      "step: 105070 train: 0.08627843856811523 elapsed, loss: 5.550188e-06\n",
      "step: 105080 train: 0.0749359130859375 elapsed, loss: 6.6970924e-06\n",
      "step: 105090 train: 0.07839179039001465 elapsed, loss: 6.269163e-06\n",
      "step: 105100 train: 0.08462238311767578 elapsed, loss: 3.0123483e-06\n",
      "step: 105110 train: 0.08162736892700195 elapsed, loss: 3.2060702e-06\n",
      "step: 105120 train: 0.08645367622375488 elapsed, loss: 2.0498373e-06\n",
      "step: 105130 train: 0.08394694328308105 elapsed, loss: 1.6689276e-06\n",
      "step: 105140 train: 0.08113551139831543 elapsed, loss: 1.853793e-06\n",
      "step: 105150 train: 0.08257341384887695 elapsed, loss: 1.6377285e-06\n",
      "step: 105160 train: 0.07997798919677734 elapsed, loss: 1.6260873e-06\n",
      "step: 105170 train: 0.08161187171936035 elapsed, loss: 1.4961677e-06\n",
      "step: 105180 train: 0.08506894111633301 elapsed, loss: 1.1515795e-06\n",
      "step: 105190 train: 0.08507490158081055 elapsed, loss: 1.2470384e-06\n",
      "step: 105200 train: 0.07564401626586914 elapsed, loss: 1.7303953e-06\n",
      "step: 105210 train: 0.07840371131896973 elapsed, loss: 9.783537e-07\n",
      "step: 105220 train: 0.07993960380554199 elapsed, loss: 1.5730022e-06\n",
      "step: 105230 train: 0.08694243431091309 elapsed, loss: 9.457574e-07\n",
      "step: 105240 train: 0.07980704307556152 elapsed, loss: 1.2633379e-06\n",
      "step: 105250 train: 0.08063912391662598 elapsed, loss: 1.6819666e-06\n",
      "step: 105260 train: 0.08285760879516602 elapsed, loss: 1.2754451e-06\n",
      "step: 105270 train: 0.07825589179992676 elapsed, loss: 1.2647348e-06\n",
      "step: 105280 train: 0.07878518104553223 elapsed, loss: 1.6204995e-06\n",
      "step: 105290 train: 0.08155393600463867 elapsed, loss: 1.8933738e-06\n",
      "step: 105300 train: 0.08497476577758789 elapsed, loss: 1.1366783e-06\n",
      "step: 105310 train: 0.08265018463134766 elapsed, loss: 1.8188707e-06\n",
      "step: 105320 train: 0.0854945182800293 elapsed, loss: 1.0379583e-06\n",
      "step: 105330 train: 0.07497000694274902 elapsed, loss: 1.6661342e-06\n",
      "step: 105340 train: 0.07999110221862793 elapsed, loss: 1.3532103e-06\n",
      "step: 105350 train: 0.07970428466796875 elapsed, loss: 1.4859237e-06\n",
      "step: 105360 train: 0.07847738265991211 elapsed, loss: 1.9385443e-06\n",
      "step: 105370 train: 0.07336091995239258 elapsed, loss: 1.7685793e-06\n",
      "step: 105380 train: 0.08355903625488281 elapsed, loss: 1.3583327e-06\n",
      "step: 105390 train: 0.08454370498657227 elapsed, loss: 1.2842927e-06\n",
      "step: 105400 train: 0.08137083053588867 elapsed, loss: 1.7760299e-06\n",
      "step: 105410 train: 0.08354520797729492 elapsed, loss: 1.1925576e-06\n",
      "step: 105420 train: 0.08803892135620117 elapsed, loss: 1.1324875e-06\n",
      "step: 105430 train: 0.07658219337463379 elapsed, loss: 1.955309e-06\n",
      "step: 105440 train: 0.07846784591674805 elapsed, loss: 1.9781266e-06\n",
      "step: 105450 train: 0.07961177825927734 elapsed, loss: 1.8156088e-06\n",
      "step: 105460 train: 0.08016085624694824 elapsed, loss: 1.713631e-06\n",
      "step: 105470 train: 0.08348608016967773 elapsed, loss: 1.1669462e-06\n",
      "step: 105480 train: 0.0816044807434082 elapsed, loss: 1.46916e-06\n",
      "step: 105490 train: 0.07974886894226074 elapsed, loss: 1.4901146e-06\n",
      "step: 105500 train: 0.08287787437438965 elapsed, loss: 1.6815006e-06\n",
      "step: 105510 train: 0.0774390697479248 elapsed, loss: 1.5748649e-06\n",
      "step: 105520 train: 0.08525848388671875 elapsed, loss: 1.6498363e-06\n",
      "step: 105530 train: 0.07924699783325195 elapsed, loss: 2.550421e-06\n",
      "step: 105540 train: 0.08250260353088379 elapsed, loss: 1.4659004e-06\n",
      "step: 105550 train: 0.08023905754089355 elapsed, loss: 1.9245758e-06\n",
      "step: 105560 train: 0.08422470092773438 elapsed, loss: 2.0307461e-06\n",
      "step: 105570 train: 0.07535195350646973 elapsed, loss: 2.1597307e-06\n",
      "step: 105580 train: 0.08577299118041992 elapsed, loss: 2.1024573e-06\n",
      "step: 105590 train: 0.08278727531433105 elapsed, loss: 1.3047818e-06\n",
      "step: 105600 train: 0.08353471755981445 elapsed, loss: 2.0056002e-06\n",
      "step: 105610 train: 0.08303308486938477 elapsed, loss: 1.4472739e-06\n",
      "step: 105620 train: 0.08130478858947754 elapsed, loss: 2.2137506e-06\n",
      "step: 105630 train: 0.0840609073638916 elapsed, loss: 1.5674143e-06\n",
      "step: 105640 train: 0.08689999580383301 elapsed, loss: 1.5329554e-06\n",
      "step: 105650 train: 0.08286213874816895 elapsed, loss: 1.9366817e-06\n",
      "step: 105660 train: 0.08596110343933105 elapsed, loss: 1.5590324e-06\n",
      "step: 105670 train: 0.08638763427734375 elapsed, loss: 2.126205e-06\n",
      "step: 105680 train: 0.08759927749633789 elapsed, loss: 1.7187537e-06\n",
      "step: 105690 train: 0.0835726261138916 elapsed, loss: 1.7173567e-06\n",
      "step: 105700 train: 0.0755162239074707 elapsed, loss: 2.4209694e-06\n",
      "step: 105710 train: 0.07662725448608398 elapsed, loss: 2.56253e-06\n",
      "step: 105720 train: 0.07850503921508789 elapsed, loss: 2.2849968e-06\n",
      "step: 105730 train: 0.08149170875549316 elapsed, loss: 5.0505378e-06\n",
      "step: 105740 train: 0.08169698715209961 elapsed, loss: 1.6754475e-06\n",
      "step: 105750 train: 0.07696270942687988 elapsed, loss: 2.7641606e-06\n",
      "step: 105760 train: 0.07537102699279785 elapsed, loss: 2.3865105e-06\n",
      "step: 105770 train: 0.08106088638305664 elapsed, loss: 2.9574082e-06\n",
      "step: 105780 train: 0.08168268203735352 elapsed, loss: 1.8621772e-06\n",
      "step: 105790 train: 0.0818171501159668 elapsed, loss: 1.3890663e-06\n",
      "step: 105800 train: 0.07477784156799316 elapsed, loss: 2.975569e-06\n",
      "step: 105810 train: 0.07780623435974121 elapsed, loss: 2.849838e-06\n",
      "step: 105820 train: 0.08085870742797852 elapsed, loss: 2.9372973e-06\n",
      "step: 105830 train: 0.07849431037902832 elapsed, loss: 2.4423884e-06\n",
      "step: 105840 train: 0.08054256439208984 elapsed, loss: 2.1047845e-06\n",
      "step: 105850 train: 0.08470559120178223 elapsed, loss: 0.0005289658\n",
      "step: 105860 train: 0.08614015579223633 elapsed, loss: 4.637502e-05\n",
      "step: 105870 train: 0.0825490951538086 elapsed, loss: 1.7475813e-05\n",
      "step: 105880 train: 0.08721160888671875 elapsed, loss: 1.2409177e-05\n",
      "step: 105890 train: 0.0827932357788086 elapsed, loss: 9.065838e-06\n",
      "step: 105900 train: 0.07804203033447266 elapsed, loss: 6.910829e-06\n",
      "step: 105910 train: 0.0797271728515625 elapsed, loss: 6.2812687e-06\n",
      "step: 105920 train: 0.07880997657775879 elapsed, loss: 5.2228334e-06\n",
      "step: 105930 train: 0.07822775840759277 elapsed, loss: 4.169052e-06\n",
      "step: 105940 train: 0.07809281349182129 elapsed, loss: 4.4088665e-06\n",
      "step: 105950 train: 0.08501482009887695 elapsed, loss: 2.23843e-06\n",
      "step: 105960 train: 0.08067655563354492 elapsed, loss: 2.0079283e-06\n",
      "step: 105970 train: 0.08039546012878418 elapsed, loss: 2.72551e-06\n",
      "step: 105980 train: 0.07853937149047852 elapsed, loss: 2.2677664e-06\n",
      "step: 105990 train: 0.08685684204101562 elapsed, loss: 1.4472739e-06\n",
      "step: 106000 train: 0.08290934562683105 elapsed, loss: 1.6065297e-06\n",
      "step: 106010 train: 0.0928194522857666 elapsed, loss: 1.2004737e-06\n",
      "step: 106020 train: 0.08164262771606445 elapsed, loss: 1.995355e-06\n",
      "step: 106030 train: 0.0768132209777832 elapsed, loss: 1.505947e-06\n",
      "step: 106040 train: 0.08071351051330566 elapsed, loss: 1.7127e-06\n",
      "step: 106050 train: 0.07783102989196777 elapsed, loss: 1.9422707e-06\n",
      "step: 106060 train: 0.07758498191833496 elapsed, loss: 1.8873203e-06\n",
      "step: 106070 train: 0.07901358604431152 elapsed, loss: 0.00011908495\n",
      "step: 106080 train: 0.08777189254760742 elapsed, loss: 2.130074e-05\n",
      "step: 106090 train: 0.08079862594604492 elapsed, loss: 2.551639e-05\n",
      "step: 106100 train: 0.08461809158325195 elapsed, loss: 1.637809e-05\n",
      "step: 106110 train: 0.08140993118286133 elapsed, loss: 1.5915044e-05\n",
      "step: 106120 train: 0.08456277847290039 elapsed, loss: 1.0794816e-05\n",
      "step: 106130 train: 0.08525276184082031 elapsed, loss: 1.1869796e-05\n",
      "step: 106140 train: 0.0816488265991211 elapsed, loss: 7.490556e-06\n",
      "step: 106150 train: 0.08695673942565918 elapsed, loss: 3.6656752e-06\n",
      "step: 106160 train: 0.08569002151489258 elapsed, loss: 2.8256254e-06\n",
      "step: 106170 train: 0.08204460144042969 elapsed, loss: 3.3029119e-06\n",
      "step: 106180 train: 0.08548855781555176 elapsed, loss: 2.6891826e-06\n",
      "step: 106190 train: 0.07805204391479492 elapsed, loss: 2.6416906e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 106200 train: 0.08353877067565918 elapsed, loss: 1.7695106e-06\n",
      "step: 106210 train: 0.0847921371459961 elapsed, loss: 1.3816157e-06\n",
      "step: 106220 train: 0.07918238639831543 elapsed, loss: 2.1960554e-06\n",
      "step: 106230 train: 0.07819318771362305 elapsed, loss: 1.4309758e-06\n",
      "step: 106240 train: 0.07970690727233887 elapsed, loss: 2.1997785e-06\n",
      "step: 106250 train: 0.08392620086669922 elapsed, loss: 1.1250366e-06\n",
      "step: 106260 train: 0.08107876777648926 elapsed, loss: 1.8943023e-05\n",
      "step: 106270 train: 0.08386754989624023 elapsed, loss: 5.8107516e-06\n",
      "step: 106280 train: 0.07953214645385742 elapsed, loss: 3.2666003e-06\n",
      "step: 106290 train: 0.0867304801940918 elapsed, loss: 2.8656682e-06\n",
      "step: 106300 train: 0.08110690116882324 elapsed, loss: 2.1620617e-06\n",
      "step: 106310 train: 0.08384180068969727 elapsed, loss: 2.095936e-06\n",
      "step: 106320 train: 0.07626867294311523 elapsed, loss: 2.4349338e-06\n",
      "step: 106330 train: 0.08063888549804688 elapsed, loss: 1.5343517e-06\n",
      "step: 106340 train: 0.08909273147583008 elapsed, loss: 1.3029189e-06\n",
      "step: 106350 train: 0.08204078674316406 elapsed, loss: 1.6153771e-06\n",
      "step: 106360 train: 0.07593536376953125 elapsed, loss: 1.8719561e-06\n",
      "step: 106370 train: 0.08242058753967285 elapsed, loss: 1.2847585e-06\n",
      "step: 106380 train: 0.08634352684020996 elapsed, loss: 1.1324863e-06\n",
      "step: 106390 train: 0.08190321922302246 elapsed, loss: 1.2116495e-06\n",
      "step: 106400 train: 0.086151123046875 elapsed, loss: 1.4831294e-06\n",
      "step: 106410 train: 0.07790923118591309 elapsed, loss: 1.2698572e-06\n",
      "step: 106420 train: 0.08869409561157227 elapsed, loss: 1.3983779e-06\n",
      "step: 106430 train: 0.08428645133972168 elapsed, loss: 1.3667141e-06\n",
      "step: 106440 train: 0.08258056640625 elapsed, loss: 2.4614812e-06\n",
      "step: 106450 train: 0.08339381217956543 elapsed, loss: 1.9744e-06\n",
      "step: 106460 train: 0.07807064056396484 elapsed, loss: 2.0218986e-06\n",
      "step: 106470 train: 0.08165979385375977 elapsed, loss: 1.4798702e-06\n",
      "step: 106480 train: 0.08378720283508301 elapsed, loss: 1.3052475e-06\n",
      "step: 106490 train: 0.07353997230529785 elapsed, loss: 1.9948898e-06\n",
      "step: 106500 train: 0.07802557945251465 elapsed, loss: 1.5776586e-06\n",
      "step: 106510 train: 0.07633805274963379 elapsed, loss: 1.6745162e-06\n",
      "step: 106520 train: 0.08343672752380371 elapsed, loss: 1.0817305e-06\n",
      "step: 106530 train: 0.07790088653564453 elapsed, loss: 1.7401743e-06\n",
      "step: 106540 train: 0.08181214332580566 elapsed, loss: 2.3054833e-06\n",
      "step: 106550 train: 0.08120346069335938 elapsed, loss: 1.7178227e-06\n",
      "step: 106560 train: 0.0726325511932373 elapsed, loss: 2.2030401e-06\n",
      "step: 106570 train: 0.07752084732055664 elapsed, loss: 1.8733531e-06\n",
      "step: 106580 train: 0.0847160816192627 elapsed, loss: 1.6028044e-06\n",
      "step: 106590 train: 0.07946133613586426 elapsed, loss: 1.8118858e-06\n",
      "step: 106600 train: 0.0862741470336914 elapsed, loss: 1.2963999e-06\n",
      "step: 106610 train: 0.07700061798095703 elapsed, loss: 3.5692678e-06\n",
      "step: 106620 train: 0.08080458641052246 elapsed, loss: 1.4193345e-06\n",
      "step: 106630 train: 0.08375668525695801 elapsed, loss: 1.5106039e-06\n",
      "step: 106640 train: 0.08538532257080078 elapsed, loss: 1.6293463e-06\n",
      "step: 106650 train: 0.0845487117767334 elapsed, loss: 0.00019556136\n",
      "step: 106660 train: 0.0803534984588623 elapsed, loss: 0.0011750666\n",
      "step: 106670 train: 0.08415699005126953 elapsed, loss: 3.1149433e-05\n",
      "step: 106680 train: 0.0835268497467041 elapsed, loss: 3.5803678e-05\n",
      "step: 106690 train: 0.07948112487792969 elapsed, loss: 3.1323405e-05\n",
      "step: 106700 train: 0.08469533920288086 elapsed, loss: 1.6423277e-05\n",
      "step: 106710 train: 0.07795262336730957 elapsed, loss: 1.0000838e-05\n",
      "step: 106720 train: 0.07767701148986816 elapsed, loss: 8.8372e-06\n",
      "step: 106730 train: 0.07975912094116211 elapsed, loss: 4.9904515e-06\n",
      "step: 106740 train: 0.0811150074005127 elapsed, loss: 5.050076e-06\n",
      "step: 106750 train: 0.08033108711242676 elapsed, loss: 3.711308e-06\n",
      "step: 106760 train: 0.07698822021484375 elapsed, loss: 4.206298e-06\n",
      "step: 106770 train: 0.0785377025604248 elapsed, loss: 2.6654393e-06\n",
      "step: 106780 train: 0.08249425888061523 elapsed, loss: 1.711303e-06\n",
      "step: 106790 train: 0.08492708206176758 elapsed, loss: 1.748556e-06\n",
      "step: 106800 train: 0.08018040657043457 elapsed, loss: 1.8016414e-06\n",
      "step: 106810 train: 0.08457374572753906 elapsed, loss: 1.8146799e-06\n",
      "step: 106820 train: 0.09100151062011719 elapsed, loss: 1.5106026e-06\n",
      "step: 106830 train: 0.07788205146789551 elapsed, loss: 1.6288814e-06\n",
      "step: 106840 train: 0.08090949058532715 elapsed, loss: 1.5408717e-06\n",
      "step: 106850 train: 0.0789496898651123 elapsed, loss: 1.7420369e-06\n",
      "step: 106860 train: 0.0810856819152832 elapsed, loss: 1.6991949e-06\n",
      "step: 106870 train: 0.08655595779418945 elapsed, loss: 1.0193319e-06\n",
      "step: 106880 train: 0.07882308959960938 elapsed, loss: 1.9073461e-06\n",
      "step: 106890 train: 0.07967448234558105 elapsed, loss: 1.5231765e-06\n",
      "step: 106900 train: 0.07674288749694824 elapsed, loss: 5.4668362e-06\n",
      "step: 106910 train: 0.08253097534179688 elapsed, loss: 2.0256239e-06\n",
      "step: 106920 train: 0.08295249938964844 elapsed, loss: 1.7052497e-06\n",
      "step: 106930 train: 0.07704997062683105 elapsed, loss: 1.5804528e-06\n",
      "step: 106940 train: 0.08081459999084473 elapsed, loss: 1.3653178e-06\n",
      "step: 106950 train: 0.08244729042053223 elapsed, loss: 2.2053382e-06\n",
      "step: 106960 train: 0.0824887752532959 elapsed, loss: 1.5599633e-06\n",
      "step: 106970 train: 0.0762169361114502 elapsed, loss: 1.5851094e-06\n",
      "step: 106980 train: 0.07854723930358887 elapsed, loss: 1.4640376e-06\n",
      "step: 106990 train: 0.08234429359436035 elapsed, loss: 1.945996e-06\n",
      "step: 107000 train: 0.08315730094909668 elapsed, loss: 1.4519304e-06\n",
      "step: 107010 train: 0.085693359375 elapsed, loss: 1.5012902e-06\n",
      "step: 107020 train: 0.07952404022216797 elapsed, loss: 2.471726e-06\n",
      "step: 107030 train: 0.07783341407775879 elapsed, loss: 2.7241124e-06\n",
      "step: 107040 train: 0.07749462127685547 elapsed, loss: 1.8528636e-06\n",
      "step: 107050 train: 0.07461190223693848 elapsed, loss: 2.752046e-06\n",
      "step: 107060 train: 0.07918524742126465 elapsed, loss: 1.8677651e-06\n",
      "step: 107070 train: 0.08271360397338867 elapsed, loss: 1.4849925e-06\n",
      "step: 107080 train: 0.07978701591491699 elapsed, loss: 1.7061811e-06\n",
      "step: 107090 train: 0.0838022232055664 elapsed, loss: 1.4943047e-06\n",
      "step: 107100 train: 0.08101987838745117 elapsed, loss: 1.3672025e-05\n",
      "step: 107110 train: 0.08216595649719238 elapsed, loss: 2.7539145e-06\n",
      "step: 107120 train: 0.08832240104675293 elapsed, loss: 2.4367996e-06\n",
      "step: 107130 train: 0.08177471160888672 elapsed, loss: 1.645179e-06\n",
      "step: 107140 train: 0.08159804344177246 elapsed, loss: 1.521314e-06\n",
      "step: 107150 train: 0.08447027206420898 elapsed, loss: 1.7494873e-06\n",
      "step: 107160 train: 0.08702421188354492 elapsed, loss: 1.3946542e-06\n",
      "step: 107170 train: 0.08182477951049805 elapsed, loss: 1.5930257e-06\n",
      "step: 107180 train: 0.0784463882446289 elapsed, loss: 1.8198015e-06\n",
      "step: 107190 train: 0.09043240547180176 elapsed, loss: 1.2121152e-06\n",
      "step: 107200 train: 0.07961869239807129 elapsed, loss: 2.6929124e-06\n",
      "step: 107210 train: 0.08500313758850098 elapsed, loss: 2.0298148e-06\n",
      "step: 107220 train: 0.08217072486877441 elapsed, loss: 1.4626407e-06\n",
      "step: 107230 train: 0.07736873626708984 elapsed, loss: 2.149955e-06\n",
      "step: 107240 train: 0.0826568603515625 elapsed, loss: 1.8402912e-06\n",
      "step: 107250 train: 0.07277655601501465 elapsed, loss: 2.3632274e-06\n",
      "step: 107260 train: 0.07982087135314941 elapsed, loss: 1.9217764e-06\n",
      "step: 107270 train: 0.08449029922485352 elapsed, loss: 1.467763e-06\n",
      "step: 107280 train: 0.08013701438903809 elapsed, loss: 1.7662512e-06\n",
      "step: 107290 train: 0.08313775062561035 elapsed, loss: 2.1541462e-06\n",
      "step: 107300 train: 0.0858011245727539 elapsed, loss: 2.3036232e-06\n",
      "step: 107310 train: 0.07931852340698242 elapsed, loss: 2.101061e-06\n",
      "step: 107320 train: 0.0768272876739502 elapsed, loss: 2.392564e-06\n",
      "step: 107330 train: 0.08552026748657227 elapsed, loss: 2.327371e-06\n",
      "step: 107340 train: 0.0830235481262207 elapsed, loss: 2.7911658e-06\n",
      "step: 107350 train: 0.07533407211303711 elapsed, loss: 1.9604308e-06\n",
      "step: 107360 train: 0.0823507308959961 elapsed, loss: 2.1797343e-06\n",
      "step: 107370 train: 0.08805584907531738 elapsed, loss: 2.0265556e-06\n",
      "step: 107380 train: 0.08126235008239746 elapsed, loss: 2.004669e-06\n",
      "step: 107390 train: 0.07657122611999512 elapsed, loss: 2.6160808e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 107400 train: 0.0832977294921875 elapsed, loss: 2.0866246e-06\n",
      "step: 107410 train: 0.08114409446716309 elapsed, loss: 1.7769614e-06\n",
      "step: 107420 train: 0.07669997215270996 elapsed, loss: 2.4512374e-06\n",
      "step: 107430 train: 0.08517217636108398 elapsed, loss: 0.0001866839\n",
      "step: 107440 train: 0.08720874786376953 elapsed, loss: 0.00046130258\n",
      "step: 107450 train: 0.08603525161743164 elapsed, loss: 4.6733483e-05\n",
      "step: 107460 train: 0.08108973503112793 elapsed, loss: 2.8907538e-05\n",
      "step: 107470 train: 0.0823206901550293 elapsed, loss: 1.8656272e-05\n",
      "step: 107480 train: 0.08332586288452148 elapsed, loss: 1.7593029e-05\n",
      "step: 107490 train: 0.07377386093139648 elapsed, loss: 1.716819e-05\n",
      "step: 107500 train: 0.07821965217590332 elapsed, loss: 6.855884e-06\n",
      "step: 107510 train: 0.07923197746276855 elapsed, loss: 6.745048e-06\n",
      "step: 107520 train: 0.08298254013061523 elapsed, loss: 5.4179454e-06\n",
      "step: 107530 train: 0.0846548080444336 elapsed, loss: 3.5995522e-06\n",
      "step: 107540 train: 0.07676410675048828 elapsed, loss: 3.695011e-06\n",
      "step: 107550 train: 0.08054423332214355 elapsed, loss: 3.3024603e-06\n",
      "step: 107560 train: 0.08495378494262695 elapsed, loss: 2.4773121e-06\n",
      "step: 107570 train: 0.08465290069580078 elapsed, loss: 1.8589174e-06\n",
      "step: 107580 train: 0.08174347877502441 elapsed, loss: 1.8412222e-06\n",
      "step: 107590 train: 0.07770538330078125 elapsed, loss: 2.0605482e-06\n",
      "step: 107600 train: 0.08515071868896484 elapsed, loss: 1.6470424e-06\n",
      "step: 107610 train: 0.0808877944946289 elapsed, loss: 1.9534461e-06\n",
      "step: 107620 train: 0.0818018913269043 elapsed, loss: 1.7327236e-06\n",
      "step: 107630 train: 0.08295989036560059 elapsed, loss: 1.5823151e-06\n",
      "step: 107640 train: 0.08424210548400879 elapsed, loss: 1.4831294e-06\n",
      "step: 107650 train: 0.084930419921875 elapsed, loss: 1.4496011e-06\n",
      "step: 107660 train: 0.08286190032958984 elapsed, loss: 1.3932573e-06\n",
      "step: 107670 train: 0.07973670959472656 elapsed, loss: 1.8742846e-06\n",
      "step: 107680 train: 0.08555173873901367 elapsed, loss: 1.1799847e-06\n",
      "step: 107690 train: 0.07674169540405273 elapsed, loss: 1.8225954e-06\n",
      "step: 107700 train: 0.08361935615539551 elapsed, loss: 1.5012906e-06\n",
      "step: 107710 train: 0.08667635917663574 elapsed, loss: 1.3872038e-06\n",
      "step: 107720 train: 0.0801401138305664 elapsed, loss: 6.9708376e-06\n",
      "step: 107730 train: 0.0814058780670166 elapsed, loss: 0.006197615\n",
      "step: 107740 train: 0.07748889923095703 elapsed, loss: 5.673438e-05\n",
      "step: 107750 train: 0.07596850395202637 elapsed, loss: 3.194086e-05\n",
      "step: 107760 train: 0.08505940437316895 elapsed, loss: 3.6883186e-05\n",
      "step: 107770 train: 0.08466267585754395 elapsed, loss: 1.0573643e-05\n",
      "step: 107780 train: 0.07874226570129395 elapsed, loss: 1.1800361e-05\n",
      "step: 107790 train: 0.07936620712280273 elapsed, loss: 8.064263e-06\n",
      "step: 107800 train: 0.08218908309936523 elapsed, loss: 6.9392154e-06\n",
      "step: 107810 train: 0.08615422248840332 elapsed, loss: 4.0605523e-06\n",
      "step: 107820 train: 0.08423113822937012 elapsed, loss: 1.5279435e-05\n",
      "step: 107830 train: 0.08288240432739258 elapsed, loss: 3.8456055e-05\n",
      "step: 107840 train: 0.0865325927734375 elapsed, loss: 2.3429373e-05\n",
      "step: 107850 train: 0.08066129684448242 elapsed, loss: 5.2274685e-05\n",
      "step: 107860 train: 0.08296847343444824 elapsed, loss: 1.5562095e-05\n",
      "step: 107870 train: 0.08366775512695312 elapsed, loss: 6.665883e-06\n",
      "step: 107880 train: 0.08139967918395996 elapsed, loss: 9.012332e-06\n",
      "step: 107890 train: 0.07860231399536133 elapsed, loss: 5.6107024e-06\n",
      "step: 107900 train: 0.07838606834411621 elapsed, loss: 5.0913973e-06\n",
      "step: 107910 train: 0.08562803268432617 elapsed, loss: 3.0290894e-06\n",
      "step: 107920 train: 0.07598400115966797 elapsed, loss: 4.279823e-06\n",
      "step: 107930 train: 0.08092594146728516 elapsed, loss: 2.5830177e-06\n",
      "step: 107940 train: 0.07920694351196289 elapsed, loss: 1.612583e-06\n",
      "step: 107950 train: 0.08168911933898926 elapsed, loss: 1.8235255e-06\n",
      "step: 107960 train: 0.08732891082763672 elapsed, loss: 1.3252708e-06\n",
      "step: 107970 train: 0.08261609077453613 elapsed, loss: 1.3424997e-06\n",
      "step: 107980 train: 0.08732080459594727 elapsed, loss: 1.1296931e-06\n",
      "step: 107990 train: 0.0811009407043457 elapsed, loss: 9.3970385e-07\n",
      "step: 108000 train: 0.08229184150695801 elapsed, loss: 9.806815e-07\n",
      "step: 108010 train: 0.08091926574707031 elapsed, loss: 1.2055962e-06\n",
      "step: 108020 train: 0.07981657981872559 elapsed, loss: 1.2335356e-06\n",
      "step: 108030 train: 0.08097672462463379 elapsed, loss: 1.4035011e-06\n",
      "step: 108040 train: 0.08283710479736328 elapsed, loss: 1.0738142e-06\n",
      "step: 108050 train: 0.0756998062133789 elapsed, loss: 1.3401718e-06\n",
      "step: 108060 train: 0.08606553077697754 elapsed, loss: 1.311301e-06\n",
      "step: 108070 train: 0.08491110801696777 elapsed, loss: 1.0565846e-06\n",
      "step: 108080 train: 0.08310914039611816 elapsed, loss: 1.5292298e-06\n",
      "step: 108090 train: 0.08306026458740234 elapsed, loss: 1.2353985e-06\n",
      "step: 108100 train: 0.08479619026184082 elapsed, loss: 1.0258511e-06\n",
      "step: 108110 train: 0.08030223846435547 elapsed, loss: 1.1799848e-06\n",
      "step: 108120 train: 0.08103013038635254 elapsed, loss: 1.2177034e-06\n",
      "step: 108130 train: 0.08198857307434082 elapsed, loss: 1.4132806e-06\n",
      "step: 108140 train: 0.08075451850891113 elapsed, loss: 1.2982626e-06\n",
      "step: 108150 train: 0.0819849967956543 elapsed, loss: 1.1832444e-06\n",
      "step: 108160 train: 0.08194971084594727 elapsed, loss: 1.7220135e-06\n",
      "step: 108170 train: 0.08099508285522461 elapsed, loss: 1.4440143e-06\n",
      "step: 108180 train: 0.0795588493347168 elapsed, loss: 1.5050155e-06\n",
      "step: 108190 train: 0.08029723167419434 elapsed, loss: 1.07102e-06\n",
      "step: 108200 train: 0.08832073211669922 elapsed, loss: 1.3718368e-06\n",
      "step: 108210 train: 0.08385705947875977 elapsed, loss: 1.4323728e-06\n",
      "step: 108220 train: 0.08057498931884766 elapsed, loss: 1.5627577e-06\n",
      "step: 108230 train: 0.08021283149719238 elapsed, loss: 1.5576355e-06\n",
      "step: 108240 train: 0.0786588191986084 elapsed, loss: 1.6996618e-06\n",
      "step: 108250 train: 0.07835555076599121 elapsed, loss: 1.3872038e-06\n",
      "step: 108260 train: 0.08633112907409668 elapsed, loss: 1.50129e-06\n",
      "step: 108270 train: 0.07876729965209961 elapsed, loss: 1.5045497e-06\n",
      "step: 108280 train: 0.08040380477905273 elapsed, loss: 1.7886025e-06\n",
      "step: 108290 train: 0.07733774185180664 elapsed, loss: 2.403258e-06\n",
      "step: 108300 train: 0.0825650691986084 elapsed, loss: 1.352279e-06\n",
      "step: 108310 train: 0.08140063285827637 elapsed, loss: 1.349019e-06\n",
      "step: 108320 train: 0.08111000061035156 elapsed, loss: 1.6875547e-06\n",
      "step: 108330 train: 0.08239054679870605 elapsed, loss: 1.8477418e-06\n",
      "step: 108340 train: 0.08532190322875977 elapsed, loss: 1.3392407e-06\n",
      "step: 108350 train: 0.08738923072814941 elapsed, loss: 1.7127002e-06\n",
      "step: 108360 train: 0.0834207534790039 elapsed, loss: 1.4156088e-06\n",
      "step: 108370 train: 0.08438420295715332 elapsed, loss: 1.444014e-06\n",
      "step: 108380 train: 0.08082437515258789 elapsed, loss: 1.5087412e-06\n",
      "step: 108390 train: 0.08426022529602051 elapsed, loss: 1.6358649e-06\n",
      "step: 108400 train: 0.07803487777709961 elapsed, loss: 1.958103e-06\n",
      "step: 108410 train: 0.08280515670776367 elapsed, loss: 1.5501848e-06\n",
      "step: 108420 train: 0.08114147186279297 elapsed, loss: 2.2188729e-06\n",
      "step: 108430 train: 0.08896183967590332 elapsed, loss: 1.837011e-06\n",
      "step: 108440 train: 0.07905244827270508 elapsed, loss: 1.6787071e-06\n",
      "step: 108450 train: 0.07999300956726074 elapsed, loss: 2.2472705e-06\n",
      "step: 108460 train: 0.08539843559265137 elapsed, loss: 2.2193385e-06\n",
      "step: 108470 train: 0.07867646217346191 elapsed, loss: 1.8910482e-06\n",
      "step: 108480 train: 0.08063793182373047 elapsed, loss: 2.2887218e-06\n",
      "step: 108490 train: 0.08518505096435547 elapsed, loss: 1.4239909e-06\n",
      "step: 108500 train: 0.08155608177185059 elapsed, loss: 0.10647631\n",
      "step: 108510 train: 0.08853888511657715 elapsed, loss: 0.00016910635\n",
      "step: 108520 train: 0.07739520072937012 elapsed, loss: 4.3349828e-05\n",
      "step: 108530 train: 0.08389592170715332 elapsed, loss: 2.3434644e-05\n",
      "step: 108540 train: 0.0811159610748291 elapsed, loss: 2.468357e-05\n",
      "step: 108550 train: 0.07573747634887695 elapsed, loss: 1.4093503e-05\n",
      "step: 108560 train: 0.07448291778564453 elapsed, loss: 1.1262759e-05\n",
      "step: 108570 train: 0.08311009407043457 elapsed, loss: 7.45051e-06\n",
      "step: 108580 train: 0.0756828784942627 elapsed, loss: 6.4363353e-06\n",
      "step: 108590 train: 0.07681965827941895 elapsed, loss: 5.914332e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 108600 train: 0.07844829559326172 elapsed, loss: 3.6922158e-06\n",
      "step: 108610 train: 0.0842745304107666 elapsed, loss: 2.7790595e-06\n",
      "step: 108620 train: 0.08952689170837402 elapsed, loss: 2.741284e-06\n",
      "step: 108630 train: 0.08461737632751465 elapsed, loss: 1.6889514e-06\n",
      "step: 108640 train: 0.08083200454711914 elapsed, loss: 1.9469273e-06\n",
      "step: 108650 train: 0.07765460014343262 elapsed, loss: 1.7448307e-06\n",
      "step: 108660 train: 0.08352160453796387 elapsed, loss: 1.3248051e-06\n",
      "step: 108670 train: 0.08307385444641113 elapsed, loss: 1.5110668e-06\n",
      "step: 108680 train: 0.07889270782470703 elapsed, loss: 1.7038527e-06\n",
      "step: 108690 train: 0.08250546455383301 elapsed, loss: 1.3238738e-06\n",
      "step: 108700 train: 0.0807657241821289 elapsed, loss: 1.137144e-06\n",
      "step: 108710 train: 0.079620361328125 elapsed, loss: 4.1229464e-06\n",
      "step: 108720 train: 0.08186030387878418 elapsed, loss: 1.4337695e-06\n",
      "step: 108730 train: 0.08781647682189941 elapsed, loss: 1.1930233e-06\n",
      "step: 108740 train: 0.0852818489074707 elapsed, loss: 1.5357492e-06\n",
      "step: 108750 train: 0.0818016529083252 elapsed, loss: 2.1741685e-06\n",
      "step: 108760 train: 0.07977509498596191 elapsed, loss: 0.00018927001\n",
      "step: 108770 train: 0.08250737190246582 elapsed, loss: 3.3924982e-05\n",
      "step: 108780 train: 0.0805201530456543 elapsed, loss: 9.88529e-06\n",
      "step: 108790 train: 0.08679509162902832 elapsed, loss: 3.5636936e-06\n",
      "step: 108800 train: 0.08203721046447754 elapsed, loss: 5.628409e-06\n",
      "step: 108810 train: 0.08349442481994629 elapsed, loss: 8.1697235e-06\n",
      "step: 108820 train: 0.08312582969665527 elapsed, loss: 2.7809192e-06\n",
      "step: 108830 train: 0.0841054916381836 elapsed, loss: 2.0829e-06\n",
      "step: 108840 train: 0.08287310600280762 elapsed, loss: 1.8854599e-06\n",
      "step: 108850 train: 0.08735013008117676 elapsed, loss: 1.9143306e-06\n",
      "step: 108860 train: 0.08864736557006836 elapsed, loss: 1.562292e-06\n",
      "step: 108870 train: 0.08785748481750488 elapsed, loss: 1.4798688e-06\n",
      "step: 108880 train: 0.0802147388458252 elapsed, loss: 1.5087411e-06\n",
      "step: 108890 train: 0.07855081558227539 elapsed, loss: 1.5036177e-06\n",
      "step: 108900 train: 0.07808589935302734 elapsed, loss: 1.788603e-06\n",
      "step: 108910 train: 0.08113765716552734 elapsed, loss: 1.4118837e-06\n",
      "step: 108920 train: 0.08062577247619629 elapsed, loss: 1.4132806e-06\n",
      "step: 108930 train: 0.08925056457519531 elapsed, loss: 1.2149094e-06\n",
      "step: 108940 train: 0.09141707420349121 elapsed, loss: 1.2279479e-06\n",
      "step: 108950 train: 0.0853414535522461 elapsed, loss: 1.4868547e-06\n",
      "step: 108960 train: 0.08141136169433594 elapsed, loss: 2.5923284e-06\n",
      "step: 108970 train: 0.07766962051391602 elapsed, loss: 1.4118837e-06\n",
      "step: 108980 train: 0.08978843688964844 elapsed, loss: 1.7513493e-06\n",
      "step: 108990 train: 0.08171463012695312 elapsed, loss: 1.4463425e-06\n",
      "step: 109000 train: 0.08105897903442383 elapsed, loss: 1.4519304e-06\n",
      "step: 109010 train: 0.08573055267333984 elapsed, loss: 1.140869e-06\n",
      "step: 109020 train: 0.07878804206848145 elapsed, loss: 1.905456e-06\n",
      "step: 109030 train: 0.07841777801513672 elapsed, loss: 1.8062981e-06\n",
      "step: 109040 train: 0.08417034149169922 elapsed, loss: 1.6414515e-06\n",
      "step: 109050 train: 0.07988905906677246 elapsed, loss: 1.7713728e-06\n",
      "step: 109060 train: 0.08557248115539551 elapsed, loss: 1.6158408e-06\n",
      "step: 109070 train: 0.0864253044128418 elapsed, loss: 2.077778e-06\n",
      "step: 109080 train: 0.08004546165466309 elapsed, loss: 1.7979162e-06\n",
      "step: 109090 train: 0.08393454551696777 elapsed, loss: 1.3690428e-06\n",
      "step: 109100 train: 0.07097721099853516 elapsed, loss: 2.470329e-06\n",
      "step: 109110 train: 0.08831167221069336 elapsed, loss: 1.2409863e-06\n",
      "step: 109120 train: 0.08323860168457031 elapsed, loss: 1.6936083e-06\n",
      "step: 109130 train: 0.07751631736755371 elapsed, loss: 2.3269058e-06\n",
      "step: 109140 train: 0.0816950798034668 elapsed, loss: 1.3639207e-06\n",
      "step: 109150 train: 0.07827162742614746 elapsed, loss: 1.9026899e-06\n",
      "step: 109160 train: 0.08065080642700195 elapsed, loss: 1.7527469e-06\n",
      "step: 109170 train: 0.08623027801513672 elapsed, loss: 1.2204972e-06\n",
      "step: 109180 train: 0.0758066177368164 elapsed, loss: 2.2221325e-06\n",
      "step: 109190 train: 0.0826728343963623 elapsed, loss: 2.2179415e-06\n",
      "step: 109200 train: 0.07961368560791016 elapsed, loss: 1.8426197e-06\n",
      "step: 109210 train: 0.08568549156188965 elapsed, loss: 1.8416879e-06\n",
      "step: 109220 train: 0.07808685302734375 elapsed, loss: 1.8049011e-06\n",
      "step: 109230 train: 0.07997512817382812 elapsed, loss: 2.2458812e-06\n",
      "step: 109240 train: 0.0771934986114502 elapsed, loss: 2.2508088e-05\n",
      "step: 109250 train: 0.08021664619445801 elapsed, loss: 0.003483237\n",
      "step: 109260 train: 0.07384586334228516 elapsed, loss: 0.00020035145\n",
      "step: 109270 train: 0.07838249206542969 elapsed, loss: 3.854373e-05\n",
      "step: 109280 train: 0.07761406898498535 elapsed, loss: 3.889566e-05\n",
      "step: 109290 train: 0.08637714385986328 elapsed, loss: 4.3211177e-05\n",
      "step: 109300 train: 0.0827949047088623 elapsed, loss: 1.503587e-05\n",
      "step: 109310 train: 0.08364629745483398 elapsed, loss: 1.2997165e-05\n",
      "step: 109320 train: 0.08504843711853027 elapsed, loss: 6.1313176e-06\n",
      "step: 109330 train: 0.07990813255310059 elapsed, loss: 7.3811434e-06\n",
      "step: 109340 train: 0.08057212829589844 elapsed, loss: 9.933821e-06\n",
      "step: 109350 train: 0.08368444442749023 elapsed, loss: 3.5864987e-06\n",
      "step: 109360 train: 0.08567333221435547 elapsed, loss: 2.7129363e-06\n",
      "step: 109370 train: 0.08462786674499512 elapsed, loss: 2.313865e-06\n",
      "step: 109380 train: 0.08801531791687012 elapsed, loss: 2.0023383e-06\n",
      "step: 109390 train: 0.09232449531555176 elapsed, loss: 1.8137432e-06\n",
      "step: 109400 train: 0.08370161056518555 elapsed, loss: 2.613184e-06\n",
      "step: 109410 train: 0.0869741439819336 elapsed, loss: 1.3257329e-06\n",
      "step: 109420 train: 0.08213567733764648 elapsed, loss: 1.980914e-06\n",
      "step: 109430 train: 0.07960295677185059 elapsed, loss: 1.6079257e-06\n",
      "step: 109440 train: 0.08176898956298828 elapsed, loss: 1.2391235e-06\n",
      "step: 109450 train: 0.09274911880493164 elapsed, loss: 1.0258509e-06\n",
      "step: 109460 train: 0.08165717124938965 elapsed, loss: 1.9464612e-06\n",
      "step: 109470 train: 0.07741618156433105 elapsed, loss: 1.5716053e-06\n",
      "step: 109480 train: 0.0731499195098877 elapsed, loss: 1.7555398e-06\n",
      "step: 109490 train: 0.08820343017578125 elapsed, loss: 1.1185164e-06\n",
      "step: 109500 train: 0.08075737953186035 elapsed, loss: 1.4831296e-06\n",
      "step: 109510 train: 0.08199357986450195 elapsed, loss: 1.3848753e-06\n",
      "step: 109520 train: 0.07911276817321777 elapsed, loss: 0.0002241117\n",
      "step: 109530 train: 0.07739400863647461 elapsed, loss: 3.1615666e-05\n",
      "step: 109540 train: 0.07772111892700195 elapsed, loss: 2.7991859e-05\n",
      "step: 109550 train: 0.07930636405944824 elapsed, loss: 1.663109e-05\n",
      "step: 109560 train: 0.08062911033630371 elapsed, loss: 1.0097685e-05\n",
      "step: 109570 train: 0.07973980903625488 elapsed, loss: 8.461436e-06\n",
      "step: 109580 train: 0.0797739028930664 elapsed, loss: 1.0873058e-05\n",
      "step: 109590 train: 0.08317756652832031 elapsed, loss: 3.726203e-06\n",
      "step: 109600 train: 0.07957077026367188 elapsed, loss: 3.5110752e-06\n",
      "step: 109610 train: 0.084716796875 elapsed, loss: 2.8852305e-06\n",
      "step: 109620 train: 0.07748866081237793 elapsed, loss: 2.7972196e-06\n",
      "step: 109630 train: 0.08740901947021484 elapsed, loss: 1.7513494e-06\n",
      "step: 109640 train: 0.08169674873352051 elapsed, loss: 1.9711172e-06\n",
      "step: 109650 train: 0.08135700225830078 elapsed, loss: 1.5613539e-06\n",
      "step: 109660 train: 0.07854795455932617 elapsed, loss: 1.6991959e-06\n",
      "step: 109670 train: 0.08366155624389648 elapsed, loss: 1.1096699e-06\n",
      "step: 109680 train: 0.07916522026062012 elapsed, loss: 1.2055962e-06\n",
      "step: 109690 train: 0.08071494102478027 elapsed, loss: 1.5045501e-06\n",
      "step: 109700 train: 0.08594655990600586 elapsed, loss: 1.0523938e-06\n",
      "step: 109710 train: 0.08571314811706543 elapsed, loss: 9.3039057e-07\n",
      "step: 109720 train: 0.08214116096496582 elapsed, loss: 1.5483222e-06\n",
      "step: 109730 train: 0.09242796897888184 elapsed, loss: 8.7870234e-07\n",
      "step: 109740 train: 0.08175349235534668 elapsed, loss: 1.4025694e-06\n",
      "step: 109750 train: 0.08592844009399414 elapsed, loss: 1.1529763e-06\n",
      "step: 109760 train: 0.0839080810546875 elapsed, loss: 1.2801017e-06\n",
      "step: 109770 train: 0.08082318305969238 elapsed, loss: 1.4393577e-06\n",
      "step: 109780 train: 0.0805964469909668 elapsed, loss: 1.1683414e-06\n",
      "step: 109790 train: 0.0849921703338623 elapsed, loss: 1.1129296e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 109800 train: 0.07710957527160645 elapsed, loss: 2.1094424e-06\n",
      "step: 109810 train: 0.07657670974731445 elapsed, loss: 1.6023384e-06\n",
      "step: 109820 train: 0.08379983901977539 elapsed, loss: 1.415609e-06\n",
      "step: 109830 train: 0.08032584190368652 elapsed, loss: 2.2551912e-06\n",
      "step: 109840 train: 0.07507538795471191 elapsed, loss: 1.8859259e-06\n",
      "step: 109850 train: 0.08105945587158203 elapsed, loss: 1.4104864e-06\n",
      "step: 109860 train: 0.0756235122680664 elapsed, loss: 1.5767275e-06\n",
      "step: 109870 train: 0.08057475090026855 elapsed, loss: 1.7774269e-06\n",
      "step: 109880 train: 0.07872629165649414 elapsed, loss: 1.8593832e-06\n",
      "step: 109890 train: 0.08130002021789551 elapsed, loss: 1.7448308e-06\n",
      "step: 109900 train: 0.07345724105834961 elapsed, loss: 2.0726552e-06\n",
      "step: 109910 train: 0.08263754844665527 elapsed, loss: 1.279636e-06\n",
      "step: 109920 train: 0.07738399505615234 elapsed, loss: 1.6940738e-06\n",
      "step: 109930 train: 0.07941460609436035 elapsed, loss: 1.6093237e-06\n",
      "step: 109940 train: 0.08070135116577148 elapsed, loss: 1.5310927e-06\n",
      "step: 109950 train: 0.07879757881164551 elapsed, loss: 1.819802e-06\n",
      "step: 109960 train: 0.0815269947052002 elapsed, loss: 2.1457645e-06\n",
      "step: 109970 train: 0.08709406852722168 elapsed, loss: 1.3685772e-06\n",
      "step: 109980 train: 0.08020210266113281 elapsed, loss: 1.8016408e-06\n",
      "step: 109990 train: 0.07915496826171875 elapsed, loss: 1.6107205e-06\n",
      "step: 110000 train: 0.08132314682006836 elapsed, loss: 1.4244565e-06\n",
      "step: 110010 train: 0.07966971397399902 elapsed, loss: 1.5445969e-06\n",
      "step: 110020 train: 0.08366847038269043 elapsed, loss: 0.011633916\n",
      "step: 110030 train: 0.07962465286254883 elapsed, loss: 8.3545674e-05\n",
      "step: 110040 train: 0.08024358749389648 elapsed, loss: 9.4593015e-05\n",
      "step: 110050 train: 0.08162355422973633 elapsed, loss: 3.3810986e-05\n",
      "step: 110060 train: 0.08389759063720703 elapsed, loss: 2.5631693e-05\n",
      "step: 110070 train: 0.0805809497833252 elapsed, loss: 1.1760831e-05\n",
      "step: 110080 train: 0.07930970191955566 elapsed, loss: 1.6404325e-05\n",
      "step: 110090 train: 0.08361983299255371 elapsed, loss: 8.265433e-06\n",
      "step: 110100 train: 0.076416015625 elapsed, loss: 9.639565e-06\n",
      "step: 110110 train: 0.0805962085723877 elapsed, loss: 5.40909e-06\n",
      "step: 110120 train: 0.0835733413696289 elapsed, loss: 4.151356e-06\n",
      "step: 110130 train: 0.08163738250732422 elapsed, loss: 3.836106e-06\n",
      "step: 110140 train: 0.07976436614990234 elapsed, loss: 2.4046694e-06\n",
      "step: 110150 train: 0.08033299446105957 elapsed, loss: 2.1140986e-06\n",
      "step: 110160 train: 0.08155012130737305 elapsed, loss: 2.0488958e-06\n",
      "step: 110170 train: 0.08073115348815918 elapsed, loss: 1.6619433e-06\n",
      "step: 110180 train: 0.08301639556884766 elapsed, loss: 1.6330723e-06\n",
      "step: 110190 train: 0.0783991813659668 elapsed, loss: 2.111305e-06\n",
      "step: 110200 train: 0.08661794662475586 elapsed, loss: 1.676844e-06\n",
      "step: 110210 train: 0.0859982967376709 elapsed, loss: 1.6088513e-06\n",
      "step: 110220 train: 0.08498668670654297 elapsed, loss: 2.0419218e-06\n",
      "step: 110230 train: 0.07990884780883789 elapsed, loss: 1.4766101e-06\n",
      "step: 110240 train: 0.079376220703125 elapsed, loss: 1.5348181e-06\n",
      "step: 110250 train: 0.07804107666015625 elapsed, loss: 1.5869721e-06\n",
      "step: 110260 train: 0.07916879653930664 elapsed, loss: 1.977661e-06\n",
      "step: 110270 train: 0.08563113212585449 elapsed, loss: 1.866355e-06\n",
      "step: 110280 train: 0.08233261108398438 elapsed, loss: 7.743151e-05\n",
      "step: 110290 train: 0.07749795913696289 elapsed, loss: 5.0599345e-05\n",
      "step: 110300 train: 0.08402752876281738 elapsed, loss: 1.953566e-05\n",
      "step: 110310 train: 0.07719540596008301 elapsed, loss: 2.822945e-05\n",
      "step: 110320 train: 0.08547782897949219 elapsed, loss: 9.863987e-06\n",
      "step: 110330 train: 0.07463479042053223 elapsed, loss: 1.34371585e-05\n",
      "step: 110340 train: 0.08296561241149902 elapsed, loss: 7.3588003e-06\n",
      "step: 110350 train: 0.07497334480285645 elapsed, loss: 8.459148e-06\n",
      "step: 110360 train: 0.08456802368164062 elapsed, loss: 3.4943112e-06\n",
      "step: 110370 train: 0.08614230155944824 elapsed, loss: 3.166023e-06\n",
      "step: 110380 train: 0.07889223098754883 elapsed, loss: 2.6936406e-05\n",
      "step: 110390 train: 0.08183145523071289 elapsed, loss: 4.0135196e-06\n",
      "step: 110400 train: 0.08014059066772461 elapsed, loss: 3.7280645e-06\n",
      "step: 110410 train: 0.07903003692626953 elapsed, loss: 2.015844e-06\n",
      "step: 110420 train: 0.08094358444213867 elapsed, loss: 1.6381939e-06\n",
      "step: 110430 train: 0.08190345764160156 elapsed, loss: 1.5632233e-06\n",
      "step: 110440 train: 0.08812236785888672 elapsed, loss: 1.1133952e-06\n",
      "step: 110450 train: 0.08923006057739258 elapsed, loss: 9.774224e-07\n",
      "step: 110460 train: 0.08733010292053223 elapsed, loss: 1.2703225e-06\n",
      "step: 110470 train: 0.0879511833190918 elapsed, loss: 1.1203803e-06\n",
      "step: 110480 train: 0.08298635482788086 elapsed, loss: 1.4021049e-06\n",
      "step: 110490 train: 0.08414030075073242 elapsed, loss: 1.4025704e-06\n",
      "step: 110500 train: 0.08416414260864258 elapsed, loss: 1.5981473e-06\n",
      "step: 110510 train: 0.09007430076599121 elapsed, loss: 1.1851067e-06\n",
      "step: 110520 train: 0.07966113090515137 elapsed, loss: 1.5171229e-06\n",
      "step: 110530 train: 0.07994198799133301 elapsed, loss: 1.4426173e-06\n",
      "step: 110540 train: 0.07680439949035645 elapsed, loss: 1.621425e-06\n",
      "step: 110550 train: 0.0800483226776123 elapsed, loss: 1.4337699e-06\n",
      "step: 110560 train: 0.08315777778625488 elapsed, loss: 1.3527449e-06\n",
      "step: 110570 train: 0.08620023727416992 elapsed, loss: 1.1580987e-06\n",
      "step: 110580 train: 0.08886075019836426 elapsed, loss: 9.573989e-07\n",
      "step: 110590 train: 0.09022665023803711 elapsed, loss: 1.2023364e-06\n",
      "step: 110600 train: 0.08409810066223145 elapsed, loss: 0.0011908314\n",
      "step: 110610 train: 0.0827338695526123 elapsed, loss: 0.000103282044\n",
      "step: 110620 train: 0.08159399032592773 elapsed, loss: 7.149962e-05\n",
      "step: 110630 train: 0.08358383178710938 elapsed, loss: 2.100912e-05\n",
      "step: 110640 train: 0.08181643486022949 elapsed, loss: 1.5524805e-05\n",
      "step: 110650 train: 0.07776355743408203 elapsed, loss: 2.1186142e-05\n",
      "step: 110660 train: 0.07849335670471191 elapsed, loss: 1.04749215e-05\n",
      "step: 110670 train: 0.07879996299743652 elapsed, loss: 8.793902e-06\n",
      "step: 110680 train: 0.08582711219787598 elapsed, loss: 4.646344e-06\n",
      "step: 110690 train: 0.08392119407653809 elapsed, loss: 3.7085038e-06\n",
      "step: 110700 train: 0.08446002006530762 elapsed, loss: 3.4570403e-06\n",
      "step: 110710 train: 0.08831334114074707 elapsed, loss: 2.7310957e-06\n",
      "step: 110720 train: 0.08555126190185547 elapsed, loss: 1.7369133e-06\n",
      "step: 110730 train: 0.08549666404724121 elapsed, loss: 1.4756793e-06\n",
      "step: 110740 train: 0.07930493354797363 elapsed, loss: 1.796519e-06\n",
      "step: 110750 train: 0.08345818519592285 elapsed, loss: 1.1418006e-06\n",
      "step: 110760 train: 0.08408355712890625 elapsed, loss: 1.3955856e-06\n",
      "step: 110770 train: 0.08615303039550781 elapsed, loss: 1.1678775e-06\n",
      "step: 110780 train: 0.0839543342590332 elapsed, loss: 9.466888e-07\n",
      "step: 110790 train: 0.08718013763427734 elapsed, loss: 1.0309727e-06\n",
      "step: 110800 train: 0.08479118347167969 elapsed, loss: 8.02334e-07\n",
      "step: 110810 train: 0.08586692810058594 elapsed, loss: 1.6195008e-06\n",
      "step: 110820 train: 0.08741497993469238 elapsed, loss: 1.4747444e-06\n",
      "step: 110830 train: 0.0804443359375 elapsed, loss: 1.1287622e-06\n",
      "step: 110840 train: 0.08136940002441406 elapsed, loss: 9.373755e-07\n",
      "step: 110850 train: 0.07761931419372559 elapsed, loss: 2.4568226e-06\n",
      "step: 110860 train: 0.08863067626953125 elapsed, loss: 7.9814305e-07\n",
      "step: 110870 train: 0.08426880836486816 elapsed, loss: 1.0738138e-06\n",
      "step: 110880 train: 0.08684635162353516 elapsed, loss: 1.0868525e-06\n",
      "step: 110890 train: 0.07868647575378418 elapsed, loss: 1.349485e-06\n",
      "step: 110900 train: 0.08471012115478516 elapsed, loss: 1.0267825e-06\n",
      "step: 110910 train: 0.08483433723449707 elapsed, loss: 1.1268995e-06\n",
      "step: 110920 train: 0.0820314884185791 elapsed, loss: 1.2600781e-06\n",
      "step: 110930 train: 0.07897067070007324 elapsed, loss: 0.00015158573\n",
      "step: 110940 train: 0.08902311325073242 elapsed, loss: 4.77951e-06\n",
      "step: 110950 train: 0.0829770565032959 elapsed, loss: 3.3080487e-06\n",
      "step: 110960 train: 0.08214211463928223 elapsed, loss: 2.4544959e-06\n",
      "step: 110970 train: 0.08342552185058594 elapsed, loss: 2.4158462e-06\n",
      "step: 110980 train: 0.08269214630126953 elapsed, loss: 2.0014095e-06\n",
      "step: 110990 train: 0.08584046363830566 elapsed, loss: 1.6340036e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 111000 train: 0.07744312286376953 elapsed, loss: 1.9017577e-06\n",
      "step: 111010 train: 0.07419562339782715 elapsed, loss: 1.6675313e-06\n",
      "step: 111020 train: 0.07810306549072266 elapsed, loss: 1.3653176e-06\n",
      "step: 111030 train: 0.07571864128112793 elapsed, loss: 1.497099e-06\n",
      "step: 111040 train: 0.08316612243652344 elapsed, loss: 1.2987282e-06\n",
      "step: 111050 train: 0.08814477920532227 elapsed, loss: 1.0281794e-06\n",
      "step: 111060 train: 0.0862114429473877 elapsed, loss: 1.2144405e-06\n",
      "step: 111070 train: 0.08193182945251465 elapsed, loss: 1.23307e-06\n",
      "step: 111080 train: 0.0768439769744873 elapsed, loss: 1.1520452e-06\n",
      "step: 111090 train: 0.08074331283569336 elapsed, loss: 1.4514646e-06\n",
      "step: 111100 train: 0.0786750316619873 elapsed, loss: 1.6125834e-06\n",
      "step: 111110 train: 0.07765841484069824 elapsed, loss: 1.5269019e-06\n",
      "step: 111120 train: 0.07847404479980469 elapsed, loss: 1.3434317e-06\n",
      "step: 111130 train: 0.07753515243530273 elapsed, loss: 1.3350498e-06\n",
      "step: 111140 train: 0.08289194107055664 elapsed, loss: 1.135281e-06\n",
      "step: 111150 train: 0.08125901222229004 elapsed, loss: 1.2693916e-06\n",
      "step: 111160 train: 0.08782601356506348 elapsed, loss: 1.2773078e-06\n",
      "step: 111170 train: 0.0759572982788086 elapsed, loss: 2.3780472e-06\n",
      "step: 111180 train: 0.07811093330383301 elapsed, loss: 4.479331e-05\n",
      "step: 111190 train: 0.08680939674377441 elapsed, loss: 2.1871798e-05\n",
      "step: 111200 train: 0.07947874069213867 elapsed, loss: 1.7759932e-05\n",
      "step: 111210 train: 0.09277915954589844 elapsed, loss: 9.036033e-06\n",
      "step: 111220 train: 0.09194016456604004 elapsed, loss: 1.4052148e-05\n",
      "step: 111230 train: 0.0836484432220459 elapsed, loss: 9.916278e-06\n",
      "step: 111240 train: 0.08268165588378906 elapsed, loss: 4.8987354e-06\n",
      "step: 111250 train: 0.0811920166015625 elapsed, loss: 4.084299e-06\n",
      "step: 111260 train: 0.08416557312011719 elapsed, loss: 4.0996633e-06\n",
      "step: 111270 train: 0.08749675750732422 elapsed, loss: 3.0756855e-06\n",
      "step: 111280 train: 0.08664250373840332 elapsed, loss: 3.3284905e-06\n",
      "step: 111290 train: 0.08234024047851562 elapsed, loss: 2.6365683e-06\n",
      "step: 111300 train: 0.07823777198791504 elapsed, loss: 2.1769538e-06\n",
      "step: 111310 train: 0.08896565437316895 elapsed, loss: 1.7983812e-06\n",
      "step: 111320 train: 0.08372163772583008 elapsed, loss: 1.3117663e-06\n",
      "step: 111330 train: 0.08537912368774414 elapsed, loss: 1.1622897e-06\n",
      "step: 111340 train: 0.07823872566223145 elapsed, loss: 1.2540247e-06\n",
      "step: 111350 train: 0.07818913459777832 elapsed, loss: 2.2067263e-06\n",
      "step: 111360 train: 0.0846867561340332 elapsed, loss: 1.2884818e-06\n",
      "step: 111370 train: 0.08153343200683594 elapsed, loss: 1.3355152e-06\n",
      "step: 111380 train: 0.08278393745422363 elapsed, loss: 9.713688e-07\n",
      "step: 111390 train: 0.08172821998596191 elapsed, loss: 1.13435e-06\n",
      "step: 111400 train: 0.08826971054077148 elapsed, loss: 1.037027e-06\n",
      "step: 111410 train: 0.08309483528137207 elapsed, loss: 1.2475028e-06\n",
      "step: 111420 train: 0.08020138740539551 elapsed, loss: 1.6866234e-06\n",
      "step: 111430 train: 0.0812675952911377 elapsed, loss: 1.1790534e-06\n",
      "step: 111440 train: 0.07934975624084473 elapsed, loss: 0.0007463141\n",
      "step: 111450 train: 0.07687115669250488 elapsed, loss: 0.0008854609\n",
      "step: 111460 train: 0.07861876487731934 elapsed, loss: 0.00024437188\n",
      "step: 111470 train: 0.08450007438659668 elapsed, loss: 4.5315403e-05\n",
      "step: 111480 train: 0.07886528968811035 elapsed, loss: 3.599157e-05\n",
      "step: 111490 train: 0.08206343650817871 elapsed, loss: 2.1188534e-05\n",
      "step: 111500 train: 0.08385801315307617 elapsed, loss: 7.955293e-06\n",
      "step: 111510 train: 0.08370661735534668 elapsed, loss: 7.2768316e-06\n",
      "step: 111520 train: 0.08497238159179688 elapsed, loss: 7.629305e-06\n",
      "step: 111530 train: 0.08390569686889648 elapsed, loss: 4.5946617e-06\n",
      "step: 111540 train: 0.07718324661254883 elapsed, loss: 4.28126e-06\n",
      "step: 111550 train: 0.08696889877319336 elapsed, loss: 2.683134e-06\n",
      "step: 111560 train: 0.0802457332611084 elapsed, loss: 4.2849724e-06\n",
      "step: 111570 train: 0.07563447952270508 elapsed, loss: 3.1967566e-06\n",
      "step: 111580 train: 0.08765435218811035 elapsed, loss: 1.6931413e-06\n",
      "step: 111590 train: 0.08217954635620117 elapsed, loss: 2.4647388e-06\n",
      "step: 111600 train: 0.08252930641174316 elapsed, loss: 1.6214307e-06\n",
      "step: 111610 train: 0.08564496040344238 elapsed, loss: 1.1222428e-06\n",
      "step: 111620 train: 0.08289909362792969 elapsed, loss: 1.333187e-06\n",
      "step: 111630 train: 0.08208179473876953 elapsed, loss: 1.25775e-06\n",
      "step: 111640 train: 0.07440352439880371 elapsed, loss: 1.6856894e-06\n",
      "step: 111650 train: 0.07744503021240234 elapsed, loss: 6.413227e-05\n",
      "step: 111660 train: 0.0831146240234375 elapsed, loss: 3.839831e-06\n",
      "step: 111670 train: 0.07726573944091797 elapsed, loss: 2.8270233e-06\n",
      "step: 111680 train: 0.07552480697631836 elapsed, loss: 2.1778942e-06\n",
      "step: 111690 train: 0.07303333282470703 elapsed, loss: 2.196055e-06\n",
      "step: 111700 train: 0.08332991600036621 elapsed, loss: 1.0947689e-06\n",
      "step: 111710 train: 0.0869283676147461 elapsed, loss: 1.5194491e-06\n",
      "step: 111720 train: 0.0836801528930664 elapsed, loss: 1.3536755e-06\n",
      "step: 111730 train: 0.07651972770690918 elapsed, loss: 1.6596084e-06\n",
      "step: 111740 train: 0.08395218849182129 elapsed, loss: 1.1823129e-06\n",
      "step: 111750 train: 0.077667236328125 elapsed, loss: 1.0994254e-06\n",
      "step: 111760 train: 0.0830392837524414 elapsed, loss: 1.1362118e-06\n",
      "step: 111770 train: 0.07998943328857422 elapsed, loss: 9.709031e-07\n",
      "step: 111780 train: 0.08205080032348633 elapsed, loss: 1.2097871e-06\n",
      "step: 111790 train: 0.08017587661743164 elapsed, loss: 1.0072247e-06\n",
      "step: 111800 train: 0.08194255828857422 elapsed, loss: 1.0486685e-06\n",
      "step: 111810 train: 0.0846400260925293 elapsed, loss: 9.727657e-07\n",
      "step: 111820 train: 0.08620476722717285 elapsed, loss: 9.1409254e-07\n",
      "step: 111830 train: 0.08268857002258301 elapsed, loss: 8.414494e-07\n",
      "step: 111840 train: 0.07778096199035645 elapsed, loss: 1.186504e-06\n",
      "step: 111850 train: 0.07537150382995605 elapsed, loss: 1.3560043e-06\n",
      "step: 111860 train: 0.08647608757019043 elapsed, loss: 1.0794014e-06\n",
      "step: 111870 train: 0.08184027671813965 elapsed, loss: 1.289415e-06\n",
      "step: 111880 train: 0.08046817779541016 elapsed, loss: 1.3024535e-06\n",
      "step: 111890 train: 0.08313345909118652 elapsed, loss: 1.0724167e-06\n",
      "step: 111900 train: 0.08882689476013184 elapsed, loss: 1.3611265e-06\n",
      "step: 111910 train: 0.07921504974365234 elapsed, loss: 1.0901124e-06\n",
      "step: 111920 train: 0.08228635787963867 elapsed, loss: 1.377425e-06\n",
      "step: 111930 train: 0.07809114456176758 elapsed, loss: 1.1827788e-06\n",
      "step: 111940 train: 0.08006715774536133 elapsed, loss: 1.0565848e-06\n",
      "step: 111950 train: 0.0835721492767334 elapsed, loss: 1.4435486e-06\n",
      "step: 111960 train: 0.07767581939697266 elapsed, loss: 1.3113012e-06\n",
      "step: 111970 train: 0.08515262603759766 elapsed, loss: 1.068692e-06\n",
      "step: 111980 train: 0.07734298706054688 elapsed, loss: 1.7625259e-06\n",
      "step: 111990 train: 0.08261561393737793 elapsed, loss: 1.1241054e-06\n",
      "step: 112000 train: 0.08257579803466797 elapsed, loss: 1.3341183e-06\n",
      "step: 112010 train: 0.07626986503601074 elapsed, loss: 1.8663683e-06\n",
      "step: 112020 train: 0.08427619934082031 elapsed, loss: 1.6191024e-06\n",
      "step: 112030 train: 0.08117008209228516 elapsed, loss: 1.8267865e-06\n",
      "step: 112040 train: 0.08336997032165527 elapsed, loss: 1.9655536e-06\n",
      "step: 112050 train: 0.0784299373626709 elapsed, loss: 1.9473932e-06\n",
      "step: 112060 train: 0.07777190208435059 elapsed, loss: 1.4035018e-06\n",
      "step: 112070 train: 0.07902908325195312 elapsed, loss: 1.8179393e-06\n",
      "step: 112080 train: 0.0813605785369873 elapsed, loss: 2.4740534e-06\n",
      "step: 112090 train: 0.08182406425476074 elapsed, loss: 2.1057176e-06\n",
      "step: 112100 train: 0.0786139965057373 elapsed, loss: 1.6558897e-06\n",
      "step: 112110 train: 0.08221602439880371 elapsed, loss: 1.5618265e-06\n",
      "step: 112120 train: 0.07894253730773926 elapsed, loss: 1.5697426e-06\n",
      "step: 112130 train: 0.09128189086914062 elapsed, loss: 1.128762e-06\n",
      "step: 112140 train: 0.07796072959899902 elapsed, loss: 2.0773123e-06\n",
      "step: 112150 train: 0.0794377326965332 elapsed, loss: 1.4961684e-06\n",
      "step: 112160 train: 0.07707548141479492 elapsed, loss: 0.00032237842\n",
      "step: 112170 train: 0.08063054084777832 elapsed, loss: 0.00012032315\n",
      "step: 112180 train: 0.081024169921875 elapsed, loss: 2.8700442e-05\n",
      "step: 112190 train: 0.09207940101623535 elapsed, loss: 0.0013572613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 112200 train: 0.09216570854187012 elapsed, loss: 4.766572e-05\n",
      "step: 112210 train: 0.08007144927978516 elapsed, loss: 3.42768e-05\n",
      "step: 112220 train: 0.08300471305847168 elapsed, loss: 3.281227e-05\n",
      "step: 112230 train: 0.08504438400268555 elapsed, loss: 1.09843495e-05\n",
      "step: 112240 train: 0.08961367607116699 elapsed, loss: 1.4041603e-05\n",
      "step: 112250 train: 0.08831191062927246 elapsed, loss: 1.0223937e-05\n",
      "step: 112260 train: 0.08805632591247559 elapsed, loss: 6.6443163e-06\n",
      "step: 112270 train: 0.08552408218383789 elapsed, loss: 8.243134e-06\n",
      "step: 112280 train: 0.07370686531066895 elapsed, loss: 4.9350483e-06\n",
      "step: 112290 train: 0.08504629135131836 elapsed, loss: 3.0556448e-06\n",
      "step: 112300 train: 0.08201861381530762 elapsed, loss: 2.6663715e-06\n",
      "step: 112310 train: 0.08908700942993164 elapsed, loss: 1.7126998e-06\n",
      "step: 112320 train: 0.07957673072814941 elapsed, loss: 1.7690437e-06\n",
      "step: 112330 train: 0.07984185218811035 elapsed, loss: 2.5066502e-06\n",
      "step: 112340 train: 0.08084797859191895 elapsed, loss: 1.6894164e-06\n",
      "step: 112350 train: 0.07543492317199707 elapsed, loss: 1.5846437e-06\n",
      "step: 112360 train: 0.07775998115539551 elapsed, loss: 1.3522792e-06\n",
      "step: 112370 train: 0.0881648063659668 elapsed, loss: 9.774224e-07\n",
      "step: 112380 train: 0.07688498497009277 elapsed, loss: 1.3317901e-06\n",
      "step: 112390 train: 0.08245849609375 elapsed, loss: 1.6186369e-06\n",
      "step: 112400 train: 0.08031725883483887 elapsed, loss: 1.1948841e-06\n",
      "step: 112410 train: 0.08011746406555176 elapsed, loss: 1.6032695e-06\n",
      "step: 112420 train: 0.08003687858581543 elapsed, loss: 1.270323e-06\n",
      "step: 112430 train: 0.08540701866149902 elapsed, loss: 1.2251538e-06\n",
      "step: 112440 train: 0.0880892276763916 elapsed, loss: 1.1525108e-06\n",
      "step: 112450 train: 0.0767221450805664 elapsed, loss: 0.00011923845\n",
      "step: 112460 train: 0.08269143104553223 elapsed, loss: 1.1915066e-05\n",
      "step: 112470 train: 0.07908129692077637 elapsed, loss: 1.1567851e-05\n",
      "step: 112480 train: 0.07894039154052734 elapsed, loss: 1.5164862e-05\n",
      "step: 112490 train: 0.08178496360778809 elapsed, loss: 5.8593837e-06\n",
      "step: 112500 train: 0.08160543441772461 elapsed, loss: 3.8761377e-06\n",
      "step: 112510 train: 0.07918071746826172 elapsed, loss: 4.2910287e-06\n",
      "step: 112520 train: 0.07607197761535645 elapsed, loss: 3.7633927e-06\n",
      "step: 112530 train: 0.07627344131469727 elapsed, loss: 2.996521e-06\n",
      "step: 112540 train: 0.07835817337036133 elapsed, loss: 4.3390155e-06\n",
      "step: 112550 train: 0.07885909080505371 elapsed, loss: 1.8621772e-06\n",
      "step: 112560 train: 0.07911825180053711 elapsed, loss: 2.4777794e-06\n",
      "step: 112570 train: 0.07787561416625977 elapsed, loss: 2.2360796e-06\n",
      "step: 112580 train: 0.08759474754333496 elapsed, loss: 9.872009e-07\n",
      "step: 112590 train: 0.07827115058898926 elapsed, loss: 1.6167726e-06\n",
      "step: 112600 train: 0.08974719047546387 elapsed, loss: 1.4086231e-06\n",
      "step: 112610 train: 0.07840752601623535 elapsed, loss: 1.2717198e-06\n",
      "step: 112620 train: 0.07543826103210449 elapsed, loss: 1.3750963e-06\n",
      "step: 112630 train: 0.07670998573303223 elapsed, loss: 4.752793e-06\n",
      "step: 112640 train: 0.08440637588500977 elapsed, loss: 5.4669115e-05\n",
      "step: 112650 train: 0.08670902252197266 elapsed, loss: 3.6586848e-06\n",
      "step: 112660 train: 0.08477401733398438 elapsed, loss: 2.6510038e-06\n",
      "step: 112670 train: 0.08096837997436523 elapsed, loss: 2.312934e-06\n",
      "step: 112680 train: 0.07937121391296387 elapsed, loss: 2.6081636e-06\n",
      "step: 112690 train: 0.07715010643005371 elapsed, loss: 1.6568208e-06\n",
      "step: 112700 train: 0.07934165000915527 elapsed, loss: 1.5501846e-06\n",
      "step: 112710 train: 0.07968902587890625 elapsed, loss: 1.5487876e-06\n",
      "step: 112720 train: 0.07550430297851562 elapsed, loss: 1.6125832e-06\n",
      "step: 112730 train: 0.08267045021057129 elapsed, loss: 1.186504e-06\n",
      "step: 112740 train: 0.08238792419433594 elapsed, loss: 1.4356324e-06\n",
      "step: 112750 train: 0.08605790138244629 elapsed, loss: 9.359786e-07\n",
      "step: 112760 train: 0.0846853256225586 elapsed, loss: 1.2926745e-06\n",
      "step: 112770 train: 0.08286356925964355 elapsed, loss: 1.0114156e-06\n",
      "step: 112780 train: 0.0786139965057373 elapsed, loss: 1.261941e-06\n",
      "step: 112790 train: 0.08038020133972168 elapsed, loss: 1.2251537e-06\n",
      "step: 112800 train: 0.07910990715026855 elapsed, loss: 1.3248042e-06\n",
      "step: 112810 train: 0.07890105247497559 elapsed, loss: 1.3080409e-06\n",
      "step: 112820 train: 0.07382965087890625 elapsed, loss: 1.6437826e-06\n",
      "step: 112830 train: 0.08339953422546387 elapsed, loss: 1.3597298e-06\n",
      "step: 112840 train: 0.07629609107971191 elapsed, loss: 1.2558874e-06\n",
      "step: 112850 train: 0.08311581611633301 elapsed, loss: 1.1818476e-06\n",
      "step: 112860 train: 0.08179473876953125 elapsed, loss: 1.0691576e-06\n",
      "step: 112870 train: 0.08431601524353027 elapsed, loss: 1.2093215e-06\n",
      "step: 112880 train: 0.08432531356811523 elapsed, loss: 1.1348156e-06\n",
      "step: 112890 train: 0.07687187194824219 elapsed, loss: 1.451671e-05\n",
      "step: 112900 train: 0.07768034934997559 elapsed, loss: 1.1765272e-05\n",
      "step: 112910 train: 0.07664942741394043 elapsed, loss: 8.459683e-06\n",
      "step: 112920 train: 0.07889175415039062 elapsed, loss: 4.4233016e-06\n",
      "step: 112930 train: 0.08546257019042969 elapsed, loss: 3.424464e-06\n",
      "step: 112940 train: 0.0800778865814209 elapsed, loss: 4.3967593e-06\n",
      "step: 112950 train: 0.08550381660461426 elapsed, loss: 3.0011806e-06\n",
      "step: 112960 train: 0.08008813858032227 elapsed, loss: 2.4074639e-06\n",
      "step: 112970 train: 0.0837106704711914 elapsed, loss: 3.0775445e-06\n",
      "step: 112980 train: 0.08518743515014648 elapsed, loss: 1.8011756e-06\n",
      "step: 112990 train: 0.07941436767578125 elapsed, loss: 2.119687e-06\n",
      "step: 113000 train: 0.0786435604095459 elapsed, loss: 1.410021e-06\n",
      "step: 113010 train: 0.08564329147338867 elapsed, loss: 1.3480883e-06\n",
      "step: 113020 train: 0.08391666412353516 elapsed, loss: 1.1753283e-06\n",
      "step: 113030 train: 0.08329176902770996 elapsed, loss: 2.0623602e-06\n",
      "step: 113040 train: 0.07526040077209473 elapsed, loss: 3.986146e-05\n",
      "step: 113050 train: 0.08843684196472168 elapsed, loss: 8.16106e-06\n",
      "step: 113060 train: 0.0758051872253418 elapsed, loss: 9.0966205e-06\n",
      "step: 113070 train: 0.08260321617126465 elapsed, loss: 5.152887e-06\n",
      "step: 113080 train: 0.07944154739379883 elapsed, loss: 4.320849e-06\n",
      "step: 113090 train: 0.07941508293151855 elapsed, loss: 4.158339e-06\n",
      "step: 113100 train: 0.08169126510620117 elapsed, loss: 3.0645083e-06\n",
      "step: 113110 train: 0.07919859886169434 elapsed, loss: 3.4593886e-06\n",
      "step: 113120 train: 0.07709145545959473 elapsed, loss: 3.0975302e-06\n",
      "step: 113130 train: 0.07808327674865723 elapsed, loss: 2.0558916e-06\n",
      "step: 113140 train: 0.08303260803222656 elapsed, loss: 1.8426183e-06\n",
      "step: 113150 train: 0.08097696304321289 elapsed, loss: 1.2959338e-06\n",
      "step: 113160 train: 0.08189988136291504 elapsed, loss: 1.2330702e-06\n",
      "step: 113170 train: 0.0792531967163086 elapsed, loss: 1.1627552e-06\n",
      "step: 113180 train: 0.08187413215637207 elapsed, loss: 1.5408714e-06\n",
      "step: 113190 train: 0.08459734916687012 elapsed, loss: 1.083593e-06\n",
      "step: 113200 train: 0.0849764347076416 elapsed, loss: 1.1702059e-06\n",
      "step: 113210 train: 0.07826638221740723 elapsed, loss: 1.6530928e-06\n",
      "step: 113220 train: 0.07790660858154297 elapsed, loss: 1.5925598e-06\n",
      "step: 113230 train: 0.08154582977294922 elapsed, loss: 1.3187516e-06\n",
      "step: 113240 train: 0.07403945922851562 elapsed, loss: 1.5865064e-06\n",
      "step: 113250 train: 0.08458566665649414 elapsed, loss: 1.210718e-06\n",
      "step: 113260 train: 0.08347535133361816 elapsed, loss: 1.4151434e-06\n",
      "step: 113270 train: 0.08147764205932617 elapsed, loss: 9.448262e-07\n",
      "step: 113280 train: 0.0853414535522461 elapsed, loss: 1.1813812e-06\n",
      "step: 113290 train: 0.0799558162689209 elapsed, loss: 1.1399379e-06\n",
      "step: 113300 train: 0.076324462890625 elapsed, loss: 1.6665999e-06\n",
      "step: 113310 train: 0.08875751495361328 elapsed, loss: 1.4263192e-06\n",
      "step: 113320 train: 0.08507180213928223 elapsed, loss: 1.4272505e-06\n",
      "step: 113330 train: 0.0765066146850586 elapsed, loss: 2.0810376e-06\n",
      "step: 113340 train: 0.07808494567871094 elapsed, loss: 1.6503018e-06\n",
      "step: 113350 train: 0.07599091529846191 elapsed, loss: 1.8319095e-06\n",
      "step: 113360 train: 0.08347344398498535 elapsed, loss: 1.285224e-06\n",
      "step: 113370 train: 0.07680869102478027 elapsed, loss: 1.6135089e-06\n",
      "step: 113380 train: 0.08803677558898926 elapsed, loss: 1.1553047e-06\n",
      "step: 113390 train: 0.07962489128112793 elapsed, loss: 1.8952389e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 113400 train: 0.07685375213623047 elapsed, loss: 1.6014075e-06\n",
      "step: 113410 train: 0.08986210823059082 elapsed, loss: 1.4849924e-06\n",
      "step: 113420 train: 0.07813239097595215 elapsed, loss: 1.6046672e-06\n",
      "step: 113430 train: 0.08867549896240234 elapsed, loss: 1.2023365e-06\n",
      "step: 113440 train: 0.08603405952453613 elapsed, loss: 1.3778906e-06\n",
      "step: 113450 train: 0.08275127410888672 elapsed, loss: 2.0908165e-06\n",
      "step: 113460 train: 0.08292174339294434 elapsed, loss: 1.569277e-06\n",
      "step: 113470 train: 0.07415986061096191 elapsed, loss: 1.9925624e-06\n",
      "step: 113480 train: 0.07343935966491699 elapsed, loss: 4.3911714e-06\n",
      "step: 113490 train: 0.0813283920288086 elapsed, loss: 1.7187538e-06\n",
      "step: 113500 train: 0.08088374137878418 elapsed, loss: 1.4957025e-06\n",
      "step: 113510 train: 0.07917380332946777 elapsed, loss: 2.018639e-06\n",
      "step: 113520 train: 0.0819547176361084 elapsed, loss: 1.396517e-06\n",
      "step: 113530 train: 0.08113336563110352 elapsed, loss: 2.237032e-06\n",
      "step: 113540 train: 0.08133220672607422 elapsed, loss: 2.020036e-06\n",
      "step: 113550 train: 0.08598542213439941 elapsed, loss: 1.3504167e-06\n",
      "step: 113560 train: 0.08229875564575195 elapsed, loss: 2.2905845e-06\n",
      "step: 113570 train: 0.08778858184814453 elapsed, loss: 1.8477406e-06\n",
      "step: 113580 train: 0.08715152740478516 elapsed, loss: 2.4591504e-06\n",
      "step: 113590 train: 0.08516383171081543 elapsed, loss: 1.4887175e-06\n",
      "step: 113600 train: 0.08545613288879395 elapsed, loss: 1.5571699e-06\n",
      "step: 113610 train: 0.08016586303710938 elapsed, loss: 1.7313268e-06\n",
      "step: 113620 train: 0.08048510551452637 elapsed, loss: 2.144833e-06\n",
      "step: 113630 train: 0.08003783226013184 elapsed, loss: 1.6740506e-06\n",
      "step: 113640 train: 0.07680845260620117 elapsed, loss: 1.9082777e-06\n",
      "step: 113650 train: 0.0806427001953125 elapsed, loss: 1.6363316e-06\n",
      "step: 113660 train: 0.0755460262298584 elapsed, loss: 0.005722186\n",
      "step: 113670 train: 0.07961249351501465 elapsed, loss: 0.00023917435\n",
      "step: 113680 train: 0.08082246780395508 elapsed, loss: 8.6294465e-05\n",
      "step: 113690 train: 0.09169149398803711 elapsed, loss: 2.1432432e-05\n",
      "step: 113700 train: 0.08113408088684082 elapsed, loss: 1.4558573e-05\n",
      "step: 113710 train: 0.0870664119720459 elapsed, loss: 9.309357e-06\n",
      "step: 113720 train: 0.07431674003601074 elapsed, loss: 1.5666476e-05\n",
      "step: 113730 train: 0.08441424369812012 elapsed, loss: 8.842237e-06\n",
      "step: 113740 train: 0.08293938636779785 elapsed, loss: 5.6950043e-06\n",
      "step: 113750 train: 0.08124995231628418 elapsed, loss: 4.431685e-06\n",
      "step: 113760 train: 0.08098220825195312 elapsed, loss: 4.3487944e-06\n",
      "step: 113770 train: 0.09018969535827637 elapsed, loss: 2.8437832e-06\n",
      "step: 113780 train: 0.0797429084777832 elapsed, loss: 2.3129348e-06\n",
      "step: 113790 train: 0.07969474792480469 elapsed, loss: 3.7476311e-06\n",
      "step: 113800 train: 0.07684922218322754 elapsed, loss: 1.9771946e-06\n",
      "step: 113810 train: 0.0801689624786377 elapsed, loss: 1.6242249e-06\n",
      "step: 113820 train: 0.08520221710205078 elapsed, loss: 1.5068779e-06\n",
      "step: 113830 train: 0.07829689979553223 elapsed, loss: 2.2039712e-06\n",
      "step: 113840 train: 0.07899880409240723 elapsed, loss: 1.8924451e-06\n",
      "step: 113850 train: 0.08249092102050781 elapsed, loss: 1.2270165e-06\n",
      "step: 113860 train: 0.07806396484375 elapsed, loss: 2.4293427e-06\n",
      "step: 113870 train: 0.0786745548248291 elapsed, loss: 1.5208484e-06\n",
      "step: 113880 train: 0.08286476135253906 elapsed, loss: 0.00023231893\n",
      "step: 113890 train: 0.08111286163330078 elapsed, loss: 0.0010741511\n",
      "step: 113900 train: 0.07957243919372559 elapsed, loss: 4.1136023e-05\n",
      "step: 113910 train: 0.07650423049926758 elapsed, loss: 1.4197801e-05\n",
      "step: 113920 train: 0.08186531066894531 elapsed, loss: 1.5154278e-05\n",
      "step: 113930 train: 0.07418298721313477 elapsed, loss: 9.445854e-06\n",
      "step: 113940 train: 0.08450794219970703 elapsed, loss: 7.8192725e-06\n",
      "step: 113950 train: 0.07987236976623535 elapsed, loss: 5.951579e-06\n",
      "step: 113960 train: 0.07466483116149902 elapsed, loss: 5.017942e-06\n",
      "step: 113970 train: 0.0786433219909668 elapsed, loss: 2.8451846e-06\n",
      "step: 113980 train: 0.08281588554382324 elapsed, loss: 2.4819694e-06\n",
      "step: 113990 train: 0.08334112167358398 elapsed, loss: 2.4819697e-06\n",
      "step: 114000 train: 0.08040380477905273 elapsed, loss: 2.516424e-06\n",
      "step: 114010 train: 0.07966804504394531 elapsed, loss: 2.1867422e-06\n",
      "step: 114020 train: 0.08510470390319824 elapsed, loss: 1.8142106e-06\n",
      "step: 114030 train: 0.08379101753234863 elapsed, loss: 1.4412201e-06\n",
      "step: 114040 train: 0.07863664627075195 elapsed, loss: 1.538077e-06\n",
      "step: 114050 train: 0.08904242515563965 elapsed, loss: 1.3979138e-06\n",
      "step: 114060 train: 0.09257960319519043 elapsed, loss: 1.1888321e-06\n",
      "step: 114070 train: 0.08417367935180664 elapsed, loss: 1.2121153e-06\n",
      "step: 114080 train: 0.08297514915466309 elapsed, loss: 1.1641516e-06\n",
      "step: 114090 train: 0.08573174476623535 elapsed, loss: 1.2903461e-06\n",
      "step: 114100 train: 0.08039426803588867 elapsed, loss: 1.2540248e-06\n",
      "step: 114110 train: 0.08205699920654297 elapsed, loss: 1.2242224e-06\n",
      "step: 114120 train: 0.08129763603210449 elapsed, loss: 1.6349351e-06\n",
      "step: 114130 train: 0.07568812370300293 elapsed, loss: 1.8035039e-06\n",
      "step: 114140 train: 0.08414888381958008 elapsed, loss: 1.161824e-06\n",
      "step: 114150 train: 0.07956671714782715 elapsed, loss: 8.117857e-05\n",
      "step: 114160 train: 0.08144116401672363 elapsed, loss: 0.00018208794\n",
      "step: 114170 train: 0.08699321746826172 elapsed, loss: 3.300263e-05\n",
      "step: 114180 train: 0.07827472686767578 elapsed, loss: 2.1337393e-05\n",
      "step: 114190 train: 0.08511686325073242 elapsed, loss: 8.9620335e-06\n",
      "step: 114200 train: 0.07161283493041992 elapsed, loss: 1.6850041e-05\n",
      "step: 114210 train: 0.08351278305053711 elapsed, loss: 6.3249945e-06\n",
      "step: 114220 train: 0.08590412139892578 elapsed, loss: 6.9340986e-06\n",
      "step: 114230 train: 0.08157229423522949 elapsed, loss: 5.0798735e-06\n",
      "step: 114240 train: 0.08624601364135742 elapsed, loss: 3.3229467e-06\n",
      "step: 114250 train: 0.08294486999511719 elapsed, loss: 2.587674e-06\n",
      "step: 114260 train: 0.0890800952911377 elapsed, loss: 2.6202708e-06\n",
      "step: 114270 train: 0.08540987968444824 elapsed, loss: 2.5252764e-06\n",
      "step: 114280 train: 0.07921671867370605 elapsed, loss: 1.6503013e-06\n",
      "step: 114290 train: 0.0811309814453125 elapsed, loss: 1.9022236e-06\n",
      "step: 114300 train: 0.07887458801269531 elapsed, loss: 1.5865062e-06\n",
      "step: 114310 train: 0.07626986503601074 elapsed, loss: 1.4835953e-06\n",
      "step: 114320 train: 0.07788681983947754 elapsed, loss: 1.6302782e-06\n",
      "step: 114330 train: 0.08632922172546387 elapsed, loss: 1.1413349e-06\n",
      "step: 114340 train: 0.08339881896972656 elapsed, loss: 9.667122e-07\n",
      "step: 114350 train: 0.08573532104492188 elapsed, loss: 1.0714857e-06\n",
      "step: 114360 train: 0.07536911964416504 elapsed, loss: 1.2647351e-06\n",
      "step: 114370 train: 0.07672977447509766 elapsed, loss: 1.8249225e-06\n",
      "step: 114380 train: 0.08542943000793457 elapsed, loss: 1.0118813e-06\n",
      "step: 114390 train: 0.08195257186889648 elapsed, loss: 1.5380773e-06\n",
      "step: 114400 train: 0.08687591552734375 elapsed, loss: 9.704374e-07\n",
      "step: 114410 train: 0.08170914649963379 elapsed, loss: 1.3657834e-06\n",
      "step: 114420 train: 0.0776360034942627 elapsed, loss: 1.5567039e-06\n",
      "step: 114430 train: 0.08234620094299316 elapsed, loss: 1.0575161e-06\n",
      "step: 114440 train: 0.08702254295349121 elapsed, loss: 1.0556535e-06\n",
      "step: 114450 train: 0.07624101638793945 elapsed, loss: 1.69128e-06\n",
      "step: 114460 train: 0.09369635581970215 elapsed, loss: 1.3736995e-06\n",
      "step: 114470 train: 0.08265161514282227 elapsed, loss: 1.6577525e-06\n",
      "step: 114480 train: 0.09011411666870117 elapsed, loss: 1.2377268e-06\n",
      "step: 114490 train: 0.0821528434753418 elapsed, loss: 1.576723e-06\n",
      "step: 114500 train: 0.07826662063598633 elapsed, loss: 1.2079246e-06\n",
      "step: 114510 train: 0.07391929626464844 elapsed, loss: 2.0326088e-06\n",
      "step: 114520 train: 0.08072495460510254 elapsed, loss: 2.257057e-06\n",
      "step: 114530 train: 0.07608580589294434 elapsed, loss: 1.969745e-06\n",
      "step: 114540 train: 0.08246994018554688 elapsed, loss: 1.2116496e-06\n",
      "step: 114550 train: 0.07976508140563965 elapsed, loss: 1.7643887e-06\n",
      "step: 114560 train: 0.08181214332580566 elapsed, loss: 1.3089727e-06\n",
      "step: 114570 train: 0.08751797676086426 elapsed, loss: 1.4244562e-06\n",
      "step: 114580 train: 0.08673238754272461 elapsed, loss: 1.2856897e-06\n",
      "step: 114590 train: 0.0834658145904541 elapsed, loss: 1.9012925e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 114600 train: 0.08416891098022461 elapsed, loss: 1.4686943e-06\n",
      "step: 114610 train: 0.07291698455810547 elapsed, loss: 2.1485585e-06\n",
      "step: 114620 train: 0.0794227123260498 elapsed, loss: 1.5986134e-06\n",
      "step: 114630 train: 0.08194851875305176 elapsed, loss: 1.929698e-06\n",
      "step: 114640 train: 0.0810232162475586 elapsed, loss: 1.8700937e-06\n",
      "step: 114650 train: 0.08316946029663086 elapsed, loss: 4.4144304e-06\n",
      "step: 114660 train: 0.08904480934143066 elapsed, loss: 1.5841779e-06\n",
      "step: 114670 train: 0.08367490768432617 elapsed, loss: 1.4705553e-06\n",
      "step: 114680 train: 0.08169674873352051 elapsed, loss: 1.982783e-06\n",
      "step: 114690 train: 0.07951474189758301 elapsed, loss: 1.8649712e-06\n",
      "step: 114700 train: 0.08388447761535645 elapsed, loss: 1.9385454e-06\n",
      "step: 114710 train: 0.08450794219970703 elapsed, loss: 1.5320202e-06\n",
      "step: 114720 train: 0.08079409599304199 elapsed, loss: 1.5324897e-06\n",
      "step: 114730 train: 0.07768511772155762 elapsed, loss: 1.946462e-06\n",
      "step: 114740 train: 0.0763711929321289 elapsed, loss: 3.395592e-06\n",
      "step: 114750 train: 0.08035397529602051 elapsed, loss: 1.632141e-06\n",
      "step: 114760 train: 0.08243727684020996 elapsed, loss: 1.8095567e-06\n",
      "step: 114770 train: 0.08391809463500977 elapsed, loss: 2.244484e-06\n",
      "step: 114780 train: 0.08067178726196289 elapsed, loss: 1.4356324e-06\n",
      "step: 114790 train: 0.08336496353149414 elapsed, loss: 1.7406392e-06\n",
      "step: 114800 train: 0.08305478096008301 elapsed, loss: 1.8333064e-06\n",
      "step: 114810 train: 0.0764012336730957 elapsed, loss: 2.473589e-06\n",
      "step: 114820 train: 0.08843517303466797 elapsed, loss: 1.7546097e-06\n",
      "step: 114830 train: 0.09242486953735352 elapsed, loss: 1.8202677e-06\n",
      "step: 114840 train: 0.08414840698242188 elapsed, loss: 1.6991961e-06\n",
      "step: 114850 train: 0.07988286018371582 elapsed, loss: 2.1099086e-06\n",
      "step: 114860 train: 0.07990837097167969 elapsed, loss: 1.5585667e-06\n",
      "step: 114870 train: 0.0813150405883789 elapsed, loss: 2.268233e-06\n",
      "step: 114880 train: 0.08074116706848145 elapsed, loss: 1.6447141e-06\n",
      "step: 114890 train: 0.0817422866821289 elapsed, loss: 3.0985043e-06\n",
      "step: 114900 train: 0.08666706085205078 elapsed, loss: 2.0312118e-06\n",
      "step: 114910 train: 0.08888602256774902 elapsed, loss: 1.3755622e-06\n",
      "step: 114920 train: 0.0831911563873291 elapsed, loss: 2.7348242e-06\n",
      "step: 114930 train: 0.0785377025604248 elapsed, loss: 2.259851e-06\n",
      "step: 114940 train: 0.0788874626159668 elapsed, loss: 1.5743993e-06\n",
      "step: 114950 train: 0.08027529716491699 elapsed, loss: 1.9976842e-06\n",
      "step: 114960 train: 0.0742647647857666 elapsed, loss: 2.4652068e-06\n",
      "step: 114970 train: 0.07944798469543457 elapsed, loss: 2.857757e-06\n",
      "step: 114980 train: 0.0775904655456543 elapsed, loss: 1.8919797e-06\n",
      "step: 114990 train: 0.08675336837768555 elapsed, loss: 1.595354e-06\n",
      "step: 115000 train: 0.08029508590698242 elapsed, loss: 0.00023872631\n",
      "step: 115010 train: 0.0777580738067627 elapsed, loss: 3.7735168e-05\n",
      "step: 115020 train: 0.0756232738494873 elapsed, loss: 5.0504845e-05\n",
      "step: 115030 train: 0.08742356300354004 elapsed, loss: 1.5353246e-05\n",
      "step: 115040 train: 0.0824277400970459 elapsed, loss: 9.802066e-06\n",
      "step: 115050 train: 0.08406996726989746 elapsed, loss: 6.4386586e-06\n",
      "step: 115060 train: 0.08246970176696777 elapsed, loss: 5.741542e-06\n",
      "step: 115070 train: 0.07908892631530762 elapsed, loss: 5.6386507e-06\n",
      "step: 115080 train: 0.0815129280090332 elapsed, loss: 4.45124e-06\n",
      "step: 115090 train: 0.08640193939208984 elapsed, loss: 7.648758e-06\n",
      "step: 115100 train: 0.07937145233154297 elapsed, loss: 3.7588065e-06\n",
      "step: 115110 train: 0.07647013664245605 elapsed, loss: 3.8323815e-06\n",
      "step: 115120 train: 0.08117794990539551 elapsed, loss: 2.945767e-06\n",
      "step: 115130 train: 0.08927655220031738 elapsed, loss: 2.519221e-06\n",
      "step: 115140 train: 0.07865643501281738 elapsed, loss: 2.5401782e-06\n",
      "step: 115150 train: 0.08240342140197754 elapsed, loss: 1.5925596e-06\n",
      "step: 115160 train: 0.08161234855651855 elapsed, loss: 2.0125854e-06\n",
      "step: 115170 train: 0.0849006175994873 elapsed, loss: 1.3806846e-06\n",
      "step: 115180 train: 0.08191585540771484 elapsed, loss: 1.7452962e-06\n",
      "step: 115190 train: 0.08522319793701172 elapsed, loss: 1.6014048e-06\n",
      "step: 115200 train: 0.07656621932983398 elapsed, loss: 1.9270803e-05\n",
      "step: 115210 train: 0.0838472843170166 elapsed, loss: 3.6223691e-06\n",
      "step: 115220 train: 0.08234500885009766 elapsed, loss: 3.459389e-06\n",
      "step: 115230 train: 0.08542466163635254 elapsed, loss: 2.9471644e-06\n",
      "step: 115240 train: 0.08994221687316895 elapsed, loss: 1.7122345e-06\n",
      "step: 115250 train: 0.08800721168518066 elapsed, loss: 1.6055975e-06\n",
      "step: 115260 train: 0.08491635322570801 elapsed, loss: 1.5790555e-06\n",
      "step: 115270 train: 0.08474040031433105 elapsed, loss: 1.7709054e-06\n",
      "step: 115280 train: 0.08519673347473145 elapsed, loss: 1.8188707e-06\n",
      "step: 115290 train: 0.07909274101257324 elapsed, loss: 1.9548434e-06\n",
      "step: 115300 train: 0.07546496391296387 elapsed, loss: 1.9660138e-06\n",
      "step: 115310 train: 0.08776569366455078 elapsed, loss: 1.2023365e-06\n",
      "step: 115320 train: 0.07777595520019531 elapsed, loss: 1.6526301e-06\n",
      "step: 115330 train: 0.07933497428894043 elapsed, loss: 1.9539114e-06\n",
      "step: 115340 train: 0.07522153854370117 elapsed, loss: 1.7210823e-06\n",
      "step: 115350 train: 0.07595539093017578 elapsed, loss: 1.4659004e-06\n",
      "step: 115360 train: 0.08020758628845215 elapsed, loss: 1.4686939e-06\n",
      "step: 115370 train: 0.08500289916992188 elapsed, loss: 1.4598468e-06\n",
      "step: 115380 train: 0.0846109390258789 elapsed, loss: 1.2288791e-06\n",
      "step: 115390 train: 0.08394432067871094 elapsed, loss: 1.4370295e-06\n",
      "step: 115400 train: 0.08366584777832031 elapsed, loss: 7.125816e-06\n",
      "step: 115410 train: 0.07965755462646484 elapsed, loss: 0.0046937717\n",
      "step: 115420 train: 0.07622122764587402 elapsed, loss: 5.3794723e-05\n",
      "step: 115430 train: 0.07812833786010742 elapsed, loss: 1.7648108e-05\n",
      "step: 115440 train: 0.0766744613647461 elapsed, loss: 1.3040133e-05\n",
      "step: 115450 train: 0.0850822925567627 elapsed, loss: 9.087734e-06\n",
      "step: 115460 train: 0.08311724662780762 elapsed, loss: 7.77415e-06\n",
      "step: 115470 train: 0.08057904243469238 elapsed, loss: 6.3635894e-06\n",
      "step: 115480 train: 0.08615422248840332 elapsed, loss: 3.4388993e-06\n",
      "step: 115490 train: 0.07613086700439453 elapsed, loss: 5.204671e-06\n",
      "step: 115500 train: 0.07578158378601074 elapsed, loss: 3.021669e-06\n",
      "step: 115510 train: 0.07794976234436035 elapsed, loss: 2.6314437e-06\n",
      "step: 115520 train: 0.08017206192016602 elapsed, loss: 4.6184746e-06\n",
      "step: 115530 train: 0.07411956787109375 elapsed, loss: 2.559269e-06\n",
      "step: 115540 train: 0.08007621765136719 elapsed, loss: 1.463572e-06\n",
      "step: 115550 train: 0.08345174789428711 elapsed, loss: 1.6014073e-06\n",
      "step: 115560 train: 0.0806882381439209 elapsed, loss: 1.3397062e-06\n",
      "step: 115570 train: 0.08834505081176758 elapsed, loss: 1.164618e-06\n",
      "step: 115580 train: 0.0825190544128418 elapsed, loss: 1.56788e-06\n",
      "step: 115590 train: 0.07357478141784668 elapsed, loss: 1.8891851e-06\n",
      "step: 115600 train: 0.08743453025817871 elapsed, loss: 1.4118837e-06\n",
      "step: 115610 train: 0.08165168762207031 elapsed, loss: 1.5404062e-06\n",
      "step: 115620 train: 0.07908248901367188 elapsed, loss: 1.5469252e-06\n",
      "step: 115630 train: 0.08641695976257324 elapsed, loss: 1.0193319e-06\n",
      "step: 115640 train: 0.08429718017578125 elapsed, loss: 1.2041991e-06\n",
      "step: 115650 train: 0.08196401596069336 elapsed, loss: 1.4328384e-06\n",
      "step: 115660 train: 0.08017873764038086 elapsed, loss: 1.3625236e-06\n",
      "step: 115670 train: 0.0768883228302002 elapsed, loss: 1.3229426e-06\n",
      "step: 115680 train: 0.08274531364440918 elapsed, loss: 1.2079585e-05\n",
      "step: 115690 train: 0.07925868034362793 elapsed, loss: 0.00013607334\n",
      "step: 115700 train: 0.0881505012512207 elapsed, loss: 0.00013467517\n",
      "step: 115710 train: 0.08589339256286621 elapsed, loss: 1.6507398e-05\n",
      "step: 115720 train: 0.08812212944030762 elapsed, loss: 9.789502e-06\n",
      "step: 115730 train: 0.07958078384399414 elapsed, loss: 9.500303e-06\n",
      "step: 115740 train: 0.08639717102050781 elapsed, loss: 7.423044e-06\n",
      "step: 115750 train: 0.08032059669494629 elapsed, loss: 8.257501e-06\n",
      "step: 115760 train: 0.07912278175354004 elapsed, loss: 1.50612905e-05\n",
      "step: 115770 train: 0.07808136940002441 elapsed, loss: 5.05147e-06\n",
      "step: 115780 train: 0.0812678337097168 elapsed, loss: 4.6873292e-06\n",
      "step: 115790 train: 0.0764153003692627 elapsed, loss: 4.162057e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 115800 train: 0.07462000846862793 elapsed, loss: 2.975568e-06\n",
      "step: 115810 train: 0.08906197547912598 elapsed, loss: 1.5688112e-06\n",
      "step: 115820 train: 0.07685232162475586 elapsed, loss: 2.377662e-06\n",
      "step: 115830 train: 0.0820772647857666 elapsed, loss: 1.3713698e-06\n",
      "step: 115840 train: 0.08072853088378906 elapsed, loss: 1.6377289e-06\n",
      "step: 115850 train: 0.08007335662841797 elapsed, loss: 1.3173544e-06\n",
      "step: 115860 train: 0.07806229591369629 elapsed, loss: 1.4454112e-06\n",
      "step: 115870 train: 0.08455991744995117 elapsed, loss: 1.327127e-06\n",
      "step: 115880 train: 0.08545517921447754 elapsed, loss: 9.848729e-07\n",
      "step: 115890 train: 0.08135223388671875 elapsed, loss: 1.3075758e-06\n",
      "step: 115900 train: 0.08439040184020996 elapsed, loss: 1.1911607e-06\n",
      "step: 115910 train: 0.07960653305053711 elapsed, loss: 0.00048435104\n",
      "step: 115920 train: 0.07533383369445801 elapsed, loss: 6.058431e-05\n",
      "step: 115930 train: 0.08605670928955078 elapsed, loss: 2.3909564e-05\n",
      "step: 115940 train: 0.07772111892700195 elapsed, loss: 1.6760401e-05\n",
      "step: 115950 train: 0.0842292308807373 elapsed, loss: 9.806669e-06\n",
      "step: 115960 train: 0.08389711380004883 elapsed, loss: 8.723575e-06\n",
      "step: 115970 train: 0.08147788047790527 elapsed, loss: 5.5478595e-06\n",
      "step: 115980 train: 0.08455038070678711 elapsed, loss: 5.344355e-06\n",
      "step: 115990 train: 0.08648252487182617 elapsed, loss: 2.7571743e-06\n",
      "step: 116000 train: 0.0811464786529541 elapsed, loss: 4.2277134e-06\n",
      "step: 116010 train: 0.09062576293945312 elapsed, loss: 2.401407e-06\n",
      "step: 116020 train: 0.0855867862701416 elapsed, loss: 4.0125606e-06\n",
      "step: 116030 train: 0.08622145652770996 elapsed, loss: 2.6881378e-06\n",
      "step: 116040 train: 0.07571148872375488 elapsed, loss: 2.0544942e-06\n",
      "step: 116050 train: 0.08205962181091309 elapsed, loss: 1.2163058e-06\n",
      "step: 116060 train: 0.0802767276763916 elapsed, loss: 1.3504164e-06\n",
      "step: 116070 train: 0.08529305458068848 elapsed, loss: 1.099891e-06\n",
      "step: 116080 train: 0.0914304256439209 elapsed, loss: 9.760254e-07\n",
      "step: 116090 train: 0.09134221076965332 elapsed, loss: 5.4246693e-06\n",
      "step: 116100 train: 0.08749079704284668 elapsed, loss: 2.152281e-06\n",
      "step: 116110 train: 0.08536839485168457 elapsed, loss: 1.3662489e-06\n",
      "step: 116120 train: 0.0760340690612793 elapsed, loss: 1.3434316e-06\n",
      "step: 116130 train: 0.08026623725891113 elapsed, loss: 1.2032665e-06\n",
      "step: 116140 train: 0.07671904563903809 elapsed, loss: 1.596285e-06\n",
      "step: 116150 train: 0.08231639862060547 elapsed, loss: 1.1278308e-06\n",
      "step: 116160 train: 0.08180356025695801 elapsed, loss: 1.0291108e-06\n",
      "step: 116170 train: 0.08454203605651855 elapsed, loss: 0.071767956\n",
      "step: 116180 train: 0.0806417465209961 elapsed, loss: 0.00021662793\n",
      "step: 116190 train: 0.08380270004272461 elapsed, loss: 8.8486195e-05\n",
      "step: 116200 train: 0.08319878578186035 elapsed, loss: 1.9857182e-05\n",
      "step: 116210 train: 0.08358263969421387 elapsed, loss: 2.0299516e-05\n",
      "step: 116220 train: 0.07956480979919434 elapsed, loss: 1.3292032e-05\n",
      "step: 116230 train: 0.07597970962524414 elapsed, loss: 8.820951e-06\n",
      "step: 116240 train: 0.0835874080657959 elapsed, loss: 6.0149014e-06\n",
      "step: 116250 train: 0.08466029167175293 elapsed, loss: 5.151568e-06\n",
      "step: 116260 train: 0.08106184005737305 elapsed, loss: 3.881275e-06\n",
      "step: 116270 train: 0.08426523208618164 elapsed, loss: 2.9713751e-06\n",
      "step: 116280 train: 0.07944941520690918 elapsed, loss: 2.9350567e-06\n",
      "step: 116290 train: 0.08474445343017578 elapsed, loss: 1.8835972e-06\n",
      "step: 116300 train: 0.0850369930267334 elapsed, loss: 2.6345704e-06\n",
      "step: 116310 train: 0.08482527732849121 elapsed, loss: 1.4589152e-06\n",
      "step: 116320 train: 0.09053373336791992 elapsed, loss: 1.1282965e-06\n",
      "step: 116330 train: 0.0794684886932373 elapsed, loss: 1.4062953e-06\n",
      "step: 116340 train: 0.08281183242797852 elapsed, loss: 1.5958192e-06\n",
      "step: 116350 train: 0.08125615119934082 elapsed, loss: 1.3280647e-06\n",
      "step: 116360 train: 0.08045244216918945 elapsed, loss: 9.927892e-07\n",
      "step: 116370 train: 0.08525919914245605 elapsed, loss: 8.43312e-07\n",
      "step: 116380 train: 0.08422279357910156 elapsed, loss: 8.861529e-07\n",
      "step: 116390 train: 0.08646249771118164 elapsed, loss: 9.583302e-07\n",
      "step: 116400 train: 0.07964372634887695 elapsed, loss: 1.0887153e-06\n",
      "step: 116410 train: 0.08036518096923828 elapsed, loss: 1.0710202e-06\n",
      "step: 116420 train: 0.08189034461975098 elapsed, loss: 1.3289961e-06\n",
      "step: 116430 train: 0.0762472152709961 elapsed, loss: 1.1529764e-06\n",
      "step: 116440 train: 0.08695244789123535 elapsed, loss: 9.3178767e-07\n",
      "step: 116450 train: 0.0782172679901123 elapsed, loss: 1.2237568e-06\n",
      "step: 116460 train: 0.08278298377990723 elapsed, loss: 1.3448287e-06\n",
      "step: 116470 train: 0.07534933090209961 elapsed, loss: 1.5120006e-06\n",
      "step: 116480 train: 0.07926464080810547 elapsed, loss: 1.4076927e-06\n",
      "step: 116490 train: 0.07838129997253418 elapsed, loss: 1.4863895e-06\n",
      "step: 116500 train: 0.0832819938659668 elapsed, loss: 9.634525e-07\n",
      "step: 116510 train: 0.08188796043395996 elapsed, loss: 1.0896467e-06\n",
      "step: 116520 train: 0.08037233352661133 elapsed, loss: 1.4146765e-06\n",
      "step: 116530 train: 0.07587218284606934 elapsed, loss: 1.2717198e-06\n",
      "step: 116540 train: 0.08236289024353027 elapsed, loss: 1.3438973e-06\n",
      "step: 116550 train: 0.08694171905517578 elapsed, loss: 1.1790519e-06\n",
      "step: 116560 train: 0.08011698722839355 elapsed, loss: 1.4970947e-06\n",
      "step: 116570 train: 0.07450723648071289 elapsed, loss: 1.6889517e-06\n",
      "step: 116580 train: 0.08136653900146484 elapsed, loss: 1.3522792e-06\n",
      "step: 116590 train: 0.07686734199523926 elapsed, loss: 2.0978014e-06\n",
      "step: 116600 train: 0.0789189338684082 elapsed, loss: 1.3490194e-06\n",
      "step: 116610 train: 0.08617138862609863 elapsed, loss: 1.0379582e-06\n",
      "step: 116620 train: 0.08609843254089355 elapsed, loss: 1.187901e-06\n",
      "step: 116630 train: 0.08199524879455566 elapsed, loss: 1.7494874e-06\n",
      "step: 116640 train: 0.08590364456176758 elapsed, loss: 1.3858062e-06\n",
      "step: 116650 train: 0.08030843734741211 elapsed, loss: 1.7099064e-06\n",
      "step: 116660 train: 0.0863492488861084 elapsed, loss: 1.2237567e-06\n",
      "step: 116670 train: 0.07834863662719727 elapsed, loss: 1.5776588e-06\n",
      "step: 116680 train: 0.08404946327209473 elapsed, loss: 1.3601955e-06\n",
      "step: 116690 train: 0.0852041244506836 elapsed, loss: 2.0633422e-06\n",
      "step: 116700 train: 0.08314132690429688 elapsed, loss: 1.8044353e-06\n",
      "step: 116710 train: 0.08653068542480469 elapsed, loss: 1.1273651e-06\n",
      "step: 116720 train: 0.08005309104919434 elapsed, loss: 2.213283e-06\n",
      "step: 116730 train: 0.07971644401550293 elapsed, loss: 1.3378437e-06\n",
      "step: 116740 train: 0.07931661605834961 elapsed, loss: 2.0181733e-06\n",
      "step: 116750 train: 0.08258342742919922 elapsed, loss: 1.3392408e-06\n",
      "step: 116760 train: 0.08167123794555664 elapsed, loss: 2.4512365e-06\n",
      "step: 116770 train: 0.07809019088745117 elapsed, loss: 1.3899977e-06\n",
      "step: 116780 train: 0.08362889289855957 elapsed, loss: 1.286621e-06\n",
      "step: 116790 train: 0.08395934104919434 elapsed, loss: 2.0051205e-06\n",
      "step: 116800 train: 0.0831902027130127 elapsed, loss: 1.4784732e-06\n",
      "step: 116810 train: 0.08434128761291504 elapsed, loss: 1.8505359e-06\n",
      "step: 116820 train: 0.08647894859313965 elapsed, loss: 1.4239905e-06\n",
      "step: 116830 train: 0.08067727088928223 elapsed, loss: 2.3161958e-06\n",
      "step: 116840 train: 0.0795743465423584 elapsed, loss: 2.3096763e-06\n",
      "step: 116850 train: 0.08738875389099121 elapsed, loss: 1.5352825e-06\n",
      "step: 116860 train: 0.08060479164123535 elapsed, loss: 2.4079304e-06\n",
      "step: 116870 train: 0.08107900619506836 elapsed, loss: 1.912003e-06\n",
      "step: 116880 train: 0.07556533813476562 elapsed, loss: 2.3469293e-06\n",
      "step: 116890 train: 0.08027458190917969 elapsed, loss: 1.8328406e-06\n",
      "step: 116900 train: 0.08039665222167969 elapsed, loss: 1.5995449e-06\n",
      "step: 116910 train: 0.08526396751403809 elapsed, loss: 1.7723044e-06\n",
      "step: 116920 train: 0.09046530723571777 elapsed, loss: 1.6307441e-06\n",
      "step: 116930 train: 0.08883976936340332 elapsed, loss: 1.8812696e-06\n",
      "step: 116940 train: 0.0842585563659668 elapsed, loss: 1.5236423e-06\n",
      "step: 116950 train: 0.07716250419616699 elapsed, loss: 2.6128214e-06\n",
      "step: 116960 train: 0.08872151374816895 elapsed, loss: 1.4975652e-06\n",
      "step: 116970 train: 0.07877731323242188 elapsed, loss: 2.134123e-06\n",
      "step: 116980 train: 0.08577299118041992 elapsed, loss: 1.5194513e-06\n",
      "step: 116990 train: 0.08332681655883789 elapsed, loss: 1.9948905e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 117000 train: 0.08017992973327637 elapsed, loss: 2.5760337e-06\n",
      "step: 117010 train: 0.08307433128356934 elapsed, loss: 2.1802216e-06\n",
      "step: 117020 train: 0.08071327209472656 elapsed, loss: 2.138313e-06\n",
      "step: 117030 train: 0.0820622444152832 elapsed, loss: 2.070327e-06\n",
      "step: 117040 train: 0.08152222633361816 elapsed, loss: 6.167181e-06\n",
      "step: 117050 train: 0.08055496215820312 elapsed, loss: 5.0053727e-06\n",
      "step: 117060 train: 0.08814835548400879 elapsed, loss: 2.504788e-06\n",
      "step: 117070 train: 0.08137774467468262 elapsed, loss: 2.4633441e-06\n",
      "step: 117080 train: 0.07622241973876953 elapsed, loss: 2.1820845e-06\n",
      "step: 117090 train: 0.07914590835571289 elapsed, loss: 2.3497237e-06\n",
      "step: 117100 train: 0.07768630981445312 elapsed, loss: 2.3567081e-06\n",
      "step: 117110 train: 0.08015012741088867 elapsed, loss: 2.109908e-06\n",
      "step: 117120 train: 0.08370137214660645 elapsed, loss: 1.6987292e-06\n",
      "step: 117130 train: 0.08536100387573242 elapsed, loss: 1.6200338e-06\n",
      "step: 117140 train: 0.07506227493286133 elapsed, loss: 2.3297002e-06\n",
      "step: 117150 train: 0.07950615882873535 elapsed, loss: 1.9585684e-06\n",
      "step: 117160 train: 0.08007049560546875 elapsed, loss: 2.044716e-06\n",
      "step: 117170 train: 0.07996797561645508 elapsed, loss: 2.3557768e-06\n",
      "step: 117180 train: 0.08461475372314453 elapsed, loss: 2.0810367e-06\n",
      "step: 117190 train: 0.08127140998840332 elapsed, loss: 1.6456452e-06\n",
      "step: 117200 train: 0.07767367362976074 elapsed, loss: 4.5248125e-06\n",
      "step: 117210 train: 0.08410930633544922 elapsed, loss: 1.5399403e-06\n",
      "step: 117220 train: 0.08153009414672852 elapsed, loss: 2.3706775e-06\n",
      "step: 117230 train: 0.08402442932128906 elapsed, loss: 2.1350538e-06\n",
      "step: 117240 train: 0.08624124526977539 elapsed, loss: 3.4365594e-06\n",
      "step: 117250 train: 0.08567333221435547 elapsed, loss: 1.9441331e-06\n",
      "step: 117260 train: 0.07919716835021973 elapsed, loss: 2.461482e-06\n",
      "step: 117270 train: 0.08478331565856934 elapsed, loss: 2.1471615e-06\n",
      "step: 117280 train: 0.0801694393157959 elapsed, loss: 2.5080478e-06\n",
      "step: 117290 train: 0.08185720443725586 elapsed, loss: 1.6069952e-06\n",
      "step: 117300 train: 0.08047056198120117 elapsed, loss: 2.6128203e-06\n",
      "step: 117310 train: 0.08010005950927734 elapsed, loss: 2.0908165e-06\n",
      "step: 117320 train: 0.09308314323425293 elapsed, loss: 1.6065298e-06\n",
      "step: 117330 train: 0.08024883270263672 elapsed, loss: 1.8309764e-06\n",
      "step: 117340 train: 0.08294963836669922 elapsed, loss: 1.9357517e-06\n",
      "step: 117350 train: 0.08135414123535156 elapsed, loss: 2.7981537e-06\n",
      "step: 117360 train: 0.08770871162414551 elapsed, loss: 2.9941957e-06\n",
      "step: 117370 train: 0.08333873748779297 elapsed, loss: 2.3143323e-06\n",
      "step: 117380 train: 0.08205389976501465 elapsed, loss: 2.0498385e-06\n",
      "step: 117390 train: 0.08212423324584961 elapsed, loss: 1.8277153e-06\n",
      "step: 117400 train: 0.08175802230834961 elapsed, loss: 2.1113055e-06\n",
      "step: 117410 train: 0.07922172546386719 elapsed, loss: 2.900133e-06\n",
      "step: 117420 train: 0.08466529846191406 elapsed, loss: 2.1541464e-06\n",
      "step: 117430 train: 0.08185720443725586 elapsed, loss: 1.9464617e-06\n",
      "step: 117440 train: 0.08257269859313965 elapsed, loss: 1.2503453e-05\n",
      "step: 117450 train: 0.08200693130493164 elapsed, loss: 2.9304008e-06\n",
      "step: 117460 train: 0.08754086494445801 elapsed, loss: 2.051701e-06\n",
      "step: 117470 train: 0.07936835289001465 elapsed, loss: 2.377663e-06\n",
      "step: 117480 train: 0.07642602920532227 elapsed, loss: 2.3348225e-06\n",
      "step: 117490 train: 0.07807374000549316 elapsed, loss: 2.1425046e-06\n",
      "step: 117500 train: 0.08250904083251953 elapsed, loss: 1.9087433e-06\n",
      "step: 117510 train: 0.08489561080932617 elapsed, loss: 1.6246905e-06\n",
      "step: 117520 train: 0.08179879188537598 elapsed, loss: 2.364159e-06\n",
      "step: 117530 train: 0.07939934730529785 elapsed, loss: 2.4316328e-06\n",
      "step: 117540 train: 0.08779025077819824 elapsed, loss: 1.5981478e-06\n",
      "step: 117550 train: 0.07435011863708496 elapsed, loss: 2.8898887e-06\n",
      "step: 117560 train: 0.08318281173706055 elapsed, loss: 2.4074652e-06\n",
      "step: 117570 train: 0.08520340919494629 elapsed, loss: 2.2873244e-06\n",
      "step: 117580 train: 0.0744333267211914 elapsed, loss: 2.1872079e-06\n",
      "step: 117590 train: 0.08412337303161621 elapsed, loss: 1.5934911e-06\n",
      "step: 117600 train: 0.08053112030029297 elapsed, loss: 1.811886e-06\n",
      "step: 117610 train: 0.08038544654846191 elapsed, loss: 2.3334242e-06\n",
      "step: 117620 train: 0.08538246154785156 elapsed, loss: 2.4922151e-06\n",
      "step: 117630 train: 0.09092283248901367 elapsed, loss: 0.013044925\n",
      "step: 117640 train: 0.08298826217651367 elapsed, loss: 6.558919e-05\n",
      "step: 117650 train: 0.08407330513000488 elapsed, loss: 3.229268e-05\n",
      "step: 117660 train: 0.08433032035827637 elapsed, loss: 2.0428373e-05\n",
      "step: 117670 train: 0.08190655708312988 elapsed, loss: 1.20925415e-05\n",
      "step: 117680 train: 0.08435416221618652 elapsed, loss: 8.645848e-06\n",
      "step: 117690 train: 0.07735037803649902 elapsed, loss: 1.0383856e-05\n",
      "step: 117700 train: 0.08774971961975098 elapsed, loss: 4.323636e-06\n",
      "step: 117710 train: 0.08594822883605957 elapsed, loss: 3.8975727e-06\n",
      "step: 117720 train: 0.08123326301574707 elapsed, loss: 4.6593686e-06\n",
      "step: 117730 train: 0.07780647277832031 elapsed, loss: 4.079172e-06\n",
      "step: 117740 train: 0.08054113388061523 elapsed, loss: 2.523413e-06\n",
      "step: 117750 train: 0.07603597640991211 elapsed, loss: 2.9667221e-06\n",
      "step: 117760 train: 0.07776951789855957 elapsed, loss: 1.936683e-06\n",
      "step: 117770 train: 0.07982182502746582 elapsed, loss: 2.0135167e-06\n",
      "step: 117780 train: 0.08390212059020996 elapsed, loss: 1.914331e-06\n",
      "step: 117790 train: 0.08327126502990723 elapsed, loss: 1.7224793e-06\n",
      "step: 117800 train: 0.08393669128417969 elapsed, loss: 1.657281e-06\n",
      "step: 117810 train: 0.08440804481506348 elapsed, loss: 2.259385e-06\n",
      "step: 117820 train: 0.08130669593811035 elapsed, loss: 1.7429684e-06\n",
      "step: 117830 train: 0.08247232437133789 elapsed, loss: 2.1434357e-06\n",
      "step: 117840 train: 0.08346271514892578 elapsed, loss: 1.8798723e-06\n",
      "step: 117850 train: 0.08171296119689941 elapsed, loss: 1.4919773e-06\n",
      "step: 117860 train: 0.0780181884765625 elapsed, loss: 1.476144e-06\n",
      "step: 117870 train: 0.08460783958435059 elapsed, loss: 1.3397064e-06\n",
      "step: 117880 train: 0.08038830757141113 elapsed, loss: 8.345338e-06\n",
      "step: 117890 train: 0.07502937316894531 elapsed, loss: 0.0020693848\n",
      "step: 117900 train: 0.08527350425720215 elapsed, loss: 3.8366645e-05\n",
      "step: 117910 train: 0.08565425872802734 elapsed, loss: 2.64134e-05\n",
      "step: 117920 train: 0.08163237571716309 elapsed, loss: 2.64609e-05\n",
      "step: 117930 train: 0.07930350303649902 elapsed, loss: 2.1248023e-05\n",
      "step: 117940 train: 0.07897353172302246 elapsed, loss: 1.2751967e-05\n",
      "step: 117950 train: 0.08130216598510742 elapsed, loss: 6.2249183e-06\n",
      "step: 117960 train: 0.08829355239868164 elapsed, loss: 5.799314e-06\n",
      "step: 117970 train: 0.0792536735534668 elapsed, loss: 4.878716e-06\n",
      "step: 117980 train: 0.08484768867492676 elapsed, loss: 4.1713665e-06\n",
      "step: 117990 train: 0.08330225944519043 elapsed, loss: 2.9965242e-06\n",
      "step: 118000 train: 0.08168649673461914 elapsed, loss: 2.675685e-06\n",
      "step: 118010 train: 0.08401298522949219 elapsed, loss: 1.7043184e-06\n",
      "step: 118020 train: 0.08809423446655273 elapsed, loss: 2.155077e-06\n",
      "step: 118030 train: 0.08730053901672363 elapsed, loss: 1.4654343e-06\n",
      "step: 118040 train: 0.09035062789916992 elapsed, loss: 1.7033872e-06\n",
      "step: 118050 train: 0.08533239364624023 elapsed, loss: 1.2996594e-06\n",
      "step: 118060 train: 0.08989191055297852 elapsed, loss: 1.0398207e-06\n",
      "step: 118070 train: 0.07845950126647949 elapsed, loss: 1.7168913e-06\n",
      "step: 118080 train: 0.08274054527282715 elapsed, loss: 1.3625233e-06\n",
      "step: 118090 train: 0.07930779457092285 elapsed, loss: 3.2265557e-06\n",
      "step: 118100 train: 0.07751679420471191 elapsed, loss: 1.5720709e-06\n",
      "step: 118110 train: 0.08260798454284668 elapsed, loss: 1.3369124e-06\n",
      "step: 118120 train: 0.08509373664855957 elapsed, loss: 1.4426174e-06\n",
      "step: 118130 train: 0.08933043479919434 elapsed, loss: 1.4863772e-06\n",
      "step: 118140 train: 0.08450746536254883 elapsed, loss: 1.3089729e-06\n",
      "step: 118150 train: 0.08149218559265137 elapsed, loss: 1.5068786e-06\n",
      "step: 118160 train: 0.08618879318237305 elapsed, loss: 1.3750966e-06\n",
      "step: 118170 train: 0.0898895263671875 elapsed, loss: 1.1096699e-06\n",
      "step: 118180 train: 0.07901334762573242 elapsed, loss: 1.3606611e-06\n",
      "step: 118190 train: 0.08062243461608887 elapsed, loss: 1.6735848e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 118200 train: 0.08374524116516113 elapsed, loss: 1.2060616e-06\n",
      "step: 118210 train: 0.0807645320892334 elapsed, loss: 1.2270166e-06\n",
      "step: 118220 train: 0.08196806907653809 elapsed, loss: 1.7904651e-06\n",
      "step: 118230 train: 0.08294892311096191 elapsed, loss: 0.0010700091\n",
      "step: 118240 train: 0.08498454093933105 elapsed, loss: 0.00016696585\n",
      "step: 118250 train: 0.08290457725524902 elapsed, loss: 6.901528e-05\n",
      "step: 118260 train: 0.07959961891174316 elapsed, loss: 3.5072182e-05\n",
      "step: 118270 train: 0.08499860763549805 elapsed, loss: 3.714067e-05\n",
      "step: 118280 train: 0.07974505424499512 elapsed, loss: 0.00031703984\n",
      "step: 118290 train: 0.0797276496887207 elapsed, loss: 5.4919175e-05\n",
      "step: 118300 train: 0.08429718017578125 elapsed, loss: 2.9424165e-05\n",
      "step: 118310 train: 0.07815885543823242 elapsed, loss: 3.4286262e-05\n",
      "step: 118320 train: 0.08199262619018555 elapsed, loss: 1.3737252e-05\n",
      "step: 118330 train: 0.08193421363830566 elapsed, loss: 1.2172248e-05\n",
      "step: 118340 train: 0.07478785514831543 elapsed, loss: 9.109188e-06\n",
      "step: 118350 train: 0.08847403526306152 elapsed, loss: 4.5471315e-06\n",
      "step: 118360 train: 0.08125424385070801 elapsed, loss: 3.93899e-06\n",
      "step: 118370 train: 0.08617329597473145 elapsed, loss: 3.9222e-06\n",
      "step: 118380 train: 0.08314371109008789 elapsed, loss: 4.297168e-06\n",
      "step: 118390 train: 0.08536148071289062 elapsed, loss: 1.7108366e-06\n",
      "step: 118400 train: 0.08080577850341797 elapsed, loss: 1.7452965e-06\n",
      "step: 118410 train: 0.08168697357177734 elapsed, loss: 1.5874357e-06\n",
      "step: 118420 train: 0.08362197875976562 elapsed, loss: 1.5874372e-06\n",
      "step: 118430 train: 0.07300424575805664 elapsed, loss: 2.1317942e-06\n",
      "step: 118440 train: 0.07779574394226074 elapsed, loss: 2.5415738e-06\n",
      "step: 118450 train: 0.08024168014526367 elapsed, loss: 1.7210823e-06\n",
      "step: 118460 train: 0.07998871803283691 elapsed, loss: 1.5846438e-06\n",
      "step: 118470 train: 0.08382272720336914 elapsed, loss: 1.2624059e-06\n",
      "step: 118480 train: 0.08954238891601562 elapsed, loss: 9.620555e-07\n",
      "step: 118490 train: 0.08525562286376953 elapsed, loss: 1.0319047e-06\n",
      "step: 118500 train: 0.08182263374328613 elapsed, loss: 1.2083881e-06\n",
      "step: 118510 train: 0.0766603946685791 elapsed, loss: 1.4011734e-06\n",
      "step: 118520 train: 0.08374309539794922 elapsed, loss: 1.0952344e-06\n",
      "step: 118530 train: 0.07877230644226074 elapsed, loss: 1.6530958e-06\n",
      "step: 118540 train: 0.07871317863464355 elapsed, loss: 1.9264385e-06\n",
      "step: 118550 train: 0.082763671875 elapsed, loss: 1.062172e-06\n",
      "step: 118560 train: 0.08676671981811523 elapsed, loss: 1.0845243e-06\n",
      "step: 118570 train: 0.08308792114257812 elapsed, loss: 0.00010632061\n",
      "step: 118580 train: 0.07938718795776367 elapsed, loss: 4.5517427e-05\n",
      "step: 118590 train: 0.07645463943481445 elapsed, loss: 4.699481e-05\n",
      "step: 118600 train: 0.07816696166992188 elapsed, loss: 1.8442106e-05\n",
      "step: 118610 train: 0.08086848258972168 elapsed, loss: 1.2641068e-05\n",
      "step: 118620 train: 0.0822746753692627 elapsed, loss: 1.1273802e-05\n",
      "step: 118630 train: 0.08317780494689941 elapsed, loss: 8.126179e-06\n",
      "step: 118640 train: 0.08385992050170898 elapsed, loss: 4.827022e-06\n",
      "step: 118650 train: 0.0777425765991211 elapsed, loss: 7.28752e-06\n",
      "step: 118660 train: 0.08800530433654785 elapsed, loss: 4.26426e-06\n",
      "step: 118670 train: 0.08022737503051758 elapsed, loss: 4.184884e-06\n",
      "step: 118680 train: 0.080841064453125 elapsed, loss: 2.8642726e-06\n",
      "step: 118690 train: 0.07843518257141113 elapsed, loss: 2.0931443e-06\n",
      "step: 118700 train: 0.07762980461120605 elapsed, loss: 2.9136318e-06\n",
      "step: 118710 train: 0.0795590877532959 elapsed, loss: 1.7695068e-06\n",
      "step: 118720 train: 0.0762166976928711 elapsed, loss: 1.4915116e-06\n",
      "step: 118730 train: 0.08350419998168945 elapsed, loss: 1.367646e-06\n",
      "step: 118740 train: 0.07732534408569336 elapsed, loss: 1.2763762e-06\n",
      "step: 118750 train: 0.07819390296936035 elapsed, loss: 1.116655e-06\n",
      "step: 118760 train: 0.07892370223999023 elapsed, loss: 1.0579818e-06\n",
      "step: 118770 train: 0.07946181297302246 elapsed, loss: 1.4435432e-06\n",
      "step: 118780 train: 0.08278989791870117 elapsed, loss: 9.3923825e-07\n",
      "step: 118790 train: 0.0843958854675293 elapsed, loss: 9.4389486e-07\n",
      "step: 118800 train: 0.0878453254699707 elapsed, loss: 1.5785895e-06\n",
      "step: 118810 train: 0.08114814758300781 elapsed, loss: 1.2037335e-06\n",
      "step: 118820 train: 0.08284187316894531 elapsed, loss: 1.5324897e-06\n",
      "step: 118830 train: 0.08465862274169922 elapsed, loss: 9.881322e-07\n",
      "step: 118840 train: 0.08393692970275879 elapsed, loss: 1.3615917e-06\n",
      "step: 118850 train: 0.08323502540588379 elapsed, loss: 1.6852262e-06\n",
      "step: 118860 train: 0.08733534812927246 elapsed, loss: 1.0202632e-06\n",
      "step: 118870 train: 0.08771657943725586 elapsed, loss: 9.126953e-07\n",
      "step: 118880 train: 0.08367490768432617 elapsed, loss: 1.3606611e-06\n",
      "step: 118890 train: 0.08393597602844238 elapsed, loss: 1.4686946e-06\n",
      "step: 118900 train: 0.08237671852111816 elapsed, loss: 1.218169e-06\n",
      "step: 118910 train: 0.08388781547546387 elapsed, loss: 1.261941e-06\n",
      "step: 118920 train: 0.0763249397277832 elapsed, loss: 2.2328425e-06\n",
      "step: 118930 train: 0.07987451553344727 elapsed, loss: 1.6628737e-06\n",
      "step: 118940 train: 0.08000969886779785 elapsed, loss: 1.732258e-06\n",
      "step: 118950 train: 0.08336949348449707 elapsed, loss: 1.4943057e-06\n",
      "step: 118960 train: 0.0862572193145752 elapsed, loss: 2.4990924e-05\n",
      "step: 118970 train: 0.08367276191711426 elapsed, loss: 6.478242e-06\n",
      "step: 118980 train: 0.07777619361877441 elapsed, loss: 7.691981e-06\n",
      "step: 118990 train: 0.08310461044311523 elapsed, loss: 2.4880228e-06\n",
      "step: 119000 train: 0.08637475967407227 elapsed, loss: 2.3231805e-06\n",
      "step: 119010 train: 0.08075428009033203 elapsed, loss: 2.0284178e-06\n",
      "step: 119020 train: 0.08268189430236816 elapsed, loss: 1.4505335e-06\n",
      "step: 119030 train: 0.08977484703063965 elapsed, loss: 1.2223599e-06\n",
      "step: 119040 train: 0.07849335670471191 elapsed, loss: 1.7974505e-06\n",
      "step: 119050 train: 0.08031177520751953 elapsed, loss: 1.7643886e-06\n",
      "step: 119060 train: 0.08205485343933105 elapsed, loss: 1.5906967e-06\n",
      "step: 119070 train: 0.0802164077758789 elapsed, loss: 1.6158428e-06\n",
      "step: 119080 train: 0.08090853691101074 elapsed, loss: 1.2600784e-06\n",
      "step: 119090 train: 0.0797433853149414 elapsed, loss: 1.7317923e-06\n",
      "step: 119100 train: 0.08004641532897949 elapsed, loss: 1.7699763e-06\n",
      "step: 119110 train: 0.08286452293395996 elapsed, loss: 3.5655448e-06\n",
      "step: 119120 train: 0.08570337295532227 elapsed, loss: 1.3145607e-06\n",
      "step: 119130 train: 0.07851243019104004 elapsed, loss: 1.6689282e-06\n",
      "step: 119140 train: 0.08101487159729004 elapsed, loss: 1.7732291e-06\n",
      "step: 119150 train: 0.09120345115661621 elapsed, loss: 1.4314412e-06\n",
      "step: 119160 train: 0.08126378059387207 elapsed, loss: 1.5799873e-06\n",
      "step: 119170 train: 0.08222150802612305 elapsed, loss: 1.4910461e-06\n",
      "step: 119180 train: 0.08262014389038086 elapsed, loss: 1.7993132e-06\n",
      "step: 119190 train: 0.0762929916381836 elapsed, loss: 1.6954709e-06\n",
      "step: 119200 train: 0.08134126663208008 elapsed, loss: 2.3553112e-06\n",
      "step: 119210 train: 0.08396053314208984 elapsed, loss: 1.6111865e-06\n",
      "step: 119220 train: 0.08534908294677734 elapsed, loss: 1.586506e-06\n",
      "step: 119230 train: 0.0804133415222168 elapsed, loss: 3.1948584e-06\n",
      "step: 119240 train: 0.08146309852600098 elapsed, loss: 2.0201209e-05\n",
      "step: 119250 train: 0.08500957489013672 elapsed, loss: 4.1592743e-06\n",
      "step: 119260 train: 0.09081149101257324 elapsed, loss: 2.78884e-06\n",
      "step: 119270 train: 0.08516049385070801 elapsed, loss: 2.8540305e-06\n",
      "step: 119280 train: 0.0740811824798584 elapsed, loss: 3.8742915e-06\n",
      "step: 119290 train: 0.08593440055847168 elapsed, loss: 1.8114202e-06\n",
      "step: 119300 train: 0.07952189445495605 elapsed, loss: 2.6770797e-06\n",
      "step: 119310 train: 0.07373523712158203 elapsed, loss: 2.6128205e-06\n",
      "step: 119320 train: 0.0738368034362793 elapsed, loss: 2.1164278e-06\n",
      "step: 119330 train: 0.07869958877563477 elapsed, loss: 2.0470443e-06\n",
      "step: 119340 train: 0.08241462707519531 elapsed, loss: 1.2502995e-06\n",
      "step: 119350 train: 0.07910466194152832 elapsed, loss: 1.7001277e-06\n",
      "step: 119360 train: 0.07616376876831055 elapsed, loss: 1.7955867e-06\n",
      "step: 119370 train: 0.08234190940856934 elapsed, loss: 1.6316753e-06\n",
      "step: 119380 train: 0.07892274856567383 elapsed, loss: 2.127602e-06\n",
      "step: 119390 train: 0.0788276195526123 elapsed, loss: 1.548788e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 119400 train: 0.07685089111328125 elapsed, loss: 3.187134e-06\n",
      "step: 119410 train: 0.08263421058654785 elapsed, loss: 1.8863917e-06\n",
      "step: 119420 train: 0.080169677734375 elapsed, loss: 1.4970997e-06\n",
      "step: 119430 train: 0.08052897453308105 elapsed, loss: 1.4291131e-06\n",
      "step: 119440 train: 0.08231925964355469 elapsed, loss: 2.6491334e-06\n",
      "step: 119450 train: 0.07610225677490234 elapsed, loss: 1.8235276e-06\n",
      "step: 119460 train: 0.08878302574157715 elapsed, loss: 1.8356342e-06\n",
      "step: 119470 train: 0.07725191116333008 elapsed, loss: 1.6521647e-06\n",
      "step: 119480 train: 0.08357548713684082 elapsed, loss: 1.8114204e-06\n",
      "step: 119490 train: 0.07604074478149414 elapsed, loss: 1.8374974e-06\n",
      "step: 119500 train: 0.0792074203491211 elapsed, loss: 1.7085094e-06\n",
      "step: 119510 train: 0.07961535453796387 elapsed, loss: 2.0293492e-06\n",
      "step: 119520 train: 0.07450437545776367 elapsed, loss: 2.0032724e-06\n",
      "step: 119530 train: 0.07919526100158691 elapsed, loss: 3.0854644e-06\n",
      "step: 119540 train: 0.07949972152709961 elapsed, loss: 1.9818522e-06\n",
      "step: 119550 train: 0.07733726501464844 elapsed, loss: 1.7276016e-06\n",
      "step: 119560 train: 0.08737874031066895 elapsed, loss: 3.3634403e-06\n",
      "step: 119570 train: 0.0843651294708252 elapsed, loss: 1.8812677e-06\n",
      "step: 119580 train: 0.08452367782592773 elapsed, loss: 1.7685796e-06\n",
      "step: 119590 train: 0.08144235610961914 elapsed, loss: 1.4547247e-06\n",
      "step: 119600 train: 0.07945895195007324 elapsed, loss: 2.1122369e-06\n",
      "step: 119610 train: 0.08134245872497559 elapsed, loss: 2.448909e-06\n",
      "step: 119620 train: 0.08013725280761719 elapsed, loss: 1.4915115e-06\n",
      "step: 119630 train: 0.0844874382019043 elapsed, loss: 1.7415708e-06\n",
      "step: 119640 train: 0.07975006103515625 elapsed, loss: 2.7543801e-06\n",
      "step: 119650 train: 0.08196187019348145 elapsed, loss: 1.5608948e-06\n",
      "step: 119660 train: 0.08581805229187012 elapsed, loss: 1.4193341e-06\n",
      "step: 119670 train: 0.08257079124450684 elapsed, loss: 1.7345865e-06\n",
      "step: 119680 train: 0.07622504234313965 elapsed, loss: 2.4563597e-06\n",
      "step: 119690 train: 0.0820004940032959 elapsed, loss: 1.907812e-06\n",
      "step: 119700 train: 0.08452916145324707 elapsed, loss: 1.8449477e-06\n",
      "step: 119710 train: 0.08451700210571289 elapsed, loss: 1.8016416e-06\n",
      "step: 119720 train: 0.07654190063476562 elapsed, loss: 2.114565e-06\n",
      "step: 119730 train: 0.08370232582092285 elapsed, loss: 1.9078107e-06\n",
      "step: 119740 train: 0.08103752136230469 elapsed, loss: 2.1606654e-06\n",
      "step: 119750 train: 0.08039498329162598 elapsed, loss: 7.0467663e-06\n",
      "step: 119760 train: 0.07943487167358398 elapsed, loss: 7.876144e-06\n",
      "step: 119770 train: 0.0782327651977539 elapsed, loss: 3.80118e-06\n",
      "step: 119780 train: 0.07674694061279297 elapsed, loss: 2.5080474e-06\n",
      "step: 119790 train: 0.0825960636138916 elapsed, loss: 2.3255088e-06\n",
      "step: 119800 train: 0.07900738716125488 elapsed, loss: 2.8535608e-06\n",
      "step: 119810 train: 0.08701229095458984 elapsed, loss: 1.9236443e-06\n",
      "step: 119820 train: 0.08409547805786133 elapsed, loss: 1.4929085e-06\n",
      "step: 119830 train: 0.08374571800231934 elapsed, loss: 2.3054854e-06\n",
      "step: 119840 train: 0.09204435348510742 elapsed, loss: 1.3844096e-06\n",
      "step: 119850 train: 0.08703088760375977 elapsed, loss: 0.0005357814\n",
      "step: 119860 train: 0.07651925086975098 elapsed, loss: 8.198608e-05\n",
      "step: 119870 train: 0.08467793464660645 elapsed, loss: 2.3014627e-05\n",
      "step: 119880 train: 0.08676576614379883 elapsed, loss: 1.4224679e-05\n",
      "step: 119890 train: 0.08053970336914062 elapsed, loss: 1.3427567e-05\n",
      "step: 119900 train: 0.07826471328735352 elapsed, loss: 0.0050046816\n",
      "step: 119910 train: 0.08160877227783203 elapsed, loss: 0.00018462248\n",
      "step: 119920 train: 0.08168148994445801 elapsed, loss: 3.545347e-05\n",
      "step: 119930 train: 0.08671283721923828 elapsed, loss: 1.5069154e-05\n",
      "step: 119940 train: 0.08230137825012207 elapsed, loss: 1.5358197e-05\n",
      "step: 119950 train: 0.08153700828552246 elapsed, loss: 1.1425802e-05\n",
      "step: 119960 train: 0.08527016639709473 elapsed, loss: 1.19093165e-05\n",
      "step: 119970 train: 0.07701373100280762 elapsed, loss: 6.0046737e-06\n",
      "step: 119980 train: 0.07797837257385254 elapsed, loss: 3.954377e-06\n",
      "step: 119990 train: 0.07711982727050781 elapsed, loss: 4.4149197e-06\n",
      "step: 120000 train: 0.08283472061157227 elapsed, loss: 5.95768e-06\n",
      "step: 120010 train: 0.08775067329406738 elapsed, loss: 1.9720708e-06\n",
      "step: 120020 train: 0.07856488227844238 elapsed, loss: 2.638431e-06\n",
      "step: 120030 train: 0.07810258865356445 elapsed, loss: 2.3823181e-06\n",
      "step: 120040 train: 0.08257865905761719 elapsed, loss: 1.4649684e-06\n",
      "step: 120050 train: 0.07833170890808105 elapsed, loss: 2.2244599e-06\n",
      "step: 120060 train: 0.07873272895812988 elapsed, loss: 1.7946561e-06\n",
      "step: 120070 train: 0.07774734497070312 elapsed, loss: 1.2964e-06\n",
      "step: 120080 train: 0.07903409004211426 elapsed, loss: 1.1692745e-06\n",
      "step: 120090 train: 0.07438850402832031 elapsed, loss: 1.4002421e-06\n",
      "step: 120100 train: 0.08382177352905273 elapsed, loss: 1.3355151e-06\n",
      "step: 120110 train: 0.08734989166259766 elapsed, loss: 8.712517e-07\n",
      "step: 120120 train: 0.07916569709777832 elapsed, loss: 9.1362676e-07\n",
      "step: 120130 train: 0.0782327651977539 elapsed, loss: 1.2698572e-06\n",
      "step: 120140 train: 0.07309222221374512 elapsed, loss: 1.382547e-06\n",
      "step: 120150 train: 0.07979822158813477 elapsed, loss: 1.4398231e-06\n",
      "step: 120160 train: 0.08725905418395996 elapsed, loss: 1.012347e-06\n",
      "step: 120170 train: 0.08693408966064453 elapsed, loss: 9.913922e-07\n",
      "step: 120180 train: 0.08040809631347656 elapsed, loss: 1.040752e-06\n",
      "step: 120190 train: 0.08192896842956543 elapsed, loss: 1.1334187e-06\n",
      "step: 120200 train: 0.07647585868835449 elapsed, loss: 1.3476226e-06\n",
      "step: 120210 train: 0.0804135799407959 elapsed, loss: 1.0509967e-06\n",
      "step: 120220 train: 0.07818055152893066 elapsed, loss: 1.3499509e-06\n",
      "step: 120230 train: 0.08434700965881348 elapsed, loss: 1.1641523e-06\n",
      "step: 120240 train: 0.08315253257751465 elapsed, loss: 1.3136275e-06\n",
      "step: 120250 train: 0.08030462265014648 elapsed, loss: 1.4621751e-06\n",
      "step: 120260 train: 0.08043050765991211 elapsed, loss: 1.4230595e-06\n",
      "step: 120270 train: 0.08420848846435547 elapsed, loss: 1.28243e-06\n",
      "step: 120280 train: 0.08221316337585449 elapsed, loss: 1.8067636e-06\n",
      "step: 120290 train: 0.0784597396850586 elapsed, loss: 1.591628e-06\n",
      "step: 120300 train: 0.08504891395568848 elapsed, loss: 1.077073e-06\n",
      "step: 120310 train: 0.08582758903503418 elapsed, loss: 1.2610096e-06\n",
      "step: 120320 train: 0.08396196365356445 elapsed, loss: 1.1324869e-06\n",
      "step: 120330 train: 0.08167839050292969 elapsed, loss: 0.00024248721\n",
      "step: 120340 train: 0.08295512199401855 elapsed, loss: 4.0166866e-05\n",
      "step: 120350 train: 0.0826728343963623 elapsed, loss: 1.4437561e-05\n",
      "step: 120360 train: 0.08211803436279297 elapsed, loss: 8.352019e-06\n",
      "step: 120370 train: 0.0845785140991211 elapsed, loss: 4.8661304e-06\n",
      "step: 120380 train: 0.07897043228149414 elapsed, loss: 7.482622e-06\n",
      "step: 120390 train: 0.08756041526794434 elapsed, loss: 3.137617e-06\n",
      "step: 120400 train: 0.08331179618835449 elapsed, loss: 3.0966394e-06\n",
      "step: 120410 train: 0.07975935935974121 elapsed, loss: 2.71899e-06\n",
      "step: 120420 train: 0.09320354461669922 elapsed, loss: 1.9916297e-06\n",
      "step: 120430 train: 0.0886235237121582 elapsed, loss: 1.7033868e-06\n",
      "step: 120440 train: 0.07530641555786133 elapsed, loss: 2.2905842e-06\n",
      "step: 120450 train: 0.08178853988647461 elapsed, loss: 2.6219118e-06\n",
      "step: 120460 train: 0.08495426177978516 elapsed, loss: 1.7764946e-06\n",
      "step: 120470 train: 0.07678508758544922 elapsed, loss: 2.1322599e-06\n",
      "step: 120480 train: 0.08301782608032227 elapsed, loss: 1.3387748e-06\n",
      "step: 120490 train: 0.08519840240478516 elapsed, loss: 1.3210797e-06\n",
      "step: 120500 train: 0.08699893951416016 elapsed, loss: 1.1580987e-06\n",
      "step: 120510 train: 0.0822441577911377 elapsed, loss: 1.3415689e-06\n",
      "step: 120520 train: 0.08159995079040527 elapsed, loss: 1.5250394e-06\n",
      "step: 120530 train: 0.0815126895904541 elapsed, loss: 1.1692746e-06\n",
      "step: 120540 train: 0.08947134017944336 elapsed, loss: 1.0118813e-06\n",
      "step: 120550 train: 0.07704639434814453 elapsed, loss: 1.9310946e-06\n",
      "step: 120560 train: 0.08612823486328125 elapsed, loss: 1.3401717e-06\n",
      "step: 120570 train: 0.08399581909179688 elapsed, loss: 1.2889492e-06\n",
      "step: 120580 train: 0.07898664474487305 elapsed, loss: 1.4579823e-06\n",
      "step: 120590 train: 0.08770990371704102 elapsed, loss: 7.415884e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 120600 train: 0.08493852615356445 elapsed, loss: 6.036437e-05\n",
      "step: 120610 train: 0.08067822456359863 elapsed, loss: 2.9103161e-05\n",
      "step: 120620 train: 0.07648515701293945 elapsed, loss: 2.0069556e-05\n",
      "step: 120630 train: 0.07776808738708496 elapsed, loss: 1.8131082e-05\n",
      "step: 120640 train: 0.08230733871459961 elapsed, loss: 1.2530042e-05\n",
      "step: 120650 train: 0.07959389686584473 elapsed, loss: 7.696817e-06\n",
      "step: 120660 train: 0.0856027603149414 elapsed, loss: 6.0682223e-06\n",
      "step: 120670 train: 0.08060002326965332 elapsed, loss: 7.573932e-06\n",
      "step: 120680 train: 0.08405566215515137 elapsed, loss: 4.1327257e-06\n",
      "step: 120690 train: 0.07373690605163574 elapsed, loss: 4.1732424e-06\n",
      "step: 120700 train: 0.0817265510559082 elapsed, loss: 2.5937231e-06\n",
      "step: 120710 train: 0.077392578125 elapsed, loss: 2.873588e-06\n",
      "step: 120720 train: 0.07706117630004883 elapsed, loss: 2.1727724e-06\n",
      "step: 120730 train: 0.07946443557739258 elapsed, loss: 1.4333041e-06\n",
      "step: 120740 train: 0.07869839668273926 elapsed, loss: 1.598148e-06\n",
      "step: 120750 train: 0.08777928352355957 elapsed, loss: 1.2759101e-06\n",
      "step: 120760 train: 0.08197999000549316 elapsed, loss: 1.1580987e-06\n",
      "step: 120770 train: 0.07407498359680176 elapsed, loss: 2.1522815e-06\n",
      "step: 120780 train: 0.0819859504699707 elapsed, loss: 1.3890664e-06\n",
      "step: 120790 train: 0.0791175365447998 elapsed, loss: 1.7434336e-06\n",
      "step: 120800 train: 0.07662487030029297 elapsed, loss: 2.0777775e-06\n",
      "step: 120810 train: 0.08296370506286621 elapsed, loss: 1.2768414e-06\n",
      "step: 120820 train: 0.08097720146179199 elapsed, loss: 1.411418e-06\n",
      "step: 120830 train: 0.07479524612426758 elapsed, loss: 1.8640399e-06\n",
      "step: 120840 train: 0.0813758373260498 elapsed, loss: 1.266132e-06\n",
      "step: 120850 train: 0.08434724807739258 elapsed, loss: 1.3643864e-06\n",
      "step: 120860 train: 0.07562923431396484 elapsed, loss: 1.5227108e-06\n",
      "step: 120870 train: 0.07439255714416504 elapsed, loss: 2.0293485e-06\n",
      "step: 120880 train: 0.08662772178649902 elapsed, loss: 1.2968653e-06\n",
      "step: 120890 train: 0.08431148529052734 elapsed, loss: 1.6572856e-06\n",
      "step: 120900 train: 0.08514118194580078 elapsed, loss: 1.1231743e-06\n",
      "step: 120910 train: 0.08471298217773438 elapsed, loss: 1.5497192e-06\n",
      "step: 120920 train: 0.07927918434143066 elapsed, loss: 1.4482054e-06\n",
      "step: 120930 train: 0.08159947395324707 elapsed, loss: 1.3462243e-06\n",
      "step: 120940 train: 0.08064985275268555 elapsed, loss: 1.5073441e-06\n",
      "step: 120950 train: 0.08417916297912598 elapsed, loss: 1.2856898e-06\n",
      "step: 120960 train: 0.08148550987243652 elapsed, loss: 1.6549586e-06\n",
      "step: 120970 train: 0.08451509475708008 elapsed, loss: 1.3895319e-06\n",
      "step: 120980 train: 0.08244514465332031 elapsed, loss: 1.3443631e-06\n",
      "step: 120990 train: 0.08288788795471191 elapsed, loss: 3.6508213e-06\n",
      "step: 121000 train: 0.08086204528808594 elapsed, loss: 2.2258573e-06\n",
      "step: 121010 train: 0.08290934562683105 elapsed, loss: 1.6395913e-06\n",
      "step: 121020 train: 0.08461546897888184 elapsed, loss: 1.4300432e-06\n",
      "step: 121030 train: 0.07892417907714844 elapsed, loss: 1.8989645e-06\n",
      "step: 121040 train: 0.0827188491821289 elapsed, loss: 1.2353985e-06\n",
      "step: 121050 train: 0.08411478996276855 elapsed, loss: 1.4659003e-06\n",
      "step: 121060 train: 0.08134698867797852 elapsed, loss: 1.5124665e-06\n",
      "step: 121070 train: 0.07827973365783691 elapsed, loss: 1.7653199e-06\n",
      "step: 121080 train: 0.08657240867614746 elapsed, loss: 1.6023388e-06\n",
      "step: 121090 train: 0.07759571075439453 elapsed, loss: 2.3166615e-06\n",
      "step: 121100 train: 0.08489394187927246 elapsed, loss: 1.534818e-06\n",
      "step: 121110 train: 0.08161163330078125 elapsed, loss: 1.8682308e-06\n",
      "step: 121120 train: 0.08604073524475098 elapsed, loss: 2.0209668e-06\n",
      "step: 121130 train: 0.08781981468200684 elapsed, loss: 1.5264362e-06\n",
      "step: 121140 train: 0.08372068405151367 elapsed, loss: 1.4896491e-06\n",
      "step: 121150 train: 0.08159470558166504 elapsed, loss: 1.3307199e-05\n",
      "step: 121160 train: 0.08213329315185547 elapsed, loss: 1.9231786e-06\n",
      "step: 121170 train: 0.07691645622253418 elapsed, loss: 2.1024575e-06\n",
      "step: 121180 train: 0.07757973670959473 elapsed, loss: 1.6847607e-06\n",
      "step: 121190 train: 0.08361601829528809 elapsed, loss: 1.9823178e-06\n",
      "step: 121200 train: 0.07879781723022461 elapsed, loss: 1.7941909e-06\n",
      "step: 121210 train: 0.08106470108032227 elapsed, loss: 2.1634594e-06\n",
      "step: 121220 train: 0.08591413497924805 elapsed, loss: 2.579756e-06\n",
      "step: 121230 train: 0.07893776893615723 elapsed, loss: 2.1448323e-06\n",
      "step: 121240 train: 0.08682560920715332 elapsed, loss: 1.6232937e-06\n",
      "step: 121250 train: 0.08237981796264648 elapsed, loss: 1.5920943e-06\n",
      "step: 121260 train: 0.07899761199951172 elapsed, loss: 3.8188764e-06\n",
      "step: 121270 train: 0.07883977890014648 elapsed, loss: 0.010722506\n",
      "step: 121280 train: 0.0755763053894043 elapsed, loss: 0.00015283705\n",
      "step: 121290 train: 0.07757186889648438 elapsed, loss: 3.9658345e-05\n",
      "step: 121300 train: 0.07924723625183105 elapsed, loss: 2.8001874e-05\n",
      "step: 121310 train: 0.08596920967102051 elapsed, loss: 2.1046722e-05\n",
      "step: 121320 train: 0.09319400787353516 elapsed, loss: 1.6595877e-05\n",
      "step: 121330 train: 0.08210968971252441 elapsed, loss: 1.1974799e-05\n",
      "step: 121340 train: 0.08154058456420898 elapsed, loss: 1.1640842e-05\n",
      "step: 121350 train: 0.08134126663208008 elapsed, loss: 6.557863e-06\n",
      "step: 121360 train: 0.08418679237365723 elapsed, loss: 3.8328467e-06\n",
      "step: 121370 train: 0.08283519744873047 elapsed, loss: 4.087993e-06\n",
      "step: 121380 train: 0.08232545852661133 elapsed, loss: 3.89478e-06\n",
      "step: 121390 train: 0.07665491104125977 elapsed, loss: 2.8344743e-06\n",
      "step: 121400 train: 0.08445501327514648 elapsed, loss: 2.138779e-06\n",
      "step: 121410 train: 0.07735753059387207 elapsed, loss: 1.9343543e-06\n",
      "step: 121420 train: 0.07442593574523926 elapsed, loss: 2.6249281e-06\n",
      "step: 121430 train: 0.08505606651306152 elapsed, loss: 1.2661319e-06\n",
      "step: 121440 train: 0.07527446746826172 elapsed, loss: 2.1466954e-06\n",
      "step: 121450 train: 0.0780951976776123 elapsed, loss: 1.7997788e-06\n",
      "step: 121460 train: 0.08284282684326172 elapsed, loss: 1.3420347e-06\n",
      "step: 121470 train: 0.07909154891967773 elapsed, loss: 1.8258557e-06\n",
      "step: 121480 train: 0.08097577095031738 elapsed, loss: 2.2369102e-06\n",
      "step: 121490 train: 0.07610487937927246 elapsed, loss: 1.8319095e-06\n",
      "step: 121500 train: 0.08001542091369629 elapsed, loss: 1.3075756e-06\n",
      "step: 121510 train: 0.08023500442504883 elapsed, loss: 1.4645034e-06\n",
      "step: 121520 train: 0.07797384262084961 elapsed, loss: 1.8775443e-06\n",
      "step: 121530 train: 0.0821981430053711 elapsed, loss: 1.4454115e-06\n",
      "step: 121540 train: 0.08374881744384766 elapsed, loss: 1.6065297e-06\n",
      "step: 121550 train: 0.08394908905029297 elapsed, loss: 1.5301616e-06\n",
      "step: 121560 train: 0.08314800262451172 elapsed, loss: 1.6880203e-06\n",
      "step: 121570 train: 0.0798490047454834 elapsed, loss: 1.859849e-06\n",
      "step: 121580 train: 0.08791995048522949 elapsed, loss: 1.4333041e-06\n",
      "step: 121590 train: 0.07804584503173828 elapsed, loss: 2.0344717e-06\n",
      "step: 121600 train: 0.0830070972442627 elapsed, loss: 1.8188709e-06\n",
      "step: 121610 train: 0.07320880889892578 elapsed, loss: 2.4624128e-06\n",
      "step: 121620 train: 0.08669447898864746 elapsed, loss: 1.4933742e-06\n",
      "step: 121630 train: 0.08450007438659668 elapsed, loss: 2.0195705e-06\n",
      "step: 121640 train: 0.09324193000793457 elapsed, loss: 1.3662491e-06\n",
      "step: 121650 train: 0.08207011222839355 elapsed, loss: 1.6801034e-06\n",
      "step: 121660 train: 0.07625961303710938 elapsed, loss: 3.16183e-06\n",
      "step: 121670 train: 0.08404827117919922 elapsed, loss: 2.089885e-06\n",
      "step: 121680 train: 0.08070039749145508 elapsed, loss: 1.4095554e-06\n",
      "step: 121690 train: 0.07764625549316406 elapsed, loss: 3.9320325e-06\n",
      "step: 121700 train: 0.07764601707458496 elapsed, loss: 1.8007102e-06\n",
      "step: 121710 train: 0.08073592185974121 elapsed, loss: 2.3781279e-06\n",
      "step: 121720 train: 0.07998466491699219 elapsed, loss: 0.057923853\n",
      "step: 121730 train: 0.07526063919067383 elapsed, loss: 5.751595e-05\n",
      "step: 121740 train: 0.07393741607666016 elapsed, loss: 1.95977e-05\n",
      "step: 121750 train: 0.08174300193786621 elapsed, loss: 1.3724246e-05\n",
      "step: 121760 train: 0.08304047584533691 elapsed, loss: 1.2211518e-05\n",
      "step: 121770 train: 0.07946276664733887 elapsed, loss: 1.2681644e-05\n",
      "step: 121780 train: 0.08725929260253906 elapsed, loss: 5.1501765e-06\n",
      "step: 121790 train: 0.0825347900390625 elapsed, loss: 5.504558e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 121800 train: 0.07742810249328613 elapsed, loss: 4.6780115e-06\n",
      "step: 121810 train: 0.08110284805297852 elapsed, loss: 3.304325e-06\n",
      "step: 121820 train: 0.07754063606262207 elapsed, loss: 4.0270215e-06\n",
      "step: 121830 train: 0.08102798461914062 elapsed, loss: 2.7776643e-06\n",
      "step: 121840 train: 0.08441042900085449 elapsed, loss: 1.7574037e-06\n",
      "step: 121850 train: 0.08324813842773438 elapsed, loss: 2.4498333e-06\n",
      "step: 121860 train: 0.09175419807434082 elapsed, loss: 1.7974502e-06\n",
      "step: 121870 train: 0.08227014541625977 elapsed, loss: 1.8966356e-06\n",
      "step: 121880 train: 0.08403491973876953 elapsed, loss: 1.4523962e-06\n",
      "step: 121890 train: 0.07562994956970215 elapsed, loss: 1.8430844e-06\n",
      "step: 121900 train: 0.07871317863464355 elapsed, loss: 1.8780097e-06\n",
      "step: 121910 train: 0.08372092247009277 elapsed, loss: 1.9845597e-06\n",
      "step: 121920 train: 0.0781254768371582 elapsed, loss: 1.5604293e-06\n",
      "step: 121930 train: 0.08115506172180176 elapsed, loss: 1.5697426e-06\n",
      "step: 121940 train: 0.07455301284790039 elapsed, loss: 1.9688132e-06\n",
      "step: 121950 train: 0.08185052871704102 elapsed, loss: 1.6624083e-06\n",
      "step: 121960 train: 0.08135271072387695 elapsed, loss: 1.3844098e-06\n",
      "step: 121970 train: 0.08137631416320801 elapsed, loss: 1.3713707e-06\n",
      "step: 121980 train: 0.08347153663635254 elapsed, loss: 1.2475055e-06\n",
      "step: 121990 train: 0.07617306709289551 elapsed, loss: 2.0801058e-06\n",
      "step: 122000 train: 0.08320975303649902 elapsed, loss: 1.7345865e-06\n",
      "step: 122010 train: 0.08116960525512695 elapsed, loss: 1.3033848e-06\n",
      "step: 122020 train: 0.08036422729492188 elapsed, loss: 0.27068186\n",
      "step: 122030 train: 0.08267974853515625 elapsed, loss: 5.567602e-05\n",
      "step: 122040 train: 0.08760452270507812 elapsed, loss: 2.2930657e-05\n",
      "step: 122050 train: 0.0821695327758789 elapsed, loss: 2.365119e-05\n",
      "step: 122060 train: 0.08490872383117676 elapsed, loss: 1.7989922e-05\n",
      "step: 122070 train: 0.08528733253479004 elapsed, loss: 1.1038844e-05\n",
      "step: 122080 train: 0.08292984962463379 elapsed, loss: 9.052374e-06\n",
      "step: 122090 train: 0.08271241188049316 elapsed, loss: 5.4747525e-06\n",
      "step: 122100 train: 0.07966470718383789 elapsed, loss: 8.398118e-06\n",
      "step: 122110 train: 0.08566546440124512 elapsed, loss: 3.2498383e-06\n",
      "step: 122120 train: 0.08301949501037598 elapsed, loss: 2.7376168e-06\n",
      "step: 122130 train: 0.08298850059509277 elapsed, loss: 2.2021088e-06\n",
      "step: 122140 train: 0.08040452003479004 elapsed, loss: 1.9068805e-06\n",
      "step: 122150 train: 0.0824277400970459 elapsed, loss: 1.5450626e-06\n",
      "step: 122160 train: 0.08490777015686035 elapsed, loss: 1.6335371e-06\n",
      "step: 122170 train: 0.08665657043457031 elapsed, loss: 1.8822005e-06\n",
      "step: 122180 train: 0.08502483367919922 elapsed, loss: 1.6842951e-06\n",
      "step: 122190 train: 0.08417510986328125 elapsed, loss: 0.0009105645\n",
      "step: 122200 train: 0.07932162284851074 elapsed, loss: 0.00041026165\n",
      "step: 122210 train: 0.08080220222473145 elapsed, loss: 7.2945746e-05\n",
      "step: 122220 train: 0.08519840240478516 elapsed, loss: 0.00014084022\n",
      "step: 122230 train: 0.08388161659240723 elapsed, loss: 3.5687088e-05\n",
      "step: 122240 train: 0.07985877990722656 elapsed, loss: 1.7148886e-05\n",
      "step: 122250 train: 0.0796201229095459 elapsed, loss: 1.4984085e-05\n",
      "step: 122260 train: 0.08141946792602539 elapsed, loss: 9.959942e-06\n",
      "step: 122270 train: 0.08453249931335449 elapsed, loss: 7.988771e-06\n",
      "step: 122280 train: 0.08246040344238281 elapsed, loss: 6.1806777e-06\n",
      "step: 122290 train: 0.07933473587036133 elapsed, loss: 6.365077e-06\n",
      "step: 122300 train: 0.08275055885314941 elapsed, loss: 3.6293504e-06\n",
      "step: 122310 train: 0.07976722717285156 elapsed, loss: 3.3192e-06\n",
      "step: 122320 train: 0.08196592330932617 elapsed, loss: 2.002806e-06\n",
      "step: 122330 train: 0.08412528038024902 elapsed, loss: 2.0284176e-06\n",
      "step: 122340 train: 0.08160567283630371 elapsed, loss: 1.6712559e-06\n",
      "step: 122350 train: 0.08112549781799316 elapsed, loss: 1.6302773e-06\n",
      "step: 122360 train: 0.0801701545715332 elapsed, loss: 1.5762616e-06\n",
      "step: 122370 train: 0.08491110801696777 elapsed, loss: 1.3215454e-06\n",
      "step: 122380 train: 0.07826042175292969 elapsed, loss: 1.8202676e-06\n",
      "step: 122390 train: 0.08301568031311035 elapsed, loss: 1.2172372e-06\n",
      "step: 122400 train: 0.07987546920776367 elapsed, loss: 1.1702057e-06\n",
      "step: 122410 train: 0.08377456665039062 elapsed, loss: 1.2116491e-06\n",
      "step: 122420 train: 0.08006596565246582 elapsed, loss: 1.3243396e-06\n",
      "step: 122430 train: 0.07939982414245605 elapsed, loss: 1.1655493e-06\n",
      "step: 122440 train: 0.08196091651916504 elapsed, loss: 1.1068761e-06\n",
      "step: 122450 train: 0.07679939270019531 elapsed, loss: 1.3438972e-06\n",
      "step: 122460 train: 0.0809469223022461 elapsed, loss: 1.7063223e-05\n",
      "step: 122470 train: 0.08197999000549316 elapsed, loss: 2.2402921e-06\n",
      "step: 122480 train: 0.08228063583374023 elapsed, loss: 1.8789399e-06\n",
      "step: 122490 train: 0.08370041847229004 elapsed, loss: 1.2540247e-06\n",
      "step: 122500 train: 0.07938742637634277 elapsed, loss: 1.2693915e-06\n",
      "step: 122510 train: 0.08348774909973145 elapsed, loss: 1.1874354e-06\n",
      "step: 122520 train: 0.09028482437133789 elapsed, loss: 1.0142091e-06\n",
      "step: 122530 train: 0.08464646339416504 elapsed, loss: 1.130159e-06\n",
      "step: 122540 train: 0.08232855796813965 elapsed, loss: 1.3713712e-06\n",
      "step: 122550 train: 0.0798792839050293 elapsed, loss: 1.3033848e-06\n",
      "step: 122560 train: 0.0811314582824707 elapsed, loss: 1.0840586e-06\n",
      "step: 122570 train: 0.08123970031738281 elapsed, loss: 1.3131635e-06\n",
      "step: 122580 train: 0.07708072662353516 elapsed, loss: 1.4011734e-06\n",
      "step: 122590 train: 0.08529329299926758 elapsed, loss: 1.0989597e-06\n",
      "step: 122600 train: 0.07915687561035156 elapsed, loss: 1.4244565e-06\n",
      "step: 122610 train: 0.07633805274963379 elapsed, loss: 1.7490219e-06\n",
      "step: 122620 train: 0.08131647109985352 elapsed, loss: 1.2456428e-06\n",
      "step: 122630 train: 0.08161783218383789 elapsed, loss: 1.252628e-06\n",
      "step: 122640 train: 0.0841379165649414 elapsed, loss: 1.5501848e-06\n",
      "step: 122650 train: 0.07982707023620605 elapsed, loss: 1.3303932e-06\n",
      "step: 122660 train: 0.08084821701049805 elapsed, loss: 1.5147945e-06\n",
      "step: 122670 train: 0.08314371109008789 elapsed, loss: 0.0018941641\n",
      "step: 122680 train: 0.07578778266906738 elapsed, loss: 4.2580454e-05\n",
      "step: 122690 train: 0.07858943939208984 elapsed, loss: 6.500952e-05\n",
      "step: 122700 train: 0.07903504371643066 elapsed, loss: 2.9875864e-05\n",
      "step: 122710 train: 0.08067631721496582 elapsed, loss: 1.2659543e-05\n",
      "step: 122720 train: 0.08593153953552246 elapsed, loss: 5.4910497e-06\n",
      "step: 122730 train: 0.08188605308532715 elapsed, loss: 6.5080285e-06\n",
      "step: 122740 train: 0.0790562629699707 elapsed, loss: 4.8163165e-06\n",
      "step: 122750 train: 0.08336734771728516 elapsed, loss: 3.647034e-06\n",
      "step: 122760 train: 0.07847309112548828 elapsed, loss: 4.5262036e-06\n",
      "step: 122770 train: 0.08377480506896973 elapsed, loss: 2.6789444e-06\n",
      "step: 122780 train: 0.08578181266784668 elapsed, loss: 1.8645036e-06\n",
      "step: 122790 train: 0.08698701858520508 elapsed, loss: 1.7648539e-06\n",
      "step: 122800 train: 0.07651925086975098 elapsed, loss: 2.0964037e-06\n",
      "step: 122810 train: 0.08358454704284668 elapsed, loss: 1.4188681e-06\n",
      "step: 122820 train: 0.08274292945861816 elapsed, loss: 1.391394e-06\n",
      "step: 122830 train: 0.0804452896118164 elapsed, loss: 1.3932573e-06\n",
      "step: 122840 train: 0.0811913013458252 elapsed, loss: 1.3979138e-06\n",
      "step: 122850 train: 0.07611870765686035 elapsed, loss: 1.663806e-06\n",
      "step: 122860 train: 0.08385086059570312 elapsed, loss: 1.1678775e-06\n",
      "step: 122870 train: 0.0808262825012207 elapsed, loss: 1.6791727e-06\n",
      "step: 122880 train: 0.08382821083068848 elapsed, loss: 1.2349328e-06\n",
      "step: 122890 train: 0.08048009872436523 elapsed, loss: 1.4468083e-06\n",
      "step: 122900 train: 0.08062076568603516 elapsed, loss: 1.7406385e-06\n",
      "step: 122910 train: 0.08701515197753906 elapsed, loss: 1.3983796e-06\n",
      "step: 122920 train: 0.0793452262878418 elapsed, loss: 1.5189855e-06\n",
      "step: 122930 train: 0.0763707160949707 elapsed, loss: 2.3907007e-06\n",
      "step: 122940 train: 0.08386898040771484 elapsed, loss: 1.4961684e-06\n",
      "step: 122950 train: 0.08136868476867676 elapsed, loss: 1.4970997e-06\n",
      "step: 122960 train: 0.07933211326599121 elapsed, loss: 1.3667145e-06\n",
      "step: 122970 train: 0.08382463455200195 elapsed, loss: 2.6775465e-06\n",
      "step: 122980 train: 0.08531332015991211 elapsed, loss: 1.1194489e-06\n",
      "step: 122990 train: 0.07683181762695312 elapsed, loss: 1.7997788e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 123000 train: 0.08426976203918457 elapsed, loss: 1.5376121e-06\n",
      "step: 123010 train: 0.07892298698425293 elapsed, loss: 1.5567039e-06\n",
      "step: 123020 train: 0.08106112480163574 elapsed, loss: 1.3671802e-06\n",
      "step: 123030 train: 0.0816037654876709 elapsed, loss: 1.420265e-06\n",
      "step: 123040 train: 0.08086585998535156 elapsed, loss: 1.7909313e-06\n",
      "step: 123050 train: 0.07906198501586914 elapsed, loss: 1.7113032e-06\n",
      "step: 123060 train: 0.08109617233276367 elapsed, loss: 2.0405246e-06\n",
      "step: 123070 train: 0.08064842224121094 elapsed, loss: 3.172542e-06\n",
      "step: 123080 train: 0.08114385604858398 elapsed, loss: 1.6405227e-06\n",
      "step: 123090 train: 0.08106040954589844 elapsed, loss: 1.6721879e-06\n",
      "step: 123100 train: 0.08332252502441406 elapsed, loss: 0.0097769\n",
      "step: 123110 train: 0.08591675758361816 elapsed, loss: 6.881054e-05\n",
      "step: 123120 train: 0.08068251609802246 elapsed, loss: 2.3938672e-05\n",
      "step: 123130 train: 0.07598686218261719 elapsed, loss: 3.400819e-05\n",
      "step: 123140 train: 0.08220863342285156 elapsed, loss: 1.2443148e-05\n",
      "step: 123150 train: 0.07854247093200684 elapsed, loss: 1.44771075e-05\n",
      "step: 123160 train: 0.08031606674194336 elapsed, loss: 1.9483126e-05\n",
      "step: 123170 train: 0.08080816268920898 elapsed, loss: 6.644472e-06\n",
      "step: 123180 train: 0.08386087417602539 elapsed, loss: 7.5743965e-06\n",
      "step: 123190 train: 0.08023524284362793 elapsed, loss: 3.660554e-06\n",
      "step: 123200 train: 0.07856059074401855 elapsed, loss: 3.8938406e-06\n",
      "step: 123210 train: 0.08125877380371094 elapsed, loss: 2.7790607e-06\n",
      "step: 123220 train: 0.08713912963867188 elapsed, loss: 1.8887191e-06\n",
      "step: 123230 train: 0.0865170955657959 elapsed, loss: 1.9613628e-06\n",
      "step: 123240 train: 0.07760429382324219 elapsed, loss: 2.1369165e-06\n",
      "step: 123250 train: 0.08238363265991211 elapsed, loss: 1.6712565e-06\n",
      "step: 123260 train: 0.08214259147644043 elapsed, loss: 1.512929e-06\n",
      "step: 123270 train: 0.0807802677154541 elapsed, loss: 1.816542e-06\n",
      "step: 123280 train: 0.0793302059173584 elapsed, loss: 1.200008e-06\n",
      "step: 123290 train: 0.08099222183227539 elapsed, loss: 1.8081605e-06\n",
      "step: 123300 train: 0.07545876502990723 elapsed, loss: 2.0232956e-06\n",
      "step: 123310 train: 0.08202242851257324 elapsed, loss: 1.7713721e-06\n",
      "step: 123320 train: 0.08513808250427246 elapsed, loss: 1.0812648e-06\n",
      "step: 123330 train: 0.07981562614440918 elapsed, loss: 1.7499531e-06\n",
      "step: 123340 train: 0.08690476417541504 elapsed, loss: 1.2451774e-06\n",
      "step: 123350 train: 0.0799562931060791 elapsed, loss: 2.1229457e-06\n",
      "step: 123360 train: 0.07953310012817383 elapsed, loss: 1.5646203e-06\n",
      "step: 123370 train: 0.07960081100463867 elapsed, loss: 1.2363294e-06\n",
      "step: 123380 train: 0.08393120765686035 elapsed, loss: 1.6642716e-06\n",
      "step: 123390 train: 0.08852362632751465 elapsed, loss: 1.7182883e-06\n",
      "step: 123400 train: 0.0778510570526123 elapsed, loss: 1.6521647e-06\n",
      "step: 123410 train: 0.07476568222045898 elapsed, loss: 2.3464486e-06\n",
      "step: 123420 train: 0.0871129035949707 elapsed, loss: 1.3555384e-06\n",
      "step: 123430 train: 0.0816495418548584 elapsed, loss: 1.6773099e-06\n",
      "step: 123440 train: 0.08055686950683594 elapsed, loss: 1.6535616e-06\n",
      "step: 123450 train: 0.0851755142211914 elapsed, loss: 1.6083924e-06\n",
      "step: 123460 train: 0.08249545097351074 elapsed, loss: 1.6703253e-06\n",
      "step: 123470 train: 0.08401679992675781 elapsed, loss: 4.8684133e-06\n",
      "step: 123480 train: 0.08057546615600586 elapsed, loss: 2.0442499e-06\n",
      "step: 123490 train: 0.08236336708068848 elapsed, loss: 1.9604304e-06\n",
      "step: 123500 train: 0.07681632041931152 elapsed, loss: 1.8025721e-06\n",
      "step: 123510 train: 0.08199810981750488 elapsed, loss: 1.5431991e-06\n",
      "step: 123520 train: 0.08320140838623047 elapsed, loss: 1.6968672e-06\n",
      "step: 123530 train: 0.08504867553710938 elapsed, loss: 1.8291137e-06\n",
      "step: 123540 train: 0.0910799503326416 elapsed, loss: 1.4617094e-06\n",
      "step: 123550 train: 0.07976388931274414 elapsed, loss: 3.3764877e-06\n",
      "step: 123560 train: 0.07690119743347168 elapsed, loss: 1.732258e-06\n",
      "step: 123570 train: 0.08221650123596191 elapsed, loss: 1.4044332e-06\n",
      "step: 123580 train: 0.08483004570007324 elapsed, loss: 1.311301e-06\n",
      "step: 123590 train: 0.07854986190795898 elapsed, loss: 2.3124703e-06\n",
      "step: 123600 train: 0.08254170417785645 elapsed, loss: 1.6475078e-06\n",
      "step: 123610 train: 0.08383488655090332 elapsed, loss: 1.7052498e-06\n",
      "step: 123620 train: 0.08093953132629395 elapsed, loss: 1.7639229e-06\n",
      "step: 123630 train: 0.0905613899230957 elapsed, loss: 1.5175887e-06\n",
      "step: 123640 train: 0.07982420921325684 elapsed, loss: 1.8891842e-06\n",
      "step: 123650 train: 0.08485865592956543 elapsed, loss: 1.5059472e-06\n",
      "step: 123660 train: 0.07811236381530762 elapsed, loss: 2.3054859e-06\n",
      "step: 123670 train: 0.07437872886657715 elapsed, loss: 0.00049717\n",
      "step: 123680 train: 0.07653594017028809 elapsed, loss: 1.2662103e-05\n",
      "step: 123690 train: 0.08352398872375488 elapsed, loss: 4.9536784e-06\n",
      "step: 123700 train: 0.08299612998962402 elapsed, loss: 3.2721932e-06\n",
      "step: 123710 train: 0.08013796806335449 elapsed, loss: 4.3180607e-06\n",
      "step: 123720 train: 0.08687829971313477 elapsed, loss: 2.8554275e-06\n",
      "step: 123730 train: 0.07490944862365723 elapsed, loss: 4.9531945e-06\n",
      "step: 123740 train: 0.08485674858093262 elapsed, loss: 2.2756828e-06\n",
      "step: 123750 train: 0.07849788665771484 elapsed, loss: 1.82539e-06\n",
      "step: 123760 train: 0.08539772033691406 elapsed, loss: 1.8025726e-06\n",
      "step: 123770 train: 0.08242964744567871 elapsed, loss: 1.620034e-06\n",
      "step: 123780 train: 0.08466219902038574 elapsed, loss: 1.2437802e-06\n",
      "step: 123790 train: 0.07935905456542969 elapsed, loss: 1.533421e-06\n",
      "step: 123800 train: 0.077789306640625 elapsed, loss: 1.438892e-06\n",
      "step: 123810 train: 0.08436083793640137 elapsed, loss: 1.4612438e-06\n",
      "step: 123820 train: 0.08007407188415527 elapsed, loss: 2.6752186e-06\n",
      "step: 123830 train: 0.08339953422546387 elapsed, loss: 1.5227109e-06\n",
      "step: 123840 train: 0.08350634574890137 elapsed, loss: 1.8393598e-06\n",
      "step: 123850 train: 0.0843050479888916 elapsed, loss: 1.3224769e-06\n",
      "step: 123860 train: 0.07999181747436523 elapsed, loss: 1.4682288e-06\n",
      "step: 123870 train: 0.08122062683105469 elapsed, loss: 1.9636911e-06\n",
      "step: 123880 train: 0.08632206916809082 elapsed, loss: 1.357867e-06\n",
      "step: 123890 train: 0.08592104911804199 elapsed, loss: 1.2605442e-06\n",
      "step: 123900 train: 0.08299446105957031 elapsed, loss: 1.2316731e-06\n",
      "step: 123910 train: 0.07929372787475586 elapsed, loss: 1.536215e-06\n",
      "step: 123920 train: 0.07913589477539062 elapsed, loss: 1.985577e-06\n",
      "step: 123930 train: 0.07843470573425293 elapsed, loss: 3.2221064e-06\n",
      "step: 123940 train: 0.08699870109558105 elapsed, loss: 0.0002190285\n",
      "step: 123950 train: 0.08286619186401367 elapsed, loss: 0.00015242476\n",
      "step: 123960 train: 0.07811331748962402 elapsed, loss: 0.00022922496\n",
      "step: 123970 train: 0.08034920692443848 elapsed, loss: 4.1991967e-05\n",
      "step: 123980 train: 0.07610917091369629 elapsed, loss: 3.5761183e-05\n",
      "step: 123990 train: 0.07793807983398438 elapsed, loss: 2.492777e-05\n",
      "step: 124000 train: 0.08057498931884766 elapsed, loss: 2.1826885e-05\n",
      "step: 124010 train: 0.08472204208374023 elapsed, loss: 8.111744e-06\n",
      "step: 124020 train: 0.08217191696166992 elapsed, loss: 1.2734175e-05\n",
      "step: 124030 train: 0.07945704460144043 elapsed, loss: 6.377195e-06\n",
      "step: 124040 train: 0.08417391777038574 elapsed, loss: 4.8903416e-06\n",
      "step: 124050 train: 0.08199071884155273 elapsed, loss: 4.3972195e-06\n",
      "step: 124060 train: 0.08673286437988281 elapsed, loss: 2.6458765e-06\n",
      "step: 124070 train: 0.07988762855529785 elapsed, loss: 3.2884907e-06\n",
      "step: 124080 train: 0.08189058303833008 elapsed, loss: 2.2952386e-06\n",
      "step: 124090 train: 0.08208584785461426 elapsed, loss: 2.376265e-06\n",
      "step: 124100 train: 0.07793593406677246 elapsed, loss: 1.3816157e-06\n",
      "step: 124110 train: 0.08205628395080566 elapsed, loss: 1.5846437e-06\n",
      "step: 124120 train: 0.07926678657531738 elapsed, loss: 1.5310927e-06\n",
      "step: 124130 train: 0.08557915687561035 elapsed, loss: 1.2186342e-06\n",
      "step: 124140 train: 0.08498954772949219 elapsed, loss: 1.2214286e-06\n",
      "step: 124150 train: 0.0821840763092041 elapsed, loss: 1.2712542e-06\n",
      "step: 124160 train: 0.07854604721069336 elapsed, loss: 1.6437826e-06\n",
      "step: 124170 train: 0.08145761489868164 elapsed, loss: 1.4821984e-06\n",
      "step: 124180 train: 0.07527399063110352 elapsed, loss: 1.5562384e-06\n",
      "step: 124190 train: 0.08452224731445312 elapsed, loss: 1.0896466e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 124200 train: 0.08473086357116699 elapsed, loss: 9.923236e-07\n",
      "step: 124210 train: 0.08246350288391113 elapsed, loss: 1.2568187e-06\n",
      "step: 124220 train: 0.07495594024658203 elapsed, loss: 1.6731193e-06\n",
      "step: 124230 train: 0.08850979804992676 elapsed, loss: 1.1106014e-06\n",
      "step: 124240 train: 0.08400774002075195 elapsed, loss: 1.3257362e-06\n",
      "step: 124250 train: 0.08094453811645508 elapsed, loss: 1.157633e-06\n",
      "step: 124260 train: 0.0768289566040039 elapsed, loss: 1.5483222e-06\n",
      "step: 124270 train: 0.07571697235107422 elapsed, loss: 2.1825508e-06\n",
      "step: 124280 train: 0.08123254776000977 elapsed, loss: 1.268926e-06\n",
      "step: 124290 train: 0.07799577713012695 elapsed, loss: 1.3527449e-06\n",
      "step: 124300 train: 0.08248472213745117 elapsed, loss: 1.4952369e-06\n",
      "step: 124310 train: 0.07869815826416016 elapsed, loss: 1.8277185e-06\n",
      "step: 124320 train: 0.07602667808532715 elapsed, loss: 2.4479773e-06\n",
      "step: 124330 train: 0.08327102661132812 elapsed, loss: 1.5348182e-06\n",
      "step: 124340 train: 0.08318328857421875 elapsed, loss: 2.1322603e-06\n",
      "step: 124350 train: 0.08362770080566406 elapsed, loss: 1.3923261e-06\n",
      "step: 124360 train: 0.08005976676940918 elapsed, loss: 1.4812655e-06\n",
      "step: 124370 train: 0.08502316474914551 elapsed, loss: 1.3513479e-06\n",
      "step: 124380 train: 0.08131217956542969 elapsed, loss: 1.7476248e-06\n",
      "step: 124390 train: 0.08396744728088379 elapsed, loss: 1.6009417e-06\n",
      "step: 124400 train: 0.08038187026977539 elapsed, loss: 2.1359845e-06\n",
      "step: 124410 train: 0.0791773796081543 elapsed, loss: 1.8668338e-06\n",
      "step: 124420 train: 0.0780181884765625 elapsed, loss: 1.4509981e-06\n",
      "step: 124430 train: 0.0817880630493164 elapsed, loss: 2.0889536e-06\n",
      "step: 124440 train: 0.08628201484680176 elapsed, loss: 1.5380779e-06\n",
      "step: 124450 train: 0.07625460624694824 elapsed, loss: 2.4940778e-06\n",
      "step: 124460 train: 0.08624577522277832 elapsed, loss: 1.5399403e-06\n",
      "step: 124470 train: 0.08058571815490723 elapsed, loss: 1.7629898e-06\n",
      "step: 124480 train: 0.0797421932220459 elapsed, loss: 1.8826639e-06\n",
      "step: 124490 train: 0.08252668380737305 elapsed, loss: 1.8724219e-06\n",
      "step: 124500 train: 0.07659912109375 elapsed, loss: 2.2323773e-06\n",
      "step: 124510 train: 0.07854819297790527 elapsed, loss: 0.011118143\n",
      "step: 124520 train: 0.08342933654785156 elapsed, loss: 6.897081e-05\n",
      "step: 124530 train: 0.08359909057617188 elapsed, loss: 1.9361538e-05\n",
      "step: 124540 train: 0.0837397575378418 elapsed, loss: 1.1654409e-05\n",
      "step: 124550 train: 0.087799072265625 elapsed, loss: 1.0253239e-05\n",
      "step: 124560 train: 0.0838308334350586 elapsed, loss: 9.29296e-06\n",
      "step: 124570 train: 0.08248710632324219 elapsed, loss: 4.863812e-06\n",
      "step: 124580 train: 0.07980966567993164 elapsed, loss: 6.6980274e-06\n",
      "step: 124590 train: 0.08601927757263184 elapsed, loss: 3.8258577e-06\n",
      "step: 124600 train: 0.07768869400024414 elapsed, loss: 4.1872127e-06\n",
      "step: 124610 train: 0.07840919494628906 elapsed, loss: 3.2824378e-06\n",
      "step: 124620 train: 0.07874679565429688 elapsed, loss: 3.2810417e-06\n",
      "step: 124630 train: 0.0827336311340332 elapsed, loss: 2.3669525e-06\n",
      "step: 124640 train: 0.0829930305480957 elapsed, loss: 1.9995468e-06\n",
      "step: 124650 train: 0.07806062698364258 elapsed, loss: 2.3273712e-06\n",
      "step: 124660 train: 0.08587336540222168 elapsed, loss: 0.0065061804\n",
      "step: 124670 train: 0.08544254302978516 elapsed, loss: 6.394734e-05\n",
      "step: 124680 train: 0.08223581314086914 elapsed, loss: 4.1899846e-05\n",
      "step: 124690 train: 0.08071660995483398 elapsed, loss: 2.263359e-05\n",
      "step: 124700 train: 0.08358955383300781 elapsed, loss: 1.3012403e-05\n",
      "step: 124710 train: 0.0897676944732666 elapsed, loss: 9.092351e-06\n",
      "step: 124720 train: 0.08010721206665039 elapsed, loss: 8.511242e-06\n",
      "step: 124730 train: 0.07948088645935059 elapsed, loss: 5.26427e-06\n",
      "step: 124740 train: 0.0844278335571289 elapsed, loss: 4.3809237e-06\n",
      "step: 124750 train: 0.08494186401367188 elapsed, loss: 4.0870946e-06\n",
      "step: 124760 train: 0.08320784568786621 elapsed, loss: 3.1138684e-06\n",
      "step: 124770 train: 0.08522891998291016 elapsed, loss: 2.7599658e-06\n",
      "step: 124780 train: 0.07938718795776367 elapsed, loss: 2.5960528e-06\n",
      "step: 124790 train: 0.08485651016235352 elapsed, loss: 1.6819663e-06\n",
      "step: 124800 train: 0.07903218269348145 elapsed, loss: 1.674516e-06\n",
      "step: 124810 train: 0.07740354537963867 elapsed, loss: 1.6381946e-06\n",
      "step: 124820 train: 0.07735323905944824 elapsed, loss: 1.5008247e-06\n",
      "step: 124830 train: 0.0816042423248291 elapsed, loss: 1.6586723e-06\n",
      "step: 124840 train: 0.07945537567138672 elapsed, loss: 1.9599656e-06\n",
      "step: 124850 train: 0.0872654914855957 elapsed, loss: 9.220088e-07\n",
      "step: 124860 train: 0.08672022819519043 elapsed, loss: 1.3159577e-06\n",
      "step: 124870 train: 0.08811450004577637 elapsed, loss: 1.0449431e-06\n",
      "step: 124880 train: 0.08845067024230957 elapsed, loss: 9.611243e-07\n",
      "step: 124890 train: 0.07924532890319824 elapsed, loss: 1.1459915e-06\n",
      "step: 124900 train: 0.08446836471557617 elapsed, loss: 1.0407523e-06\n",
      "step: 124910 train: 0.08398580551147461 elapsed, loss: 9.75094e-07\n",
      "step: 124920 train: 0.07983803749084473 elapsed, loss: 1.9497215e-06\n",
      "step: 124930 train: 0.08363008499145508 elapsed, loss: 1.4002421e-06\n",
      "step: 124940 train: 0.07896184921264648 elapsed, loss: 1.3094384e-06\n",
      "step: 124950 train: 0.08330869674682617 elapsed, loss: 1.5846437e-06\n",
      "step: 124960 train: 0.08138871192932129 elapsed, loss: 1.309438e-06\n",
      "step: 124970 train: 0.08280801773071289 elapsed, loss: 2.508813e-06\n",
      "step: 124980 train: 0.08312630653381348 elapsed, loss: 0.00010601002\n",
      "step: 124990 train: 0.07477021217346191 elapsed, loss: 7.611561e-05\n",
      "step: 125000 train: 0.07925796508789062 elapsed, loss: 3.582151e-05\n",
      "step: 125010 train: 0.0790410041809082 elapsed, loss: 2.0831227e-05\n",
      "step: 125020 train: 0.08396720886230469 elapsed, loss: 8.962028e-06\n",
      "step: 125030 train: 0.08130192756652832 elapsed, loss: 1.2161963e-05\n",
      "step: 125040 train: 0.08513593673706055 elapsed, loss: 6.5983772e-06\n",
      "step: 125050 train: 0.0848848819732666 elapsed, loss: 5.7085153e-06\n",
      "step: 125060 train: 0.08055496215820312 elapsed, loss: 3.7527502e-06\n",
      "step: 125070 train: 0.08218860626220703 elapsed, loss: 4.6928894e-06\n",
      "step: 125080 train: 0.08038544654846191 elapsed, loss: 2.6747527e-06\n",
      "step: 125090 train: 0.08765578269958496 elapsed, loss: 4.3269047e-06\n",
      "step: 125100 train: 0.08197951316833496 elapsed, loss: 2.1276028e-06\n",
      "step: 125110 train: 0.08117032051086426 elapsed, loss: 1.7564716e-06\n",
      "step: 125120 train: 0.0755605697631836 elapsed, loss: 1.9352854e-06\n",
      "step: 125130 train: 0.08486151695251465 elapsed, loss: 1.801641e-06\n",
      "step: 125140 train: 0.08199429512023926 elapsed, loss: 1.3313243e-06\n",
      "step: 125150 train: 0.08110666275024414 elapsed, loss: 1.2051303e-06\n",
      "step: 125160 train: 0.08362197875976562 elapsed, loss: 1.4761449e-06\n",
      "step: 125170 train: 0.08086872100830078 elapsed, loss: 1.7764954e-06\n",
      "step: 125180 train: 0.08240723609924316 elapsed, loss: 1.0407523e-06\n",
      "step: 125190 train: 0.08151054382324219 elapsed, loss: 1.215838e-06\n",
      "step: 125200 train: 0.07863807678222656 elapsed, loss: 1.505947e-06\n",
      "step: 125210 train: 0.08145022392272949 elapsed, loss: 1.6614775e-06\n",
      "step: 125220 train: 0.0751638412475586 elapsed, loss: 1.5958196e-06\n",
      "step: 125230 train: 0.08619523048400879 elapsed, loss: 9.690406e-07\n",
      "step: 125240 train: 0.08279895782470703 elapsed, loss: 1.166015e-06\n",
      "step: 125250 train: 0.07946968078613281 elapsed, loss: 1.4253874e-06\n",
      "step: 125260 train: 0.08550906181335449 elapsed, loss: 1.1506479e-06\n",
      "step: 125270 train: 0.08280682563781738 elapsed, loss: 1.3709057e-06\n",
      "step: 125280 train: 0.0866236686706543 elapsed, loss: 1.3820802e-06\n",
      "step: 125290 train: 0.08140134811401367 elapsed, loss: 1.3089727e-06\n",
      "step: 125300 train: 0.08051276206970215 elapsed, loss: 1.1217771e-06\n",
      "step: 125310 train: 0.0772242546081543 elapsed, loss: 1.3913947e-06\n",
      "step: 125320 train: 0.08003544807434082 elapsed, loss: 1.4253878e-06\n",
      "step: 125330 train: 0.07690644264221191 elapsed, loss: 2.2542627e-06\n",
      "step: 125340 train: 0.08316946029663086 elapsed, loss: 1.1227087e-06\n",
      "step: 125350 train: 0.08564424514770508 elapsed, loss: 1.1389788e-05\n",
      "step: 125360 train: 0.08084440231323242 elapsed, loss: 6.741332e-06\n",
      "step: 125370 train: 0.07757711410522461 elapsed, loss: 2.5448344e-06\n",
      "step: 125380 train: 0.08011770248413086 elapsed, loss: 2.2710265e-06\n",
      "step: 125390 train: 0.08131575584411621 elapsed, loss: 2.4726573e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 125400 train: 0.07978367805480957 elapsed, loss: 2.221201e-06\n",
      "step: 125410 train: 0.07703018188476562 elapsed, loss: 2.3408747e-06\n",
      "step: 125420 train: 0.08313107490539551 elapsed, loss: 1.6125833e-06\n",
      "step: 125430 train: 0.08724737167358398 elapsed, loss: 1.1855727e-06\n",
      "step: 125440 train: 0.08626103401184082 elapsed, loss: 1.3359809e-06\n",
      "step: 125450 train: 0.07866978645324707 elapsed, loss: 1.607461e-06\n",
      "step: 125460 train: 0.08526372909545898 elapsed, loss: 1.2689259e-06\n",
      "step: 125470 train: 0.08440351486206055 elapsed, loss: 1.2316731e-06\n",
      "step: 125480 train: 0.08283710479736328 elapsed, loss: 1.2796361e-06\n",
      "step: 125490 train: 0.09544682502746582 elapsed, loss: 1.6218949e-06\n",
      "step: 125500 train: 0.08416295051574707 elapsed, loss: 1.3010562e-06\n",
      "step: 125510 train: 0.07994198799133301 elapsed, loss: 1.3178204e-06\n",
      "step: 125520 train: 0.07508277893066406 elapsed, loss: 4.240761e-06\n",
      "step: 125530 train: 0.07642412185668945 elapsed, loss: 1.8156113e-06\n",
      "step: 125540 train: 0.08619523048400879 elapsed, loss: 1.6703252e-06\n",
      "step: 125550 train: 0.08976221084594727 elapsed, loss: 2.0111884e-06\n",
      "step: 125560 train: 0.08149266242980957 elapsed, loss: 2.0530972e-06\n",
      "step: 125570 train: 0.08862757682800293 elapsed, loss: 1.9068807e-06\n",
      "step: 125580 train: 0.08527803421020508 elapsed, loss: 1.2642694e-06\n",
      "step: 125590 train: 0.07873082160949707 elapsed, loss: 2.5099102e-06\n",
      "step: 125600 train: 0.0808725357055664 elapsed, loss: 1.5245737e-06\n",
      "step: 125610 train: 0.0779881477355957 elapsed, loss: 1.621431e-06\n",
      "step: 125620 train: 0.0782310962677002 elapsed, loss: 1.6107208e-06\n",
      "step: 125630 train: 0.08110761642456055 elapsed, loss: 1.7867403e-06\n",
      "step: 125640 train: 0.08423256874084473 elapsed, loss: 1.560895e-06\n",
      "step: 125650 train: 0.08091068267822266 elapsed, loss: 1.6931425e-06\n",
      "step: 125660 train: 0.07966828346252441 elapsed, loss: 1.8537953e-06\n",
      "step: 125670 train: 0.07921147346496582 elapsed, loss: 2.0596171e-06\n",
      "step: 125680 train: 0.07510185241699219 elapsed, loss: 1.8431567e-05\n",
      "step: 125690 train: 0.07969188690185547 elapsed, loss: 5.4286465e-06\n",
      "step: 125700 train: 0.08802008628845215 elapsed, loss: 2.3008288e-06\n",
      "step: 125710 train: 0.07500219345092773 elapsed, loss: 3.0151486e-06\n",
      "step: 125720 train: 0.0833289623260498 elapsed, loss: 1.8561237e-06\n",
      "step: 125730 train: 0.08305478096008301 elapsed, loss: 2.0279522e-06\n",
      "step: 125740 train: 0.08568048477172852 elapsed, loss: 1.883132e-06\n",
      "step: 125750 train: 0.07663869857788086 elapsed, loss: 1.9883712e-06\n",
      "step: 125760 train: 0.07813644409179688 elapsed, loss: 1.5739331e-06\n",
      "step: 125770 train: 0.08406424522399902 elapsed, loss: 1.6121178e-06\n",
      "step: 125780 train: 0.08458113670349121 elapsed, loss: 1.6288815e-06\n",
      "step: 125790 train: 0.08531761169433594 elapsed, loss: 1.5874372e-06\n",
      "step: 125800 train: 0.08487272262573242 elapsed, loss: 1.2298106e-06\n",
      "step: 125810 train: 0.08003711700439453 elapsed, loss: 1.5632229e-06\n",
      "step: 125820 train: 0.08236503601074219 elapsed, loss: 1.5050157e-06\n",
      "step: 125830 train: 0.07909011840820312 elapsed, loss: 1.7313267e-06\n",
      "step: 125840 train: 0.07927584648132324 elapsed, loss: 1.7206162e-06\n",
      "step: 125850 train: 0.0879666805267334 elapsed, loss: 1.2842928e-06\n",
      "step: 125860 train: 0.08177518844604492 elapsed, loss: 3.1473955e-06\n",
      "step: 125870 train: 0.09018492698669434 elapsed, loss: 1.7816182e-06\n",
      "step: 125880 train: 0.08729195594787598 elapsed, loss: 1.6977992e-06\n",
      "step: 125890 train: 0.07961344718933105 elapsed, loss: 2.428419e-06\n",
      "step: 125900 train: 0.07787466049194336 elapsed, loss: 4.685461e-06\n",
      "step: 125910 train: 0.08319759368896484 elapsed, loss: 1.8938423e-06\n",
      "step: 125920 train: 0.08231163024902344 elapsed, loss: 1.8062981e-06\n",
      "step: 125930 train: 0.08094954490661621 elapsed, loss: 1.1780565e-05\n",
      "step: 125940 train: 0.08197522163391113 elapsed, loss: 2.841923e-06\n",
      "step: 125950 train: 0.08192586898803711 elapsed, loss: 2.521085e-06\n",
      "step: 125960 train: 0.08092904090881348 elapsed, loss: 2.3725406e-06\n",
      "step: 125970 train: 0.07762885093688965 elapsed, loss: 1.929698e-06\n",
      "step: 125980 train: 0.07941818237304688 elapsed, loss: 2.0740526e-06\n",
      "step: 125990 train: 0.07921481132507324 elapsed, loss: 2.0526322e-06\n",
      "step: 126000 train: 0.0798637866973877 elapsed, loss: 1.4519305e-06\n",
      "step: 126010 train: 0.08427095413208008 elapsed, loss: 2.4503024e-06\n",
      "step: 126020 train: 0.08195209503173828 elapsed, loss: 1.515726e-06\n",
      "step: 126030 train: 0.0915067195892334 elapsed, loss: 1.584177e-06\n",
      "step: 126040 train: 0.08660578727722168 elapsed, loss: 1.3164233e-06\n",
      "step: 126050 train: 0.08065342903137207 elapsed, loss: 1.7164257e-06\n",
      "step: 126060 train: 0.07908487319946289 elapsed, loss: 6.043936e-05\n",
      "step: 126070 train: 0.08445954322814941 elapsed, loss: 3.8100238e-06\n",
      "step: 126080 train: 0.08992385864257812 elapsed, loss: 2.6146836e-06\n",
      "step: 126090 train: 0.07642483711242676 elapsed, loss: 2.71806e-06\n",
      "step: 126100 train: 0.0881352424621582 elapsed, loss: 1.6419199e-06\n",
      "step: 126110 train: 0.07921004295349121 elapsed, loss: 1.7248074e-06\n",
      "step: 126120 train: 0.07792782783508301 elapsed, loss: 2.2402917e-06\n",
      "step: 126130 train: 0.0797884464263916 elapsed, loss: 1.5390074e-06\n",
      "step: 126140 train: 0.08493375778198242 elapsed, loss: 1.1580987e-06\n",
      "step: 126150 train: 0.08156704902648926 elapsed, loss: 1.9958215e-06\n",
      "step: 126160 train: 0.0814363956451416 elapsed, loss: 1.6042011e-06\n",
      "step: 126170 train: 0.07780265808105469 elapsed, loss: 1.7615947e-06\n",
      "step: 126180 train: 0.08715343475341797 elapsed, loss: 1.360661e-06\n",
      "step: 126190 train: 0.08781647682189941 elapsed, loss: 1.4440135e-06\n",
      "step: 126200 train: 0.08506011962890625 elapsed, loss: 1.3979139e-06\n",
      "step: 126210 train: 0.08679461479187012 elapsed, loss: 1.5906967e-06\n",
      "step: 126220 train: 0.07890152931213379 elapsed, loss: 1.7485563e-06\n",
      "step: 126230 train: 0.08177351951599121 elapsed, loss: 1.6582177e-06\n",
      "step: 126240 train: 0.07880878448486328 elapsed, loss: 1.8160771e-06\n",
      "step: 126250 train: 0.0871572494506836 elapsed, loss: 1.6014076e-06\n",
      "step: 126260 train: 0.08432674407958984 elapsed, loss: 1.6614774e-06\n",
      "step: 126270 train: 0.07729387283325195 elapsed, loss: 2.332959e-06\n",
      "step: 126280 train: 0.08118510246276855 elapsed, loss: 1.7709079e-06\n",
      "step: 126290 train: 0.08738350868225098 elapsed, loss: 2.0326086e-06\n",
      "step: 126300 train: 0.08268857002258301 elapsed, loss: 3.8929047e-06\n",
      "step: 126310 train: 0.07803964614868164 elapsed, loss: 2.077778e-06\n",
      "step: 126320 train: 0.08098626136779785 elapsed, loss: 1.9343547e-06\n",
      "step: 126330 train: 0.08503246307373047 elapsed, loss: 1.6260875e-06\n",
      "step: 126340 train: 0.08443975448608398 elapsed, loss: 1.8053668e-06\n",
      "step: 126350 train: 0.08488130569458008 elapsed, loss: 1.6274844e-06\n",
      "step: 126360 train: 0.07938337326049805 elapsed, loss: 1.8007095e-06\n",
      "step: 126370 train: 0.08182072639465332 elapsed, loss: 2.2156132e-06\n",
      "step: 126380 train: 0.07466340065002441 elapsed, loss: 2.5015283e-06\n",
      "step: 126390 train: 0.08523440361022949 elapsed, loss: 1.5124665e-06\n",
      "step: 126400 train: 0.08079862594604492 elapsed, loss: 1.4118839e-06\n",
      "step: 126410 train: 0.08291268348693848 elapsed, loss: 1.751816e-06\n",
      "step: 126420 train: 0.0744314193725586 elapsed, loss: 2.1532146e-06\n",
      "step: 126430 train: 0.07970881462097168 elapsed, loss: 1.5874377e-06\n",
      "step: 126440 train: 0.08209967613220215 elapsed, loss: 0.02522799\n",
      "step: 126450 train: 0.08243727684020996 elapsed, loss: 0.00012441221\n",
      "step: 126460 train: 0.0797739028930664 elapsed, loss: 3.9546965e-05\n",
      "step: 126470 train: 0.08368444442749023 elapsed, loss: 2.327341e-05\n",
      "step: 126480 train: 0.07995939254760742 elapsed, loss: 2.8755601e-05\n",
      "step: 126490 train: 0.0804753303527832 elapsed, loss: 1.8107927e-05\n",
      "step: 126500 train: 0.07170748710632324 elapsed, loss: 2.6292912e-05\n",
      "step: 126510 train: 0.07947206497192383 elapsed, loss: 5.9273707e-06\n",
      "step: 126520 train: 0.08073925971984863 elapsed, loss: 7.735945e-06\n",
      "step: 126530 train: 0.08172845840454102 elapsed, loss: 4.0735895e-06\n",
      "step: 126540 train: 0.0768132209777832 elapsed, loss: 5.6400604e-06\n",
      "step: 126550 train: 0.08708429336547852 elapsed, loss: 2.6449502e-06\n",
      "step: 126560 train: 0.08250117301940918 elapsed, loss: 2.4693975e-06\n",
      "step: 126570 train: 0.08097243309020996 elapsed, loss: 2.271958e-06\n",
      "step: 126580 train: 0.08136415481567383 elapsed, loss: 3.2549221e-06\n",
      "step: 126590 train: 0.08785724639892578 elapsed, loss: 1.672651e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 126600 train: 0.0797567367553711 elapsed, loss: 1.5879023e-06\n",
      "step: 126610 train: 0.08216023445129395 elapsed, loss: 1.7993129e-06\n",
      "step: 126620 train: 0.08436703681945801 elapsed, loss: 1.4356324e-06\n",
      "step: 126630 train: 0.08253645896911621 elapsed, loss: 1.5739332e-06\n",
      "step: 126640 train: 0.07694721221923828 elapsed, loss: 1.7029215e-06\n",
      "step: 126650 train: 0.08189773559570312 elapsed, loss: 0.013344568\n",
      "step: 126660 train: 0.07562470436096191 elapsed, loss: 7.2404e-05\n",
      "step: 126670 train: 0.08124661445617676 elapsed, loss: 4.7172878e-05\n",
      "step: 126680 train: 0.07884335517883301 elapsed, loss: 2.854371e-05\n",
      "step: 126690 train: 0.08140158653259277 elapsed, loss: 1.32310615e-05\n",
      "step: 126700 train: 0.07904767990112305 elapsed, loss: 1.40398815e-05\n",
      "step: 126710 train: 0.08688521385192871 elapsed, loss: 1.1492725e-05\n",
      "step: 126720 train: 0.0841226577758789 elapsed, loss: 6.551341e-06\n",
      "step: 126730 train: 0.0938253402709961 elapsed, loss: 9.435e-06\n",
      "step: 126740 train: 0.0861809253692627 elapsed, loss: 3.4421569e-06\n",
      "step: 126750 train: 0.08613276481628418 elapsed, loss: 4.2845345e-06\n",
      "step: 126760 train: 0.08145904541015625 elapsed, loss: 2.6849964e-06\n",
      "step: 126770 train: 0.07877063751220703 elapsed, loss: 2.844718e-06\n",
      "step: 126780 train: 0.08215713500976562 elapsed, loss: 1.699196e-06\n",
      "step: 126790 train: 0.07933163642883301 elapsed, loss: 2.5210852e-06\n",
      "step: 126800 train: 0.07652711868286133 elapsed, loss: 1.9120025e-06\n",
      "step: 126810 train: 0.08808588981628418 elapsed, loss: 1.4090895e-06\n",
      "step: 126820 train: 0.07801651954650879 elapsed, loss: 1.8975672e-06\n",
      "step: 126830 train: 0.07935047149658203 elapsed, loss: 1.2600783e-06\n",
      "step: 126840 train: 0.08157730102539062 elapsed, loss: 1.4272505e-06\n",
      "step: 126850 train: 0.08036565780639648 elapsed, loss: 1.0291108e-06\n",
      "step: 126860 train: 0.07932519912719727 elapsed, loss: 1.10967e-06\n",
      "step: 126870 train: 0.08066010475158691 elapsed, loss: 1.3858064e-06\n",
      "step: 126880 train: 0.08087897300720215 elapsed, loss: 1.187901e-06\n",
      "step: 126890 train: 0.08464813232421875 elapsed, loss: 1.4235251e-06\n",
      "step: 126900 train: 0.08506393432617188 elapsed, loss: 1.0561191e-06\n",
      "step: 126910 train: 0.07800102233886719 elapsed, loss: 1.4975651e-06\n",
      "step: 126920 train: 0.08272004127502441 elapsed, loss: 1.1464567e-06\n",
      "step: 126930 train: 0.07982110977172852 elapsed, loss: 1.4710226e-06\n",
      "step: 126940 train: 0.09053277969360352 elapsed, loss: 9.634521e-07\n",
      "step: 126950 train: 0.07929444313049316 elapsed, loss: 1.5068775e-06\n",
      "step: 126960 train: 0.07743120193481445 elapsed, loss: 1.2614753e-06\n",
      "step: 126970 train: 0.08564352989196777 elapsed, loss: 1.5306272e-06\n",
      "step: 126980 train: 0.08135294914245605 elapsed, loss: 1.2814987e-06\n",
      "step: 126990 train: 0.08395957946777344 elapsed, loss: 1.7448308e-06\n",
      "step: 127000 train: 0.08812689781188965 elapsed, loss: 1.0286451e-06\n",
      "step: 127010 train: 0.07843589782714844 elapsed, loss: 1.4472738e-06\n",
      "step: 127020 train: 0.08193850517272949 elapsed, loss: 1.1525108e-06\n",
      "step: 127030 train: 0.08444881439208984 elapsed, loss: 1.3080414e-06\n",
      "step: 127040 train: 0.0845944881439209 elapsed, loss: 1.6647373e-06\n",
      "step: 127050 train: 0.0816643238067627 elapsed, loss: 1.4104868e-06\n",
      "step: 127060 train: 0.08018064498901367 elapsed, loss: 1.6451795e-06\n",
      "step: 127070 train: 0.08038973808288574 elapsed, loss: 1.6889518e-06\n",
      "step: 127080 train: 0.08185243606567383 elapsed, loss: 1.6782415e-06\n",
      "step: 127090 train: 0.0824289321899414 elapsed, loss: 1.621431e-06\n",
      "step: 127100 train: 0.08068966865539551 elapsed, loss: 1.640523e-06\n",
      "step: 127110 train: 0.08477258682250977 elapsed, loss: 1.6079259e-06\n",
      "step: 127120 train: 0.0827183723449707 elapsed, loss: 1.9054837e-06\n",
      "step: 127130 train: 0.0854177474975586 elapsed, loss: 1.2940715e-06\n",
      "step: 127140 train: 0.08019709587097168 elapsed, loss: 2.1411076e-06\n",
      "step: 127150 train: 0.08029532432556152 elapsed, loss: 2.1024573e-06\n",
      "step: 127160 train: 0.08087444305419922 elapsed, loss: 5.1482866e-06\n",
      "step: 127170 train: 0.07417535781860352 elapsed, loss: 2.1089772e-06\n",
      "step: 127180 train: 0.08609366416931152 elapsed, loss: 1.2731168e-06\n",
      "step: 127190 train: 0.07677030563354492 elapsed, loss: 0.0004099303\n",
      "step: 127200 train: 0.08225893974304199 elapsed, loss: 1.6634373e-05\n",
      "step: 127210 train: 0.07842421531677246 elapsed, loss: 1.2468835e-05\n",
      "step: 127220 train: 0.08591437339782715 elapsed, loss: 8.650991e-06\n",
      "step: 127230 train: 0.0896909236907959 elapsed, loss: 7.5160633e-06\n",
      "step: 127240 train: 0.08262968063354492 elapsed, loss: 4.8540323e-06\n",
      "step: 127250 train: 0.08126139640808105 elapsed, loss: 1.16128285e-05\n",
      "step: 127260 train: 0.08292245864868164 elapsed, loss: 3.9441397e-06\n",
      "step: 127270 train: 0.08510208129882812 elapsed, loss: 3.022592e-06\n",
      "step: 127280 train: 0.07748579978942871 elapsed, loss: 2.623065e-06\n",
      "step: 127290 train: 0.07881855964660645 elapsed, loss: 4.0680006e-06\n",
      "step: 127300 train: 0.07662343978881836 elapsed, loss: 2.5969882e-06\n",
      "step: 127310 train: 0.0840308666229248 elapsed, loss: 1.5418029e-06\n",
      "step: 127320 train: 0.07909440994262695 elapsed, loss: 2.442855e-06\n",
      "step: 127330 train: 0.07866263389587402 elapsed, loss: 1.8910481e-06\n",
      "step: 127340 train: 0.0848236083984375 elapsed, loss: 1.7802199e-06\n",
      "step: 127350 train: 0.08231115341186523 elapsed, loss: 1.301988e-06\n",
      "step: 127360 train: 0.08710837364196777 elapsed, loss: 3.110141e-06\n",
      "step: 127370 train: 0.08200883865356445 elapsed, loss: 1.99815e-06\n",
      "step: 127380 train: 0.08599686622619629 elapsed, loss: 2.0428533e-06\n",
      "step: 127390 train: 0.08049798011779785 elapsed, loss: 1.4523962e-06\n",
      "step: 127400 train: 0.08539557456970215 elapsed, loss: 5.2951946e-06\n",
      "step: 127410 train: 0.07412958145141602 elapsed, loss: 3.3534256e-05\n",
      "step: 127420 train: 0.08320474624633789 elapsed, loss: 1.6236148e-05\n",
      "step: 127430 train: 0.08568978309631348 elapsed, loss: 1.2420525e-05\n",
      "step: 127440 train: 0.07996892929077148 elapsed, loss: 7.902214e-06\n",
      "step: 127450 train: 0.08128952980041504 elapsed, loss: 8.765956e-06\n",
      "step: 127460 train: 0.08215618133544922 elapsed, loss: 8.593061e-06\n",
      "step: 127470 train: 0.0908200740814209 elapsed, loss: 3.224229e-06\n",
      "step: 127480 train: 0.08537721633911133 elapsed, loss: 3.035171e-06\n",
      "step: 127490 train: 0.09311413764953613 elapsed, loss: 2.7422707e-06\n",
      "step: 127500 train: 0.08134055137634277 elapsed, loss: 5.743369e-06\n",
      "step: 127510 train: 0.07783627510070801 elapsed, loss: 2.7357546e-06\n",
      "step: 127520 train: 0.08072733879089355 elapsed, loss: 2.155543e-06\n",
      "step: 127530 train: 0.08308887481689453 elapsed, loss: 1.6419195e-06\n",
      "step: 127540 train: 0.07903218269348145 elapsed, loss: 2.207231e-06\n",
      "step: 127550 train: 0.08479690551757812 elapsed, loss: 1.3238739e-06\n",
      "step: 127560 train: 0.08229374885559082 elapsed, loss: 1.5557728e-06\n",
      "step: 127570 train: 0.08482217788696289 elapsed, loss: 1.2391238e-06\n",
      "step: 127580 train: 0.07452607154846191 elapsed, loss: 1.6209651e-06\n",
      "step: 127590 train: 0.08492660522460938 elapsed, loss: 1.2656664e-06\n",
      "step: 127600 train: 0.08184957504272461 elapsed, loss: 1.2707886e-06\n",
      "step: 127610 train: 0.0826866626739502 elapsed, loss: 1.5175888e-06\n",
      "step: 127620 train: 0.08627986907958984 elapsed, loss: 1.0938363e-06\n",
      "step: 127630 train: 0.08534812927246094 elapsed, loss: 1.3071092e-06\n",
      "step: 127640 train: 0.08457517623901367 elapsed, loss: 9.858043e-07\n",
      "step: 127650 train: 0.08113408088684082 elapsed, loss: 1.1320218e-06\n",
      "step: 127660 train: 0.08636093139648438 elapsed, loss: 1.139938e-06\n",
      "step: 127670 train: 0.07781863212585449 elapsed, loss: 2.1751002e-06\n",
      "step: 127680 train: 0.07993698120117188 elapsed, loss: 1.07102e-06\n",
      "step: 127690 train: 0.07877659797668457 elapsed, loss: 1.6312099e-06\n",
      "step: 127700 train: 0.08752894401550293 elapsed, loss: 1.2675291e-06\n",
      "step: 127710 train: 0.0807335376739502 elapsed, loss: 1.7085094e-06\n",
      "step: 127720 train: 0.07776618003845215 elapsed, loss: 2.076846e-06\n",
      "step: 127730 train: 0.08168888092041016 elapsed, loss: 1.7015245e-06\n",
      "step: 127740 train: 0.07974767684936523 elapsed, loss: 2.3059513e-06\n",
      "step: 127750 train: 0.08394861221313477 elapsed, loss: 2.9010644e-06\n",
      "step: 127760 train: 0.08069849014282227 elapsed, loss: 1.7941909e-06\n",
      "step: 127770 train: 0.08265376091003418 elapsed, loss: 1.3825472e-06\n",
      "step: 127780 train: 0.08493518829345703 elapsed, loss: 1.200008e-06\n",
      "step: 127790 train: 0.07955336570739746 elapsed, loss: 1.4947707e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 127800 train: 0.08548355102539062 elapsed, loss: 2.7469312e-06\n",
      "step: 127810 train: 0.07828569412231445 elapsed, loss: 1.6731193e-06\n",
      "step: 127820 train: 0.08406400680541992 elapsed, loss: 1.4305101e-06\n",
      "step: 127830 train: 0.08111262321472168 elapsed, loss: 0.0027311656\n",
      "step: 127840 train: 0.0877530574798584 elapsed, loss: 0.00013523594\n",
      "step: 127850 train: 0.0760488510131836 elapsed, loss: 4.1903648e-05\n",
      "step: 127860 train: 0.08366656303405762 elapsed, loss: 2.0945892e-05\n",
      "step: 127870 train: 0.08061552047729492 elapsed, loss: 2.2249793e-05\n",
      "step: 127880 train: 0.08424878120422363 elapsed, loss: 1.0175507e-05\n",
      "step: 127890 train: 0.0794379711151123 elapsed, loss: 1.1782945e-05\n",
      "step: 127900 train: 0.07885336875915527 elapsed, loss: 6.4675282e-06\n",
      "step: 127910 train: 0.08216309547424316 elapsed, loss: 6.2816807e-06\n",
      "step: 127920 train: 0.08235859870910645 elapsed, loss: 3.6754527e-06\n",
      "step: 127930 train: 0.07853579521179199 elapsed, loss: 4.3967575e-06\n",
      "step: 127940 train: 0.08249664306640625 elapsed, loss: 2.7753279e-06\n",
      "step: 127950 train: 0.08342862129211426 elapsed, loss: 2.5900038e-06\n",
      "step: 127960 train: 0.07629895210266113 elapsed, loss: 2.543436e-06\n",
      "step: 127970 train: 0.08355998992919922 elapsed, loss: 1.6605456e-06\n",
      "step: 127980 train: 0.08014869689941406 elapsed, loss: 1.8514668e-06\n",
      "step: 127990 train: 0.08181262016296387 elapsed, loss: 1.393257e-06\n",
      "step: 128000 train: 0.08108258247375488 elapsed, loss: 1.2810331e-06\n",
      "step: 128010 train: 0.08333230018615723 elapsed, loss: 1.2582159e-06\n",
      "step: 128020 train: 0.0872182846069336 elapsed, loss: 1.1110669e-06\n",
      "step: 128030 train: 0.08855986595153809 elapsed, loss: 1.1869697e-06\n",
      "step: 128040 train: 0.08248138427734375 elapsed, loss: 1.3518132e-06\n",
      "step: 128050 train: 0.08189249038696289 elapsed, loss: 1.3560044e-06\n",
      "step: 128060 train: 0.0785830020904541 elapsed, loss: 6.4302235e-06\n",
      "step: 128070 train: 0.0806741714477539 elapsed, loss: 1.8142142e-06\n",
      "step: 128080 train: 0.07908129692077637 elapsed, loss: 2.438664e-06\n",
      "step: 128090 train: 0.07659149169921875 elapsed, loss: 3.1322932e-06\n",
      "step: 128100 train: 0.08543777465820312 elapsed, loss: 6.7699366e-05\n",
      "step: 128110 train: 0.08528733253479004 elapsed, loss: 2.4793077e-05\n",
      "step: 128120 train: 0.0774848461151123 elapsed, loss: 3.4718694e-05\n",
      "step: 128130 train: 0.08009767532348633 elapsed, loss: 1.0323107e-05\n",
      "step: 128140 train: 0.0796971321105957 elapsed, loss: 1.1758242e-05\n",
      "step: 128150 train: 0.09148788452148438 elapsed, loss: 7.740896e-06\n",
      "step: 128160 train: 0.08221960067749023 elapsed, loss: 1.0203134e-05\n",
      "step: 128170 train: 0.08125185966491699 elapsed, loss: 5.423989e-06\n",
      "step: 128180 train: 0.08268094062805176 elapsed, loss: 4.5899906e-06\n",
      "step: 128190 train: 0.07708525657653809 elapsed, loss: 4.953221e-06\n",
      "step: 128200 train: 0.08526992797851562 elapsed, loss: 3.0421556e-06\n",
      "step: 128210 train: 0.08100318908691406 elapsed, loss: 6.332529e-06\n",
      "step: 128220 train: 0.08025074005126953 elapsed, loss: 2.5145666e-06\n",
      "step: 128230 train: 0.0841372013092041 elapsed, loss: 1.766251e-06\n",
      "step: 128240 train: 0.09147119522094727 elapsed, loss: 1.3234081e-06\n",
      "step: 128250 train: 0.08627009391784668 elapsed, loss: 1.410021e-06\n",
      "step: 128260 train: 0.07976055145263672 elapsed, loss: 1.3401719e-06\n",
      "step: 128270 train: 0.08373427391052246 elapsed, loss: 1.369043e-06\n",
      "step: 128280 train: 0.08085131645202637 elapsed, loss: 2.1611295e-06\n",
      "step: 128290 train: 0.08154487609863281 elapsed, loss: 1.2246882e-06\n",
      "step: 128300 train: 0.08398151397705078 elapsed, loss: 1.2787049e-06\n",
      "step: 128310 train: 0.07827019691467285 elapsed, loss: 2.066135e-06\n",
      "step: 128320 train: 0.08497166633605957 elapsed, loss: 1.2582157e-06\n",
      "step: 128330 train: 0.08153700828552246 elapsed, loss: 1.5608949e-06\n",
      "step: 128340 train: 0.07435393333435059 elapsed, loss: 2.0004782e-06\n",
      "step: 128350 train: 0.0770730972290039 elapsed, loss: 4.2554726e-05\n",
      "step: 128360 train: 0.08048439025878906 elapsed, loss: 3.547618e-05\n",
      "step: 128370 train: 0.07668614387512207 elapsed, loss: 1.8880291e-05\n",
      "step: 128380 train: 0.08148646354675293 elapsed, loss: 1.8201263e-05\n",
      "step: 128390 train: 0.0832209587097168 elapsed, loss: 1.1791978e-05\n",
      "step: 128400 train: 0.08503603935241699 elapsed, loss: 7.779285e-06\n",
      "step: 128410 train: 0.0798335075378418 elapsed, loss: 6.809788e-06\n",
      "step: 128420 train: 0.073577880859375 elapsed, loss: 1.2179133e-05\n",
      "step: 128430 train: 0.08524942398071289 elapsed, loss: 4.3990763e-06\n",
      "step: 128440 train: 0.0778806209564209 elapsed, loss: 3.1138688e-06\n",
      "step: 128450 train: 0.0875239372253418 elapsed, loss: 2.3106072e-06\n",
      "step: 128460 train: 0.07822680473327637 elapsed, loss: 2.0898847e-06\n",
      "step: 128470 train: 0.07878398895263672 elapsed, loss: 1.5315583e-06\n",
      "step: 128480 train: 0.08090686798095703 elapsed, loss: 1.9492495e-06\n",
      "step: 128490 train: 0.0802311897277832 elapsed, loss: 1.3886006e-06\n",
      "step: 128500 train: 0.08510422706604004 elapsed, loss: 1.4519297e-06\n",
      "step: 128510 train: 0.08087038993835449 elapsed, loss: 1.3499478e-06\n",
      "step: 128520 train: 0.08236050605773926 elapsed, loss: 1.0379583e-06\n",
      "step: 128530 train: 0.07863545417785645 elapsed, loss: 1.6214306e-06\n",
      "step: 128540 train: 0.08136367797851562 elapsed, loss: 1.3695085e-06\n",
      "step: 128550 train: 0.0753018856048584 elapsed, loss: 1.4859238e-06\n",
      "step: 128560 train: 0.08479738235473633 elapsed, loss: 1.3816154e-06\n",
      "step: 128570 train: 0.07903695106506348 elapsed, loss: 1.2158405e-06\n",
      "step: 128580 train: 0.0778815746307373 elapsed, loss: 1.2870868e-06\n",
      "step: 128590 train: 0.0786886215209961 elapsed, loss: 1.3578672e-06\n",
      "step: 128600 train: 0.07525825500488281 elapsed, loss: 1.4621742e-06\n",
      "step: 128610 train: 0.07561492919921875 elapsed, loss: 1.7462278e-06\n",
      "step: 128620 train: 0.0834646224975586 elapsed, loss: 1.5799869e-06\n",
      "step: 128630 train: 0.07636260986328125 elapsed, loss: 1.6931424e-06\n",
      "step: 128640 train: 0.07815861701965332 elapsed, loss: 1.5348181e-06\n",
      "step: 128650 train: 0.08492302894592285 elapsed, loss: 1.4053635e-06\n",
      "step: 128660 train: 0.08028531074523926 elapsed, loss: 1.2163063e-06\n",
      "step: 128670 train: 0.07619166374206543 elapsed, loss: 1.7401744e-06\n",
      "step: 128680 train: 0.08367443084716797 elapsed, loss: 1.5180543e-06\n",
      "step: 128690 train: 0.08132648468017578 elapsed, loss: 1.4277161e-06\n",
      "step: 128700 train: 0.07692909240722656 elapsed, loss: 1.8849944e-06\n",
      "step: 128710 train: 0.08255577087402344 elapsed, loss: 1.3043162e-06\n",
      "step: 128720 train: 0.08006715774536133 elapsed, loss: 1.665203e-06\n",
      "step: 128730 train: 0.08398008346557617 elapsed, loss: 1.7588006e-06\n",
      "step: 128740 train: 0.08909440040588379 elapsed, loss: 1.4081584e-06\n",
      "step: 128750 train: 0.08234477043151855 elapsed, loss: 1.3820813e-06\n",
      "step: 128760 train: 0.07976984977722168 elapsed, loss: 1.7643886e-06\n",
      "step: 128770 train: 0.07896018028259277 elapsed, loss: 5.802104e-06\n",
      "step: 128780 train: 0.08466434478759766 elapsed, loss: 2.7250449e-06\n",
      "step: 128790 train: 0.08208823204040527 elapsed, loss: 2.7501774e-06\n",
      "step: 128800 train: 0.08173561096191406 elapsed, loss: 2.0940736e-06\n",
      "step: 128810 train: 0.08379316329956055 elapsed, loss: 1.6540273e-06\n",
      "step: 128820 train: 0.08666467666625977 elapsed, loss: 1.4603121e-06\n",
      "step: 128830 train: 0.08410167694091797 elapsed, loss: 0.002474374\n",
      "step: 128840 train: 0.07921624183654785 elapsed, loss: 0.002520901\n",
      "step: 128850 train: 0.07436633110046387 elapsed, loss: 3.6341367e-05\n",
      "step: 128860 train: 0.08237648010253906 elapsed, loss: 2.0160023e-05\n",
      "step: 128870 train: 0.08169770240783691 elapsed, loss: 1.0746844e-05\n",
      "step: 128880 train: 0.07463717460632324 elapsed, loss: 2.1370452e-05\n",
      "step: 128890 train: 0.08315801620483398 elapsed, loss: 8.128507e-06\n",
      "step: 128900 train: 0.08234429359436035 elapsed, loss: 5.8076957e-06\n",
      "step: 128910 train: 0.08250808715820312 elapsed, loss: 5.258208e-06\n",
      "step: 128920 train: 0.08577609062194824 elapsed, loss: 4.0782443e-06\n",
      "step: 128930 train: 0.07724332809448242 elapsed, loss: 4.861943e-06\n",
      "step: 128940 train: 0.07701706886291504 elapsed, loss: 3.4198074e-06\n",
      "step: 128950 train: 0.07882475852966309 elapsed, loss: 3.149724e-06\n",
      "step: 128960 train: 0.0793304443359375 elapsed, loss: 2.1425046e-06\n",
      "step: 128970 train: 0.08774828910827637 elapsed, loss: 1.6912797e-06\n",
      "step: 128980 train: 0.08164811134338379 elapsed, loss: 1.6177056e-06\n",
      "step: 128990 train: 0.08596253395080566 elapsed, loss: 1.2805675e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 129000 train: 0.08081674575805664 elapsed, loss: 1.8752155e-06\n",
      "step: 129010 train: 0.08072042465209961 elapsed, loss: 1.3587983e-06\n",
      "step: 129020 train: 0.07651042938232422 elapsed, loss: 1.528764e-06\n",
      "step: 129030 train: 0.07740902900695801 elapsed, loss: 1.2628724e-06\n",
      "step: 129040 train: 0.0828709602355957 elapsed, loss: 1.5222449e-06\n",
      "step: 129050 train: 0.08598804473876953 elapsed, loss: 1.6554243e-06\n",
      "step: 129060 train: 0.0811913013458252 elapsed, loss: 1.4253878e-06\n",
      "step: 129070 train: 0.08019781112670898 elapsed, loss: 1.4682286e-06\n",
      "step: 129080 train: 0.08037614822387695 elapsed, loss: 1.3909289e-06\n",
      "step: 129090 train: 0.08574485778808594 elapsed, loss: 1.6950007e-06\n",
      "step: 129100 train: 0.0844411849975586 elapsed, loss: 2.4177084e-06\n",
      "step: 129110 train: 0.08205628395080566 elapsed, loss: 1.4933743e-06\n",
      "step: 129120 train: 0.07936787605285645 elapsed, loss: 1.3997766e-06\n",
      "step: 129130 train: 0.07865190505981445 elapsed, loss: 1.5203827e-06\n",
      "step: 129140 train: 0.08218526840209961 elapsed, loss: 1.722945e-06\n",
      "step: 129150 train: 0.09457278251647949 elapsed, loss: 1.2223593e-06\n",
      "step: 129160 train: 0.0766294002532959 elapsed, loss: 2.1327255e-06\n",
      "step: 129170 train: 0.08559060096740723 elapsed, loss: 1.6596149e-06\n",
      "step: 129180 train: 0.08293294906616211 elapsed, loss: 1.3844099e-06\n",
      "step: 129190 train: 0.07654643058776855 elapsed, loss: 6.659313e-05\n",
      "step: 129200 train: 0.08392810821533203 elapsed, loss: 9.296293e-05\n",
      "step: 129210 train: 0.07911205291748047 elapsed, loss: 6.246679e-05\n",
      "step: 129220 train: 0.08308529853820801 elapsed, loss: 0.0007754434\n",
      "step: 129230 train: 0.0842580795288086 elapsed, loss: 3.627456e-05\n",
      "step: 129240 train: 0.08188724517822266 elapsed, loss: 2.3793535e-05\n",
      "step: 129250 train: 0.08007144927978516 elapsed, loss: 2.0758409e-05\n",
      "step: 129260 train: 0.07812833786010742 elapsed, loss: 0.00029010832\n",
      "step: 129270 train: 0.08417606353759766 elapsed, loss: 9.706995e-06\n",
      "step: 129280 train: 0.08038997650146484 elapsed, loss: 1.0058449e-05\n",
      "step: 129290 train: 0.08228278160095215 elapsed, loss: 6.3357425e-06\n",
      "step: 129300 train: 0.0798957347869873 elapsed, loss: 4.5196916e-06\n",
      "step: 129310 train: 0.08071565628051758 elapsed, loss: 3.8281887e-06\n",
      "step: 129320 train: 0.08275318145751953 elapsed, loss: 2.2789413e-06\n",
      "step: 129330 train: 0.08655023574829102 elapsed, loss: 2.26078e-06\n",
      "step: 129340 train: 0.07883358001708984 elapsed, loss: 3.1617856e-06\n",
      "step: 129350 train: 0.0833425521850586 elapsed, loss: 1.7173566e-06\n",
      "step: 129360 train: 0.07652616500854492 elapsed, loss: 1.8398255e-06\n",
      "step: 129370 train: 0.08036947250366211 elapsed, loss: 1.2074587e-06\n",
      "step: 129380 train: 0.08006811141967773 elapsed, loss: 1.0635695e-06\n",
      "step: 129390 train: 0.07645034790039062 elapsed, loss: 1.4533275e-06\n",
      "step: 129400 train: 0.0783548355102539 elapsed, loss: 1.8733531e-06\n",
      "step: 129410 train: 0.07649993896484375 elapsed, loss: 1.5706732e-06\n",
      "step: 129420 train: 0.08376288414001465 elapsed, loss: 8.959316e-07\n",
      "step: 129430 train: 0.08256673812866211 elapsed, loss: 1.0686917e-06\n",
      "step: 129440 train: 0.08022570610046387 elapsed, loss: 1.1804505e-06\n",
      "step: 129450 train: 0.08573484420776367 elapsed, loss: 1.2451771e-06\n",
      "step: 129460 train: 0.08172082901000977 elapsed, loss: 1.2367955e-06\n",
      "step: 129470 train: 0.08106875419616699 elapsed, loss: 1.1906948e-06\n",
      "step: 129480 train: 0.08506369590759277 elapsed, loss: 1.0062934e-06\n",
      "step: 129490 train: 0.07986760139465332 elapsed, loss: 1.2819644e-06\n",
      "step: 129500 train: 0.0837242603302002 elapsed, loss: 1.0738141e-06\n",
      "step: 129510 train: 0.08219051361083984 elapsed, loss: 9.70903e-07\n",
      "step: 129520 train: 0.07325005531311035 elapsed, loss: 1.8337718e-06\n",
      "step: 129530 train: 0.07668566703796387 elapsed, loss: 1.316889e-06\n",
      "step: 129540 train: 0.07992029190063477 elapsed, loss: 1.5269018e-06\n",
      "step: 129550 train: 0.08408093452453613 elapsed, loss: 1.1422662e-06\n",
      "step: 129560 train: 0.07932734489440918 elapsed, loss: 1.8477419e-06\n",
      "step: 129570 train: 0.08717203140258789 elapsed, loss: 9.383069e-07\n",
      "step: 129580 train: 0.08115792274475098 elapsed, loss: 1.9380795e-06\n",
      "step: 129590 train: 0.08473968505859375 elapsed, loss: 1.2754447e-06\n",
      "step: 129600 train: 0.08584070205688477 elapsed, loss: 1.8626409e-06\n",
      "step: 129610 train: 0.07987618446350098 elapsed, loss: 1.8444815e-06\n",
      "step: 129620 train: 0.0763096809387207 elapsed, loss: 1.6861576e-06\n",
      "step: 129630 train: 0.08101725578308105 elapsed, loss: 1.2754452e-06\n",
      "step: 129640 train: 0.09207534790039062 elapsed, loss: 1.0654322e-06\n",
      "step: 129650 train: 0.07804656028747559 elapsed, loss: 1.4356325e-06\n",
      "step: 129660 train: 0.07891249656677246 elapsed, loss: 1.2884835e-06\n",
      "step: 129670 train: 0.0806117057800293 elapsed, loss: 1.6461107e-06\n",
      "step: 129680 train: 0.08074569702148438 elapsed, loss: 1.7848777e-06\n",
      "step: 129690 train: 0.086090087890625 elapsed, loss: 1.2121154e-06\n",
      "step: 129700 train: 0.08661484718322754 elapsed, loss: 1.4742819e-06\n",
      "step: 129710 train: 0.08050131797790527 elapsed, loss: 2.100595e-06\n",
      "step: 129720 train: 0.07917261123657227 elapsed, loss: 1.6838295e-06\n",
      "step: 129730 train: 0.07776093482971191 elapsed, loss: 1.9283011e-06\n",
      "step: 129740 train: 0.08242511749267578 elapsed, loss: 1.7061807e-06\n",
      "step: 129750 train: 0.08239912986755371 elapsed, loss: 4.3874406e-06\n",
      "step: 129760 train: 0.08900904655456543 elapsed, loss: 1.7471591e-06\n",
      "step: 129770 train: 0.08036279678344727 elapsed, loss: 1.454724e-06\n",
      "step: 129780 train: 0.0846261978149414 elapsed, loss: 1.4258535e-06\n",
      "step: 129790 train: 0.0849604606628418 elapsed, loss: 1.4170059e-06\n",
      "step: 129800 train: 0.08127832412719727 elapsed, loss: 1.5720709e-06\n",
      "step: 129810 train: 0.08357691764831543 elapsed, loss: 1.3629895e-06\n",
      "step: 129820 train: 0.07706642150878906 elapsed, loss: 1.8812691e-06\n",
      "step: 129830 train: 0.07940125465393066 elapsed, loss: 1.9683475e-06\n",
      "step: 129840 train: 0.08002376556396484 elapsed, loss: 2.0814678e-06\n",
      "step: 129850 train: 0.08439350128173828 elapsed, loss: 1.5669484e-06\n",
      "step: 129860 train: 0.0886225700378418 elapsed, loss: 1.8351689e-06\n",
      "step: 129870 train: 0.08963656425476074 elapsed, loss: 1.4421516e-06\n",
      "step: 129880 train: 0.08298611640930176 elapsed, loss: 1.633538e-06\n",
      "step: 129890 train: 0.08507227897644043 elapsed, loss: 1.4672974e-06\n",
      "step: 129900 train: 0.08184981346130371 elapsed, loss: 2.0246916e-06\n",
      "step: 129910 train: 0.07651519775390625 elapsed, loss: 2.3483258e-06\n",
      "step: 129920 train: 0.07862067222595215 elapsed, loss: 2.0367997e-06\n",
      "step: 129930 train: 0.08082985877990723 elapsed, loss: 2.0149141e-06\n",
      "step: 129940 train: 0.08373498916625977 elapsed, loss: 0.00017853064\n",
      "step: 129950 train: 0.08418583869934082 elapsed, loss: 0.00037035317\n",
      "step: 129960 train: 0.08225536346435547 elapsed, loss: 0.00032146194\n",
      "step: 129970 train: 0.08989500999450684 elapsed, loss: 5.0072333e-05\n",
      "step: 129980 train: 0.08104729652404785 elapsed, loss: 3.0629224e-05\n",
      "step: 129990 train: 0.08313870429992676 elapsed, loss: 2.5720374e-05\n",
      "step: 130000 train: 0.08080339431762695 elapsed, loss: 2.1020765e-05\n",
      "step: 130010 train: 0.08066105842590332 elapsed, loss: 1.9517558e-05\n",
      "step: 130020 train: 0.0747077465057373 elapsed, loss: 1.5346499e-05\n",
      "step: 130030 train: 0.08129167556762695 elapsed, loss: 1.03220755e-05\n",
      "step: 130040 train: 0.08005237579345703 elapsed, loss: 4.980689e-06\n",
      "step: 130050 train: 0.07761216163635254 elapsed, loss: 5.098958e-06\n",
      "step: 130060 train: 0.07789802551269531 elapsed, loss: 4.167655e-06\n",
      "step: 130070 train: 0.08230090141296387 elapsed, loss: 2.8652073e-06\n",
      "step: 130080 train: 0.07970356941223145 elapsed, loss: 2.784183e-06\n",
      "step: 130090 train: 0.08593416213989258 elapsed, loss: 1.8454128e-06\n",
      "step: 130100 train: 0.07535076141357422 elapsed, loss: 2.59185e-06\n",
      "step: 130110 train: 0.08416986465454102 elapsed, loss: 2.1550659e-06\n",
      "step: 130120 train: 0.08168911933898926 elapsed, loss: 1.163221e-06\n",
      "step: 130130 train: 0.07967782020568848 elapsed, loss: 1.3075753e-06\n",
      "step: 130140 train: 0.08096814155578613 elapsed, loss: 1.4249222e-06\n",
      "step: 130150 train: 0.07646489143371582 elapsed, loss: 1.6726534e-06\n",
      "step: 130160 train: 0.0803678035736084 elapsed, loss: 1.0980283e-06\n",
      "step: 130170 train: 0.0790853500366211 elapsed, loss: 1.2060616e-06\n",
      "step: 130180 train: 0.08349084854125977 elapsed, loss: 9.573989e-07\n",
      "step: 130190 train: 0.08171653747558594 elapsed, loss: 2.005995e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 130200 train: 0.08031916618347168 elapsed, loss: 1.3480882e-06\n",
      "step: 130210 train: 0.07714366912841797 elapsed, loss: 1.3974484e-06\n",
      "step: 130220 train: 0.08425784111022949 elapsed, loss: 9.913922e-07\n",
      "step: 130230 train: 0.08186793327331543 elapsed, loss: 1.2475057e-06\n",
      "step: 130240 train: 0.0795440673828125 elapsed, loss: 1.5306271e-06\n",
      "step: 130250 train: 0.08223652839660645 elapsed, loss: 1.7308607e-06\n",
      "step: 130260 train: 0.08589911460876465 elapsed, loss: 1.2582158e-06\n",
      "step: 130270 train: 0.08194613456726074 elapsed, loss: 1.3695085e-06\n",
      "step: 130280 train: 0.08711004257202148 elapsed, loss: 1.0444775e-06\n",
      "step: 130290 train: 0.08432769775390625 elapsed, loss: 2.0381954e-06\n",
      "step: 130300 train: 0.07573080062866211 elapsed, loss: 1.6475079e-06\n",
      "step: 130310 train: 0.08205699920654297 elapsed, loss: 1.1860378e-06\n",
      "step: 130320 train: 0.08014559745788574 elapsed, loss: 1.4044331e-06\n",
      "step: 130330 train: 0.08639001846313477 elapsed, loss: 1.2889492e-06\n",
      "step: 130340 train: 0.07890701293945312 elapsed, loss: 1.6097894e-06\n",
      "step: 130350 train: 0.08063983917236328 elapsed, loss: 1.2656664e-06\n",
      "step: 130360 train: 0.07658791542053223 elapsed, loss: 1.5883691e-06\n",
      "step: 130370 train: 0.08320212364196777 elapsed, loss: 1.3993108e-06\n",
      "step: 130380 train: 0.0758066177368164 elapsed, loss: 3.030508e-06\n",
      "step: 130390 train: 0.08453226089477539 elapsed, loss: 1.312698e-06\n",
      "step: 130400 train: 0.08164620399475098 elapsed, loss: 1.435632e-06\n",
      "step: 130410 train: 0.07418370246887207 elapsed, loss: 1.9865086e-06\n",
      "step: 130420 train: 0.08327817916870117 elapsed, loss: 1.7173556e-06\n",
      "step: 130430 train: 0.0811927318572998 elapsed, loss: 1.6358662e-06\n",
      "step: 130440 train: 0.08385467529296875 elapsed, loss: 2.113633e-06\n",
      "step: 130450 train: 0.07978582382202148 elapsed, loss: 3.4384339e-06\n",
      "step: 130460 train: 0.07396125793457031 elapsed, loss: 2.135054e-06\n",
      "step: 130470 train: 0.08112144470214844 elapsed, loss: 1.728067e-06\n",
      "step: 130480 train: 0.08181953430175781 elapsed, loss: 1.9664853e-06\n",
      "step: 130490 train: 0.0858161449432373 elapsed, loss: 2.286392e-06\n",
      "step: 130500 train: 0.08189201354980469 elapsed, loss: 2.3567077e-06\n",
      "step: 130510 train: 0.08704543113708496 elapsed, loss: 1.4402891e-06\n",
      "step: 130520 train: 0.08038449287414551 elapsed, loss: 1.881267e-06\n",
      "step: 130530 train: 0.08253884315490723 elapsed, loss: 1.547391e-06\n",
      "step: 130540 train: 0.08236813545227051 elapsed, loss: 2.0274865e-06\n",
      "step: 130550 train: 0.08784341812133789 elapsed, loss: 1.3806844e-06\n",
      "step: 130560 train: 0.08530569076538086 elapsed, loss: 0.00027230033\n",
      "step: 130570 train: 0.07885360717773438 elapsed, loss: 0.0006434949\n",
      "step: 130580 train: 0.08736872673034668 elapsed, loss: 7.743909e-05\n",
      "step: 130590 train: 0.08605051040649414 elapsed, loss: 2.1572789e-05\n",
      "step: 130600 train: 0.08084321022033691 elapsed, loss: 1.7674261e-05\n",
      "step: 130610 train: 0.08138585090637207 elapsed, loss: 1.0157809e-05\n",
      "step: 130620 train: 0.08155441284179688 elapsed, loss: 7.139004e-06\n",
      "step: 130630 train: 0.08185338973999023 elapsed, loss: 7.4448863e-06\n",
      "step: 130640 train: 0.08661746978759766 elapsed, loss: 4.9229384e-06\n",
      "step: 130650 train: 0.08727049827575684 elapsed, loss: 3.8328417e-06\n",
      "step: 130660 train: 0.08694076538085938 elapsed, loss: 3.2647395e-06\n",
      "step: 130670 train: 0.0798182487487793 elapsed, loss: 2.6724244e-06\n",
      "step: 130680 train: 0.08085751533508301 elapsed, loss: 2.3026914e-06\n",
      "step: 130690 train: 0.07666873931884766 elapsed, loss: 2.1201527e-06\n",
      "step: 130700 train: 0.07877850532531738 elapsed, loss: 2.3231803e-06\n",
      "step: 130710 train: 0.08076930046081543 elapsed, loss: 1.7913966e-06\n",
      "step: 130720 train: 0.08118343353271484 elapsed, loss: 1.8132823e-06\n",
      "step: 130730 train: 0.08127045631408691 elapsed, loss: 1.624225e-06\n",
      "step: 130740 train: 0.08503603935241699 elapsed, loss: 1.402105e-06\n",
      "step: 130750 train: 0.08671808242797852 elapsed, loss: 2.1257406e-06\n",
      "step: 130760 train: 0.08003544807434082 elapsed, loss: 1.512466e-06\n",
      "step: 130770 train: 0.08133530616760254 elapsed, loss: 1.1748625e-06\n",
      "step: 130780 train: 0.0771946907043457 elapsed, loss: 1.6535614e-06\n",
      "step: 130790 train: 0.07736849784851074 elapsed, loss: 1.5515818e-06\n",
      "step: 130800 train: 0.08291053771972656 elapsed, loss: 1.4980309e-06\n",
      "step: 130810 train: 0.07756853103637695 elapsed, loss: 1.8193366e-06\n",
      "step: 130820 train: 0.07958006858825684 elapsed, loss: 3.3103724e-06\n",
      "step: 130830 train: 0.07277393341064453 elapsed, loss: 2.177429e-06\n",
      "step: 130840 train: 0.08676886558532715 elapsed, loss: 1.3555389e-06\n",
      "step: 130850 train: 0.07862520217895508 elapsed, loss: 1.6894173e-06\n",
      "step: 130860 train: 0.08246231079101562 elapsed, loss: 1.8957047e-06\n",
      "step: 130870 train: 0.09277534484863281 elapsed, loss: 1.2163059e-06\n",
      "step: 130880 train: 0.08048009872436523 elapsed, loss: 2.2645074e-06\n",
      "step: 130890 train: 0.07778310775756836 elapsed, loss: 1.9101403e-06\n",
      "step: 130900 train: 0.08330750465393066 elapsed, loss: 1.1376097e-06\n",
      "step: 130910 train: 0.08729934692382812 elapsed, loss: 1.6335377e-06\n",
      "step: 130920 train: 0.07869315147399902 elapsed, loss: 2.1075803e-06\n",
      "step: 130930 train: 0.07515597343444824 elapsed, loss: 2.1364508e-06\n",
      "step: 130940 train: 0.08314180374145508 elapsed, loss: 1.4035018e-06\n",
      "step: 130950 train: 0.08152556419372559 elapsed, loss: 1.8840633e-06\n",
      "step: 130960 train: 0.0832822322845459 elapsed, loss: 1.8533299e-06\n",
      "step: 130970 train: 0.08383464813232422 elapsed, loss: 1.8761472e-06\n",
      "step: 130980 train: 0.08738183975219727 elapsed, loss: 1.5431999e-06\n",
      "step: 130990 train: 0.08132457733154297 elapsed, loss: 1.8379627e-06\n",
      "step: 131000 train: 0.08307719230651855 elapsed, loss: 1.6260876e-06\n",
      "step: 131010 train: 0.08473563194274902 elapsed, loss: 2.7478509e-06\n",
      "step: 131020 train: 0.0826263427734375 elapsed, loss: 1.7103721e-06\n",
      "step: 131030 train: 0.08276486396789551 elapsed, loss: 1.6787071e-06\n",
      "step: 131040 train: 0.08554792404174805 elapsed, loss: 1.7690452e-06\n",
      "step: 131050 train: 0.08022665977478027 elapsed, loss: 2.122481e-06\n",
      "step: 131060 train: 0.08189272880554199 elapsed, loss: 2.3450662e-06\n",
      "step: 131070 train: 0.08248662948608398 elapsed, loss: 1.7727705e-06\n",
      "step: 131080 train: 0.08303976058959961 elapsed, loss: 2.101061e-06\n",
      "step: 131090 train: 0.0807187557220459 elapsed, loss: 2.1429705e-06\n",
      "step: 131100 train: 0.08232784271240234 elapsed, loss: 2.1508863e-06\n",
      "step: 131110 train: 0.07384347915649414 elapsed, loss: 2.4689325e-06\n",
      "step: 131120 train: 0.07773303985595703 elapsed, loss: 1.6600809e-06\n",
      "step: 131130 train: 0.08046221733093262 elapsed, loss: 1.6107199e-06\n",
      "step: 131140 train: 0.08727741241455078 elapsed, loss: 1.5064129e-06\n",
      "step: 131150 train: 0.0823664665222168 elapsed, loss: 2.2281856e-06\n",
      "step: 131160 train: 0.08542370796203613 elapsed, loss: 1.7588006e-06\n",
      "step: 131170 train: 0.08245682716369629 elapsed, loss: 2.533659e-06\n",
      "step: 131180 train: 0.0904698371887207 elapsed, loss: 2.1248097e-06\n",
      "step: 131190 train: 0.08459639549255371 elapsed, loss: 1.9287668e-06\n",
      "step: 131200 train: 0.07916259765625 elapsed, loss: 2.6756843e-06\n",
      "step: 131210 train: 0.08146309852600098 elapsed, loss: 2.0908165e-06\n",
      "step: 131220 train: 0.07937192916870117 elapsed, loss: 2.002807e-06\n",
      "step: 131230 train: 0.07988500595092773 elapsed, loss: 1.742037e-06\n",
      "step: 131240 train: 0.08379936218261719 elapsed, loss: 1.8319095e-06\n",
      "step: 131250 train: 0.08437371253967285 elapsed, loss: 2.0056004e-06\n",
      "step: 131260 train: 0.08584833145141602 elapsed, loss: 1.6204995e-06\n",
      "step: 131270 train: 0.07859659194946289 elapsed, loss: 2.2714926e-06\n",
      "step: 131280 train: 0.08223247528076172 elapsed, loss: 1.9995468e-06\n",
      "step: 131290 train: 0.07706046104431152 elapsed, loss: 3.23308e-06\n",
      "step: 131300 train: 0.08288764953613281 elapsed, loss: 2.2277206e-06\n",
      "step: 131310 train: 0.08855724334716797 elapsed, loss: 0.0004944742\n",
      "step: 131320 train: 0.07924652099609375 elapsed, loss: 1.9103218e-05\n",
      "step: 131330 train: 0.0756998062133789 elapsed, loss: 3.1779473e-05\n",
      "step: 131340 train: 0.08089590072631836 elapsed, loss: 9.0700705e-06\n",
      "step: 131350 train: 0.07354283332824707 elapsed, loss: 1.0771117e-05\n",
      "step: 131360 train: 0.08330368995666504 elapsed, loss: 8.22529e-06\n",
      "step: 131370 train: 0.08443474769592285 elapsed, loss: 5.4305174e-06\n",
      "step: 131380 train: 0.08890509605407715 elapsed, loss: 4.0386644e-06\n",
      "step: 131390 train: 0.08057355880737305 elapsed, loss: 3.907813e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 131400 train: 0.0857391357421875 elapsed, loss: 2.9858134e-06\n",
      "step: 131410 train: 0.07849788665771484 elapsed, loss: 4.704082e-06\n",
      "step: 131420 train: 0.08573412895202637 elapsed, loss: 2.454959e-06\n",
      "step: 131430 train: 0.08785319328308105 elapsed, loss: 2.1569388e-06\n",
      "step: 131440 train: 0.08136487007141113 elapsed, loss: 2.1895362e-06\n",
      "step: 131450 train: 0.0773916244506836 elapsed, loss: 2.4032745e-06\n",
      "step: 131460 train: 0.08032608032226562 elapsed, loss: 2.276614e-06\n",
      "step: 131470 train: 0.08106851577758789 elapsed, loss: 1.4207309e-06\n",
      "step: 131480 train: 0.07973051071166992 elapsed, loss: 1.46916e-06\n",
      "step: 131490 train: 0.08328843116760254 elapsed, loss: 1.6144459e-06\n",
      "step: 131500 train: 0.07907295227050781 elapsed, loss: 1.6801041e-06\n",
      "step: 131510 train: 0.07835602760314941 elapsed, loss: 0.00014489645\n",
      "step: 131520 train: 0.07889914512634277 elapsed, loss: 0.0015831585\n",
      "step: 131530 train: 0.08419322967529297 elapsed, loss: 0.0011229808\n",
      "step: 131540 train: 0.0778801441192627 elapsed, loss: 0.000121675475\n",
      "step: 131550 train: 0.08358311653137207 elapsed, loss: 4.541709e-05\n",
      "step: 131560 train: 0.09078288078308105 elapsed, loss: 1.3941103e-05\n",
      "step: 131570 train: 0.08161664009094238 elapsed, loss: 1.6507802e-05\n",
      "step: 131580 train: 0.08533334732055664 elapsed, loss: 1.4560116e-05\n",
      "step: 131590 train: 0.07912707328796387 elapsed, loss: 1.34766615e-05\n",
      "step: 131600 train: 0.08129572868347168 elapsed, loss: 5.7960215e-06\n",
      "step: 131610 train: 0.08626914024353027 elapsed, loss: 6.952002e-06\n",
      "step: 131620 train: 0.08182787895202637 elapsed, loss: 3.8458847e-06\n",
      "step: 131630 train: 0.08583998680114746 elapsed, loss: 3.1925665e-06\n",
      "step: 131640 train: 0.08155632019042969 elapsed, loss: 9.335011e-06\n",
      "step: 131650 train: 0.08085513114929199 elapsed, loss: 3.0686938e-06\n",
      "step: 131660 train: 0.0811011791229248 elapsed, loss: 3.0319115e-06\n",
      "step: 131670 train: 0.0824134349822998 elapsed, loss: 1.6465758e-06\n",
      "step: 131680 train: 0.0797414779663086 elapsed, loss: 1.4770761e-06\n",
      "step: 131690 train: 0.07612061500549316 elapsed, loss: 1.6149115e-06\n",
      "step: 131700 train: 0.08695864677429199 elapsed, loss: 1.2023364e-06\n",
      "step: 131710 train: 0.08095860481262207 elapsed, loss: 1.2353983e-06\n",
      "step: 131720 train: 0.07976102828979492 elapsed, loss: 1.6200338e-06\n",
      "step: 131730 train: 0.08545899391174316 elapsed, loss: 1.1837101e-06\n",
      "step: 131740 train: 0.08253240585327148 elapsed, loss: 0.00076675724\n",
      "step: 131750 train: 0.07773041725158691 elapsed, loss: 0.00010497794\n",
      "step: 131760 train: 0.08511710166931152 elapsed, loss: 6.1559986e-05\n",
      "step: 131770 train: 0.0803825855255127 elapsed, loss: 1.7803675e-05\n",
      "step: 131780 train: 0.07966208457946777 elapsed, loss: 1.8954175e-05\n",
      "step: 131790 train: 0.07655787467956543 elapsed, loss: 1.4974905e-05\n",
      "step: 131800 train: 0.083648681640625 elapsed, loss: 6.273812e-06\n",
      "step: 131810 train: 0.08054924011230469 elapsed, loss: 6.04005e-06\n",
      "step: 131820 train: 0.08053779602050781 elapsed, loss: 5.251699e-06\n",
      "step: 131830 train: 0.08001565933227539 elapsed, loss: 5.6921863e-06\n",
      "step: 131840 train: 0.07994532585144043 elapsed, loss: 3.2796365e-06\n",
      "step: 131850 train: 0.0820932388305664 elapsed, loss: 4.13924e-06\n",
      "step: 131860 train: 0.07826089859008789 elapsed, loss: 2.275683e-06\n",
      "step: 131870 train: 0.08060550689697266 elapsed, loss: 2.2975685e-06\n",
      "step: 131880 train: 0.07847309112548828 elapsed, loss: 2.1103735e-06\n",
      "step: 131890 train: 0.08075976371765137 elapsed, loss: 1.4873208e-06\n",
      "step: 131900 train: 0.07803583145141602 elapsed, loss: 1.8631083e-06\n",
      "step: 131910 train: 0.08497500419616699 elapsed, loss: 1.4128148e-06\n",
      "step: 131920 train: 0.08317327499389648 elapsed, loss: 1.4500679e-06\n",
      "step: 131930 train: 0.08323931694030762 elapsed, loss: 1.3043158e-06\n",
      "step: 131940 train: 0.07881498336791992 elapsed, loss: 1.797916e-06\n",
      "step: 131950 train: 0.0807352066040039 elapsed, loss: 1.3904632e-06\n",
      "step: 131960 train: 0.07702779769897461 elapsed, loss: 1.3131637e-06\n",
      "step: 131970 train: 0.08652496337890625 elapsed, loss: 1.1790531e-06\n",
      "step: 131980 train: 0.07903718948364258 elapsed, loss: 1.0961659e-06\n",
      "step: 131990 train: 0.08705329895019531 elapsed, loss: 8.8708424e-07\n",
      "step: 132000 train: 0.08155012130737305 elapsed, loss: 1.2144436e-06\n",
      "step: 132010 train: 0.08372926712036133 elapsed, loss: 1.0072247e-06\n",
      "step: 132020 train: 0.08169913291931152 elapsed, loss: 1.5101382e-06\n",
      "step: 132030 train: 0.08468914031982422 elapsed, loss: 1.2367955e-06\n",
      "step: 132040 train: 0.08326196670532227 elapsed, loss: 1.1273652e-06\n",
      "step: 132050 train: 0.08343243598937988 elapsed, loss: 1.2419177e-06\n",
      "step: 132060 train: 0.08301806449890137 elapsed, loss: 1.6873282e-05\n",
      "step: 132070 train: 0.08316850662231445 elapsed, loss: 5.014628e-05\n",
      "step: 132080 train: 0.09317326545715332 elapsed, loss: 5.3108333e-06\n",
      "step: 132090 train: 0.07980871200561523 elapsed, loss: 1.4247518e-05\n",
      "step: 132100 train: 0.08283090591430664 elapsed, loss: 7.0831106e-06\n",
      "step: 132110 train: 0.08126449584960938 elapsed, loss: 6.2453882e-06\n",
      "step: 132120 train: 0.08019781112670898 elapsed, loss: 5.016045e-06\n",
      "step: 132130 train: 0.08091187477111816 elapsed, loss: 4.7152616e-06\n",
      "step: 132140 train: 0.08077216148376465 elapsed, loss: 3.7867453e-06\n",
      "step: 132150 train: 0.0816950798034668 elapsed, loss: 3.4388952e-06\n",
      "step: 132160 train: 0.08021712303161621 elapsed, loss: 2.9490218e-06\n",
      "step: 132170 train: 0.07755661010742188 elapsed, loss: 2.1820852e-06\n",
      "step: 132180 train: 0.08133673667907715 elapsed, loss: 2.0167763e-06\n",
      "step: 132190 train: 0.0785825252532959 elapsed, loss: 1.822596e-06\n",
      "step: 132200 train: 0.08434605598449707 elapsed, loss: 1.537612e-06\n",
      "step: 132210 train: 0.08042716979980469 elapsed, loss: 1.4384261e-06\n",
      "step: 132220 train: 0.07938909530639648 elapsed, loss: 1.783946e-06\n",
      "step: 132230 train: 0.08147406578063965 elapsed, loss: 1.3671804e-06\n",
      "step: 132240 train: 0.07644963264465332 elapsed, loss: 1.7783561e-06\n",
      "step: 132250 train: 0.0816354751586914 elapsed, loss: 1.453326e-06\n",
      "step: 132260 train: 0.09010839462280273 elapsed, loss: 1.094769e-06\n",
      "step: 132270 train: 0.0794527530670166 elapsed, loss: 1.2898806e-06\n",
      "step: 132280 train: 0.0825507640838623 elapsed, loss: 1.608856e-06\n",
      "step: 132290 train: 0.0848090648651123 elapsed, loss: 1.1404036e-06\n",
      "step: 132300 train: 0.07545614242553711 elapsed, loss: 1.3806828e-06\n",
      "step: 132310 train: 0.07938551902770996 elapsed, loss: 1.6083924e-06\n",
      "step: 132320 train: 0.08404541015625 elapsed, loss: 1.3010565e-06\n",
      "step: 132330 train: 0.08833122253417969 elapsed, loss: 2.3608955e-06\n",
      "step: 132340 train: 0.07855892181396484 elapsed, loss: 1.9133995e-06\n",
      "step: 132350 train: 0.08463168144226074 elapsed, loss: 1.3303932e-06\n",
      "step: 132360 train: 0.08303546905517578 elapsed, loss: 1.2270166e-06\n",
      "step: 132370 train: 0.08507180213928223 elapsed, loss: 1.7690452e-06\n",
      "step: 132380 train: 0.08333516120910645 elapsed, loss: 1.4612438e-06\n",
      "step: 132390 train: 0.07848882675170898 elapsed, loss: 1.6749818e-06\n",
      "step: 132400 train: 0.07878351211547852 elapsed, loss: 1.7546099e-06\n",
      "step: 132410 train: 0.0883474349975586 elapsed, loss: 1.249834e-06\n",
      "step: 132420 train: 0.08956789970397949 elapsed, loss: 1.5324897e-06\n",
      "step: 132430 train: 0.07908487319946289 elapsed, loss: 2.5108411e-06\n",
      "step: 132440 train: 0.07930183410644531 elapsed, loss: 1.4798702e-06\n",
      "step: 132450 train: 0.08044815063476562 elapsed, loss: 1.9082777e-06\n",
      "step: 132460 train: 0.07657098770141602 elapsed, loss: 1.6787071e-06\n",
      "step: 132470 train: 0.08339095115661621 elapsed, loss: 1.954843e-06\n",
      "step: 132480 train: 0.07909750938415527 elapsed, loss: 1.8510015e-06\n",
      "step: 132490 train: 0.08681154251098633 elapsed, loss: 3.1065001e-06\n",
      "step: 132500 train: 0.08624267578125 elapsed, loss: 6.483353e-06\n",
      "step: 132510 train: 0.07841968536376953 elapsed, loss: 5.8565747e-06\n",
      "step: 132520 train: 0.08140897750854492 elapsed, loss: 3.7676518e-06\n",
      "step: 132530 train: 0.08685898780822754 elapsed, loss: 3.037968e-06\n",
      "step: 132540 train: 0.08183598518371582 elapsed, loss: 2.6808066e-06\n",
      "step: 132550 train: 0.08356142044067383 elapsed, loss: 2.1476264e-06\n",
      "step: 132560 train: 0.08221197128295898 elapsed, loss: 1.6731192e-06\n",
      "step: 132570 train: 0.08685493469238281 elapsed, loss: 1.6833619e-06\n",
      "step: 132580 train: 0.07527995109558105 elapsed, loss: 2.1331866e-06\n",
      "step: 132590 train: 0.07965636253356934 elapsed, loss: 1.3266673e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 132600 train: 0.0826723575592041 elapsed, loss: 2.6849948e-05\n",
      "step: 132610 train: 0.07733750343322754 elapsed, loss: 0.2088709\n",
      "step: 132620 train: 0.08464264869689941 elapsed, loss: 8.1268954e-05\n",
      "step: 132630 train: 0.08228039741516113 elapsed, loss: 2.2562755e-05\n",
      "step: 132640 train: 0.08179163932800293 elapsed, loss: 1.7406363e-05\n",
      "step: 132650 train: 0.07784295082092285 elapsed, loss: 1.8740455e-05\n",
      "step: 132660 train: 0.08172965049743652 elapsed, loss: 1.0858564e-05\n",
      "step: 132670 train: 0.08173942565917969 elapsed, loss: 6.9308535e-06\n",
      "step: 132680 train: 0.08414435386657715 elapsed, loss: 6.7911606e-06\n",
      "step: 132690 train: 0.07993412017822266 elapsed, loss: 5.870097e-06\n",
      "step: 132700 train: 0.0827186107635498 elapsed, loss: 4.631914e-06\n",
      "step: 132710 train: 0.08146166801452637 elapsed, loss: 3.3671884e-06\n",
      "step: 132720 train: 0.0821375846862793 elapsed, loss: 2.428419e-06\n",
      "step: 132730 train: 0.08430027961730957 elapsed, loss: 2.5690467e-06\n",
      "step: 132740 train: 0.08162927627563477 elapsed, loss: 2.0288833e-06\n",
      "step: 132750 train: 0.08098006248474121 elapsed, loss: 1.5082753e-06\n",
      "step: 132760 train: 0.08509659767150879 elapsed, loss: 1.6316751e-06\n",
      "step: 132770 train: 0.08245587348937988 elapsed, loss: 1.4877864e-06\n",
      "step: 132780 train: 0.07893919944763184 elapsed, loss: 1.2377268e-06\n",
      "step: 132790 train: 0.07642292976379395 elapsed, loss: 2.30874e-06\n",
      "step: 132800 train: 0.0866091251373291 elapsed, loss: 1.0565847e-06\n",
      "step: 132810 train: 0.08092403411865234 elapsed, loss: 1.484061e-06\n",
      "step: 132820 train: 0.07753777503967285 elapsed, loss: 1.198611e-06\n",
      "step: 132830 train: 0.08543968200683594 elapsed, loss: 1.1823125e-06\n",
      "step: 132840 train: 0.08593535423278809 elapsed, loss: 1.2586813e-06\n",
      "step: 132850 train: 0.08536338806152344 elapsed, loss: 9.1874915e-07\n",
      "step: 132860 train: 0.0789022445678711 elapsed, loss: 1.3224753e-06\n",
      "step: 132870 train: 0.07975602149963379 elapsed, loss: 1.5022218e-06\n",
      "step: 132880 train: 0.08008575439453125 elapsed, loss: 8.904758e-06\n",
      "step: 132890 train: 0.08261275291442871 elapsed, loss: 4.1452854e-06\n",
      "step: 132900 train: 0.0802164077758789 elapsed, loss: 2.5536792e-06\n",
      "step: 132910 train: 0.08351922035217285 elapsed, loss: 1.8128169e-06\n",
      "step: 132920 train: 0.08230042457580566 elapsed, loss: 1.8901162e-06\n",
      "step: 132930 train: 0.08655452728271484 elapsed, loss: 1.4915081e-06\n",
      "step: 132940 train: 0.08188319206237793 elapsed, loss: 1.3005908e-06\n",
      "step: 132950 train: 0.08426022529602051 elapsed, loss: 1.2195655e-06\n",
      "step: 132960 train: 0.07738542556762695 elapsed, loss: 1.5771916e-06\n",
      "step: 132970 train: 0.08005213737487793 elapsed, loss: 1.1972143e-06\n",
      "step: 132980 train: 0.07913589477539062 elapsed, loss: 1.5171227e-06\n",
      "step: 132990 train: 0.08149528503417969 elapsed, loss: 1.3425004e-06\n",
      "step: 133000 train: 0.08075475692749023 elapsed, loss: 1.5231759e-06\n",
      "step: 133010 train: 0.08707571029663086 elapsed, loss: 1.2349326e-06\n",
      "step: 133020 train: 0.07589077949523926 elapsed, loss: 1.941805e-06\n",
      "step: 133030 train: 0.08264517784118652 elapsed, loss: 1.2838271e-06\n",
      "step: 133040 train: 0.08109092712402344 elapsed, loss: 1.3839442e-06\n",
      "step: 133050 train: 0.08306145668029785 elapsed, loss: 1.5813841e-06\n",
      "step: 133060 train: 0.07732772827148438 elapsed, loss: 1.81468e-06\n",
      "step: 133070 train: 0.08307361602783203 elapsed, loss: 1.9166546e-06\n",
      "step: 133080 train: 0.083984375 elapsed, loss: 1.319683e-06\n",
      "step: 133090 train: 0.0754547119140625 elapsed, loss: 1.9180557e-06\n",
      "step: 133100 train: 0.08548998832702637 elapsed, loss: 1.4477382e-06\n",
      "step: 133110 train: 0.08277320861816406 elapsed, loss: 1.781618e-06\n",
      "step: 133120 train: 0.08822107315063477 elapsed, loss: 1.254956e-06\n",
      "step: 133130 train: 0.08069729804992676 elapsed, loss: 2.4218991e-06\n",
      "step: 133140 train: 0.0811004638671875 elapsed, loss: 1.418403e-06\n",
      "step: 133150 train: 0.08469009399414062 elapsed, loss: 1.3057132e-06\n",
      "step: 133160 train: 0.07758808135986328 elapsed, loss: 1.6069955e-06\n",
      "step: 133170 train: 0.08572673797607422 elapsed, loss: 1.668928e-06\n",
      "step: 133180 train: 0.09034919738769531 elapsed, loss: 1.3178203e-06\n",
      "step: 133190 train: 0.07632184028625488 elapsed, loss: 1.9310946e-06\n",
      "step: 133200 train: 0.07700562477111816 elapsed, loss: 1.6251563e-06\n",
      "step: 133210 train: 0.07568550109863281 elapsed, loss: 6.1888577e-06\n",
      "step: 133220 train: 0.08121633529663086 elapsed, loss: 2.6868606e-06\n",
      "step: 133230 train: 0.08295917510986328 elapsed, loss: 2.1615967e-06\n",
      "step: 133240 train: 0.07758235931396484 elapsed, loss: 2.0712587e-06\n",
      "step: 133250 train: 0.07625889778137207 elapsed, loss: 1.9413396e-06\n",
      "step: 133260 train: 0.07624936103820801 elapsed, loss: 1.927835e-06\n",
      "step: 133270 train: 0.07996392250061035 elapsed, loss: 2.1494884e-06\n",
      "step: 133280 train: 0.08585524559020996 elapsed, loss: 1.4388921e-06\n",
      "step: 133290 train: 0.07761240005493164 elapsed, loss: 1.7709079e-06\n",
      "step: 133300 train: 0.07847189903259277 elapsed, loss: 2.144833e-06\n",
      "step: 133310 train: 0.08045053482055664 elapsed, loss: 2.0870912e-06\n",
      "step: 133320 train: 0.08288073539733887 elapsed, loss: 2.4437868e-06\n",
      "step: 133330 train: 0.08680319786071777 elapsed, loss: 1.620034e-06\n",
      "step: 133340 train: 0.08257699012756348 elapsed, loss: 2.0740522e-06\n",
      "step: 133350 train: 0.07989287376403809 elapsed, loss: 1.9203849e-06\n",
      "step: 133360 train: 0.09058260917663574 elapsed, loss: 1.8253902e-06\n",
      "step: 133370 train: 0.08146357536315918 elapsed, loss: 2.0437833e-06\n",
      "step: 133380 train: 0.08852815628051758 elapsed, loss: 2.0787093e-06\n",
      "step: 133390 train: 0.07602167129516602 elapsed, loss: 2.555545e-06\n",
      "step: 133400 train: 0.08207440376281738 elapsed, loss: 1.648439e-06\n",
      "step: 133410 train: 0.08241534233093262 elapsed, loss: 2.0768468e-06\n",
      "step: 133420 train: 0.08718347549438477 elapsed, loss: 2.0498385e-06\n",
      "step: 133430 train: 0.07898187637329102 elapsed, loss: 2.3101425e-06\n",
      "step: 133440 train: 0.07662129402160645 elapsed, loss: 2.0717246e-06\n",
      "step: 133450 train: 0.0848243236541748 elapsed, loss: 1.979989e-06\n",
      "step: 133460 train: 0.08021235466003418 elapsed, loss: 1.7746331e-06\n",
      "step: 133470 train: 0.07995891571044922 elapsed, loss: 2.8023442e-06\n",
      "step: 133480 train: 0.08433079719543457 elapsed, loss: 1.6442482e-06\n",
      "step: 133490 train: 0.07828164100646973 elapsed, loss: 2.542497e-06\n",
      "step: 133500 train: 0.08224797248840332 elapsed, loss: 2.0936102e-06\n",
      "step: 133510 train: 0.08770060539245605 elapsed, loss: 1.6931427e-06\n",
      "step: 133520 train: 0.08196616172790527 elapsed, loss: 1.7965191e-06\n",
      "step: 133530 train: 0.08111977577209473 elapsed, loss: 2.0037378e-06\n",
      "step: 133540 train: 0.0864262580871582 elapsed, loss: 2.2458812e-06\n",
      "step: 133550 train: 0.08308029174804688 elapsed, loss: 2.3115388e-06\n",
      "step: 133560 train: 0.08243608474731445 elapsed, loss: 1.6824326e-06\n",
      "step: 133570 train: 0.08363127708435059 elapsed, loss: 2.3110738e-06\n",
      "step: 133580 train: 0.0775601863861084 elapsed, loss: 2.9257442e-06\n",
      "step: 133590 train: 0.08443140983581543 elapsed, loss: 1.6097895e-06\n",
      "step: 133600 train: 0.07996630668640137 elapsed, loss: 2.0409907e-06\n",
      "step: 133610 train: 0.0823814868927002 elapsed, loss: 1.6447138e-06\n",
      "step: 133620 train: 0.08252882957458496 elapsed, loss: 1.9641568e-06\n",
      "step: 133630 train: 0.08981680870056152 elapsed, loss: 1.4654347e-06\n",
      "step: 133640 train: 0.08091330528259277 elapsed, loss: 1.5269018e-06\n",
      "step: 133650 train: 0.07941436767578125 elapsed, loss: 2.542506e-06\n",
      "step: 133660 train: 0.07877230644226074 elapsed, loss: 2.1965209e-06\n",
      "step: 133670 train: 0.08342647552490234 elapsed, loss: 2.069396e-06\n",
      "step: 133680 train: 0.08248090744018555 elapsed, loss: 2.322715e-06\n",
      "step: 133690 train: 0.08031082153320312 elapsed, loss: 2.3553112e-06\n",
      "step: 133700 train: 0.0806725025177002 elapsed, loss: 2.2300326e-06\n",
      "step: 133710 train: 0.08391880989074707 elapsed, loss: 0.0025125383\n",
      "step: 133720 train: 0.08821487426757812 elapsed, loss: 0.000103618535\n",
      "step: 133730 train: 0.08008217811584473 elapsed, loss: 3.0537398e-05\n",
      "step: 133740 train: 0.08385229110717773 elapsed, loss: 1.9015955e-05\n",
      "step: 133750 train: 0.07885360717773438 elapsed, loss: 1.7163187e-05\n",
      "step: 133760 train: 0.08517837524414062 elapsed, loss: 1.324066e-05\n",
      "step: 133770 train: 0.08162569999694824 elapsed, loss: 8.963412e-06\n",
      "step: 133780 train: 0.08426952362060547 elapsed, loss: 8.069848e-06\n",
      "step: 133790 train: 0.08368086814880371 elapsed, loss: 5.160431e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 133800 train: 0.07407379150390625 elapsed, loss: 1.5488935e-05\n",
      "step: 133810 train: 0.07975053787231445 elapsed, loss: 4.704092e-06\n",
      "step: 133820 train: 0.0797739028930664 elapsed, loss: 4.183952e-06\n",
      "step: 133830 train: 0.0788733959197998 elapsed, loss: 3.2619491e-06\n",
      "step: 133840 train: 0.08079123497009277 elapsed, loss: 2.456359e-06\n",
      "step: 133850 train: 0.08127975463867188 elapsed, loss: 2.7255096e-06\n",
      "step: 133860 train: 0.07877445220947266 elapsed, loss: 2.1071146e-06\n",
      "step: 133870 train: 0.08868145942687988 elapsed, loss: 1.4947713e-06\n",
      "step: 133880 train: 0.08853983879089355 elapsed, loss: 1.5627575e-06\n",
      "step: 133890 train: 0.08190298080444336 elapsed, loss: 1.5106036e-06\n",
      "step: 133900 train: 0.08235406875610352 elapsed, loss: 4.960577e-06\n",
      "step: 133910 train: 0.08288192749023438 elapsed, loss: 1.7727705e-06\n",
      "step: 133920 train: 0.08126568794250488 elapsed, loss: 2.3855728e-06\n",
      "step: 133930 train: 0.0895395278930664 elapsed, loss: 1.524108e-06\n",
      "step: 133940 train: 0.07781076431274414 elapsed, loss: 1.8156102e-06\n",
      "step: 133950 train: 0.08563828468322754 elapsed, loss: 1.3788208e-06\n",
      "step: 133960 train: 0.08661603927612305 elapsed, loss: 1.7173329e-06\n",
      "step: 133970 train: 0.08030271530151367 elapsed, loss: 1.6642716e-06\n",
      "step: 133980 train: 0.07906341552734375 elapsed, loss: 1.3215457e-06\n",
      "step: 133990 train: 0.07322406768798828 elapsed, loss: 2.1178244e-06\n",
      "step: 134000 train: 0.08487892150878906 elapsed, loss: 2.489418e-06\n",
      "step: 134010 train: 0.07809972763061523 elapsed, loss: 2.2374993e-06\n",
      "step: 134020 train: 0.08346104621887207 elapsed, loss: 1.3667147e-06\n",
      "step: 134030 train: 0.08334064483642578 elapsed, loss: 1.4859237e-06\n",
      "step: 134040 train: 0.08194446563720703 elapsed, loss: 1.9441336e-06\n",
      "step: 134050 train: 0.08614325523376465 elapsed, loss: 1.5478568e-06\n",
      "step: 134060 train: 0.0795905590057373 elapsed, loss: 2.0363332e-06\n",
      "step: 134070 train: 0.08004927635192871 elapsed, loss: 1.9683475e-06\n",
      "step: 134080 train: 0.08087539672851562 elapsed, loss: 2.131329e-06\n",
      "step: 134090 train: 0.07929563522338867 elapsed, loss: 1.7727706e-06\n",
      "step: 134100 train: 0.07573986053466797 elapsed, loss: 1.921781e-06\n",
      "step: 134110 train: 0.07571029663085938 elapsed, loss: 2.3315624e-06\n",
      "step: 134120 train: 0.07833194732666016 elapsed, loss: 2.9453017e-06\n",
      "step: 134130 train: 0.08052563667297363 elapsed, loss: 2.179292e-06\n",
      "step: 134140 train: 0.07945036888122559 elapsed, loss: 4.759383e-06\n",
      "step: 134150 train: 0.08927345275878906 elapsed, loss: 1.9012928e-06\n",
      "step: 134160 train: 0.08880352973937988 elapsed, loss: 1.9273693e-06\n",
      "step: 134170 train: 0.08156728744506836 elapsed, loss: 2.2412244e-06\n",
      "step: 134180 train: 0.07898950576782227 elapsed, loss: 1.6959368e-06\n",
      "step: 134190 train: 0.08106493949890137 elapsed, loss: 2.1867427e-06\n",
      "step: 134200 train: 0.08568549156188965 elapsed, loss: 1.4021049e-06\n",
      "step: 134210 train: 0.07861661911010742 elapsed, loss: 2.2738209e-06\n",
      "step: 134220 train: 0.08362102508544922 elapsed, loss: 1.9813863e-06\n",
      "step: 134230 train: 0.07918787002563477 elapsed, loss: 2.3250436e-06\n",
      "step: 134240 train: 0.08022332191467285 elapsed, loss: 1.6428512e-06\n",
      "step: 134250 train: 0.08164691925048828 elapsed, loss: 1.7932587e-06\n",
      "step: 134260 train: 0.08554482460021973 elapsed, loss: 2.947068e-06\n",
      "step: 134270 train: 0.07609724998474121 elapsed, loss: 2.635173e-06\n",
      "step: 134280 train: 0.07850861549377441 elapsed, loss: 2.8056018e-06\n",
      "step: 134290 train: 0.07754015922546387 elapsed, loss: 2.4549627e-06\n",
      "step: 134300 train: 0.08407330513000488 elapsed, loss: 1.596751e-06\n",
      "step: 134310 train: 0.08044099807739258 elapsed, loss: 2.0195707e-06\n",
      "step: 134320 train: 0.07594871520996094 elapsed, loss: 8.254126e-06\n",
      "step: 134330 train: 0.08298540115356445 elapsed, loss: 2.5848815e-06\n",
      "step: 134340 train: 0.07744169235229492 elapsed, loss: 2.9499581e-06\n",
      "step: 134350 train: 0.07771706581115723 elapsed, loss: 2.1527492e-06\n",
      "step: 134360 train: 0.08287596702575684 elapsed, loss: 6.1001265e-06\n",
      "step: 134370 train: 0.08164834976196289 elapsed, loss: 3.4025527e-06\n",
      "step: 134380 train: 0.09055233001708984 elapsed, loss: 1.8845287e-06\n",
      "step: 134390 train: 0.08320903778076172 elapsed, loss: 1.8300466e-06\n",
      "step: 134400 train: 0.08652544021606445 elapsed, loss: 1.630744e-06\n",
      "step: 134410 train: 0.08742237091064453 elapsed, loss: 1.7136317e-06\n",
      "step: 134420 train: 0.0762171745300293 elapsed, loss: 2.5783625e-06\n",
      "step: 134430 train: 0.07850527763366699 elapsed, loss: 1.8742846e-06\n",
      "step: 134440 train: 0.08126258850097656 elapsed, loss: 1.872422e-06\n",
      "step: 134450 train: 0.07907962799072266 elapsed, loss: 2.3231808e-06\n",
      "step: 134460 train: 0.07888412475585938 elapsed, loss: 2.3245752e-06\n",
      "step: 134470 train: 0.08307695388793945 elapsed, loss: 2.2831337e-06\n",
      "step: 134480 train: 0.08584117889404297 elapsed, loss: 1.9823178e-06\n",
      "step: 134490 train: 0.07730269432067871 elapsed, loss: 2.101061e-06\n",
      "step: 134500 train: 0.0842585563659668 elapsed, loss: 2.1806886e-06\n",
      "step: 134510 train: 0.08407998085021973 elapsed, loss: 1.7252733e-06\n",
      "step: 134520 train: 0.08292078971862793 elapsed, loss: 1.5376114e-06\n",
      "step: 134530 train: 0.08418679237365723 elapsed, loss: 3.0542637e-06\n",
      "step: 134540 train: 0.07714700698852539 elapsed, loss: 2.2267893e-06\n",
      "step: 134550 train: 0.07100391387939453 elapsed, loss: 2.6487183e-05\n",
      "step: 134560 train: 0.08372282981872559 elapsed, loss: 0.0001240541\n",
      "step: 134570 train: 0.08335208892822266 elapsed, loss: 2.1621263e-05\n",
      "step: 134580 train: 0.0783078670501709 elapsed, loss: 1.6455548e-05\n",
      "step: 134590 train: 0.08471322059631348 elapsed, loss: 1.1700284e-05\n",
      "step: 134600 train: 0.08537650108337402 elapsed, loss: 1.6521673e-05\n",
      "step: 134610 train: 0.08700752258300781 elapsed, loss: 7.2158387e-06\n",
      "step: 134620 train: 0.08001399040222168 elapsed, loss: 1.0965702e-05\n",
      "step: 134630 train: 0.0762183666229248 elapsed, loss: 7.254957e-06\n",
      "step: 134640 train: 0.08408689498901367 elapsed, loss: 5.145538e-06\n",
      "step: 134650 train: 0.08522439002990723 elapsed, loss: 5.1172888e-06\n",
      "step: 134660 train: 0.09020233154296875 elapsed, loss: 3.977665e-06\n",
      "step: 134670 train: 0.08579277992248535 elapsed, loss: 2.559267e-06\n",
      "step: 134680 train: 0.07962822914123535 elapsed, loss: 2.2109566e-06\n",
      "step: 134690 train: 0.08002972602844238 elapsed, loss: 1.8472757e-06\n",
      "step: 134700 train: 0.07747006416320801 elapsed, loss: 2.0596171e-06\n",
      "step: 134710 train: 0.07971405982971191 elapsed, loss: 1.4668317e-06\n",
      "step: 134720 train: 0.07654738426208496 elapsed, loss: 1.8104877e-06\n",
      "step: 134730 train: 0.07880926132202148 elapsed, loss: 1.8374972e-06\n",
      "step: 134740 train: 0.07665705680847168 elapsed, loss: 1.7480905e-06\n",
      "step: 134750 train: 0.07786011695861816 elapsed, loss: 1.7178227e-06\n",
      "step: 134760 train: 0.0833284854888916 elapsed, loss: 2.2989634e-06\n",
      "step: 134770 train: 0.08398008346557617 elapsed, loss: 1.3518136e-06\n",
      "step: 134780 train: 0.08083534240722656 elapsed, loss: 1.866367e-06\n",
      "step: 134790 train: 0.0800790786743164 elapsed, loss: 1.6158431e-06\n",
      "step: 134800 train: 0.08043718338012695 elapsed, loss: 1.802573e-06\n",
      "step: 134810 train: 0.08367562294006348 elapsed, loss: 1.7085092e-06\n",
      "step: 134820 train: 0.08448982238769531 elapsed, loss: 1.7210821e-06\n",
      "step: 134830 train: 0.08177447319030762 elapsed, loss: 1.591163e-06\n",
      "step: 134840 train: 0.07396721839904785 elapsed, loss: 5.405367e-06\n",
      "step: 134850 train: 0.0810244083404541 elapsed, loss: 1.685692e-06\n",
      "step: 134860 train: 0.08537697792053223 elapsed, loss: 1.519917e-06\n",
      "step: 134870 train: 0.08489251136779785 elapsed, loss: 1.5161917e-06\n",
      "step: 134880 train: 0.07867884635925293 elapsed, loss: 2.136917e-06\n",
      "step: 134890 train: 0.07877373695373535 elapsed, loss: 1.3574015e-06\n",
      "step: 134900 train: 0.08005285263061523 elapsed, loss: 1.4426174e-06\n",
      "step: 134910 train: 0.07489848136901855 elapsed, loss: 1.9287668e-06\n",
      "step: 134920 train: 0.08899521827697754 elapsed, loss: 1.6712567e-06\n",
      "step: 134930 train: 0.08445167541503906 elapsed, loss: 1.6088582e-06\n",
      "step: 134940 train: 0.08432459831237793 elapsed, loss: 1.5697426e-06\n",
      "step: 134950 train: 0.08550858497619629 elapsed, loss: 1.6302785e-06\n",
      "step: 134960 train: 0.07794952392578125 elapsed, loss: 2.2286517e-06\n",
      "step: 134970 train: 0.08779549598693848 elapsed, loss: 1.7075781e-06\n",
      "step: 134980 train: 0.08353972434997559 elapsed, loss: 1.4398233e-06\n",
      "step: 134990 train: 0.08814406394958496 elapsed, loss: 1.5213135e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 135000 train: 0.08399271965026855 elapsed, loss: 1.9012925e-06\n",
      "step: 135010 train: 0.08721613883972168 elapsed, loss: 1.6074612e-06\n",
      "step: 135020 train: 0.08325672149658203 elapsed, loss: 1.7383117e-06\n",
      "step: 135030 train: 0.08268570899963379 elapsed, loss: 2.1387782e-06\n",
      "step: 135040 train: 0.08001828193664551 elapsed, loss: 2.3017606e-06\n",
      "step: 135050 train: 0.08015680313110352 elapsed, loss: 2.5979195e-06\n",
      "step: 135060 train: 0.07746315002441406 elapsed, loss: 1.9599659e-06\n",
      "step: 135070 train: 0.08399534225463867 elapsed, loss: 1.6493706e-06\n",
      "step: 135080 train: 0.07454562187194824 elapsed, loss: 3.2158482e-06\n",
      "step: 135090 train: 0.07738590240478516 elapsed, loss: 1.8649712e-06\n",
      "step: 135100 train: 0.07889223098754883 elapsed, loss: 2.2700954e-06\n",
      "step: 135110 train: 0.08189702033996582 elapsed, loss: 2.1480928e-06\n",
      "step: 135120 train: 0.0811471939086914 elapsed, loss: 2.3134016e-06\n",
      "step: 135130 train: 0.07797718048095703 elapsed, loss: 4.0203202e-05\n",
      "step: 135140 train: 0.08160924911499023 elapsed, loss: 4.7967537e-06\n",
      "step: 135150 train: 0.08424758911132812 elapsed, loss: 2.8596191e-06\n",
      "step: 135160 train: 0.08032512664794922 elapsed, loss: 2.6929144e-06\n",
      "step: 135170 train: 0.08619141578674316 elapsed, loss: 2.0777766e-06\n",
      "step: 135180 train: 0.08230304718017578 elapsed, loss: 2.1881394e-06\n",
      "step: 135190 train: 0.08106160163879395 elapsed, loss: 1.8957049e-06\n",
      "step: 135200 train: 0.08234930038452148 elapsed, loss: 1.7937252e-06\n",
      "step: 135210 train: 0.08660125732421875 elapsed, loss: 1.4393578e-06\n",
      "step: 135220 train: 0.08469724655151367 elapsed, loss: 3.3927995e-06\n",
      "step: 135230 train: 0.08260250091552734 elapsed, loss: 2.079175e-06\n",
      "step: 135240 train: 0.09071826934814453 elapsed, loss: 1.5506507e-06\n",
      "step: 135250 train: 0.08437538146972656 elapsed, loss: 1.7927939e-06\n",
      "step: 135260 train: 0.08280158042907715 elapsed, loss: 1.6079264e-06\n",
      "step: 135270 train: 0.08170247077941895 elapsed, loss: 1.6982651e-06\n",
      "step: 135280 train: 0.08315324783325195 elapsed, loss: 1.5106037e-06\n",
      "step: 135290 train: 0.08476710319519043 elapsed, loss: 0.00017060399\n",
      "step: 135300 train: 0.08716058731079102 elapsed, loss: 0.0005158842\n",
      "step: 135310 train: 0.07678079605102539 elapsed, loss: 5.87947e-05\n",
      "step: 135320 train: 0.0839242935180664 elapsed, loss: 2.670649e-05\n",
      "step: 135330 train: 0.0795450210571289 elapsed, loss: 1.4326352e-05\n",
      "step: 135340 train: 0.08225798606872559 elapsed, loss: 1.3851722e-05\n",
      "step: 135350 train: 0.07807588577270508 elapsed, loss: 1.3358733e-05\n",
      "step: 135360 train: 0.07804512977600098 elapsed, loss: 1.0101949e-05\n",
      "step: 135370 train: 0.07957291603088379 elapsed, loss: 6.6998764e-06\n",
      "step: 135380 train: 0.08221554756164551 elapsed, loss: 4.7529866e-06\n",
      "step: 135390 train: 0.07184267044067383 elapsed, loss: 7.2800794e-06\n",
      "step: 135400 train: 0.08045268058776855 elapsed, loss: 3.634004e-06\n",
      "step: 135410 train: 0.08549022674560547 elapsed, loss: 3.7616003e-06\n",
      "step: 135420 train: 0.08495903015136719 elapsed, loss: 1.9879053e-06\n",
      "step: 135430 train: 0.08219790458679199 elapsed, loss: 2.2053687e-06\n",
      "step: 135440 train: 0.0796971321105957 elapsed, loss: 2.57417e-06\n",
      "step: 135450 train: 0.0824747085571289 elapsed, loss: 1.6144447e-06\n",
      "step: 135460 train: 0.07939696311950684 elapsed, loss: 2.1280694e-06\n",
      "step: 135470 train: 0.07888126373291016 elapsed, loss: 2.0684647e-06\n",
      "step: 135480 train: 0.08543229103088379 elapsed, loss: 2.2863933e-06\n",
      "step: 135490 train: 0.08179283142089844 elapsed, loss: 1.7774261e-06\n",
      "step: 135500 train: 0.08728933334350586 elapsed, loss: 4.6938353e-06\n",
      "step: 135510 train: 0.08426547050476074 elapsed, loss: 2.0438963e-05\n",
      "step: 135520 train: 0.08152890205383301 elapsed, loss: 1.1253518e-05\n",
      "step: 135530 train: 0.07821202278137207 elapsed, loss: 1.0517049e-05\n",
      "step: 135540 train: 0.08974170684814453 elapsed, loss: 4.8768406e-06\n",
      "step: 135550 train: 0.0867929458618164 elapsed, loss: 5.1036195e-06\n",
      "step: 135560 train: 0.08098983764648438 elapsed, loss: 4.9276096e-06\n",
      "step: 135570 train: 0.07808446884155273 elapsed, loss: 3.5287728e-06\n",
      "step: 135580 train: 0.08094644546508789 elapsed, loss: 3.2312146e-06\n",
      "step: 135590 train: 0.07432913780212402 elapsed, loss: 4.194193e-06\n",
      "step: 135600 train: 0.07985281944274902 elapsed, loss: 3.0114213e-06\n",
      "step: 135610 train: 0.08596301078796387 elapsed, loss: 1.8682199e-06\n",
      "step: 135620 train: 0.08603024482727051 elapsed, loss: 1.8901168e-06\n",
      "step: 135630 train: 0.08551931381225586 elapsed, loss: 1.3974482e-06\n",
      "step: 135640 train: 0.09009623527526855 elapsed, loss: 1.3168888e-06\n",
      "step: 135650 train: 0.07883977890014648 elapsed, loss: 1.540406e-06\n",
      "step: 135660 train: 0.0836024284362793 elapsed, loss: 1.3485537e-06\n",
      "step: 135670 train: 0.08234429359436035 elapsed, loss: 1.3210799e-06\n",
      "step: 135680 train: 0.08263778686523438 elapsed, loss: 1.0156066e-06\n",
      "step: 135690 train: 0.0776362419128418 elapsed, loss: 4.030696e-06\n",
      "step: 135700 train: 0.08188390731811523 elapsed, loss: 4.4987995e-05\n",
      "step: 135710 train: 0.07133173942565918 elapsed, loss: 0.00011611189\n",
      "step: 135720 train: 0.07639765739440918 elapsed, loss: 3.371056e-05\n",
      "step: 135730 train: 0.08578205108642578 elapsed, loss: 1.259893e-05\n",
      "step: 135740 train: 0.0789487361907959 elapsed, loss: 1.20599925e-05\n",
      "step: 135750 train: 0.07912206649780273 elapsed, loss: 8.621537e-06\n",
      "step: 135760 train: 0.08019685745239258 elapsed, loss: 7.1278273e-06\n",
      "step: 135770 train: 0.08212494850158691 elapsed, loss: 6.370172e-06\n",
      "step: 135780 train: 0.08249449729919434 elapsed, loss: 4.1918606e-06\n",
      "step: 135790 train: 0.08068728446960449 elapsed, loss: 3.5217872e-06\n",
      "step: 135800 train: 0.08012866973876953 elapsed, loss: 2.749723e-06\n",
      "step: 135810 train: 0.08200979232788086 elapsed, loss: 2.1271373e-06\n",
      "step: 135820 train: 0.07718420028686523 elapsed, loss: 2.3762655e-06\n",
      "step: 135830 train: 0.08193683624267578 elapsed, loss: 2.7222413e-06\n",
      "step: 135840 train: 0.08121299743652344 elapsed, loss: 2.0693951e-06\n",
      "step: 135850 train: 0.09077239036560059 elapsed, loss: 1.3224769e-06\n",
      "step: 135860 train: 0.08721446990966797 elapsed, loss: 1.4612436e-06\n",
      "step: 135870 train: 0.07636570930480957 elapsed, loss: 1.2740481e-06\n",
      "step: 135880 train: 0.08327794075012207 elapsed, loss: 1.4067614e-06\n",
      "step: 135890 train: 0.08336853981018066 elapsed, loss: 1.3057131e-06\n",
      "step: 135900 train: 0.0795755386352539 elapsed, loss: 1.0477372e-06\n",
      "step: 135910 train: 0.07950425148010254 elapsed, loss: 1.2051305e-06\n",
      "step: 135920 train: 0.08222007751464844 elapsed, loss: 1.1702059e-06\n",
      "step: 135930 train: 0.08470606803894043 elapsed, loss: 8.9826017e-07\n",
      "step: 135940 train: 0.07880115509033203 elapsed, loss: 1.8249245e-06\n",
      "step: 135950 train: 0.08254265785217285 elapsed, loss: 2.7054834e-06\n",
      "step: 135960 train: 0.08689546585083008 elapsed, loss: 9.261996e-07\n",
      "step: 135970 train: 0.08543276786804199 elapsed, loss: 1.2768422e-06\n",
      "step: 135980 train: 0.08583521842956543 elapsed, loss: 1.2777734e-06\n",
      "step: 135990 train: 0.08168411254882812 elapsed, loss: 1.6409887e-06\n",
      "step: 136000 train: 0.0840151309967041 elapsed, loss: 1.4393574e-06\n",
      "step: 136010 train: 0.08346676826477051 elapsed, loss: 1.6740503e-06\n",
      "step: 136020 train: 0.08398151397705078 elapsed, loss: 1.3238737e-06\n",
      "step: 136030 train: 0.07645130157470703 elapsed, loss: 1.8738187e-06\n",
      "step: 136040 train: 0.09068036079406738 elapsed, loss: 1.4374951e-06\n",
      "step: 136050 train: 0.08337044715881348 elapsed, loss: 1.8975663e-06\n",
      "step: 136060 train: 0.08596658706665039 elapsed, loss: 1.8165417e-06\n",
      "step: 136070 train: 0.08712220191955566 elapsed, loss: 1.3229425e-06\n",
      "step: 136080 train: 0.08716654777526855 elapsed, loss: 1.2516965e-06\n",
      "step: 136090 train: 0.07984757423400879 elapsed, loss: 2.3706777e-06\n",
      "step: 136100 train: 0.08208847045898438 elapsed, loss: 1.5255039e-06\n",
      "step: 136110 train: 0.07962727546691895 elapsed, loss: 1.7266702e-06\n",
      "step: 136120 train: 0.08590984344482422 elapsed, loss: 1.4682284e-06\n",
      "step: 136130 train: 0.08249163627624512 elapsed, loss: 2.9767325e-05\n",
      "step: 136140 train: 0.07830286026000977 elapsed, loss: 0.000111195426\n",
      "step: 136150 train: 0.07903933525085449 elapsed, loss: 0.0003220594\n",
      "step: 136160 train: 0.07774758338928223 elapsed, loss: 3.1506257e-05\n",
      "step: 136170 train: 0.08402657508850098 elapsed, loss: 2.4677276e-05\n",
      "step: 136180 train: 0.08766770362854004 elapsed, loss: 4.5485067e-05\n",
      "step: 136190 train: 0.08415937423706055 elapsed, loss: 1.25424085e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 136200 train: 0.08627200126647949 elapsed, loss: 1.07096e-05\n",
      "step: 136210 train: 0.08318400382995605 elapsed, loss: 8.9885925e-06\n",
      "step: 136220 train: 0.09054040908813477 elapsed, loss: 1.1086872e-05\n",
      "step: 136230 train: 0.08820223808288574 elapsed, loss: 4.150423e-06\n",
      "step: 136240 train: 0.08345651626586914 elapsed, loss: 5.918972e-06\n",
      "step: 136250 train: 0.08136272430419922 elapsed, loss: 3.539423e-06\n",
      "step: 136260 train: 0.08668088912963867 elapsed, loss: 2.2915146e-06\n",
      "step: 136270 train: 0.08112192153930664 elapsed, loss: 2.2728889e-06\n",
      "step: 136280 train: 0.08031439781188965 elapsed, loss: 1.7527468e-06\n",
      "step: 136290 train: 0.08584976196289062 elapsed, loss: 1.5478564e-06\n",
      "step: 136300 train: 0.08049798011779785 elapsed, loss: 1.734586e-06\n",
      "step: 136310 train: 0.08076262474060059 elapsed, loss: 1.610255e-06\n",
      "step: 136320 train: 0.08819890022277832 elapsed, loss: 1.1958172e-06\n",
      "step: 136330 train: 0.08570194244384766 elapsed, loss: 1.2498338e-06\n",
      "step: 136340 train: 0.07293415069580078 elapsed, loss: 1.8039696e-06\n",
      "step: 136350 train: 0.07956218719482422 elapsed, loss: 1.2549563e-06\n",
      "step: 136360 train: 0.08136439323425293 elapsed, loss: 1.3690428e-06\n",
      "step: 136370 train: 0.08121562004089355 elapsed, loss: 1.3825472e-06\n",
      "step: 136380 train: 0.07710003852844238 elapsed, loss: 1.5036189e-06\n",
      "step: 136390 train: 0.07947731018066406 elapsed, loss: 4.3431637e-06\n",
      "step: 136400 train: 0.08629417419433594 elapsed, loss: 1.6344693e-06\n",
      "step: 136410 train: 0.08488821983337402 elapsed, loss: 1.5976821e-06\n",
      "step: 136420 train: 0.08427143096923828 elapsed, loss: 1.2512309e-06\n",
      "step: 136430 train: 0.0827171802520752 elapsed, loss: 1.4435486e-06\n",
      "step: 136440 train: 0.08045101165771484 elapsed, loss: 1.0798677e-06\n",
      "step: 136450 train: 0.0780031681060791 elapsed, loss: 1.392326e-06\n",
      "step: 136460 train: 0.08402705192565918 elapsed, loss: 1.0221258e-06\n",
      "step: 136470 train: 0.08362555503845215 elapsed, loss: 1.008156e-06\n",
      "step: 136480 train: 0.0829763412475586 elapsed, loss: 1.1841757e-06\n",
      "step: 136490 train: 0.07785439491271973 elapsed, loss: 1.389532e-06\n",
      "step: 136500 train: 0.07959318161010742 elapsed, loss: 1.1790535e-06\n",
      "step: 136510 train: 0.08731937408447266 elapsed, loss: 1.2773079e-06\n",
      "step: 136520 train: 0.08562588691711426 elapsed, loss: 1.6065298e-06\n",
      "step: 136530 train: 0.07633233070373535 elapsed, loss: 1.7099062e-06\n",
      "step: 136540 train: 0.08635401725769043 elapsed, loss: 1.2400549e-06\n",
      "step: 136550 train: 0.0789337158203125 elapsed, loss: 1.2782392e-06\n",
      "step: 136560 train: 0.07769894599914551 elapsed, loss: 1.8533298e-06\n",
      "step: 136570 train: 0.08286857604980469 elapsed, loss: 4.4386575e-06\n",
      "step: 136580 train: 0.08164024353027344 elapsed, loss: 1.8053667e-06\n",
      "step: 136590 train: 0.08344149589538574 elapsed, loss: 1.7113027e-06\n",
      "step: 136600 train: 0.0834360122680664 elapsed, loss: 1.5622921e-06\n",
      "step: 136610 train: 0.08881354331970215 elapsed, loss: 1.412815e-06\n",
      "step: 136620 train: 0.08075380325317383 elapsed, loss: 1.5739336e-06\n",
      "step: 136630 train: 0.08042430877685547 elapsed, loss: 1.5050158e-06\n",
      "step: 136640 train: 0.08335709571838379 elapsed, loss: 1.5222453e-06\n",
      "step: 136650 train: 0.07798361778259277 elapsed, loss: 1.8891855e-06\n",
      "step: 136660 train: 0.07898926734924316 elapsed, loss: 1.7564703e-06\n",
      "step: 136670 train: 0.08000850677490234 elapsed, loss: 1.7299299e-06\n",
      "step: 136680 train: 0.0823526382446289 elapsed, loss: 1.4137447e-06\n",
      "step: 136690 train: 0.07961821556091309 elapsed, loss: 1.565086e-06\n",
      "step: 136700 train: 0.07899618148803711 elapsed, loss: 1.9310946e-06\n",
      "step: 136710 train: 0.0780644416809082 elapsed, loss: 2.390701e-06\n",
      "step: 136720 train: 0.07751631736755371 elapsed, loss: 2.0242271e-06\n",
      "step: 136730 train: 0.09563994407653809 elapsed, loss: 3.8416847e-06\n",
      "step: 136740 train: 0.08469438552856445 elapsed, loss: 1.8721687e-05\n",
      "step: 136750 train: 0.07766842842102051 elapsed, loss: 1.6705588e-05\n",
      "step: 136760 train: 0.07907271385192871 elapsed, loss: 1.4391841e-05\n",
      "step: 136770 train: 0.08440494537353516 elapsed, loss: 5.786741e-06\n",
      "step: 136780 train: 0.08240365982055664 elapsed, loss: 5.884055e-06\n",
      "step: 136790 train: 0.0779871940612793 elapsed, loss: 5.151117e-06\n",
      "step: 136800 train: 0.08168458938598633 elapsed, loss: 5.1897737e-06\n",
      "step: 136810 train: 0.08176779747009277 elapsed, loss: 5.5776654e-06\n",
      "step: 136820 train: 0.08534383773803711 elapsed, loss: 2.7324788e-06\n",
      "step: 136830 train: 0.08309316635131836 elapsed, loss: 2.2286517e-06\n",
      "step: 136840 train: 0.07676434516906738 elapsed, loss: 2.4284195e-06\n",
      "step: 136850 train: 0.0821070671081543 elapsed, loss: 2.1285346e-06\n",
      "step: 136860 train: 0.0770120620727539 elapsed, loss: 1.5553071e-06\n",
      "step: 136870 train: 0.08583259582519531 elapsed, loss: 1.7355027e-06\n",
      "step: 136880 train: 0.08145713806152344 elapsed, loss: 1.6144434e-06\n",
      "step: 136890 train: 0.08509325981140137 elapsed, loss: 1.5380766e-06\n",
      "step: 136900 train: 0.0876915454864502 elapsed, loss: 1.1110669e-06\n",
      "step: 136910 train: 0.0757746696472168 elapsed, loss: 1.8766127e-06\n",
      "step: 136920 train: 0.08148455619812012 elapsed, loss: 1.3913946e-06\n",
      "step: 136930 train: 0.0832984447479248 elapsed, loss: 1.4556556e-06\n",
      "step: 136940 train: 0.0775303840637207 elapsed, loss: 2.1709052e-06\n",
      "step: 136950 train: 0.08079242706298828 elapsed, loss: 2.5052534e-06\n",
      "step: 136960 train: 0.07971358299255371 elapsed, loss: 1.8225962e-06\n",
      "step: 136970 train: 0.08312439918518066 elapsed, loss: 1.5743993e-06\n",
      "step: 136980 train: 0.0801241397857666 elapsed, loss: 1.5799872e-06\n",
      "step: 136990 train: 0.08703374862670898 elapsed, loss: 1.7816178e-06\n",
      "step: 137000 train: 0.08147311210632324 elapsed, loss: 2.2617114e-06\n",
      "step: 137010 train: 0.08174347877502441 elapsed, loss: 1.5329554e-06\n",
      "step: 137020 train: 0.08013105392456055 elapsed, loss: 1.998149e-06\n",
      "step: 137030 train: 0.08080625534057617 elapsed, loss: 2.237965e-06\n",
      "step: 137040 train: 0.08031630516052246 elapsed, loss: 1.4775419e-06\n",
      "step: 137050 train: 0.0858297348022461 elapsed, loss: 7.7244455e-05\n",
      "step: 137060 train: 0.0917363166809082 elapsed, loss: 0.00015113779\n",
      "step: 137070 train: 0.08542609214782715 elapsed, loss: 7.5401695e-05\n",
      "step: 137080 train: 0.08021211624145508 elapsed, loss: 4.018106e-05\n",
      "step: 137090 train: 0.08171343803405762 elapsed, loss: 1.2530294e-05\n",
      "step: 137100 train: 0.0814971923828125 elapsed, loss: 1.2725646e-05\n",
      "step: 137110 train: 0.08495378494262695 elapsed, loss: 8.894843e-06\n",
      "step: 137120 train: 0.08400440216064453 elapsed, loss: 7.679144e-06\n",
      "step: 137130 train: 0.07476449012756348 elapsed, loss: 6.770201e-06\n",
      "step: 137140 train: 0.08123064041137695 elapsed, loss: 4.5429756e-06\n",
      "step: 137150 train: 0.0820169448852539 elapsed, loss: 4.935519e-06\n",
      "step: 137160 train: 0.07933163642883301 elapsed, loss: 3.4943132e-06\n",
      "step: 137170 train: 0.08460164070129395 elapsed, loss: 3.019341e-06\n",
      "step: 137180 train: 0.07857060432434082 elapsed, loss: 2.010257e-06\n",
      "step: 137190 train: 0.08386468887329102 elapsed, loss: 2.119687e-06\n",
      "step: 137200 train: 0.0748894214630127 elapsed, loss: 2.4414576e-06\n",
      "step: 137210 train: 0.08496642112731934 elapsed, loss: 1.6083903e-06\n",
      "step: 137220 train: 0.08181023597717285 elapsed, loss: 1.260078e-06\n",
      "step: 137230 train: 0.0798192024230957 elapsed, loss: 1.4831295e-06\n",
      "step: 137240 train: 0.07212543487548828 elapsed, loss: 2.1751007e-06\n",
      "step: 137250 train: 0.08624911308288574 elapsed, loss: 1.3438971e-06\n",
      "step: 137260 train: 0.0769343376159668 elapsed, loss: 1.6544925e-06\n",
      "step: 137270 train: 0.08222556114196777 elapsed, loss: 1.2228255e-06\n",
      "step: 137280 train: 0.0852363109588623 elapsed, loss: 1.1762595e-06\n",
      "step: 137290 train: 0.0838773250579834 elapsed, loss: 1.6405231e-06\n",
      "step: 137300 train: 0.08803367614746094 elapsed, loss: 1.2461086e-06\n",
      "step: 137310 train: 0.08559513092041016 elapsed, loss: 1.3080415e-06\n",
      "step: 137320 train: 0.08695650100708008 elapsed, loss: 1.1143267e-06\n",
      "step: 137330 train: 0.07610130310058594 elapsed, loss: 1.6307441e-06\n",
      "step: 137340 train: 0.07684516906738281 elapsed, loss: 1.7057154e-06\n",
      "step: 137350 train: 0.0820462703704834 elapsed, loss: 1.2828958e-06\n",
      "step: 137360 train: 0.08387017250061035 elapsed, loss: 1.385341e-06\n",
      "step: 137370 train: 0.08183932304382324 elapsed, loss: 1.5995449e-06\n",
      "step: 137380 train: 0.07762455940246582 elapsed, loss: 1.5525131e-06\n",
      "step: 137390 train: 0.07981348037719727 elapsed, loss: 1.5813841e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 137400 train: 0.0783381462097168 elapsed, loss: 1.5539101e-06\n",
      "step: 137410 train: 0.07469725608825684 elapsed, loss: 8.201424e-05\n",
      "step: 137420 train: 0.07645606994628906 elapsed, loss: 0.0001317368\n",
      "step: 137430 train: 0.07937169075012207 elapsed, loss: 6.522001e-05\n",
      "step: 137440 train: 0.0790863037109375 elapsed, loss: 1.5577e-05\n",
      "step: 137450 train: 0.08359670639038086 elapsed, loss: 1.35525115e-05\n",
      "step: 137460 train: 0.07575368881225586 elapsed, loss: 1.3588993e-05\n",
      "step: 137470 train: 0.0805046558380127 elapsed, loss: 6.7413353e-06\n",
      "step: 137480 train: 0.07614588737487793 elapsed, loss: 7.4449417e-06\n",
      "step: 137490 train: 0.08350086212158203 elapsed, loss: 5.273041e-06\n",
      "step: 137500 train: 0.08298254013061523 elapsed, loss: 4.2407537e-06\n",
      "step: 137510 train: 0.08014726638793945 elapsed, loss: 3.4836e-06\n",
      "step: 137520 train: 0.08303213119506836 elapsed, loss: 3.2843002e-06\n",
      "step: 137530 train: 0.08108305931091309 elapsed, loss: 3.10316e-06\n",
      "step: 137540 train: 0.08708620071411133 elapsed, loss: 1.5613605e-06\n",
      "step: 137550 train: 0.08388876914978027 elapsed, loss: 1.5497188e-06\n",
      "step: 137560 train: 0.07660484313964844 elapsed, loss: 1.5003592e-06\n",
      "step: 137570 train: 0.07839798927307129 elapsed, loss: 0.0015628808\n",
      "step: 137580 train: 0.0831143856048584 elapsed, loss: 2.074565e-05\n",
      "step: 137590 train: 0.08273530006408691 elapsed, loss: 1.4508813e-05\n",
      "step: 137600 train: 0.08205890655517578 elapsed, loss: 1.8617093e-05\n",
      "step: 137610 train: 0.07989740371704102 elapsed, loss: 8.9958985e-06\n",
      "step: 137620 train: 0.08006739616394043 elapsed, loss: 9.658988e-06\n",
      "step: 137630 train: 0.07657742500305176 elapsed, loss: 9.125006e-06\n",
      "step: 137640 train: 0.08692646026611328 elapsed, loss: 5.605174e-06\n",
      "step: 137650 train: 0.07632970809936523 elapsed, loss: 0.00018565985\n",
      "step: 137660 train: 0.08230733871459961 elapsed, loss: 1.9807809e-05\n",
      "step: 137670 train: 0.07963395118713379 elapsed, loss: 1.3084787e-05\n",
      "step: 137680 train: 0.07977008819580078 elapsed, loss: 1.4390557e-05\n",
      "step: 137690 train: 0.08393573760986328 elapsed, loss: 1.9760424e-05\n",
      "step: 137700 train: 0.08321952819824219 elapsed, loss: 1.3692914e-05\n",
      "step: 137710 train: 0.0799417495727539 elapsed, loss: 7.1869545e-06\n",
      "step: 137720 train: 0.08541274070739746 elapsed, loss: 4.638431e-06\n",
      "step: 137730 train: 0.08213543891906738 elapsed, loss: 3.6689325e-06\n",
      "step: 137740 train: 0.08476972579956055 elapsed, loss: 3.8933495e-06\n",
      "step: 137750 train: 0.08704042434692383 elapsed, loss: 2.5867434e-06\n",
      "step: 137760 train: 0.08035778999328613 elapsed, loss: 2.6468128e-06\n",
      "step: 137770 train: 0.08478379249572754 elapsed, loss: 2.1555402e-06\n",
      "step: 137780 train: 0.0851595401763916 elapsed, loss: 1.5161914e-06\n",
      "step: 137790 train: 0.08392333984375 elapsed, loss: 1.2307416e-06\n",
      "step: 137800 train: 0.08489513397216797 elapsed, loss: 1.2572777e-06\n",
      "step: 137810 train: 0.07865238189697266 elapsed, loss: 1.6223617e-06\n",
      "step: 137820 train: 0.0817108154296875 elapsed, loss: 1.1292277e-06\n",
      "step: 137830 train: 0.09403061866760254 elapsed, loss: 8.9127514e-07\n",
      "step: 137840 train: 0.08620905876159668 elapsed, loss: 8.558849e-07\n",
      "step: 137850 train: 0.08026885986328125 elapsed, loss: 1.3071101e-06\n",
      "step: 137860 train: 0.08395242691040039 elapsed, loss: 9.3877253e-07\n",
      "step: 137870 train: 0.07826972007751465 elapsed, loss: 1.6088563e-06\n",
      "step: 137880 train: 0.08402013778686523 elapsed, loss: 9.476201e-07\n",
      "step: 137890 train: 0.08777785301208496 elapsed, loss: 7.646154e-07\n",
      "step: 137900 train: 0.08568191528320312 elapsed, loss: 7.860358e-07\n",
      "step: 137910 train: 0.0756673812866211 elapsed, loss: 1.552513e-06\n",
      "step: 137920 train: 0.08336949348449707 elapsed, loss: 1.259147e-06\n",
      "step: 137930 train: 0.08060169219970703 elapsed, loss: 1.362058e-06\n",
      "step: 137940 train: 0.08578276634216309 elapsed, loss: 1.2163056e-06\n",
      "step: 137950 train: 0.0797584056854248 elapsed, loss: 1.0160722e-06\n",
      "step: 137960 train: 0.07915496826171875 elapsed, loss: 1.1161891e-06\n",
      "step: 137970 train: 0.07843589782714844 elapsed, loss: 1.0817305e-06\n",
      "step: 137980 train: 0.0785820484161377 elapsed, loss: 1.5180538e-06\n",
      "step: 137990 train: 0.08221006393432617 elapsed, loss: 1.6158428e-06\n",
      "step: 138000 train: 0.07882452011108398 elapsed, loss: 1.1515795e-06\n",
      "step: 138010 train: 0.08800268173217773 elapsed, loss: 9.825446e-07\n",
      "step: 138020 train: 0.08440089225769043 elapsed, loss: 1.0034994e-06\n",
      "step: 138030 train: 0.07881617546081543 elapsed, loss: 1.2139781e-06\n",
      "step: 138040 train: 0.08157992362976074 elapsed, loss: 1.4929087e-06\n",
      "step: 138050 train: 0.09076714515686035 elapsed, loss: 9.704374e-07\n",
      "step: 138060 train: 0.08771681785583496 elapsed, loss: 1.187901e-06\n",
      "step: 138070 train: 0.08850574493408203 elapsed, loss: 1.0714859e-06\n",
      "step: 138080 train: 0.08484816551208496 elapsed, loss: 1.2465741e-06\n",
      "step: 138090 train: 0.0763552188873291 elapsed, loss: 2.0246925e-06\n",
      "step: 138100 train: 0.08512258529663086 elapsed, loss: 9.80682e-07\n",
      "step: 138110 train: 0.08026647567749023 elapsed, loss: 1.4072273e-06\n",
      "step: 138120 train: 0.07920622825622559 elapsed, loss: 1.3303932e-06\n",
      "step: 138130 train: 0.07495713233947754 elapsed, loss: 1.7755644e-06\n",
      "step: 138140 train: 0.07465076446533203 elapsed, loss: 1.8696276e-06\n",
      "step: 138150 train: 0.08059239387512207 elapsed, loss: 1.3164233e-06\n",
      "step: 138160 train: 0.08475852012634277 elapsed, loss: 1.1823131e-06\n",
      "step: 138170 train: 0.07960820198059082 elapsed, loss: 1.608392e-06\n",
      "step: 138180 train: 0.08318233489990234 elapsed, loss: 7.2669077e-06\n",
      "step: 138190 train: 0.0793607234954834 elapsed, loss: 0.0003226649\n",
      "step: 138200 train: 0.08148002624511719 elapsed, loss: 4.073149e-05\n",
      "step: 138210 train: 0.08469510078430176 elapsed, loss: 1.3846328e-05\n",
      "step: 138220 train: 0.08384513854980469 elapsed, loss: 8.465158e-06\n",
      "step: 138230 train: 0.08309388160705566 elapsed, loss: 1.2080049e-05\n",
      "step: 138240 train: 0.08167076110839844 elapsed, loss: 7.1986005e-06\n",
      "step: 138250 train: 0.07812786102294922 elapsed, loss: 6.5909335e-06\n",
      "step: 138260 train: 0.08014917373657227 elapsed, loss: 4.7525155e-06\n",
      "step: 138270 train: 0.0765068531036377 elapsed, loss: 4.7911703e-06\n",
      "step: 138280 train: 0.07739734649658203 elapsed, loss: 3.8230683e-06\n",
      "step: 138290 train: 0.08199906349182129 elapsed, loss: 2.0828998e-06\n",
      "step: 138300 train: 0.07840657234191895 elapsed, loss: 2.223529e-06\n",
      "step: 138310 train: 0.0814971923828125 elapsed, loss: 1.8146798e-06\n",
      "step: 138320 train: 0.08249616622924805 elapsed, loss: 1.8798718e-06\n",
      "step: 138330 train: 0.08432197570800781 elapsed, loss: 1.3229421e-06\n",
      "step: 138340 train: 0.08261537551879883 elapsed, loss: 1.2270165e-06\n",
      "step: 138350 train: 0.07902050018310547 elapsed, loss: 1.4556555e-06\n",
      "step: 138360 train: 0.08303332328796387 elapsed, loss: 1.2759108e-06\n",
      "step: 138370 train: 0.08257746696472168 elapsed, loss: 1.3392403e-06\n",
      "step: 138380 train: 0.07439374923706055 elapsed, loss: 1.7723048e-06\n",
      "step: 138390 train: 0.07890129089355469 elapsed, loss: 1.6051329e-06\n",
      "step: 138400 train: 0.07859182357788086 elapsed, loss: 1.4812671e-06\n",
      "step: 138410 train: 0.0818181037902832 elapsed, loss: 1.5166572e-06\n",
      "step: 138420 train: 0.08159232139587402 elapsed, loss: 1.6093238e-06\n",
      "step: 138430 train: 0.07944273948669434 elapsed, loss: 1.3089729e-06\n",
      "step: 138440 train: 0.07969975471496582 elapsed, loss: 1.2116498e-06\n",
      "step: 138450 train: 0.08201432228088379 elapsed, loss: 1.6023388e-06\n",
      "step: 138460 train: 0.08321905136108398 elapsed, loss: 1.1697402e-06\n",
      "step: 138470 train: 0.08221817016601562 elapsed, loss: 1.6340036e-06\n",
      "step: 138480 train: 0.08621478080749512 elapsed, loss: 1.2861552e-06\n",
      "step: 138490 train: 0.08457779884338379 elapsed, loss: 1.7113034e-06\n",
      "step: 138500 train: 0.07707691192626953 elapsed, loss: 1.7769596e-06\n",
      "step: 138510 train: 0.07863664627075195 elapsed, loss: 1.8882527e-06\n",
      "step: 138520 train: 0.07559585571289062 elapsed, loss: 2.1904675e-06\n",
      "step: 138530 train: 0.07628035545349121 elapsed, loss: 1.7541441e-06\n",
      "step: 138540 train: 0.07716250419616699 elapsed, loss: 2.0023404e-06\n",
      "step: 138550 train: 0.08521199226379395 elapsed, loss: 1.748555e-06\n",
      "step: 138560 train: 0.07902073860168457 elapsed, loss: 2.1099083e-06\n",
      "step: 138570 train: 0.07515287399291992 elapsed, loss: 1.784412e-06\n",
      "step: 138580 train: 0.09202194213867188 elapsed, loss: 0.013633522\n",
      "step: 138590 train: 0.08428525924682617 elapsed, loss: 6.3128144e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 138600 train: 0.07591676712036133 elapsed, loss: 5.015279e-05\n",
      "step: 138610 train: 0.08255171775817871 elapsed, loss: 2.4931576e-05\n",
      "step: 138620 train: 0.08116602897644043 elapsed, loss: 2.3311706e-05\n",
      "step: 138630 train: 0.08074069023132324 elapsed, loss: 1.0164311e-05\n",
      "step: 138640 train: 0.076507568359375 elapsed, loss: 1.028167e-05\n",
      "step: 138650 train: 0.07851219177246094 elapsed, loss: 9.662542e-06\n",
      "step: 138660 train: 0.07982921600341797 elapsed, loss: 8.606728e-06\n",
      "step: 138670 train: 0.08821392059326172 elapsed, loss: 4.053101e-06\n",
      "step: 138680 train: 0.08327484130859375 elapsed, loss: 3.560901e-06\n",
      "step: 138690 train: 0.07675814628601074 elapsed, loss: 3.2009466e-06\n",
      "step: 138700 train: 0.07754039764404297 elapsed, loss: 2.9578732e-06\n",
      "step: 138710 train: 0.08160662651062012 elapsed, loss: 1.7471591e-06\n",
      "step: 138720 train: 0.08296656608581543 elapsed, loss: 2.6775474e-06\n",
      "step: 138730 train: 0.08074378967285156 elapsed, loss: 1.8007088e-06\n",
      "step: 138740 train: 0.08458566665649414 elapsed, loss: 1.3709043e-06\n",
      "step: 138750 train: 0.08623099327087402 elapsed, loss: 1.2558874e-06\n",
      "step: 138760 train: 0.08715343475341797 elapsed, loss: 1.2707885e-06\n",
      "step: 138770 train: 0.07644367218017578 elapsed, loss: 1.4677629e-06\n",
      "step: 138780 train: 0.08220219612121582 elapsed, loss: 1.6991962e-06\n",
      "step: 138790 train: 0.08949089050292969 elapsed, loss: 1.1920919e-06\n",
      "step: 138800 train: 0.08498096466064453 elapsed, loss: 1.2782393e-06\n",
      "step: 138810 train: 0.0853419303894043 elapsed, loss: 1.4328377e-06\n",
      "step: 138820 train: 0.08004903793334961 elapsed, loss: 1.6000106e-06\n",
      "step: 138830 train: 0.082672119140625 elapsed, loss: 3.6754086e-06\n",
      "step: 138840 train: 0.08173394203186035 elapsed, loss: 0.00017096837\n",
      "step: 138850 train: 0.07937264442443848 elapsed, loss: 3.334789e-05\n",
      "step: 138860 train: 0.08586597442626953 elapsed, loss: 3.4788925e-05\n",
      "step: 138870 train: 0.08459901809692383 elapsed, loss: 1.1807843e-05\n",
      "step: 138880 train: 0.07781004905700684 elapsed, loss: 8.886551e-06\n",
      "step: 138890 train: 0.08347582817077637 elapsed, loss: 6.3729926e-06\n",
      "step: 138900 train: 0.07411766052246094 elapsed, loss: 9.362974e-06\n",
      "step: 138910 train: 0.0826568603515625 elapsed, loss: 6.0390485e-06\n",
      "step: 138920 train: 0.07441306114196777 elapsed, loss: 4.989518e-06\n",
      "step: 138930 train: 0.08750152587890625 elapsed, loss: 3.0998967e-06\n",
      "step: 138940 train: 0.08550500869750977 elapsed, loss: 2.5201669e-05\n",
      "step: 138950 train: 0.08133530616760254 elapsed, loss: 6.8534127e-06\n",
      "step: 138960 train: 0.08227276802062988 elapsed, loss: 4.0963873e-06\n",
      "step: 138970 train: 0.08573365211486816 elapsed, loss: 3.4808068e-06\n",
      "step: 138980 train: 0.08221077919006348 elapsed, loss: 3.293612e-06\n",
      "step: 138990 train: 0.07580900192260742 elapsed, loss: 4.044253e-06\n",
      "step: 139000 train: 0.0797572135925293 elapsed, loss: 3.2451858e-06\n",
      "step: 139010 train: 0.07972288131713867 elapsed, loss: 2.5769643e-06\n",
      "step: 139020 train: 0.07810473442077637 elapsed, loss: 1.518054e-06\n",
      "step: 139030 train: 0.07974696159362793 elapsed, loss: 1.6526272e-06\n",
      "step: 139040 train: 0.08241152763366699 elapsed, loss: 1.198611e-06\n",
      "step: 139050 train: 0.07762026786804199 elapsed, loss: 1.4845267e-06\n",
      "step: 139060 train: 0.07858061790466309 elapsed, loss: 1.51852e-06\n",
      "step: 139070 train: 0.08866333961486816 elapsed, loss: 1.0868526e-06\n",
      "step: 139080 train: 0.08207941055297852 elapsed, loss: 1.1525109e-06\n",
      "step: 139090 train: 0.08021688461303711 elapsed, loss: 1.1953516e-06\n",
      "step: 139100 train: 0.08626866340637207 elapsed, loss: 1.0817304e-06\n",
      "step: 139110 train: 0.08233451843261719 elapsed, loss: 9.2945936e-07\n",
      "step: 139120 train: 0.07876420021057129 elapsed, loss: 1.2982625e-06\n",
      "step: 139130 train: 0.08463454246520996 elapsed, loss: 1.1236395e-06\n",
      "step: 139140 train: 0.089080810546875 elapsed, loss: 1.1064104e-06\n",
      "step: 139150 train: 0.07704615592956543 elapsed, loss: 1.5161914e-06\n",
      "step: 139160 train: 0.07685327529907227 elapsed, loss: 1.5641547e-06\n",
      "step: 139170 train: 0.08387231826782227 elapsed, loss: 1.3145607e-06\n",
      "step: 139180 train: 0.08218502998352051 elapsed, loss: 1.3019878e-06\n",
      "step: 139190 train: 0.08535885810852051 elapsed, loss: 1.1231723e-06\n",
      "step: 139200 train: 0.07746505737304688 elapsed, loss: 1.323408e-06\n",
      "step: 139210 train: 0.08495426177978516 elapsed, loss: 3.990232e-06\n",
      "step: 139220 train: 0.08007001876831055 elapsed, loss: 2.0661362e-06\n",
      "step: 139230 train: 0.07697081565856934 elapsed, loss: 2.2212012e-06\n",
      "step: 139240 train: 0.08470606803894043 elapsed, loss: 1.0598444e-06\n",
      "step: 139250 train: 0.07642602920532227 elapsed, loss: 1.4570528e-06\n",
      "step: 139260 train: 0.0810856819152832 elapsed, loss: 1.0523938e-06\n",
      "step: 139270 train: 0.08195018768310547 elapsed, loss: 1.3061788e-06\n",
      "step: 139280 train: 0.082427978515625 elapsed, loss: 1.4938306e-06\n",
      "step: 139290 train: 0.08004188537597656 elapsed, loss: 1.708509e-06\n",
      "step: 139300 train: 0.07666778564453125 elapsed, loss: 1.4221283e-06\n",
      "step: 139310 train: 0.07636070251464844 elapsed, loss: 1.8924451e-06\n",
      "step: 139320 train: 0.08303403854370117 elapsed, loss: 1.3476226e-06\n",
      "step: 139330 train: 0.0846860408782959 elapsed, loss: 1.174397e-06\n",
      "step: 139340 train: 0.07773804664611816 elapsed, loss: 1.3862723e-06\n",
      "step: 139350 train: 0.07883381843566895 elapsed, loss: 1.6363322e-06\n",
      "step: 139360 train: 0.07195520401000977 elapsed, loss: 1.8971018e-06\n",
      "step: 139370 train: 0.08365249633789062 elapsed, loss: 1.6083898e-06\n",
      "step: 139380 train: 0.07839512825012207 elapsed, loss: 1.3876695e-06\n",
      "step: 139390 train: 0.08187532424926758 elapsed, loss: 1.2638036e-06\n",
      "step: 139400 train: 0.08273625373840332 elapsed, loss: 1.7406401e-06\n",
      "step: 139410 train: 0.08329033851623535 elapsed, loss: 1.3564702e-06\n",
      "step: 139420 train: 0.08139514923095703 elapsed, loss: 1.3750964e-06\n",
      "step: 139430 train: 0.07948493957519531 elapsed, loss: 1.6246905e-06\n",
      "step: 139440 train: 0.08226203918457031 elapsed, loss: 1.500825e-06\n",
      "step: 139450 train: 0.07735204696655273 elapsed, loss: 2.5643908e-06\n",
      "step: 139460 train: 0.08143901824951172 elapsed, loss: 1.4458767e-06\n",
      "step: 139470 train: 0.0786447525024414 elapsed, loss: 3.280072e-06\n",
      "step: 139480 train: 0.07463836669921875 elapsed, loss: 7.3682866e-05\n",
      "step: 139490 train: 0.07813835144042969 elapsed, loss: 5.8793985e-06\n",
      "step: 139500 train: 0.0787055492401123 elapsed, loss: 3.7937293e-06\n",
      "step: 139510 train: 0.08117294311523438 elapsed, loss: 2.9671849e-06\n",
      "step: 139520 train: 0.08399033546447754 elapsed, loss: 1.9925612e-06\n",
      "step: 139530 train: 0.08718466758728027 elapsed, loss: 1.735048e-06\n",
      "step: 139540 train: 0.08037614822387695 elapsed, loss: 2.3283028e-06\n",
      "step: 139550 train: 0.08967995643615723 elapsed, loss: 1.3052473e-06\n",
      "step: 139560 train: 0.08669710159301758 elapsed, loss: 1.3164233e-06\n",
      "step: 139570 train: 0.07663774490356445 elapsed, loss: 2.059151e-06\n",
      "step: 139580 train: 0.08186149597167969 elapsed, loss: 2.4922133e-06\n",
      "step: 139590 train: 0.07500958442687988 elapsed, loss: 6.6305147e-06\n",
      "step: 139600 train: 0.08441567420959473 elapsed, loss: 1.6698594e-06\n",
      "step: 139610 train: 0.0814363956451416 elapsed, loss: 1.5567025e-06\n",
      "step: 139620 train: 0.0742957592010498 elapsed, loss: 2.4819706e-06\n",
      "step: 139630 train: 0.082122802734375 elapsed, loss: 1.3355154e-06\n",
      "step: 139640 train: 0.08297228813171387 elapsed, loss: 1.5674143e-06\n",
      "step: 139650 train: 0.0796051025390625 elapsed, loss: 1.5012906e-06\n",
      "step: 139660 train: 0.08379173278808594 elapsed, loss: 1.4156091e-06\n",
      "step: 139670 train: 0.0859076976776123 elapsed, loss: 1.3327215e-06\n",
      "step: 139680 train: 0.0854952335357666 elapsed, loss: 1.5897662e-06\n",
      "step: 139690 train: 0.07656002044677734 elapsed, loss: 1.9199192e-06\n",
      "step: 139700 train: 0.0848836898803711 elapsed, loss: 1.3667147e-06\n",
      "step: 139710 train: 0.07474946975708008 elapsed, loss: 1.7634575e-06\n",
      "step: 139720 train: 0.0745551586151123 elapsed, loss: 2.1047863e-06\n",
      "step: 139730 train: 0.08615231513977051 elapsed, loss: 1.4123495e-06\n",
      "step: 139740 train: 0.08720755577087402 elapsed, loss: 1.3131637e-06\n",
      "step: 139750 train: 0.07941484451293945 elapsed, loss: 2.8908069e-06\n",
      "step: 139760 train: 0.0762779712677002 elapsed, loss: 1.968348e-06\n",
      "step: 139770 train: 0.0758213996887207 elapsed, loss: 1.917591e-06\n",
      "step: 139780 train: 0.0810079574584961 elapsed, loss: 1.3974484e-06\n",
      "step: 139790 train: 0.08190608024597168 elapsed, loss: 1.5450601e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 139800 train: 0.07902765274047852 elapsed, loss: 1.8561238e-06\n",
      "step: 139810 train: 0.08269357681274414 elapsed, loss: 1.743434e-06\n",
      "step: 139820 train: 0.08356475830078125 elapsed, loss: 2.0484413e-06\n",
      "step: 139830 train: 0.08233880996704102 elapsed, loss: 1.4645032e-06\n",
      "step: 139840 train: 0.07850527763366699 elapsed, loss: 2.7767164e-06\n",
      "step: 139850 train: 0.08077239990234375 elapsed, loss: 2.9271405e-06\n",
      "step: 139860 train: 0.07783913612365723 elapsed, loss: 1.8458793e-06\n",
      "step: 139870 train: 0.08155131340026855 elapsed, loss: 1.8314437e-06\n",
      "step: 139880 train: 0.07474660873413086 elapsed, loss: 2.651937e-06\n",
      "step: 139890 train: 0.08013153076171875 elapsed, loss: 2.1127025e-06\n",
      "step: 139900 train: 0.08814406394958496 elapsed, loss: 1.4677628e-06\n",
      "step: 139910 train: 0.08512210845947266 elapsed, loss: 2.4018768e-06\n",
      "step: 139920 train: 0.08014845848083496 elapsed, loss: 1.7113034e-06\n",
      "step: 139930 train: 0.08214735984802246 elapsed, loss: 1.9520496e-06\n",
      "step: 139940 train: 0.0809943675994873 elapsed, loss: 1.4752134e-06\n",
      "step: 139950 train: 0.07440352439880371 elapsed, loss: 2.5471634e-06\n",
      "step: 139960 train: 0.07668948173522949 elapsed, loss: 2.5811564e-06\n",
      "step: 139970 train: 0.0818173885345459 elapsed, loss: 1.9813863e-06\n",
      "step: 139980 train: 0.08078813552856445 elapsed, loss: 2.8940676e-06\n",
      "step: 139990 train: 0.0785682201385498 elapsed, loss: 2.2295833e-06\n",
      "step: 140000 train: 0.08024096488952637 elapsed, loss: 2.1788246e-06\n",
      "step: 140010 train: 0.08370852470397949 elapsed, loss: 2.794893e-06\n",
      "step: 140020 train: 0.08921694755554199 elapsed, loss: 2.6696316e-06\n",
      "step: 140030 train: 0.0781862735748291 elapsed, loss: 2.7110752e-06\n",
      "step: 140040 train: 0.07499289512634277 elapsed, loss: 2.472657e-06\n",
      "step: 140050 train: 0.07903456687927246 elapsed, loss: 3.1897735e-06\n",
      "step: 140060 train: 0.08045840263366699 elapsed, loss: 2.4600845e-06\n",
      "step: 140070 train: 0.08388900756835938 elapsed, loss: 2.041922e-06\n",
      "step: 140080 train: 0.08221125602722168 elapsed, loss: 1.9841805e-06\n",
      "step: 140090 train: 0.08376073837280273 elapsed, loss: 1.8211994e-06\n",
      "step: 140100 train: 0.07767581939697266 elapsed, loss: 1.7048375e-05\n",
      "step: 140110 train: 0.08859562873840332 elapsed, loss: 7.6325035e-05\n",
      "step: 140120 train: 0.08268237113952637 elapsed, loss: 6.686388e-06\n",
      "step: 140130 train: 0.08221435546875 elapsed, loss: 6.7790543e-06\n",
      "step: 140140 train: 0.07770466804504395 elapsed, loss: 5.6805748e-06\n",
      "step: 140150 train: 0.07883191108703613 elapsed, loss: 4.021903e-06\n",
      "step: 140160 train: 0.08005142211914062 elapsed, loss: 3.5879111e-06\n",
      "step: 140170 train: 0.08284687995910645 elapsed, loss: 3.1869756e-06\n",
      "step: 140180 train: 0.08237719535827637 elapsed, loss: 2.0749835e-06\n",
      "step: 140190 train: 0.07551455497741699 elapsed, loss: 2.4330761e-06\n",
      "step: 140200 train: 0.08268618583679199 elapsed, loss: 1.776029e-06\n",
      "step: 140210 train: 0.08294486999511719 elapsed, loss: 1.9627541e-06\n",
      "step: 140220 train: 0.0791168212890625 elapsed, loss: 1.7019902e-06\n",
      "step: 140230 train: 0.07901597023010254 elapsed, loss: 1.9483243e-06\n",
      "step: 140240 train: 0.08658552169799805 elapsed, loss: 1.754609e-06\n",
      "step: 140250 train: 0.08525443077087402 elapsed, loss: 1.4849924e-06\n",
      "step: 140260 train: 0.07965373992919922 elapsed, loss: 2.126672e-06\n",
      "step: 140270 train: 0.08127236366271973 elapsed, loss: 1.3872038e-06\n",
      "step: 140280 train: 0.07399201393127441 elapsed, loss: 2.069396e-06\n",
      "step: 140290 train: 0.08096814155578613 elapsed, loss: 1.7550751e-06\n",
      "step: 140300 train: 0.07994937896728516 elapsed, loss: 1.7331895e-06\n",
      "step: 140310 train: 0.08030462265014648 elapsed, loss: 2.039594e-06\n",
      "step: 140320 train: 0.07949113845825195 elapsed, loss: 0.0002196162\n",
      "step: 140330 train: 0.0867912769317627 elapsed, loss: 0.00018014849\n",
      "step: 140340 train: 0.08018875122070312 elapsed, loss: 8.1969236e-05\n",
      "step: 140350 train: 0.08280014991760254 elapsed, loss: 0.00026840917\n",
      "step: 140360 train: 0.08520126342773438 elapsed, loss: 1.9909621e-05\n",
      "step: 140370 train: 0.08034682273864746 elapsed, loss: 1.5601958e-05\n",
      "step: 140380 train: 0.08733463287353516 elapsed, loss: 9.672331e-06\n",
      "step: 140390 train: 0.0848538875579834 elapsed, loss: 6.720379e-06\n",
      "step: 140400 train: 0.08563375473022461 elapsed, loss: 5.742505e-06\n",
      "step: 140410 train: 0.08060097694396973 elapsed, loss: 5.0025765e-06\n",
      "step: 140420 train: 0.08138895034790039 elapsed, loss: 3.2349408e-06\n",
      "step: 140430 train: 0.07911443710327148 elapsed, loss: 2.972775e-06\n",
      "step: 140440 train: 0.08448338508605957 elapsed, loss: 2.1238782e-06\n",
      "step: 140450 train: 0.07926249504089355 elapsed, loss: 3.264273e-06\n",
      "step: 140460 train: 0.08378434181213379 elapsed, loss: 2.5685642e-06\n",
      "step: 140470 train: 0.08049249649047852 elapsed, loss: 1.6526301e-06\n",
      "step: 140480 train: 0.08123064041137695 elapsed, loss: 1.6032701e-06\n",
      "step: 140490 train: 0.07586812973022461 elapsed, loss: 1.8225933e-06\n",
      "step: 140500 train: 0.08200812339782715 elapsed, loss: 1.3280646e-06\n",
      "step: 140510 train: 0.09079217910766602 elapsed, loss: 1.3625236e-06\n",
      "step: 140520 train: 0.08081293106079102 elapsed, loss: 1.3271335e-06\n",
      "step: 140530 train: 0.08222246170043945 elapsed, loss: 6.5955637e-06\n",
      "step: 140540 train: 0.08452367782592773 elapsed, loss: 1.6214304e-06\n",
      "step: 140550 train: 0.07649827003479004 elapsed, loss: 1.5767273e-06\n",
      "step: 140560 train: 0.08192229270935059 elapsed, loss: 1.6451797e-06\n",
      "step: 140570 train: 0.0866537094116211 elapsed, loss: 1.8095442e-06\n",
      "step: 140580 train: 0.07825517654418945 elapsed, loss: 1.9315607e-06\n",
      "step: 140590 train: 0.07510972023010254 elapsed, loss: 1.595354e-06\n",
      "step: 140600 train: 0.08072471618652344 elapsed, loss: 1.4631064e-06\n",
      "step: 140610 train: 0.07423734664916992 elapsed, loss: 1.9608974e-06\n",
      "step: 140620 train: 0.0857243537902832 elapsed, loss: 1.4998935e-06\n",
      "step: 140630 train: 0.08397388458251953 elapsed, loss: 2.141573e-06\n",
      "step: 140640 train: 0.08983039855957031 elapsed, loss: 1.3522788e-06\n",
      "step: 140650 train: 0.08592724800109863 elapsed, loss: 1.4593797e-06\n",
      "step: 140660 train: 0.0840451717376709 elapsed, loss: 1.8472483e-06\n",
      "step: 140670 train: 0.08532977104187012 elapsed, loss: 1.2652004e-06\n",
      "step: 140680 train: 0.08253884315490723 elapsed, loss: 1.3099041e-06\n",
      "step: 140690 train: 0.08152937889099121 elapsed, loss: 1.646111e-06\n",
      "step: 140700 train: 0.08127331733703613 elapsed, loss: 1.5608953e-06\n",
      "step: 140710 train: 0.08072805404663086 elapsed, loss: 1.7802138e-06\n",
      "step: 140720 train: 0.08321046829223633 elapsed, loss: 1.9143304e-06\n",
      "step: 140730 train: 0.08212447166442871 elapsed, loss: 1.6791729e-06\n",
      "step: 140740 train: 0.08210444450378418 elapsed, loss: 1.7629908e-06\n",
      "step: 140750 train: 0.07908916473388672 elapsed, loss: 1.5292302e-06\n",
      "step: 140760 train: 0.08495092391967773 elapsed, loss: 1.7038528e-06\n",
      "step: 140770 train: 0.0816795825958252 elapsed, loss: 1.9716076e-06\n",
      "step: 140780 train: 0.08049321174621582 elapsed, loss: 2.051235e-06\n",
      "step: 140790 train: 0.08043408393859863 elapsed, loss: 2.2808044e-06\n",
      "step: 140800 train: 0.08103537559509277 elapsed, loss: 1.6358664e-06\n",
      "step: 140810 train: 0.08354330062866211 elapsed, loss: 2.1443675e-06\n",
      "step: 140820 train: 0.07515859603881836 elapsed, loss: 2.1378482e-06\n",
      "step: 140830 train: 0.0806121826171875 elapsed, loss: 3.250774e-06\n",
      "step: 140840 train: 0.08409285545349121 elapsed, loss: 2.3492573e-06\n",
      "step: 140850 train: 0.0769948959350586 elapsed, loss: 2.124344e-06\n",
      "step: 140860 train: 0.08127808570861816 elapsed, loss: 1.8640399e-06\n",
      "step: 140870 train: 0.08097696304321289 elapsed, loss: 1.5730024e-06\n",
      "step: 140880 train: 0.0820169448852539 elapsed, loss: 1.936681e-06\n",
      "step: 140890 train: 0.08136343955993652 elapsed, loss: 2.3026919e-06\n",
      "step: 140900 train: 0.0856633186340332 elapsed, loss: 2.210491e-06\n",
      "step: 140910 train: 0.08220386505126953 elapsed, loss: 2.044716e-06\n",
      "step: 140920 train: 0.08387470245361328 elapsed, loss: 1.3848755e-06\n",
      "step: 140930 train: 0.07771921157836914 elapsed, loss: 1.8817351e-06\n",
      "step: 140940 train: 0.08304595947265625 elapsed, loss: 1.9818517e-06\n",
      "step: 140950 train: 0.08021712303161621 elapsed, loss: 2.1844135e-06\n",
      "step: 140960 train: 0.08383440971374512 elapsed, loss: 1.9106042e-06\n",
      "step: 140970 train: 0.08606266975402832 elapsed, loss: 1.7881375e-06\n",
      "step: 140980 train: 0.0827796459197998 elapsed, loss: 1.6340034e-06\n",
      "step: 140990 train: 0.07805871963500977 elapsed, loss: 1.8868575e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 141000 train: 0.07972502708435059 elapsed, loss: 1.8747498e-06\n",
      "step: 141010 train: 0.08176922798156738 elapsed, loss: 2.1583373e-06\n",
      "step: 141020 train: 0.08274507522583008 elapsed, loss: 1.9157274e-06\n",
      "step: 141030 train: 0.07597208023071289 elapsed, loss: 1.9222475e-06\n",
      "step: 141040 train: 0.08274149894714355 elapsed, loss: 1.7257382e-06\n",
      "step: 141050 train: 0.08717918395996094 elapsed, loss: 2.0568234e-06\n",
      "step: 141060 train: 0.0843515396118164 elapsed, loss: 1.6605464e-06\n",
      "step: 141070 train: 0.08464622497558594 elapsed, loss: 0.03309256\n",
      "step: 141080 train: 0.08193016052246094 elapsed, loss: 4.2435782e-05\n",
      "step: 141090 train: 0.08366870880126953 elapsed, loss: 2.4524643e-05\n",
      "step: 141100 train: 0.07807779312133789 elapsed, loss: 1.6365775e-05\n",
      "step: 141110 train: 0.07900524139404297 elapsed, loss: 1.419677e-05\n",
      "step: 141120 train: 0.07709026336669922 elapsed, loss: 1.3543138e-05\n",
      "step: 141130 train: 0.07913422584533691 elapsed, loss: 7.441221e-06\n",
      "step: 141140 train: 0.08239388465881348 elapsed, loss: 7.713166e-06\n",
      "step: 141150 train: 0.08681988716125488 elapsed, loss: 4.068934e-06\n",
      "step: 141160 train: 0.08809137344360352 elapsed, loss: 3.239596e-06\n",
      "step: 141170 train: 0.08912777900695801 elapsed, loss: 2.8097945e-06\n",
      "step: 141180 train: 0.0863792896270752 elapsed, loss: 2.7106082e-06\n",
      "step: 141190 train: 0.08007574081420898 elapsed, loss: 2.853567e-06\n",
      "step: 141200 train: 0.08151102066040039 elapsed, loss: 2.2947752e-06\n",
      "step: 141210 train: 0.08144354820251465 elapsed, loss: 2.4055862e-06\n",
      "step: 141220 train: 0.07849645614624023 elapsed, loss: 1.97673e-06\n",
      "step: 141230 train: 0.08554434776306152 elapsed, loss: 1.5120008e-06\n",
      "step: 141240 train: 0.08418703079223633 elapsed, loss: 1.6093237e-06\n",
      "step: 141250 train: 0.0802464485168457 elapsed, loss: 1.8146787e-06\n",
      "step: 141260 train: 0.08019685745239258 elapsed, loss: 2.196521e-06\n",
      "step: 141270 train: 0.08225274085998535 elapsed, loss: 1.3215457e-06\n",
      "step: 141280 train: 0.08076834678649902 elapsed, loss: 2.0861598e-06\n",
      "step: 141290 train: 0.08168554306030273 elapsed, loss: 1.7816178e-06\n",
      "step: 141300 train: 0.08397150039672852 elapsed, loss: 2.7031574e-06\n",
      "step: 141310 train: 0.07947635650634766 elapsed, loss: 2.1634594e-06\n",
      "step: 141320 train: 0.08499479293823242 elapsed, loss: 1.7271359e-06\n",
      "step: 141330 train: 0.08343982696533203 elapsed, loss: 1.625622e-06\n",
      "step: 141340 train: 0.08098983764648438 elapsed, loss: 1.6051328e-06\n",
      "step: 141350 train: 0.08127570152282715 elapsed, loss: 2.4218996e-06\n",
      "step: 141360 train: 0.0815286636352539 elapsed, loss: 2.1373826e-06\n",
      "step: 141370 train: 0.07818889617919922 elapsed, loss: 2.0116543e-06\n",
      "step: 141380 train: 0.0817251205444336 elapsed, loss: 2.2449499e-06\n",
      "step: 141390 train: 0.07856559753417969 elapsed, loss: 2.261248e-06\n",
      "step: 141400 train: 0.0792093276977539 elapsed, loss: 1.6661344e-06\n",
      "step: 141410 train: 0.0877690315246582 elapsed, loss: 1.7839463e-06\n",
      "step: 141420 train: 0.08019161224365234 elapsed, loss: 1.7918626e-06\n",
      "step: 141430 train: 0.08105945587158203 elapsed, loss: 1.8724219e-06\n",
      "step: 141440 train: 0.08568310737609863 elapsed, loss: 5.820904e-05\n",
      "step: 141450 train: 0.08470559120178223 elapsed, loss: 0.00044233724\n",
      "step: 141460 train: 0.07811760902404785 elapsed, loss: 8.698061e-05\n",
      "step: 141470 train: 0.08127617835998535 elapsed, loss: 5.536039e-05\n",
      "step: 141480 train: 0.078125 elapsed, loss: 2.311794e-05\n",
      "step: 141490 train: 0.08489131927490234 elapsed, loss: 1.3565164e-05\n",
      "step: 141500 train: 0.0830080509185791 elapsed, loss: 0.00022610722\n",
      "step: 141510 train: 0.08298444747924805 elapsed, loss: 2.9892166e-05\n",
      "step: 141520 train: 0.08443617820739746 elapsed, loss: 1.190345e-05\n",
      "step: 141530 train: 0.0820164680480957 elapsed, loss: 6.6025414e-06\n",
      "step: 141540 train: 0.08066034317016602 elapsed, loss: 1.1566171e-05\n",
      "step: 141550 train: 0.08103442192077637 elapsed, loss: 4.6938453e-06\n",
      "step: 141560 train: 0.08433294296264648 elapsed, loss: 3.4849909e-06\n",
      "step: 141570 train: 0.09299230575561523 elapsed, loss: 2.3241103e-06\n",
      "step: 141580 train: 0.07707858085632324 elapsed, loss: 3.3704396e-06\n",
      "step: 141590 train: 0.08134746551513672 elapsed, loss: 1.5706737e-06\n",
      "step: 141600 train: 0.08253026008605957 elapsed, loss: 1.6004753e-06\n",
      "step: 141610 train: 0.08143162727355957 elapsed, loss: 1.7047839e-06\n",
      "step: 141620 train: 0.08137679100036621 elapsed, loss: 1.2991939e-06\n",
      "step: 141630 train: 0.08473825454711914 elapsed, loss: 9.969801e-07\n",
      "step: 141640 train: 0.08385729789733887 elapsed, loss: 1.1101357e-06\n",
      "step: 141650 train: 0.08129143714904785 elapsed, loss: 1.3010566e-06\n",
      "step: 141660 train: 0.08589720726013184 elapsed, loss: 9.774224e-07\n",
      "step: 141670 train: 0.07605576515197754 elapsed, loss: 3.3806657e-06\n",
      "step: 141680 train: 0.08063864707946777 elapsed, loss: 1.1022189e-06\n",
      "step: 141690 train: 0.08332514762878418 elapsed, loss: 1.1674103e-06\n",
      "step: 141700 train: 0.07899641990661621 elapsed, loss: 1.2191003e-06\n",
      "step: 141710 train: 0.08222126960754395 elapsed, loss: 1.1785878e-06\n",
      "step: 141720 train: 0.08226871490478516 elapsed, loss: 1.1557704e-06\n",
      "step: 141730 train: 0.07620120048522949 elapsed, loss: 1.958101e-06\n",
      "step: 141740 train: 0.07956147193908691 elapsed, loss: 1.4230591e-06\n",
      "step: 141750 train: 0.08299636840820312 elapsed, loss: 1.0975629e-06\n",
      "step: 141760 train: 0.08223247528076172 elapsed, loss: 1.142732e-06\n",
      "step: 141770 train: 0.08103704452514648 elapsed, loss: 1.6596151e-06\n",
      "step: 141780 train: 0.07650470733642578 elapsed, loss: 1.2395894e-06\n",
      "step: 141790 train: 0.0767970085144043 elapsed, loss: 1.4491363e-06\n",
      "step: 141800 train: 0.07654047012329102 elapsed, loss: 1.4356324e-06\n",
      "step: 141810 train: 0.08221697807312012 elapsed, loss: 1.315492e-06\n",
      "step: 141820 train: 0.07957863807678223 elapsed, loss: 1.9441336e-06\n",
      "step: 141830 train: 0.08033108711242676 elapsed, loss: 1.5031524e-06\n",
      "step: 141840 train: 0.08722472190856934 elapsed, loss: 1.64192e-06\n",
      "step: 141850 train: 0.0800631046295166 elapsed, loss: 1.5064129e-06\n",
      "step: 141860 train: 0.08624839782714844 elapsed, loss: 1.8300468e-06\n",
      "step: 141870 train: 0.08133506774902344 elapsed, loss: 2.2761483e-06\n",
      "step: 141880 train: 0.08968377113342285 elapsed, loss: 1.2102528e-06\n",
      "step: 141890 train: 0.07900667190551758 elapsed, loss: 1.3126981e-06\n",
      "step: 141900 train: 0.07783365249633789 elapsed, loss: 1.5338867e-06\n",
      "step: 141910 train: 0.08133220672607422 elapsed, loss: 4.7983925e-05\n",
      "step: 141920 train: 0.08704018592834473 elapsed, loss: 1.9336927e-05\n",
      "step: 141930 train: 0.08121895790100098 elapsed, loss: 6.6314296e-06\n",
      "step: 141940 train: 0.08544421195983887 elapsed, loss: 4.6528376e-06\n",
      "step: 141950 train: 0.08191657066345215 elapsed, loss: 3.6950007e-06\n",
      "step: 141960 train: 0.08007597923278809 elapsed, loss: 4.7264457e-06\n",
      "step: 141970 train: 0.08671426773071289 elapsed, loss: 2.3562416e-06\n",
      "step: 141980 train: 0.07850193977355957 elapsed, loss: 2.4908177e-06\n",
      "step: 141990 train: 0.08191943168640137 elapsed, loss: 2.8172446e-06\n",
      "step: 142000 train: 0.0805501937866211 elapsed, loss: 1.7327237e-06\n",
      "step: 142010 train: 0.07464456558227539 elapsed, loss: 2.3748685e-06\n",
      "step: 142020 train: 0.0775156021118164 elapsed, loss: 1.9748668e-06\n",
      "step: 142030 train: 0.07723593711853027 elapsed, loss: 1.5315584e-06\n",
      "step: 142040 train: 0.07591700553894043 elapsed, loss: 1.6326067e-06\n",
      "step: 142050 train: 0.08127021789550781 elapsed, loss: 1.8198023e-06\n",
      "step: 142060 train: 0.07776093482971191 elapsed, loss: 1.9180552e-06\n",
      "step: 142070 train: 0.08268451690673828 elapsed, loss: 1.3788217e-06\n",
      "step: 142080 train: 0.07936978340148926 elapsed, loss: 1.5995448e-06\n",
      "step: 142090 train: 0.07660102844238281 elapsed, loss: 1.5688113e-06\n",
      "step: 142100 train: 0.07887744903564453 elapsed, loss: 1.8244586e-06\n",
      "step: 142110 train: 0.08472013473510742 elapsed, loss: 1.2065275e-06\n",
      "step: 142120 train: 0.08239173889160156 elapsed, loss: 1.6423853e-06\n",
      "step: 142130 train: 0.08322572708129883 elapsed, loss: 1.2996595e-06\n",
      "step: 142140 train: 0.08432126045227051 elapsed, loss: 1.3322558e-06\n",
      "step: 142150 train: 0.0812833309173584 elapsed, loss: 1.5310927e-06\n",
      "step: 142160 train: 0.07925796508789062 elapsed, loss: 1.4118839e-06\n",
      "step: 142170 train: 0.08132290840148926 elapsed, loss: 1.9920965e-06\n",
      "step: 142180 train: 0.08103179931640625 elapsed, loss: 1.5068786e-06\n",
      "step: 142190 train: 0.08752012252807617 elapsed, loss: 1.4374951e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 142200 train: 0.08125019073486328 elapsed, loss: 1.533421e-06\n",
      "step: 142210 train: 0.0923006534576416 elapsed, loss: 1.540406e-06\n",
      "step: 142220 train: 0.08337068557739258 elapsed, loss: 1.7816178e-06\n",
      "step: 142230 train: 0.08028459548950195 elapsed, loss: 2.292913e-06\n",
      "step: 142240 train: 0.07975029945373535 elapsed, loss: 1.9250415e-06\n",
      "step: 142250 train: 0.08071422576904297 elapsed, loss: 1.7690453e-06\n",
      "step: 142260 train: 0.0788736343383789 elapsed, loss: 2.3394787e-06\n",
      "step: 142270 train: 0.08274960517883301 elapsed, loss: 1.4901148e-06\n",
      "step: 142280 train: 0.08452844619750977 elapsed, loss: 1.7248076e-06\n",
      "step: 142290 train: 0.08121895790100098 elapsed, loss: 1.729464e-06\n",
      "step: 142300 train: 0.07719802856445312 elapsed, loss: 3.2879634e-06\n",
      "step: 142310 train: 0.08079147338867188 elapsed, loss: 0.00029656198\n",
      "step: 142320 train: 0.08690929412841797 elapsed, loss: 0.00018384817\n",
      "step: 142330 train: 0.07519936561584473 elapsed, loss: 4.1528765e-05\n",
      "step: 142340 train: 0.08290648460388184 elapsed, loss: 2.9998288e-05\n",
      "step: 142350 train: 0.08650350570678711 elapsed, loss: 1.4989363e-05\n",
      "step: 142360 train: 0.08372974395751953 elapsed, loss: 1.6204227e-05\n",
      "step: 142370 train: 0.08232259750366211 elapsed, loss: 1.5722424e-05\n",
      "step: 142380 train: 0.08240199089050293 elapsed, loss: 6.824691e-06\n",
      "step: 142390 train: 0.08382534980773926 elapsed, loss: 1.3552659e-05\n",
      "step: 142400 train: 0.0841512680053711 elapsed, loss: 4.12667e-06\n",
      "step: 142410 train: 0.08145570755004883 elapsed, loss: 4.5192264e-06\n",
      "step: 142420 train: 0.08126163482666016 elapsed, loss: 2.973707e-06\n",
      "step: 142430 train: 0.0752406120300293 elapsed, loss: 3.610725e-06\n",
      "step: 142440 train: 0.08324718475341797 elapsed, loss: 1.8854598e-06\n",
      "step: 142450 train: 0.08243250846862793 elapsed, loss: 1.4598467e-06\n",
      "step: 142460 train: 0.0814046859741211 elapsed, loss: 2.3413413e-06\n",
      "step: 142470 train: 0.0893404483795166 elapsed, loss: 2.0298144e-06\n",
      "step: 142480 train: 0.08269262313842773 elapsed, loss: 1.5534445e-06\n",
      "step: 142490 train: 0.09003853797912598 elapsed, loss: 1.1539078e-06\n",
      "step: 142500 train: 0.08131790161132812 elapsed, loss: 1.4980308e-06\n",
      "step: 142510 train: 0.07774829864501953 elapsed, loss: 1.4919774e-06\n",
      "step: 142520 train: 0.08348846435546875 elapsed, loss: 1.1227086e-06\n",
      "step: 142530 train: 0.0814971923828125 elapsed, loss: 1.2577501e-06\n",
      "step: 142540 train: 0.07767391204833984 elapsed, loss: 1.7103717e-06\n",
      "step: 142550 train: 0.07821989059448242 elapsed, loss: 1.6246904e-06\n",
      "step: 142560 train: 0.07996320724487305 elapsed, loss: 1.7858088e-06\n",
      "step: 142570 train: 0.08336782455444336 elapsed, loss: 1.8412219e-06\n",
      "step: 142580 train: 0.08325743675231934 elapsed, loss: 1.470557e-06\n",
      "step: 142590 train: 0.0833578109741211 elapsed, loss: 1.7113033e-06\n",
      "step: 142600 train: 0.08156108856201172 elapsed, loss: 2.3688149e-06\n",
      "step: 142610 train: 0.08290600776672363 elapsed, loss: 1.9436675e-06\n",
      "step: 142620 train: 0.08908247947692871 elapsed, loss: 1.5101361e-06\n",
      "step: 142630 train: 0.08630013465881348 elapsed, loss: 1.2773079e-06\n",
      "step: 142640 train: 0.08674049377441406 elapsed, loss: 1.3015222e-06\n",
      "step: 142650 train: 0.08690524101257324 elapsed, loss: 1.5813841e-06\n",
      "step: 142660 train: 0.08557796478271484 elapsed, loss: 1.415609e-06\n",
      "step: 142670 train: 0.08226180076599121 elapsed, loss: 1.406296e-06\n",
      "step: 142680 train: 0.08626937866210938 elapsed, loss: 1.7406398e-06\n",
      "step: 142690 train: 0.08419132232666016 elapsed, loss: 1.51852e-06\n",
      "step: 142700 train: 0.08513784408569336 elapsed, loss: 1.5771932e-06\n",
      "step: 142710 train: 0.08429169654846191 elapsed, loss: 1.2130466e-06\n",
      "step: 142720 train: 0.08544921875 elapsed, loss: 1.3876692e-06\n",
      "step: 142730 train: 0.08150553703308105 elapsed, loss: 1.4957022e-06\n",
      "step: 142740 train: 0.07969427108764648 elapsed, loss: 1.6852265e-06\n",
      "step: 142750 train: 0.08905887603759766 elapsed, loss: 1.4025691e-06\n",
      "step: 142760 train: 0.07768726348876953 elapsed, loss: 1.6596152e-06\n",
      "step: 142770 train: 0.08189582824707031 elapsed, loss: 1.6214308e-06\n",
      "step: 142780 train: 0.08636093139648438 elapsed, loss: 1.6111862e-06\n",
      "step: 142790 train: 0.08308887481689453 elapsed, loss: 1.7406401e-06\n",
      "step: 142800 train: 0.08377671241760254 elapsed, loss: 2.1550773e-06\n",
      "step: 142810 train: 0.07989931106567383 elapsed, loss: 2.151352e-06\n",
      "step: 142820 train: 0.0785818099975586 elapsed, loss: 2.0205011e-06\n",
      "step: 142830 train: 0.07892942428588867 elapsed, loss: 2.3879077e-06\n",
      "step: 142840 train: 0.080047607421875 elapsed, loss: 1.818405e-06\n",
      "step: 142850 train: 0.07994341850280762 elapsed, loss: 1.846345e-06\n",
      "step: 142860 train: 0.08733487129211426 elapsed, loss: 1.8249226e-06\n",
      "step: 142870 train: 0.0792398452758789 elapsed, loss: 2.7823207e-06\n",
      "step: 142880 train: 0.08195805549621582 elapsed, loss: 2.0060666e-06\n",
      "step: 142890 train: 0.077972412109375 elapsed, loss: 6.9191938e-06\n",
      "step: 142900 train: 0.07887434959411621 elapsed, loss: 2.0209677e-06\n",
      "step: 142910 train: 0.08335113525390625 elapsed, loss: 2.0586858e-06\n",
      "step: 142920 train: 0.07990193367004395 elapsed, loss: 1.7830153e-06\n",
      "step: 142930 train: 0.0841226577758789 elapsed, loss: 1.8905828e-06\n",
      "step: 142940 train: 0.08674860000610352 elapsed, loss: 2.537838e-06\n",
      "step: 142950 train: 0.09167718887329102 elapsed, loss: 1.5366808e-06\n",
      "step: 142960 train: 0.0798187255859375 elapsed, loss: 2.5774311e-06\n",
      "step: 142970 train: 0.08162260055541992 elapsed, loss: 2.5210861e-06\n",
      "step: 142980 train: 0.08240723609924316 elapsed, loss: 2.0251578e-06\n",
      "step: 142990 train: 0.08836793899536133 elapsed, loss: 1.7308612e-06\n",
      "step: 143000 train: 0.0834805965423584 elapsed, loss: 1.845413e-06\n",
      "step: 143010 train: 0.08574056625366211 elapsed, loss: 1.7741672e-06\n",
      "step: 143020 train: 0.08392953872680664 elapsed, loss: 2.4419242e-06\n",
      "step: 143030 train: 0.07723259925842285 elapsed, loss: 2.1411079e-06\n",
      "step: 143040 train: 0.0833592414855957 elapsed, loss: 1.8444823e-06\n",
      "step: 143050 train: 0.07850265502929688 elapsed, loss: 2.4735887e-06\n",
      "step: 143060 train: 0.0803229808807373 elapsed, loss: 2.5494915e-06\n",
      "step: 143070 train: 0.08389711380004883 elapsed, loss: 1.8365652e-06\n",
      "step: 143080 train: 0.08123660087585449 elapsed, loss: 1.9362174e-06\n",
      "step: 143090 train: 0.07771086692810059 elapsed, loss: 2.2104912e-06\n",
      "step: 143100 train: 0.08434414863586426 elapsed, loss: 3.5599605e-06\n",
      "step: 143110 train: 0.07705354690551758 elapsed, loss: 2.4526344e-06\n",
      "step: 143120 train: 0.0744180679321289 elapsed, loss: 2.8298182e-06\n",
      "step: 143130 train: 0.0776968002319336 elapsed, loss: 2.7599694e-06\n",
      "step: 143140 train: 0.07957887649536133 elapsed, loss: 2.1769633e-06\n",
      "step: 143150 train: 0.08227825164794922 elapsed, loss: 1.6880203e-06\n",
      "step: 143160 train: 0.07968878746032715 elapsed, loss: 1.8784704e-06\n",
      "step: 143170 train: 0.08991527557373047 elapsed, loss: 6.833735e-06\n",
      "step: 143180 train: 0.07953619956970215 elapsed, loss: 3.348563e-06\n",
      "step: 143190 train: 0.08521485328674316 elapsed, loss: 2.1550777e-06\n",
      "step: 143200 train: 0.07717347145080566 elapsed, loss: 2.2137506e-06\n",
      "step: 143210 train: 0.0891427993774414 elapsed, loss: 1.6903488e-06\n",
      "step: 143220 train: 0.07898712158203125 elapsed, loss: 1.7848777e-06\n",
      "step: 143230 train: 0.0808403491973877 elapsed, loss: 2.991402e-06\n",
      "step: 143240 train: 0.08478426933288574 elapsed, loss: 1.9366828e-06\n",
      "step: 143250 train: 0.08010458946228027 elapsed, loss: 2.4670694e-06\n",
      "step: 143260 train: 0.08139586448669434 elapsed, loss: 1.9497215e-06\n",
      "step: 143270 train: 0.07861924171447754 elapsed, loss: 1.7886032e-06\n",
      "step: 143280 train: 0.08361577987670898 elapsed, loss: 1.5259707e-06\n",
      "step: 143290 train: 0.08427667617797852 elapsed, loss: 1.9245758e-06\n",
      "step: 143300 train: 0.08436179161071777 elapsed, loss: 1.6740506e-06\n",
      "step: 143310 train: 0.08222484588623047 elapsed, loss: 2.1769633e-06\n",
      "step: 143320 train: 0.0817265510559082 elapsed, loss: 1.5650862e-06\n",
      "step: 143330 train: 0.08080935478210449 elapsed, loss: 2.3995492e-06\n",
      "step: 143340 train: 0.08438491821289062 elapsed, loss: 1.958569e-06\n",
      "step: 143350 train: 0.07916045188903809 elapsed, loss: 2.4167782e-06\n",
      "step: 143360 train: 0.07617402076721191 elapsed, loss: 2.752049e-06\n",
      "step: 143370 train: 0.08090853691101074 elapsed, loss: 1.7266702e-06\n",
      "step: 143380 train: 0.0791466236114502 elapsed, loss: 2.1718413e-06\n",
      "step: 143390 train: 0.08205890655517578 elapsed, loss: 0.00010357515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 143400 train: 0.08592557907104492 elapsed, loss: 2.5021725e-05\n",
      "step: 143410 train: 0.08642148971557617 elapsed, loss: 1.3984545e-05\n",
      "step: 143420 train: 0.08212399482727051 elapsed, loss: 2.9176867e-05\n",
      "step: 143430 train: 0.07994413375854492 elapsed, loss: 1.1084032e-05\n",
      "step: 143440 train: 0.08600807189941406 elapsed, loss: 8.650851e-06\n",
      "step: 143450 train: 0.08040618896484375 elapsed, loss: 5.652632e-06\n",
      "step: 143460 train: 0.08274221420288086 elapsed, loss: 5.723843e-06\n",
      "step: 143470 train: 0.0796365737915039 elapsed, loss: 4.587681e-06\n",
      "step: 143480 train: 0.0792703628540039 elapsed, loss: 3.241926e-06\n",
      "step: 143490 train: 0.08427047729492188 elapsed, loss: 2.8689337e-06\n",
      "step: 143500 train: 0.0813283920288086 elapsed, loss: 2.9271405e-06\n",
      "step: 143510 train: 0.08575844764709473 elapsed, loss: 2.4191052e-06\n",
      "step: 143520 train: 0.08359479904174805 elapsed, loss: 2.304554e-06\n",
      "step: 143530 train: 0.07562780380249023 elapsed, loss: 2.4000144e-06\n",
      "step: 143540 train: 0.08008956909179688 elapsed, loss: 1.9632253e-06\n",
      "step: 143550 train: 0.08176326751708984 elapsed, loss: 2.5532152e-06\n",
      "step: 143560 train: 0.08076357841491699 elapsed, loss: 1.7629915e-06\n",
      "step: 143570 train: 0.08401656150817871 elapsed, loss: 1.625622e-06\n",
      "step: 143580 train: 0.09342384338378906 elapsed, loss: 1.4738167e-06\n",
      "step: 143590 train: 0.08413577079772949 elapsed, loss: 1.5357494e-06\n",
      "step: 143600 train: 0.08127951622009277 elapsed, loss: 1.7443654e-06\n",
      "step: 143610 train: 0.08080863952636719 elapsed, loss: 1.9730046e-06\n",
      "step: 143620 train: 0.0750586986541748 elapsed, loss: 2.4153815e-06\n",
      "step: 143630 train: 0.0819087028503418 elapsed, loss: 1.7341198e-06\n",
      "step: 143640 train: 0.08565545082092285 elapsed, loss: 2.148557e-06\n",
      "step: 143650 train: 0.0855557918548584 elapsed, loss: 2.0437842e-06\n",
      "step: 143660 train: 0.08129000663757324 elapsed, loss: 2.0200355e-06\n",
      "step: 143670 train: 0.07893824577331543 elapsed, loss: 7.954016e-05\n",
      "step: 143680 train: 0.08552980422973633 elapsed, loss: 2.6875583e-05\n",
      "step: 143690 train: 0.07854056358337402 elapsed, loss: 2.8209452e-05\n",
      "step: 143700 train: 0.09202218055725098 elapsed, loss: 9.099411e-06\n",
      "step: 143710 train: 0.08687734603881836 elapsed, loss: 1.1524467e-05\n",
      "step: 143720 train: 0.08326578140258789 elapsed, loss: 6.302691e-06\n",
      "step: 143730 train: 0.07560586929321289 elapsed, loss: 7.016929e-06\n",
      "step: 143740 train: 0.0808262825012207 elapsed, loss: 5.6731105e-06\n",
      "step: 143750 train: 0.0858762264251709 elapsed, loss: 3.1343575e-06\n",
      "step: 143760 train: 0.09293866157531738 elapsed, loss: 2.6682326e-06\n",
      "step: 143770 train: 0.08923697471618652 elapsed, loss: 2.8009467e-06\n",
      "step: 143780 train: 0.07933998107910156 elapsed, loss: 2.2812712e-06\n",
      "step: 143790 train: 0.08074760437011719 elapsed, loss: 1.8682306e-06\n",
      "step: 143800 train: 0.07725691795349121 elapsed, loss: 2.466603e-06\n",
      "step: 143810 train: 0.08068466186523438 elapsed, loss: 2.3036218e-06\n",
      "step: 143820 train: 0.08101272583007812 elapsed, loss: 1.8752157e-06\n",
      "step: 143830 train: 0.08062553405761719 elapsed, loss: 1.8924447e-06\n",
      "step: 143840 train: 0.08507609367370605 elapsed, loss: 1.3895319e-06\n",
      "step: 143850 train: 0.08395218849182129 elapsed, loss: 1.1585644e-06\n",
      "step: 143860 train: 0.0754845142364502 elapsed, loss: 0.13009183\n",
      "step: 143870 train: 0.08205676078796387 elapsed, loss: 0.00012149425\n",
      "step: 143880 train: 0.07798218727111816 elapsed, loss: 3.607312e-05\n",
      "step: 143890 train: 0.08033299446105957 elapsed, loss: 3.159722e-05\n",
      "step: 143900 train: 0.08342099189758301 elapsed, loss: 1.9169263e-05\n",
      "step: 143910 train: 0.07880640029907227 elapsed, loss: 1.6123995e-05\n",
      "step: 143920 train: 0.08788347244262695 elapsed, loss: 1.0886344e-05\n",
      "step: 143930 train: 0.08850598335266113 elapsed, loss: 7.0844976e-06\n",
      "step: 143940 train: 0.0802156925201416 elapsed, loss: 7.05237e-06\n",
      "step: 143950 train: 0.07459068298339844 elapsed, loss: 7.017008e-06\n",
      "step: 143960 train: 0.08349752426147461 elapsed, loss: 5.2456517e-06\n",
      "step: 143970 train: 0.0903329849243164 elapsed, loss: 3.130162e-06\n",
      "step: 143980 train: 0.08186936378479004 elapsed, loss: 2.7371507e-06\n",
      "step: 143990 train: 0.08134198188781738 elapsed, loss: 2.7576375e-06\n",
      "step: 144000 train: 0.07617068290710449 elapsed, loss: 2.3026914e-06\n",
      "step: 144010 train: 0.07519340515136719 elapsed, loss: 2.2738177e-06\n",
      "step: 144020 train: 0.0826880931854248 elapsed, loss: 1.6763784e-06\n",
      "step: 144030 train: 0.08889150619506836 elapsed, loss: 1.2698572e-06\n",
      "step: 144040 train: 0.08180451393127441 elapsed, loss: 1.4188687e-06\n",
      "step: 144050 train: 0.0833747386932373 elapsed, loss: 1.3709055e-06\n",
      "step: 144060 train: 0.08343887329101562 elapsed, loss: 1.2814988e-06\n",
      "step: 144070 train: 0.07791352272033691 elapsed, loss: 6.567111e-06\n",
      "step: 144080 train: 0.08237457275390625 elapsed, loss: 1.8412221e-06\n",
      "step: 144090 train: 0.08284521102905273 elapsed, loss: 1.3392405e-06\n",
      "step: 144100 train: 0.0889596939086914 elapsed, loss: 1.2759108e-06\n",
      "step: 144110 train: 0.08279991149902344 elapsed, loss: 1.4151433e-06\n",
      "step: 144120 train: 0.08821845054626465 elapsed, loss: 1.226085e-06\n",
      "step: 144130 train: 0.08482933044433594 elapsed, loss: 1.5157259e-06\n",
      "step: 144140 train: 0.08025789260864258 elapsed, loss: 1.5255048e-06\n",
      "step: 144150 train: 0.08455824851989746 elapsed, loss: 2.6002467e-06\n",
      "step: 144160 train: 0.08013629913330078 elapsed, loss: 2.5811555e-06\n",
      "step: 144170 train: 0.0823211669921875 elapsed, loss: 1.355073e-06\n",
      "step: 144180 train: 0.08874654769897461 elapsed, loss: 1.1613583e-06\n",
      "step: 144190 train: 0.08178877830505371 elapsed, loss: 1.371837e-06\n",
      "step: 144200 train: 0.08383870124816895 elapsed, loss: 1.9087431e-06\n",
      "step: 144210 train: 0.07836151123046875 elapsed, loss: 2.1848796e-06\n",
      "step: 144220 train: 0.08001899719238281 elapsed, loss: 1.2400551e-06\n",
      "step: 144230 train: 0.08260560035705566 elapsed, loss: 1.2516966e-06\n",
      "step: 144240 train: 0.08502936363220215 elapsed, loss: 1.4025699e-06\n",
      "step: 144250 train: 0.08811044692993164 elapsed, loss: 2.257518e-06\n",
      "step: 144260 train: 0.07983231544494629 elapsed, loss: 1.7988475e-06\n",
      "step: 144270 train: 0.0831296443939209 elapsed, loss: 1.8039692e-06\n",
      "step: 144280 train: 0.08126950263977051 elapsed, loss: 1.7113034e-06\n",
      "step: 144290 train: 0.07698774337768555 elapsed, loss: 1.9110716e-06\n",
      "step: 144300 train: 0.08519959449768066 elapsed, loss: 1.382547e-06\n",
      "step: 144310 train: 0.07893180847167969 elapsed, loss: 1.8519327e-06\n",
      "step: 144320 train: 0.08297204971313477 elapsed, loss: 1.6060642e-06\n",
      "step: 144330 train: 0.086181640625 elapsed, loss: 2.0000123e-06\n",
      "step: 144340 train: 0.08559536933898926 elapsed, loss: 1.8039696e-06\n",
      "step: 144350 train: 0.08221435546875 elapsed, loss: 1.6749817e-06\n",
      "step: 144360 train: 0.08345913887023926 elapsed, loss: 1.766251e-06\n",
      "step: 144370 train: 0.0789651870727539 elapsed, loss: 2.5252762e-06\n",
      "step: 144380 train: 0.08063840866088867 elapsed, loss: 2.1159622e-06\n",
      "step: 144390 train: 0.07906532287597656 elapsed, loss: 1.7886028e-06\n",
      "step: 144400 train: 0.0764472484588623 elapsed, loss: 2.187674e-06\n",
      "step: 144410 train: 0.0858151912689209 elapsed, loss: 5.1795264e-06\n",
      "step: 144420 train: 0.0819704532623291 elapsed, loss: 2.4884898e-06\n",
      "step: 144430 train: 0.08277511596679688 elapsed, loss: 1.9222475e-06\n",
      "step: 144440 train: 0.0841064453125 elapsed, loss: 1.6922112e-06\n",
      "step: 144450 train: 0.08644819259643555 elapsed, loss: 1.5576353e-06\n",
      "step: 144460 train: 0.08397531509399414 elapsed, loss: 1.7383118e-06\n",
      "step: 144470 train: 0.07775735855102539 elapsed, loss: 1.9231788e-06\n",
      "step: 144480 train: 0.08070063591003418 elapsed, loss: 2.6738223e-06\n",
      "step: 144490 train: 0.08723950386047363 elapsed, loss: 1.6773092e-06\n",
      "step: 144500 train: 0.07762408256530762 elapsed, loss: 2.4572907e-06\n",
      "step: 144510 train: 0.07836580276489258 elapsed, loss: 1.8374974e-06\n",
      "step: 144520 train: 0.07892251014709473 elapsed, loss: 2.3203866e-06\n",
      "step: 144530 train: 0.07952022552490234 elapsed, loss: 8.0753925e-06\n",
      "step: 144540 train: 0.07622051239013672 elapsed, loss: 3.588842e-06\n",
      "step: 144550 train: 0.09000730514526367 elapsed, loss: 1.7825489e-06\n",
      "step: 144560 train: 0.08171558380126953 elapsed, loss: 1.6181713e-06\n",
      "step: 144570 train: 0.08108997344970703 elapsed, loss: 1.4896491e-06\n",
      "step: 144580 train: 0.07841277122497559 elapsed, loss: 1.9269041e-06\n",
      "step: 144590 train: 0.0800940990447998 elapsed, loss: 1.9813865e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 144600 train: 0.08280777931213379 elapsed, loss: 1.8686962e-06\n",
      "step: 144610 train: 0.0865778923034668 elapsed, loss: 1.5641547e-06\n",
      "step: 144620 train: 0.08006930351257324 elapsed, loss: 2.0596165e-06\n",
      "step: 144630 train: 0.08341598510742188 elapsed, loss: 1.5399401e-06\n",
      "step: 144640 train: 0.08297848701477051 elapsed, loss: 2.1653166e-06\n",
      "step: 144650 train: 0.08191561698913574 elapsed, loss: 1.8556582e-06\n",
      "step: 144660 train: 0.08430767059326172 elapsed, loss: 1.532024e-06\n",
      "step: 144670 train: 0.08066511154174805 elapsed, loss: 1.4612439e-06\n",
      "step: 144680 train: 0.08242964744567871 elapsed, loss: 2.2561258e-06\n",
      "step: 144690 train: 0.07908010482788086 elapsed, loss: 1.9641561e-06\n",
      "step: 144700 train: 0.08522939682006836 elapsed, loss: 1.6321411e-06\n",
      "step: 144710 train: 0.07602667808532715 elapsed, loss: 2.1187557e-06\n",
      "step: 144720 train: 0.07360410690307617 elapsed, loss: 3.1455352e-06\n",
      "step: 144730 train: 0.08248519897460938 elapsed, loss: 2.2244599e-06\n",
      "step: 144740 train: 0.07872486114501953 elapsed, loss: 3.3811589e-06\n",
      "step: 144750 train: 0.08373904228210449 elapsed, loss: 1.9674167e-06\n",
      "step: 144760 train: 0.08800482749938965 elapsed, loss: 1.4849925e-06\n",
      "step: 144770 train: 0.08776164054870605 elapsed, loss: 2.4363362e-06\n",
      "step: 144780 train: 0.0804893970489502 elapsed, loss: 1.9296979e-06\n",
      "step: 144790 train: 0.08104109764099121 elapsed, loss: 2.0214331e-06\n",
      "step: 144800 train: 0.07838559150695801 elapsed, loss: 2.5369186e-06\n",
      "step: 144810 train: 0.07813787460327148 elapsed, loss: 2.4409928e-06\n",
      "step: 144820 train: 0.08032631874084473 elapsed, loss: 2.285928e-06\n",
      "step: 144830 train: 0.07272624969482422 elapsed, loss: 2.5029256e-06\n",
      "step: 144840 train: 0.07691597938537598 elapsed, loss: 0.0012609842\n",
      "step: 144850 train: 0.08311057090759277 elapsed, loss: 2.3145853e-05\n",
      "step: 144860 train: 0.08441638946533203 elapsed, loss: 1.4182394e-05\n",
      "step: 144870 train: 0.08151912689208984 elapsed, loss: 9.659565e-06\n",
      "step: 144880 train: 0.07710790634155273 elapsed, loss: 1.0984353e-05\n",
      "step: 144890 train: 0.08054804801940918 elapsed, loss: 1.31517245e-05\n",
      "step: 144900 train: 0.08340787887573242 elapsed, loss: 4.7986186e-06\n",
      "step: 144910 train: 0.07922101020812988 elapsed, loss: 4.4661383e-06\n",
      "step: 144920 train: 0.08463144302368164 elapsed, loss: 3.285687e-06\n",
      "step: 144930 train: 0.08236169815063477 elapsed, loss: 3.3574086e-06\n",
      "step: 144940 train: 0.08076071739196777 elapsed, loss: 2.9895386e-06\n",
      "step: 144950 train: 0.0830221176147461 elapsed, loss: 2.2519346e-06\n",
      "step: 144960 train: 0.07802319526672363 elapsed, loss: 2.6118882e-06\n",
      "step: 144970 train: 0.07922530174255371 elapsed, loss: 2.8721931e-06\n",
      "step: 144980 train: 0.08351731300354004 elapsed, loss: 1.7085093e-06\n",
      "step: 144990 train: 0.07543730735778809 elapsed, loss: 2.0950074e-06\n",
      "step: 145000 train: 0.07881021499633789 elapsed, loss: 2.6048447e-06\n",
      "step: 145010 train: 0.08028888702392578 elapsed, loss: 2.0591508e-06\n",
      "step: 145020 train: 0.0804133415222168 elapsed, loss: 2.1331894e-06\n",
      "step: 145030 train: 0.07846426963806152 elapsed, loss: 2.039128e-06\n",
      "step: 145040 train: 0.08366751670837402 elapsed, loss: 1.2940717e-06\n",
      "step: 145050 train: 0.07564473152160645 elapsed, loss: 0.00012411641\n",
      "step: 145060 train: 0.07577395439147949 elapsed, loss: 3.7004993e-05\n",
      "step: 145070 train: 0.0788412094116211 elapsed, loss: 3.416526e-05\n",
      "step: 145080 train: 0.08264899253845215 elapsed, loss: 8.611403e-06\n",
      "step: 145090 train: 0.08542370796203613 elapsed, loss: 6.7241053e-06\n",
      "step: 145100 train: 0.07587242126464844 elapsed, loss: 1.3176137e-05\n",
      "step: 145110 train: 0.08681654930114746 elapsed, loss: 3.9972238e-06\n",
      "step: 145120 train: 0.08626270294189453 elapsed, loss: 3.6703293e-06\n",
      "step: 145130 train: 0.07956910133361816 elapsed, loss: 4.2342426e-06\n",
      "step: 145140 train: 0.07736086845397949 elapsed, loss: 3.7797618e-06\n",
      "step: 145150 train: 0.08063817024230957 elapsed, loss: 2.3222492e-06\n",
      "step: 145160 train: 0.08428049087524414 elapsed, loss: 2.4600845e-06\n",
      "step: 145170 train: 0.0894007682800293 elapsed, loss: 1.5762603e-06\n",
      "step: 145180 train: 0.08402276039123535 elapsed, loss: 1.7066467e-06\n",
      "step: 145190 train: 0.08913826942443848 elapsed, loss: 1.4570523e-06\n",
      "step: 145200 train: 0.08179044723510742 elapsed, loss: 1.6042015e-06\n",
      "step: 145210 train: 0.08133912086486816 elapsed, loss: 1.8496045e-06\n",
      "step: 145220 train: 0.08189511299133301 elapsed, loss: 1.360661e-06\n",
      "step: 145230 train: 0.08502936363220215 elapsed, loss: 1.1404035e-06\n",
      "step: 145240 train: 0.08855485916137695 elapsed, loss: 1.3248048e-06\n",
      "step: 145250 train: 0.07893681526184082 elapsed, loss: 1.5739329e-06\n",
      "step: 145260 train: 0.07849478721618652 elapsed, loss: 1.502222e-06\n",
      "step: 145270 train: 0.08531379699707031 elapsed, loss: 1.5492536e-06\n",
      "step: 145280 train: 0.08251595497131348 elapsed, loss: 2.2802822e-06\n",
      "step: 145290 train: 0.07966494560241699 elapsed, loss: 1.2558875e-06\n",
      "step: 145300 train: 0.07960224151611328 elapsed, loss: 1.3229426e-06\n",
      "step: 145310 train: 0.07579588890075684 elapsed, loss: 8.674714e-05\n",
      "step: 145320 train: 0.08179521560668945 elapsed, loss: 0.00022767337\n",
      "step: 145330 train: 0.08210444450378418 elapsed, loss: 4.832665e-05\n",
      "step: 145340 train: 0.0822596549987793 elapsed, loss: 2.0818406e-05\n",
      "step: 145350 train: 0.08200478553771973 elapsed, loss: 1.2126064e-05\n",
      "step: 145360 train: 0.08343029022216797 elapsed, loss: 1.1652544e-05\n",
      "step: 145370 train: 0.07797718048095703 elapsed, loss: 1.35237315e-05\n",
      "step: 145380 train: 0.08046174049377441 elapsed, loss: 8.237454e-06\n",
      "step: 145390 train: 0.08181262016296387 elapsed, loss: 4.0684695e-06\n",
      "step: 145400 train: 0.07928705215454102 elapsed, loss: 6.5695185e-06\n",
      "step: 145410 train: 0.08490538597106934 elapsed, loss: 3.5962912e-06\n",
      "step: 145420 train: 0.08435630798339844 elapsed, loss: 3.0314477e-06\n",
      "step: 145430 train: 0.0921165943145752 elapsed, loss: 2.2822003e-06\n",
      "step: 145440 train: 0.07524275779724121 elapsed, loss: 3.0673045e-06\n",
      "step: 145450 train: 0.08096456527709961 elapsed, loss: 2.0624107e-06\n",
      "step: 145460 train: 0.07700562477111816 elapsed, loss: 1.8510009e-06\n",
      "step: 145470 train: 0.08158063888549805 elapsed, loss: 1.515726e-06\n",
      "step: 145480 train: 0.08434700965881348 elapsed, loss: 1.5487878e-06\n",
      "step: 145490 train: 0.08469724655151367 elapsed, loss: 1.1096699e-06\n",
      "step: 145500 train: 0.08336901664733887 elapsed, loss: 1.2679947e-06\n",
      "step: 145510 train: 0.08344912528991699 elapsed, loss: 1.7201477e-06\n",
      "step: 145520 train: 0.07960677146911621 elapsed, loss: 1.2018709e-06\n",
      "step: 145530 train: 0.08422040939331055 elapsed, loss: 1.6936083e-06\n",
      "step: 145540 train: 0.0811464786529541 elapsed, loss: 1.5287645e-06\n",
      "step: 145550 train: 0.08042454719543457 elapsed, loss: 1.9953561e-06\n",
      "step: 145560 train: 0.08478903770446777 elapsed, loss: 1.4184031e-06\n",
      "step: 145570 train: 0.08136987686157227 elapsed, loss: 1.4998936e-06\n",
      "step: 145580 train: 0.08229255676269531 elapsed, loss: 1.3466913e-06\n",
      "step: 145590 train: 0.08554196357727051 elapsed, loss: 1.0947688e-06\n",
      "step: 145600 train: 0.07978510856628418 elapsed, loss: 1.7387772e-06\n",
      "step: 145610 train: 0.07931232452392578 elapsed, loss: 1.4188686e-06\n",
      "step: 145620 train: 0.07949090003967285 elapsed, loss: 1.3643864e-06\n",
      "step: 145630 train: 0.08197188377380371 elapsed, loss: 1.6521644e-06\n",
      "step: 145640 train: 0.08040332794189453 elapsed, loss: 2.0954726e-06\n",
      "step: 145650 train: 0.08274435997009277 elapsed, loss: 1.5711396e-06\n",
      "step: 145660 train: 0.08364462852478027 elapsed, loss: 1.8691622e-06\n",
      "step: 145670 train: 0.08643746376037598 elapsed, loss: 1.3750964e-06\n",
      "step: 145680 train: 0.08031535148620605 elapsed, loss: 1.4044332e-06\n",
      "step: 145690 train: 0.08800601959228516 elapsed, loss: 1.2861553e-06\n",
      "step: 145700 train: 0.08604097366333008 elapsed, loss: 1.3387751e-06\n",
      "step: 145710 train: 0.07975053787231445 elapsed, loss: 1.6493706e-06\n",
      "step: 145720 train: 0.07835888862609863 elapsed, loss: 2.1923302e-06\n",
      "step: 145730 train: 0.0802145004272461 elapsed, loss: 1.4388912e-06\n",
      "step: 145740 train: 0.0841984748840332 elapsed, loss: 1.6237591e-06\n",
      "step: 145750 train: 0.08269929885864258 elapsed, loss: 1.8645057e-06\n",
      "step: 145760 train: 0.07961034774780273 elapsed, loss: 1.7825496e-06\n",
      "step: 145770 train: 0.07884597778320312 elapsed, loss: 1.8845291e-06\n",
      "step: 145780 train: 0.08167338371276855 elapsed, loss: 1.5930258e-06\n",
      "step: 145790 train: 0.0829305648803711 elapsed, loss: 1.5590326e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 145800 train: 0.07647848129272461 elapsed, loss: 2.0675334e-06\n",
      "step: 145810 train: 0.08010625839233398 elapsed, loss: 2.3543782e-06\n",
      "step: 145820 train: 0.07973957061767578 elapsed, loss: 1.9068806e-06\n",
      "step: 145830 train: 0.07769417762756348 elapsed, loss: 1.8309781e-06\n",
      "step: 145840 train: 0.08234214782714844 elapsed, loss: 1.8253902e-06\n",
      "step: 145850 train: 0.0786740779876709 elapsed, loss: 2.0614796e-06\n",
      "step: 145860 train: 0.07744789123535156 elapsed, loss: 2.1271383e-06\n",
      "step: 145870 train: 0.08389663696289062 elapsed, loss: 2.0856933e-06\n",
      "step: 145880 train: 0.07992315292358398 elapsed, loss: 1.7355176e-06\n",
      "step: 145890 train: 0.08361077308654785 elapsed, loss: 1.9073462e-06\n",
      "step: 145900 train: 0.07816696166992188 elapsed, loss: 2.1872079e-06\n",
      "step: 145910 train: 0.0818028450012207 elapsed, loss: 2.2924473e-06\n",
      "step: 145920 train: 0.08030223846435547 elapsed, loss: 2.611424e-06\n",
      "step: 145930 train: 0.08579635620117188 elapsed, loss: 1.8374958e-06\n",
      "step: 145940 train: 0.07434940338134766 elapsed, loss: 2.9397138e-06\n",
      "step: 145950 train: 0.07684159278869629 elapsed, loss: 2.2761492e-06\n",
      "step: 145960 train: 0.08849954605102539 elapsed, loss: 2.9103771e-06\n",
      "step: 145970 train: 0.09340071678161621 elapsed, loss: 1.9455292e-06\n",
      "step: 145980 train: 0.07954907417297363 elapsed, loss: 2.7017616e-06\n",
      "step: 145990 train: 0.07935500144958496 elapsed, loss: 2.624463e-06\n",
      "step: 146000 train: 0.08119463920593262 elapsed, loss: 1.6554243e-06\n",
      "step: 146010 train: 0.07646369934082031 elapsed, loss: 0.009477141\n",
      "step: 146020 train: 0.08212566375732422 elapsed, loss: 9.6952535e-05\n",
      "step: 146030 train: 0.07909107208251953 elapsed, loss: 4.0945582e-05\n",
      "step: 146040 train: 0.07955098152160645 elapsed, loss: 2.3560326e-05\n",
      "step: 146050 train: 0.07638812065124512 elapsed, loss: 2.9033457e-05\n",
      "step: 146060 train: 0.0754551887512207 elapsed, loss: 1.5533167e-05\n",
      "step: 146070 train: 0.0780026912689209 elapsed, loss: 1.2637831e-05\n",
      "step: 146080 train: 0.08100724220275879 elapsed, loss: 1.8529547e-05\n",
      "step: 146090 train: 0.08661079406738281 elapsed, loss: 6.4884835e-06\n",
      "step: 146100 train: 0.0808861255645752 elapsed, loss: 4.5727775e-06\n",
      "step: 146110 train: 0.0811772346496582 elapsed, loss: 4.96576e-06\n",
      "step: 146120 train: 0.08620786666870117 elapsed, loss: 4.2917686e-06\n",
      "step: 146130 train: 0.08555054664611816 elapsed, loss: 2.6891857e-06\n",
      "step: 146140 train: 0.08419132232666016 elapsed, loss: 2.734824e-06\n",
      "step: 146150 train: 0.08442902565002441 elapsed, loss: 1.8118853e-06\n",
      "step: 146160 train: 0.08785367012023926 elapsed, loss: 1.4407547e-06\n",
      "step: 146170 train: 0.08124136924743652 elapsed, loss: 1.7592663e-06\n",
      "step: 146180 train: 0.08280181884765625 elapsed, loss: 1.3941886e-06\n",
      "step: 146190 train: 0.08390450477600098 elapsed, loss: 1.2335357e-06\n",
      "step: 146200 train: 0.07414555549621582 elapsed, loss: 2.6943103e-06\n",
      "step: 146210 train: 0.08137917518615723 elapsed, loss: 1.4919772e-06\n",
      "step: 146220 train: 0.08458614349365234 elapsed, loss: 1.2242224e-06\n",
      "step: 146230 train: 0.0878598690032959 elapsed, loss: 1.2763767e-06\n",
      "step: 146240 train: 0.07898783683776855 elapsed, loss: 2.4064905e-06\n",
      "step: 146250 train: 0.08156251907348633 elapsed, loss: 2.2510033e-06\n",
      "step: 146260 train: 0.08402132987976074 elapsed, loss: 1.5022217e-06\n",
      "step: 146270 train: 0.08281779289245605 elapsed, loss: 2.3329592e-06\n",
      "step: 146280 train: 0.0825650691986084 elapsed, loss: 1.4048987e-06\n",
      "step: 146290 train: 0.08270621299743652 elapsed, loss: 1.5222454e-06\n",
      "step: 146300 train: 0.07829546928405762 elapsed, loss: 1.4123493e-06\n",
      "step: 146310 train: 0.08192610740661621 elapsed, loss: 1.5795215e-06\n",
      "step: 146320 train: 0.07841038703918457 elapsed, loss: 2.6980347e-06\n",
      "step: 146330 train: 0.08198666572570801 elapsed, loss: 1.473351e-06\n",
      "step: 146340 train: 0.07696771621704102 elapsed, loss: 1.4421516e-06\n",
      "step: 146350 train: 0.08680319786071777 elapsed, loss: 1.3010566e-06\n",
      "step: 146360 train: 0.0848989486694336 elapsed, loss: 1.7252733e-06\n",
      "step: 146370 train: 0.0787806510925293 elapsed, loss: 1.9208505e-06\n",
      "step: 146380 train: 0.08128952980041504 elapsed, loss: 1.7285329e-06\n",
      "step: 146390 train: 0.08037281036376953 elapsed, loss: 0.000514106\n",
      "step: 146400 train: 0.0797586441040039 elapsed, loss: 4.864028e-05\n",
      "step: 146410 train: 0.08377790451049805 elapsed, loss: 4.519115e-05\n",
      "step: 146420 train: 0.08454585075378418 elapsed, loss: 1.8249495e-05\n",
      "step: 146430 train: 0.08865737915039062 elapsed, loss: 1.0453895e-05\n",
      "step: 146440 train: 0.08077383041381836 elapsed, loss: 7.751331e-06\n",
      "step: 146450 train: 0.0769195556640625 elapsed, loss: 5.792326e-06\n",
      "step: 146460 train: 0.08511233329772949 elapsed, loss: 4.4531007e-06\n",
      "step: 146470 train: 0.08494162559509277 elapsed, loss: 3.4011778e-06\n",
      "step: 146480 train: 0.08020949363708496 elapsed, loss: 3.9790566e-06\n",
      "step: 146490 train: 0.08294296264648438 elapsed, loss: 0.0027605277\n",
      "step: 146500 train: 0.08087563514709473 elapsed, loss: 0.00016032542\n",
      "step: 146510 train: 0.08350419998168945 elapsed, loss: 3.0783453e-05\n",
      "step: 146520 train: 0.08047795295715332 elapsed, loss: 3.0263702e-05\n",
      "step: 146530 train: 0.08372259140014648 elapsed, loss: 1.4411637e-05\n",
      "step: 146540 train: 0.07325983047485352 elapsed, loss: 2.1170106e-05\n",
      "step: 146550 train: 0.08179807662963867 elapsed, loss: 9.008104e-06\n",
      "step: 146560 train: 0.08108019828796387 elapsed, loss: 7.765699e-06\n",
      "step: 146570 train: 0.08349394798278809 elapsed, loss: 4.1587973e-06\n",
      "step: 146580 train: 0.07653212547302246 elapsed, loss: 4.509906e-06\n",
      "step: 146590 train: 0.07652473449707031 elapsed, loss: 3.2205044e-06\n",
      "step: 146600 train: 0.08099651336669922 elapsed, loss: 2.9173598e-06\n",
      "step: 146610 train: 0.08306145668029785 elapsed, loss: 1.7727702e-06\n",
      "step: 146620 train: 0.08421468734741211 elapsed, loss: 2.4209683e-06\n",
      "step: 146630 train: 0.08238029479980469 elapsed, loss: 1.5413368e-06\n",
      "step: 146640 train: 0.08163642883300781 elapsed, loss: 1.3173546e-06\n",
      "step: 146650 train: 0.07524800300598145 elapsed, loss: 2.2225981e-06\n",
      "step: 146660 train: 0.0816812515258789 elapsed, loss: 1.4295786e-06\n",
      "step: 146670 train: 0.08171439170837402 elapsed, loss: 1.4947648e-06\n",
      "step: 146680 train: 0.08533191680908203 elapsed, loss: 1.2307419e-06\n",
      "step: 146690 train: 0.0836489200592041 elapsed, loss: 1.5231765e-06\n",
      "step: 146700 train: 0.0851435661315918 elapsed, loss: 1.0197975e-06\n",
      "step: 146710 train: 0.08028316497802734 elapsed, loss: 1.4398229e-06\n",
      "step: 146720 train: 0.08544802665710449 elapsed, loss: 1.0728829e-06\n",
      "step: 146730 train: 0.08796477317810059 elapsed, loss: 1.0519262e-06\n",
      "step: 146740 train: 0.08631587028503418 elapsed, loss: 1.2093215e-06\n",
      "step: 146750 train: 0.07677626609802246 elapsed, loss: 1.3685753e-06\n",
      "step: 146760 train: 0.08179140090942383 elapsed, loss: 1.0929061e-06\n",
      "step: 146770 train: 0.07628297805786133 elapsed, loss: 1.160427e-06\n",
      "step: 146780 train: 0.08357930183410645 elapsed, loss: 1.1497166e-06\n",
      "step: 146790 train: 0.08387136459350586 elapsed, loss: 1.2726514e-06\n",
      "step: 146800 train: 0.08369874954223633 elapsed, loss: 1.1548391e-06\n",
      "step: 146810 train: 0.08267903327941895 elapsed, loss: 1.1669463e-06\n",
      "step: 146820 train: 0.08640003204345703 elapsed, loss: 9.140926e-07\n",
      "step: 146830 train: 0.07979989051818848 elapsed, loss: 1.6866232e-06\n",
      "step: 146840 train: 0.08158159255981445 elapsed, loss: 1.1404038e-06\n",
      "step: 146850 train: 0.0817716121673584 elapsed, loss: 1.6600807e-06\n",
      "step: 146860 train: 0.07797122001647949 elapsed, loss: 1.5916285e-06\n",
      "step: 146870 train: 0.08428764343261719 elapsed, loss: 1.4621753e-06\n",
      "step: 146880 train: 0.08178830146789551 elapsed, loss: 1.9841796e-06\n",
      "step: 146890 train: 0.07901406288146973 elapsed, loss: 1.7085093e-06\n",
      "step: 146900 train: 0.0802001953125 elapsed, loss: 1.6535614e-06\n",
      "step: 146910 train: 0.08038878440856934 elapsed, loss: 1.7336552e-06\n",
      "step: 146920 train: 0.07864904403686523 elapsed, loss: 1.9222475e-06\n",
      "step: 146930 train: 0.08481001853942871 elapsed, loss: 1.2721857e-06\n",
      "step: 146940 train: 0.0818936824798584 elapsed, loss: 2.3160471e-06\n",
      "step: 146950 train: 0.07821130752563477 elapsed, loss: 0.03526\n",
      "step: 146960 train: 0.07999324798583984 elapsed, loss: 3.8274957e-05\n",
      "step: 146970 train: 0.0857701301574707 elapsed, loss: 2.530551e-05\n",
      "step: 146980 train: 0.08214187622070312 elapsed, loss: 1.4348401e-05\n",
      "step: 146990 train: 0.08533310890197754 elapsed, loss: 7.9222245e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 147000 train: 0.08296632766723633 elapsed, loss: 7.3480433e-06\n",
      "step: 147010 train: 0.0802311897277832 elapsed, loss: 9.068673e-06\n",
      "step: 147020 train: 0.08129334449768066 elapsed, loss: 6.0521706e-06\n",
      "step: 147030 train: 0.08717131614685059 elapsed, loss: 3.7937198e-06\n",
      "step: 147040 train: 0.08445262908935547 elapsed, loss: 3.89152e-06\n",
      "step: 147050 train: 0.08066153526306152 elapsed, loss: 3.3946583e-06\n",
      "step: 147060 train: 0.08725762367248535 elapsed, loss: 1.8342373e-06\n",
      "step: 147070 train: 0.0871124267578125 elapsed, loss: 1.6530955e-06\n",
      "step: 147080 train: 0.08296966552734375 elapsed, loss: 2.3175926e-06\n",
      "step: 147090 train: 0.07936716079711914 elapsed, loss: 2.118755e-06\n",
      "step: 147100 train: 0.0895988941192627 elapsed, loss: 1.2270164e-06\n",
      "step: 147110 train: 0.07775688171386719 elapsed, loss: 1.575796e-06\n",
      "step: 147120 train: 0.08261752128601074 elapsed, loss: 1.3750963e-06\n",
      "step: 147130 train: 0.07659053802490234 elapsed, loss: 1.6647373e-06\n",
      "step: 147140 train: 0.07509684562683105 elapsed, loss: 1.5520475e-06\n",
      "step: 147150 train: 0.07806730270385742 elapsed, loss: 1.5315584e-06\n",
      "step: 147160 train: 0.08224892616271973 elapsed, loss: 1.3220113e-06\n",
      "step: 147170 train: 0.08136534690856934 elapsed, loss: 1.5529788e-06\n",
      "step: 147180 train: 0.07836365699768066 elapsed, loss: 1.1962829e-06\n",
      "step: 147190 train: 0.08205509185791016 elapsed, loss: 1.3373777e-06\n",
      "step: 147200 train: 0.08732199668884277 elapsed, loss: 1.5762619e-06\n",
      "step: 147210 train: 0.08251953125 elapsed, loss: 1.5865063e-06\n",
      "step: 147220 train: 0.0834047794342041 elapsed, loss: 1.4817329e-06\n",
      "step: 147230 train: 0.07807564735412598 elapsed, loss: 1.268926e-06\n",
      "step: 147240 train: 0.08176946640014648 elapsed, loss: 1.5078097e-06\n",
      "step: 147250 train: 0.07879519462585449 elapsed, loss: 0.0002336844\n",
      "step: 147260 train: 0.08271956443786621 elapsed, loss: 2.5835043e-05\n",
      "step: 147270 train: 0.08643341064453125 elapsed, loss: 1.6477024e-05\n",
      "step: 147280 train: 0.07981228828430176 elapsed, loss: 1.250798e-05\n",
      "step: 147290 train: 0.0904996395111084 elapsed, loss: 7.287073e-06\n",
      "step: 147300 train: 0.07955050468444824 elapsed, loss: 9.073315e-06\n",
      "step: 147310 train: 0.07555913925170898 elapsed, loss: 7.130533e-06\n",
      "step: 147320 train: 0.08207058906555176 elapsed, loss: 7.526418e-06\n",
      "step: 147330 train: 0.08424806594848633 elapsed, loss: 0.00020555191\n",
      "step: 147340 train: 0.07621240615844727 elapsed, loss: 2.6773767e-05\n",
      "step: 147350 train: 0.07971620559692383 elapsed, loss: 1.8623938e-05\n",
      "step: 147360 train: 0.08661866188049316 elapsed, loss: 1.7969458e-05\n",
      "step: 147370 train: 0.07668304443359375 elapsed, loss: 3.3103133e-05\n",
      "step: 147380 train: 0.07996058464050293 elapsed, loss: 9.17897e-06\n",
      "step: 147390 train: 0.08460426330566406 elapsed, loss: 9.026162e-06\n",
      "step: 147400 train: 0.08062553405761719 elapsed, loss: 5.2591176e-06\n",
      "step: 147410 train: 0.08318591117858887 elapsed, loss: 6.4386027e-06\n",
      "step: 147420 train: 0.07831430435180664 elapsed, loss: 4.2612446e-06\n",
      "step: 147430 train: 0.0820322036743164 elapsed, loss: 4.0838363e-06\n",
      "step: 147440 train: 0.08363032341003418 elapsed, loss: 2.502924e-06\n",
      "step: 147450 train: 0.07401752471923828 elapsed, loss: 2.8619475e-06\n",
      "step: 147460 train: 0.0826408863067627 elapsed, loss: 2.2388942e-06\n",
      "step: 147470 train: 0.08114957809448242 elapsed, loss: 1.8728862e-06\n",
      "step: 147480 train: 0.08002448081970215 elapsed, loss: 2.0503035e-06\n",
      "step: 147490 train: 0.08556199073791504 elapsed, loss: 1.1944202e-06\n",
      "step: 147500 train: 0.08635163307189941 elapsed, loss: 1.3722932e-06\n",
      "step: 147510 train: 0.08308076858520508 elapsed, loss: 1.1385403e-06\n",
      "step: 147520 train: 0.08271503448486328 elapsed, loss: 1.0286449e-06\n",
      "step: 147530 train: 0.08298110961914062 elapsed, loss: 9.853386e-07\n",
      "step: 147540 train: 0.08695244789123535 elapsed, loss: 9.657807e-07\n",
      "step: 147550 train: 0.07610726356506348 elapsed, loss: 1.5622919e-06\n",
      "step: 147560 train: 0.07904839515686035 elapsed, loss: 1.1133953e-06\n",
      "step: 147570 train: 0.07373476028442383 elapsed, loss: 1.4845261e-06\n",
      "step: 147580 train: 0.08321690559387207 elapsed, loss: 9.3737555e-07\n",
      "step: 147590 train: 0.07838797569274902 elapsed, loss: 5.413266e-06\n",
      "step: 147600 train: 0.08599662780761719 elapsed, loss: 1.5315575e-06\n",
      "step: 147610 train: 0.0832667350769043 elapsed, loss: 1.15903e-06\n",
      "step: 147620 train: 0.07275962829589844 elapsed, loss: 1.4477397e-06\n",
      "step: 147630 train: 0.08186912536621094 elapsed, loss: 1.1990769e-06\n",
      "step: 147640 train: 0.08150124549865723 elapsed, loss: 1.4449378e-06\n",
      "step: 147650 train: 0.08332538604736328 elapsed, loss: 4.665431e-06\n",
      "step: 147660 train: 0.07925653457641602 elapsed, loss: 5.1827583e-06\n",
      "step: 147670 train: 0.08038735389709473 elapsed, loss: 2.474518e-06\n",
      "step: 147680 train: 0.07851576805114746 elapsed, loss: 2.4279534e-06\n",
      "step: 147690 train: 0.08177375793457031 elapsed, loss: 1.856123e-06\n",
      "step: 147700 train: 0.07763981819152832 elapsed, loss: 1.6312097e-06\n",
      "step: 147710 train: 0.08152365684509277 elapsed, loss: 1.2265507e-06\n",
      "step: 147720 train: 0.07486486434936523 elapsed, loss: 1.5199169e-06\n",
      "step: 147730 train: 0.07886838912963867 elapsed, loss: 1.4449442e-06\n",
      "step: 147740 train: 0.07747292518615723 elapsed, loss: 1.3774245e-06\n",
      "step: 147750 train: 0.07955026626586914 elapsed, loss: 1.2554217e-06\n",
      "step: 147760 train: 0.08295154571533203 elapsed, loss: 1.0728829e-06\n",
      "step: 147770 train: 0.07681488990783691 elapsed, loss: 1.4286476e-06\n",
      "step: 147780 train: 0.08122730255126953 elapsed, loss: 9.471545e-07\n",
      "step: 147790 train: 0.08022165298461914 elapsed, loss: 1.3015223e-06\n",
      "step: 147800 train: 0.07643389701843262 elapsed, loss: 1.4156087e-06\n",
      "step: 147810 train: 0.08129024505615234 elapsed, loss: 9.550707e-07\n",
      "step: 147820 train: 0.08585214614868164 elapsed, loss: 1.0230572e-06\n",
      "step: 147830 train: 0.0913400650024414 elapsed, loss: 1.1050134e-06\n",
      "step: 147840 train: 0.0736081600189209 elapsed, loss: 1.6377289e-06\n",
      "step: 147850 train: 0.08255386352539062 elapsed, loss: 1.3918605e-06\n",
      "step: 147860 train: 0.07557415962219238 elapsed, loss: 2.7730061e-06\n",
      "step: 147870 train: 0.07872748374938965 elapsed, loss: 1.6572867e-06\n",
      "step: 147880 train: 0.08659076690673828 elapsed, loss: 1.2684593e-06\n",
      "step: 147890 train: 0.08723831176757812 elapsed, loss: 1.0454089e-06\n",
      "step: 147900 train: 0.07986092567443848 elapsed, loss: 1.245643e-06\n",
      "step: 147910 train: 0.08281707763671875 elapsed, loss: 1.5040794e-06\n",
      "step: 147920 train: 0.08536887168884277 elapsed, loss: 1.50455e-06\n",
      "step: 147930 train: 0.08761334419250488 elapsed, loss: 4.1387793e-06\n",
      "step: 147940 train: 0.08099722862243652 elapsed, loss: 3.7386367e-06\n",
      "step: 147950 train: 0.08246779441833496 elapsed, loss: 1.5962853e-06\n",
      "step: 147960 train: 0.08459997177124023 elapsed, loss: 1.267529e-06\n",
      "step: 147970 train: 0.08608722686767578 elapsed, loss: 1.2060617e-06\n",
      "step: 147980 train: 0.0794520378112793 elapsed, loss: 1.4109523e-06\n",
      "step: 147990 train: 0.08432984352111816 elapsed, loss: 1.3322558e-06\n",
      "step: 148000 train: 0.08269190788269043 elapsed, loss: 1.4058302e-06\n",
      "step: 148010 train: 0.08169174194335938 elapsed, loss: 1.6842951e-06\n",
      "step: 148020 train: 0.08259224891662598 elapsed, loss: 1.7029215e-06\n",
      "step: 148030 train: 0.08018350601196289 elapsed, loss: 1.3913947e-06\n",
      "step: 148040 train: 0.08222460746765137 elapsed, loss: 1.746228e-06\n",
      "step: 148050 train: 0.08289694786071777 elapsed, loss: 1.5492535e-06\n",
      "step: 148060 train: 0.0790560245513916 elapsed, loss: 1.6652018e-06\n",
      "step: 148070 train: 0.08101534843444824 elapsed, loss: 1.400708e-06\n",
      "step: 148080 train: 0.08481979370117188 elapsed, loss: 1.8319085e-06\n",
      "step: 148090 train: 0.08754324913024902 elapsed, loss: 1.2926747e-06\n",
      "step: 148100 train: 0.08591055870056152 elapsed, loss: 1.5101279e-05\n",
      "step: 148110 train: 0.08038830757141113 elapsed, loss: 7.3867245e-06\n",
      "step: 148120 train: 0.0782160758972168 elapsed, loss: 1.0686548e-05\n",
      "step: 148130 train: 0.08585166931152344 elapsed, loss: 3.3010638e-06\n",
      "step: 148140 train: 0.07670855522155762 elapsed, loss: 4.74386e-06\n",
      "step: 148150 train: 0.08458209037780762 elapsed, loss: 3.6583083e-06\n",
      "step: 148160 train: 0.07688784599304199 elapsed, loss: 2.7925657e-06\n",
      "step: 148170 train: 0.07918906211853027 elapsed, loss: 1.6465767e-06\n",
      "step: 148180 train: 0.08012223243713379 elapsed, loss: 1.7415691e-06\n",
      "step: 148190 train: 0.07862401008605957 elapsed, loss: 1.8500702e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 148200 train: 0.08163142204284668 elapsed, loss: 1.2097869e-06\n",
      "step: 148210 train: 0.0883333683013916 elapsed, loss: 1.2898807e-06\n",
      "step: 148220 train: 0.08017945289611816 elapsed, loss: 1.4980307e-06\n",
      "step: 148230 train: 0.08775615692138672 elapsed, loss: 1.348554e-06\n",
      "step: 148240 train: 0.0912942886352539 elapsed, loss: 1.3550732e-06\n",
      "step: 148250 train: 0.08114933967590332 elapsed, loss: 1.4998935e-06\n",
      "step: 148260 train: 0.07793450355529785 elapsed, loss: 2.4330761e-06\n",
      "step: 148270 train: 0.07810068130493164 elapsed, loss: 1.692677e-06\n",
      "step: 148280 train: 0.08177018165588379 elapsed, loss: 1.3150263e-06\n",
      "step: 148290 train: 0.08073043823242188 elapsed, loss: 2.5490253e-06\n",
      "step: 148300 train: 0.08846282958984375 elapsed, loss: 1.4742699e-06\n",
      "step: 148310 train: 0.07735037803649902 elapsed, loss: 2.4558926e-06\n",
      "step: 148320 train: 0.08416295051574707 elapsed, loss: 1.6144461e-06\n",
      "step: 148330 train: 0.08148002624511719 elapsed, loss: 1.672653e-06\n",
      "step: 148340 train: 0.08898472785949707 elapsed, loss: 1.2312075e-06\n",
      "step: 148350 train: 0.08438658714294434 elapsed, loss: 1.5962853e-06\n",
      "step: 148360 train: 0.07610583305358887 elapsed, loss: 3.205138e-06\n",
      "step: 148370 train: 0.08185505867004395 elapsed, loss: 1.3732339e-06\n",
      "step: 148380 train: 0.08205533027648926 elapsed, loss: 1.7462278e-06\n",
      "step: 148390 train: 0.07878780364990234 elapsed, loss: 1.943668e-06\n",
      "step: 148400 train: 0.08006501197814941 elapsed, loss: 1.7485563e-06\n",
      "step: 148410 train: 0.08107733726501465 elapsed, loss: 1.5110693e-06\n",
      "step: 148420 train: 0.08008408546447754 elapsed, loss: 1.672188e-06\n",
      "step: 148430 train: 0.0836172103881836 elapsed, loss: 1.6079268e-06\n",
      "step: 148440 train: 0.0879371166229248 elapsed, loss: 1.2945371e-06\n",
      "step: 148450 train: 0.09110713005065918 elapsed, loss: 1.5306272e-06\n",
      "step: 148460 train: 0.0784306526184082 elapsed, loss: 2.7292353e-06\n",
      "step: 148470 train: 0.07906723022460938 elapsed, loss: 2.2738209e-06\n",
      "step: 148480 train: 0.0810387134552002 elapsed, loss: 2.1471612e-06\n",
      "step: 148490 train: 0.08228850364685059 elapsed, loss: 4.315717e-06\n",
      "step: 148500 train: 0.07647180557250977 elapsed, loss: 1.7452967e-06\n",
      "step: 148510 train: 0.07576560974121094 elapsed, loss: 2.0307461e-06\n",
      "step: 148520 train: 0.07533073425292969 elapsed, loss: 2.4456494e-06\n",
      "step: 148530 train: 0.07901430130004883 elapsed, loss: 5.939816e-06\n",
      "step: 148540 train: 0.08128523826599121 elapsed, loss: 0.00033956242\n",
      "step: 148550 train: 0.08026885986328125 elapsed, loss: 3.4257046e-05\n",
      "step: 148560 train: 0.0777738094329834 elapsed, loss: 1.952274e-05\n",
      "step: 148570 train: 0.08359718322753906 elapsed, loss: 1.5742233e-05\n",
      "step: 148580 train: 0.0886375904083252 elapsed, loss: 9.72431e-06\n",
      "step: 148590 train: 0.09064054489135742 elapsed, loss: 5.9362164e-06\n",
      "step: 148600 train: 0.08581852912902832 elapsed, loss: 7.2338853e-06\n",
      "step: 148610 train: 0.07610869407653809 elapsed, loss: 6.2295826e-06\n",
      "step: 148620 train: 0.07490992546081543 elapsed, loss: 5.25822e-06\n",
      "step: 148630 train: 0.08261466026306152 elapsed, loss: 2.9862777e-06\n",
      "step: 148640 train: 0.07903432846069336 elapsed, loss: 2.6230641e-06\n",
      "step: 148650 train: 0.08779406547546387 elapsed, loss: 1.8668336e-06\n",
      "step: 148660 train: 0.07847046852111816 elapsed, loss: 2.190933e-06\n",
      "step: 148670 train: 0.08597302436828613 elapsed, loss: 1.9357515e-06\n",
      "step: 148680 train: 0.08363652229309082 elapsed, loss: 1.3555386e-06\n",
      "step: 148690 train: 0.08161258697509766 elapsed, loss: 1.6838294e-06\n",
      "step: 148700 train: 0.08789277076721191 elapsed, loss: 1.1785878e-06\n",
      "step: 148710 train: 0.08648896217346191 elapsed, loss: 1.1147923e-06\n",
      "step: 148720 train: 0.07697105407714844 elapsed, loss: 1.732258e-06\n",
      "step: 148730 train: 0.08029317855834961 elapsed, loss: 1.3206144e-06\n",
      "step: 148740 train: 0.08439159393310547 elapsed, loss: 1.5879019e-06\n",
      "step: 148750 train: 0.08034420013427734 elapsed, loss: 1.7052498e-06\n",
      "step: 148760 train: 0.07802724838256836 elapsed, loss: 1.9171252e-06\n",
      "step: 148770 train: 0.08231425285339355 elapsed, loss: 1.4500679e-06\n",
      "step: 148780 train: 0.08130073547363281 elapsed, loss: 1.8761468e-06\n",
      "step: 148790 train: 0.08335256576538086 elapsed, loss: 1.5050159e-06\n",
      "step: 148800 train: 0.07998228073120117 elapsed, loss: 2.0731204e-06\n",
      "step: 148810 train: 0.0801992416381836 elapsed, loss: 2.0125844e-06\n",
      "step: 148820 train: 0.08075571060180664 elapsed, loss: 1.3993108e-06\n",
      "step: 148830 train: 0.0859978199005127 elapsed, loss: 1.6060642e-06\n",
      "step: 148840 train: 0.08149552345275879 elapsed, loss: 1.6195681e-06\n",
      "step: 148850 train: 0.08023667335510254 elapsed, loss: 2.2505378e-06\n",
      "step: 148860 train: 0.07947921752929688 elapsed, loss: 1.4328384e-06\n",
      "step: 148870 train: 0.08867383003234863 elapsed, loss: 1.3383094e-06\n",
      "step: 148880 train: 0.08084297180175781 elapsed, loss: 2.3115394e-06\n",
      "step: 148890 train: 0.08251523971557617 elapsed, loss: 0.00017917129\n",
      "step: 148900 train: 0.07697677612304688 elapsed, loss: 3.8149294e-05\n",
      "step: 148910 train: 0.0797574520111084 elapsed, loss: 1.4594518e-05\n",
      "step: 148920 train: 0.08317947387695312 elapsed, loss: 1.1023449e-05\n",
      "step: 148930 train: 0.08497786521911621 elapsed, loss: 8.893126e-06\n",
      "step: 148940 train: 0.0778040885925293 elapsed, loss: 6.5094473e-06\n",
      "step: 148950 train: 0.07802009582519531 elapsed, loss: 5.9674207e-06\n",
      "step: 148960 train: 0.07931995391845703 elapsed, loss: 3.7373866e-06\n",
      "step: 148970 train: 0.07748651504516602 elapsed, loss: 3.40537e-06\n",
      "step: 148980 train: 0.07749009132385254 elapsed, loss: 3.6475158e-06\n",
      "step: 148990 train: 0.0773465633392334 elapsed, loss: 2.4982683e-06\n",
      "step: 149000 train: 0.08258295059204102 elapsed, loss: 2.3706773e-06\n",
      "step: 149010 train: 0.08970427513122559 elapsed, loss: 1.3979139e-06\n",
      "step: 149020 train: 0.08101844787597656 elapsed, loss: 1.3764934e-06\n",
      "step: 149030 train: 0.07941937446594238 elapsed, loss: 2.0028065e-06\n",
      "step: 149040 train: 0.08374571800231934 elapsed, loss: 1.7341205e-06\n",
      "step: 149050 train: 0.08406448364257812 elapsed, loss: 1.1865039e-06\n",
      "step: 149060 train: 0.0818333625793457 elapsed, loss: 1.1259682e-06\n",
      "step: 149070 train: 0.08125042915344238 elapsed, loss: 1.3792873e-06\n",
      "step: 149080 train: 0.07887101173400879 elapsed, loss: 1.3131635e-06\n",
      "step: 149090 train: 0.08234786987304688 elapsed, loss: 1.540406e-06\n",
      "step: 149100 train: 0.0797114372253418 elapsed, loss: 1.6312094e-06\n",
      "step: 149110 train: 0.0826418399810791 elapsed, loss: 1.7574038e-06\n",
      "step: 149120 train: 0.07863378524780273 elapsed, loss: 1.5399404e-06\n",
      "step: 149130 train: 0.08058500289916992 elapsed, loss: 1.3965171e-06\n",
      "step: 149140 train: 0.0811452865600586 elapsed, loss: 1.1855727e-06\n",
      "step: 149150 train: 0.07972860336303711 elapsed, loss: 1.7313267e-06\n",
      "step: 149160 train: 0.08135366439819336 elapsed, loss: 1.62795e-06\n",
      "step: 149170 train: 0.07511568069458008 elapsed, loss: 1.6875543e-06\n",
      "step: 149180 train: 0.07687664031982422 elapsed, loss: 1.65589e-06\n",
      "step: 149190 train: 0.08315587043762207 elapsed, loss: 1.4337697e-06\n",
      "step: 149200 train: 0.08308219909667969 elapsed, loss: 2.328767e-06\n",
      "step: 149210 train: 0.08402633666992188 elapsed, loss: 1.2381925e-06\n",
      "step: 149220 train: 0.08154129981994629 elapsed, loss: 1.5427343e-06\n",
      "step: 149230 train: 0.0808565616607666 elapsed, loss: 1.4137463e-06\n",
      "step: 149240 train: 0.08046460151672363 elapsed, loss: 1.7001275e-06\n",
      "step: 149250 train: 0.08176469802856445 elapsed, loss: 9.573488e-05\n",
      "step: 149260 train: 0.08739137649536133 elapsed, loss: 1.3000899e-05\n",
      "step: 149270 train: 0.08397626876831055 elapsed, loss: 8.2444e-06\n",
      "step: 149280 train: 0.07752799987792969 elapsed, loss: 3.0452973e-05\n",
      "step: 149290 train: 0.08439922332763672 elapsed, loss: 5.533886e-06\n",
      "step: 149300 train: 0.08076667785644531 elapsed, loss: 9.980864e-06\n",
      "step: 149310 train: 0.08395934104919434 elapsed, loss: 5.5455357e-06\n",
      "step: 149320 train: 0.07856607437133789 elapsed, loss: 5.3312942e-06\n",
      "step: 149330 train: 0.08586382865905762 elapsed, loss: 3.8510043e-06\n",
      "step: 149340 train: 0.07318997383117676 elapsed, loss: 3.827723e-06\n",
      "step: 149350 train: 0.08249545097351074 elapsed, loss: 3.127374e-06\n",
      "step: 149360 train: 0.08220934867858887 elapsed, loss: 2.2142121e-06\n",
      "step: 149370 train: 0.07372856140136719 elapsed, loss: 2.967187e-06\n",
      "step: 149380 train: 0.07958340644836426 elapsed, loss: 2.203971e-06\n",
      "step: 149390 train: 0.0787503719329834 elapsed, loss: 1.6591492e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 149400 train: 0.07790756225585938 elapsed, loss: 1.6125833e-06\n",
      "step: 149410 train: 0.07857108116149902 elapsed, loss: 1.8943078e-06\n",
      "step: 149420 train: 0.07642722129821777 elapsed, loss: 2.7283024e-06\n",
      "step: 149430 train: 0.0832967758178711 elapsed, loss: 2.5029235e-06\n",
      "step: 149440 train: 0.08407998085021973 elapsed, loss: 1.1054791e-06\n",
      "step: 149450 train: 0.0818026065826416 elapsed, loss: 1.8565893e-06\n",
      "step: 149460 train: 0.0800924301147461 elapsed, loss: 1.7480882e-06\n",
      "step: 149470 train: 0.08082866668701172 elapsed, loss: 2.8689262e-06\n",
      "step: 149480 train: 0.082672119140625 elapsed, loss: 1.6060641e-06\n",
      "step: 149490 train: 0.0800943374633789 elapsed, loss: 1.3792876e-06\n",
      "step: 149500 train: 0.08074474334716797 elapsed, loss: 1.2405208e-06\n",
      "step: 149510 train: 0.08578920364379883 elapsed, loss: 1.1958173e-06\n",
      "step: 149520 train: 0.08437061309814453 elapsed, loss: 3.1380796e-06\n",
      "step: 149530 train: 0.08460330963134766 elapsed, loss: 1.6740503e-06\n",
      "step: 149540 train: 0.07722616195678711 elapsed, loss: 1.7993132e-06\n",
      "step: 149550 train: 0.07849669456481934 elapsed, loss: 1.6479735e-06\n",
      "step: 149560 train: 0.07934951782226562 elapsed, loss: 1.2698574e-06\n",
      "step: 149570 train: 0.07978701591491699 elapsed, loss: 1.4659005e-06\n",
      "step: 149580 train: 0.08155679702758789 elapsed, loss: 0.00017291415\n",
      "step: 149590 train: 0.0819251537322998 elapsed, loss: 2.336819e-05\n",
      "step: 149600 train: 0.08080530166625977 elapsed, loss: 1.308155e-05\n",
      "step: 149610 train: 0.08836627006530762 elapsed, loss: 6.3641432e-06\n",
      "step: 149620 train: 0.08561444282531738 elapsed, loss: 6.4055166e-06\n",
      "step: 149630 train: 0.08697915077209473 elapsed, loss: 4.1085077e-06\n",
      "step: 149640 train: 0.08285140991210938 elapsed, loss: 4.774406e-06\n",
      "step: 149650 train: 0.07857823371887207 elapsed, loss: 4.884763e-06\n",
      "step: 149660 train: 0.08411550521850586 elapsed, loss: 3.6847168e-06\n",
      "step: 149670 train: 0.08262181282043457 elapsed, loss: 3.8839835e-06\n",
      "step: 149680 train: 0.08495020866394043 elapsed, loss: 2.185811e-06\n",
      "step: 149690 train: 0.07810258865356445 elapsed, loss: 2.1918602e-06\n",
      "step: 149700 train: 0.08101677894592285 elapsed, loss: 2.0866253e-06\n",
      "step: 149710 train: 0.08104300498962402 elapsed, loss: 1.4659001e-06\n",
      "step: 149720 train: 0.08076214790344238 elapsed, loss: 1.3597297e-06\n",
      "step: 149730 train: 0.0868980884552002 elapsed, loss: 3.3342327e-05\n",
      "step: 149740 train: 0.08160519599914551 elapsed, loss: 9.619606e-05\n",
      "step: 149750 train: 0.08945178985595703 elapsed, loss: 8.114053e-06\n",
      "step: 149760 train: 0.08022046089172363 elapsed, loss: 6.1727715e-06\n",
      "step: 149770 train: 0.08419227600097656 elapsed, loss: 4.2481875e-06\n",
      "step: 149780 train: 0.08160877227783203 elapsed, loss: 4.0530986e-06\n",
      "step: 149790 train: 0.08343267440795898 elapsed, loss: 2.8349377e-06\n",
      "step: 149800 train: 0.08296847343444824 elapsed, loss: 2.094541e-06\n",
      "step: 149810 train: 0.08373427391052246 elapsed, loss: 1.8407565e-06\n",
      "step: 149820 train: 0.08176422119140625 elapsed, loss: 1.9920963e-06\n",
      "step: 149830 train: 0.08941984176635742 elapsed, loss: 1.3383092e-06\n",
      "step: 149840 train: 0.08449053764343262 elapsed, loss: 1.3057131e-06\n",
      "step: 149850 train: 0.08496260643005371 elapsed, loss: 1.135747e-06\n",
      "step: 149860 train: 0.08128952980041504 elapsed, loss: 1.2856897e-06\n",
      "step: 149870 train: 0.08451533317565918 elapsed, loss: 1.285224e-06\n",
      "step: 149880 train: 0.0771946907043457 elapsed, loss: 1.3904635e-06\n",
      "step: 149890 train: 0.08544731140136719 elapsed, loss: 3.9967013e-06\n",
      "step: 149900 train: 0.08588242530822754 elapsed, loss: 0.0011442788\n",
      "step: 149910 train: 0.08129644393920898 elapsed, loss: 5.256797e-05\n",
      "step: 149920 train: 0.0854952335357666 elapsed, loss: 1.1394529e-05\n",
      "step: 149930 train: 0.07793569564819336 elapsed, loss: 8.328709e-06\n",
      "step: 149940 train: 0.0802314281463623 elapsed, loss: 7.5762355e-06\n",
      "step: 149950 train: 0.0843653678894043 elapsed, loss: 4.6933806e-06\n",
      "step: 149960 train: 0.08153367042541504 elapsed, loss: 5.860312e-06\n",
      "step: 149970 train: 0.08353066444396973 elapsed, loss: 2.9117732e-06\n",
      "step: 149980 train: 0.08234643936157227 elapsed, loss: 2.7599685e-06\n",
      "step: 149990 train: 0.08174014091491699 elapsed, loss: 3.3401534e-06\n",
      "step: 150000 train: 0.0760798454284668 elapsed, loss: 2.5373838e-06\n",
      "step: 150010 train: 0.08216643333435059 elapsed, loss: 1.4845266e-06\n",
      "step: 150020 train: 0.08335351943969727 elapsed, loss: 1.5944224e-06\n",
      "step: 150030 train: 0.08483767509460449 elapsed, loss: 1.5678797e-06\n",
      "step: 150040 train: 0.0884702205657959 elapsed, loss: 9.5087967e-07\n",
      "step: 150050 train: 0.08046960830688477 elapsed, loss: 1.4756793e-06\n",
      "step: 150060 train: 0.0800483226776123 elapsed, loss: 1.3564702e-06\n",
      "step: 150070 train: 0.07670235633850098 elapsed, loss: 1.562292e-06\n",
      "step: 150080 train: 0.0834493637084961 elapsed, loss: 1.0416835e-06\n",
      "step: 150090 train: 0.08172893524169922 elapsed, loss: 8.4331214e-07\n",
      "step: 150100 train: 0.07505393028259277 elapsed, loss: 4.733794e-06\n",
      "step: 150110 train: 0.0736544132232666 elapsed, loss: 1.5054816e-06\n",
      "step: 150120 train: 0.07956314086914062 elapsed, loss: 1.5720422e-06\n",
      "step: 150130 train: 0.08255553245544434 elapsed, loss: 1.1944203e-06\n",
      "step: 150140 train: 0.08125829696655273 elapsed, loss: 1.1706713e-06\n",
      "step: 150150 train: 0.08051824569702148 elapsed, loss: 2.2216655e-06\n",
      "step: 150160 train: 0.08665227890014648 elapsed, loss: 1.1012881e-06\n",
      "step: 150170 train: 0.07631969451904297 elapsed, loss: 1.4770761e-06\n",
      "step: 150180 train: 0.07272219657897949 elapsed, loss: 1.6698596e-06\n",
      "step: 150190 train: 0.0820472240447998 elapsed, loss: 1.2288766e-06\n",
      "step: 150200 train: 0.08689641952514648 elapsed, loss: 1.0617068e-06\n",
      "step: 150210 train: 0.08311629295349121 elapsed, loss: 1.4188687e-06\n",
      "step: 150220 train: 0.07515954971313477 elapsed, loss: 1.6656686e-06\n",
      "step: 150230 train: 0.07812190055847168 elapsed, loss: 1.1902293e-06\n",
      "step: 150240 train: 0.08267951011657715 elapsed, loss: 1.0426148e-06\n",
      "step: 150250 train: 0.08301401138305664 elapsed, loss: 1.3387745e-06\n",
      "step: 150260 train: 0.08283591270446777 elapsed, loss: 1.3904635e-06\n",
      "step: 150270 train: 0.0829935073852539 elapsed, loss: 1.259147e-06\n",
      "step: 150280 train: 0.08109307289123535 elapsed, loss: 1.5576355e-06\n",
      "step: 150290 train: 0.0853874683380127 elapsed, loss: 1.9944237e-06\n",
      "step: 150300 train: 0.08227658271789551 elapsed, loss: 1.4035018e-06\n",
      "step: 150310 train: 0.08352255821228027 elapsed, loss: 1.2200312e-06\n",
      "step: 150320 train: 0.08268165588378906 elapsed, loss: 1.7541441e-06\n",
      "step: 150330 train: 0.07745742797851562 elapsed, loss: 1.6773101e-06\n",
      "step: 150340 train: 0.0815742015838623 elapsed, loss: 1.1408692e-06\n",
      "step: 150350 train: 0.07506632804870605 elapsed, loss: 1.9217819e-06\n",
      "step: 150360 train: 0.0764002799987793 elapsed, loss: 1.9487902e-06\n",
      "step: 150370 train: 0.08214879035949707 elapsed, loss: 1.5334213e-06\n",
      "step: 150380 train: 0.09092569351196289 elapsed, loss: 1.1748625e-06\n",
      "step: 150390 train: 0.07845735549926758 elapsed, loss: 1.5380779e-06\n",
      "step: 150400 train: 0.08106803894042969 elapsed, loss: 1.6498357e-06\n",
      "step: 150410 train: 0.08490562438964844 elapsed, loss: 5.362511e-06\n",
      "step: 150420 train: 0.08870196342468262 elapsed, loss: 1.3327215e-06\n",
      "step: 150430 train: 0.0812232494354248 elapsed, loss: 1.4812672e-06\n",
      "step: 150440 train: 0.08080554008483887 elapsed, loss: 2.0307461e-06\n",
      "step: 150450 train: 0.08484005928039551 elapsed, loss: 1.8752158e-06\n",
      "step: 150460 train: 0.07754278182983398 elapsed, loss: 0.024789805\n",
      "step: 150470 train: 0.07715082168579102 elapsed, loss: 3.972566e-05\n",
      "step: 150480 train: 0.08034729957580566 elapsed, loss: 3.6126476e-05\n",
      "step: 150490 train: 0.07671904563903809 elapsed, loss: 1.8318486e-05\n",
      "step: 150500 train: 0.08227658271789551 elapsed, loss: 1.051944e-05\n",
      "step: 150510 train: 0.0816340446472168 elapsed, loss: 0.00017216789\n",
      "step: 150520 train: 0.09110355377197266 elapsed, loss: 1.0796008e-05\n",
      "step: 150530 train: 0.07862496376037598 elapsed, loss: 8.6442615e-06\n",
      "step: 150540 train: 0.07842803001403809 elapsed, loss: 4.4358394e-06\n",
      "step: 150550 train: 0.08664202690124512 elapsed, loss: 3.1012942e-06\n",
      "step: 150560 train: 0.08587408065795898 elapsed, loss: 3.2489093e-06\n",
      "step: 150570 train: 0.08240580558776855 elapsed, loss: 2.0228276e-06\n",
      "step: 150580 train: 0.08472704887390137 elapsed, loss: 1.7760289e-06\n",
      "step: 150590 train: 0.07704806327819824 elapsed, loss: 1.8700928e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 150600 train: 0.08324289321899414 elapsed, loss: 1.5725365e-06\n",
      "step: 150610 train: 0.08282971382141113 elapsed, loss: 1.4603123e-06\n",
      "step: 150620 train: 0.0875539779663086 elapsed, loss: 1.4826636e-06\n",
      "step: 150630 train: 0.07842612266540527 elapsed, loss: 1.6097895e-06\n",
      "step: 150640 train: 0.08021998405456543 elapsed, loss: 1.7038528e-06\n",
      "step: 150650 train: 0.0796358585357666 elapsed, loss: 1.6023387e-06\n",
      "step: 150660 train: 0.07610368728637695 elapsed, loss: 1.8868551e-06\n",
      "step: 150670 train: 0.0796513557434082 elapsed, loss: 1.0705546e-06\n",
      "step: 150680 train: 0.07976412773132324 elapsed, loss: 1.7276012e-06\n",
      "step: 150690 train: 0.08018636703491211 elapsed, loss: 1.2819645e-06\n",
      "step: 150700 train: 0.07454466819763184 elapsed, loss: 1.544597e-06\n",
      "step: 150710 train: 0.07871580123901367 elapsed, loss: 1.2456429e-06\n",
      "step: 150720 train: 0.0798642635345459 elapsed, loss: 1.3196828e-06\n",
      "step: 150730 train: 0.08249855041503906 elapsed, loss: 1.6335379e-06\n",
      "step: 150740 train: 0.09024930000305176 elapsed, loss: 1.3350493e-06\n",
      "step: 150750 train: 0.08226490020751953 elapsed, loss: 1.4393565e-06\n",
      "step: 150760 train: 0.08681654930114746 elapsed, loss: 1.5902318e-06\n",
      "step: 150770 train: 0.0835561752319336 elapsed, loss: 1.3811498e-06\n",
      "step: 150780 train: 0.07953047752380371 elapsed, loss: 2.0195705e-06\n",
      "step: 150790 train: 0.09047818183898926 elapsed, loss: 1.1855727e-06\n",
      "step: 150800 train: 0.0753331184387207 elapsed, loss: 2.118289e-06\n",
      "step: 150810 train: 0.08149480819702148 elapsed, loss: 2.462878e-06\n",
      "step: 150820 train: 0.0798795223236084 elapsed, loss: 1.519917e-06\n",
      "step: 150830 train: 0.08111357688903809 elapsed, loss: 1.7890686e-06\n",
      "step: 150840 train: 0.07659721374511719 elapsed, loss: 1.6763786e-06\n",
      "step: 150850 train: 0.0789794921875 elapsed, loss: 2.720854e-06\n",
      "step: 150860 train: 0.07974982261657715 elapsed, loss: 1.7257389e-06\n",
      "step: 150870 train: 0.08370208740234375 elapsed, loss: 1.7252731e-06\n",
      "step: 150880 train: 0.07973790168762207 elapsed, loss: 1.513863e-06\n",
      "step: 150890 train: 0.08532404899597168 elapsed, loss: 1.6656686e-06\n",
      "step: 150900 train: 0.08001899719238281 elapsed, loss: 1.9827833e-06\n",
      "step: 150910 train: 0.08072996139526367 elapsed, loss: 2.2309798e-06\n",
      "step: 150920 train: 0.07489728927612305 elapsed, loss: 2.4149151e-06\n",
      "step: 150930 train: 0.0806112289428711 elapsed, loss: 1.9143313e-06\n",
      "step: 150940 train: 0.08308076858520508 elapsed, loss: 1.7937248e-06\n",
      "step: 150950 train: 0.07773733139038086 elapsed, loss: 2.0950072e-06\n",
      "step: 150960 train: 0.08539557456970215 elapsed, loss: 2.2673016e-06\n",
      "step: 150970 train: 0.08308196067810059 elapsed, loss: 2.0833659e-06\n",
      "step: 150980 train: 0.08132529258728027 elapsed, loss: 1.7341208e-06\n",
      "step: 150990 train: 0.08485937118530273 elapsed, loss: 6.6351695e-06\n",
      "step: 151000 train: 0.08779144287109375 elapsed, loss: 1.9972185e-06\n",
      "step: 151010 train: 0.08380436897277832 elapsed, loss: 2.0684645e-06\n",
      "step: 151020 train: 0.07948160171508789 elapsed, loss: 1.8156113e-06\n",
      "step: 151030 train: 0.08373785018920898 elapsed, loss: 1.5054816e-06\n",
      "step: 151040 train: 0.08025050163269043 elapsed, loss: 1.5129322e-06\n",
      "step: 151050 train: 0.08100318908691406 elapsed, loss: 2.2998977e-06\n",
      "step: 151060 train: 0.08363556861877441 elapsed, loss: 1.7480904e-06\n",
      "step: 151070 train: 0.08743596076965332 elapsed, loss: 1.3257367e-06\n",
      "step: 151080 train: 0.07941699028015137 elapsed, loss: 2.0675336e-06\n",
      "step: 151090 train: 0.07587504386901855 elapsed, loss: 2.1136339e-06\n",
      "step: 151100 train: 0.07627654075622559 elapsed, loss: 2.5401766e-06\n",
      "step: 151110 train: 0.07896161079406738 elapsed, loss: 2.1341227e-06\n",
      "step: 151120 train: 0.07865095138549805 elapsed, loss: 2.8768488e-06\n",
      "step: 151130 train: 0.08344650268554688 elapsed, loss: 1.6624087e-06\n",
      "step: 151140 train: 0.07749319076538086 elapsed, loss: 1.7438998e-06\n",
      "step: 151150 train: 0.08060216903686523 elapsed, loss: 2.1057172e-06\n",
      "step: 151160 train: 0.08472847938537598 elapsed, loss: 1.8952392e-06\n",
      "step: 151170 train: 0.08312273025512695 elapsed, loss: 1.6526299e-06\n",
      "step: 151180 train: 0.08208680152893066 elapsed, loss: 8.630509e-06\n",
      "step: 151190 train: 0.08508467674255371 elapsed, loss: 3.2880243e-06\n",
      "step: 151200 train: 0.08657002449035645 elapsed, loss: 1.977192e-06\n",
      "step: 151210 train: 0.08141779899597168 elapsed, loss: 3.2185587e-06\n",
      "step: 151220 train: 0.08466172218322754 elapsed, loss: 1.9245751e-06\n",
      "step: 151230 train: 0.08017778396606445 elapsed, loss: 2.050304e-06\n",
      "step: 151240 train: 0.08204293251037598 elapsed, loss: 1.8370317e-06\n",
      "step: 151250 train: 0.0812373161315918 elapsed, loss: 1.9385457e-06\n",
      "step: 151260 train: 0.0790395736694336 elapsed, loss: 1.6991962e-06\n",
      "step: 151270 train: 0.07683515548706055 elapsed, loss: 2.3292341e-06\n",
      "step: 151280 train: 0.08158159255981445 elapsed, loss: 1.4025707e-06\n",
      "step: 151290 train: 0.08077263832092285 elapsed, loss: 2.1764977e-06\n",
      "step: 151300 train: 0.07840704917907715 elapsed, loss: 3.7918667e-06\n",
      "step: 151310 train: 0.0774376392364502 elapsed, loss: 1.6572868e-06\n",
      "step: 151320 train: 0.07961487770080566 elapsed, loss: 1.9399424e-06\n",
      "step: 151330 train: 0.08572006225585938 elapsed, loss: 1.7504178e-06\n",
      "step: 151340 train: 0.08186888694763184 elapsed, loss: 2.1881397e-06\n",
      "step: 151350 train: 0.07920050621032715 elapsed, loss: 2.106649e-06\n",
      "step: 151360 train: 0.0808253288269043 elapsed, loss: 2.196521e-06\n",
      "step: 151370 train: 0.08503007888793945 elapsed, loss: 1.9990812e-06\n",
      "step: 151380 train: 0.07710123062133789 elapsed, loss: 2.3627617e-06\n",
      "step: 151390 train: 0.08321905136108398 elapsed, loss: 1.8277184e-06\n",
      "step: 151400 train: 0.07962393760681152 elapsed, loss: 1.9771953e-06\n",
      "step: 151410 train: 0.08108711242675781 elapsed, loss: 1.585575e-06\n",
      "step: 151420 train: 0.08577084541320801 elapsed, loss: 1.6731183e-06\n",
      "step: 151430 train: 0.08086085319519043 elapsed, loss: 1.7425021e-06\n",
      "step: 151440 train: 0.08023405075073242 elapsed, loss: 1.8873206e-06\n",
      "step: 151450 train: 0.07760810852050781 elapsed, loss: 3.1134016e-06\n",
      "step: 151460 train: 0.08971619606018066 elapsed, loss: 1.6163085e-06\n",
      "step: 151470 train: 0.09125733375549316 elapsed, loss: 1.9133995e-06\n",
      "step: 151480 train: 0.08988237380981445 elapsed, loss: 2.4218998e-06\n",
      "step: 151490 train: 0.08049297332763672 elapsed, loss: 2.4144504e-06\n",
      "step: 151500 train: 0.08427739143371582 elapsed, loss: 1.8468102e-06\n",
      "step: 151510 train: 0.07659077644348145 elapsed, loss: 2.105252e-06\n",
      "step: 151520 train: 0.08082890510559082 elapsed, loss: 2.3073485e-06\n",
      "step: 151530 train: 0.07968735694885254 elapsed, loss: 2.1285339e-06\n",
      "step: 151540 train: 0.0791010856628418 elapsed, loss: 2.0582202e-06\n",
      "step: 151550 train: 0.08137655258178711 elapsed, loss: 1.7657858e-06\n",
      "step: 151560 train: 0.07706904411315918 elapsed, loss: 2.5187583e-06\n",
      "step: 151570 train: 0.08138799667358398 elapsed, loss: 8.430319e-05\n",
      "step: 151580 train: 0.0759894847869873 elapsed, loss: 0.00044770548\n",
      "step: 151590 train: 0.07635951042175293 elapsed, loss: 5.3506898e-05\n",
      "step: 151600 train: 0.0837717056274414 elapsed, loss: 2.2353088e-05\n",
      "step: 151610 train: 0.0798337459564209 elapsed, loss: 1.903061e-05\n",
      "step: 151620 train: 0.08455371856689453 elapsed, loss: 1.3239798e-05\n",
      "step: 151630 train: 0.08041620254516602 elapsed, loss: 1.030266e-05\n",
      "step: 151640 train: 0.07771730422973633 elapsed, loss: 7.2684634e-06\n",
      "step: 151650 train: 0.07682657241821289 elapsed, loss: 7.5548346e-06\n",
      "step: 151660 train: 0.08437061309814453 elapsed, loss: 5.4085863e-06\n",
      "step: 151670 train: 0.09460163116455078 elapsed, loss: 3.928306e-06\n",
      "step: 151680 train: 0.0845344066619873 elapsed, loss: 3.2083872e-06\n",
      "step: 151690 train: 0.08213639259338379 elapsed, loss: 2.8861623e-06\n",
      "step: 151700 train: 0.08412933349609375 elapsed, loss: 2.8954762e-06\n",
      "step: 151710 train: 0.07662010192871094 elapsed, loss: 2.6575244e-06\n",
      "step: 151720 train: 0.09439730644226074 elapsed, loss: 1.6307408e-06\n",
      "step: 151730 train: 0.08092164993286133 elapsed, loss: 1.82958e-06\n",
      "step: 151740 train: 0.08382034301757812 elapsed, loss: 1.5050157e-06\n",
      "step: 151750 train: 0.08656477928161621 elapsed, loss: 1.6037354e-06\n",
      "step: 151760 train: 0.08605742454528809 elapsed, loss: 1.2936061e-06\n",
      "step: 151770 train: 0.0871274471282959 elapsed, loss: 1.9050101e-06\n",
      "step: 151780 train: 0.08889055252075195 elapsed, loss: 5.5776372e-06\n",
      "step: 151790 train: 0.0826568603515625 elapsed, loss: 1.6624083e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 151800 train: 0.08521795272827148 elapsed, loss: 2.0591513e-06\n",
      "step: 151810 train: 0.08491706848144531 elapsed, loss: 2.2718991e-06\n",
      "step: 151820 train: 0.07857441902160645 elapsed, loss: 1.2292701e-05\n",
      "step: 151830 train: 0.07625770568847656 elapsed, loss: 7.239586e-06\n",
      "step: 151840 train: 0.08109617233276367 elapsed, loss: 5.835562e-06\n",
      "step: 151850 train: 0.07947373390197754 elapsed, loss: 4.0484274e-06\n",
      "step: 151860 train: 0.08183574676513672 elapsed, loss: 3.2447128e-06\n",
      "step: 151870 train: 0.07746124267578125 elapsed, loss: 2.7254966e-06\n",
      "step: 151880 train: 0.07620382308959961 elapsed, loss: 2.6151492e-06\n",
      "step: 151890 train: 0.08226203918457031 elapsed, loss: 2.4344722e-06\n",
      "step: 151900 train: 0.08073234558105469 elapsed, loss: 1.6055983e-06\n",
      "step: 151910 train: 0.07973694801330566 elapsed, loss: 2.235171e-06\n",
      "step: 151920 train: 0.08529973030090332 elapsed, loss: 1.3234082e-06\n",
      "step: 151930 train: 0.0836648941040039 elapsed, loss: 1.6367976e-06\n",
      "step: 151940 train: 0.07985949516296387 elapsed, loss: 1.5911627e-06\n",
      "step: 151950 train: 0.08209562301635742 elapsed, loss: 1.4426173e-06\n",
      "step: 151960 train: 0.07763481140136719 elapsed, loss: 5.6721174e-06\n",
      "step: 151970 train: 0.07853913307189941 elapsed, loss: 1.6889517e-06\n",
      "step: 151980 train: 0.07861113548278809 elapsed, loss: 1.5329555e-06\n",
      "step: 151990 train: 0.08164000511169434 elapsed, loss: 1.3732339e-06\n",
      "step: 152000 train: 0.08086490631103516 elapsed, loss: 1.5320243e-06\n",
      "step: 152010 train: 0.07536196708679199 elapsed, loss: 2.521196e-05\n",
      "step: 152020 train: 0.08389949798583984 elapsed, loss: 1.5985112e-05\n",
      "step: 152030 train: 0.0810999870300293 elapsed, loss: 3.5459998e-06\n",
      "step: 152040 train: 0.08173727989196777 elapsed, loss: 4.900133e-06\n",
      "step: 152050 train: 0.08121013641357422 elapsed, loss: 3.9311003e-06\n",
      "step: 152060 train: 0.07691216468811035 elapsed, loss: 2.7292322e-06\n",
      "step: 152070 train: 0.07827401161193848 elapsed, loss: 2.0940752e-06\n",
      "step: 152080 train: 0.08739352226257324 elapsed, loss: 1.629812e-06\n",
      "step: 152090 train: 0.08185744285583496 elapsed, loss: 1.6917456e-06\n",
      "step: 152100 train: 0.08541727066040039 elapsed, loss: 1.6470422e-06\n",
      "step: 152110 train: 0.08947110176086426 elapsed, loss: 1.130159e-06\n",
      "step: 152120 train: 0.07549715042114258 elapsed, loss: 1.6665995e-06\n",
      "step: 152130 train: 0.07920169830322266 elapsed, loss: 1.7816169e-06\n",
      "step: 152140 train: 0.08310651779174805 elapsed, loss: 1.1799848e-06\n",
      "step: 152150 train: 0.08435511589050293 elapsed, loss: 1.4277159e-06\n",
      "step: 152160 train: 0.08187532424926758 elapsed, loss: 1.3299269e-06\n",
      "step: 152170 train: 0.08437538146972656 elapsed, loss: 1.2763762e-06\n",
      "step: 152180 train: 0.07958650588989258 elapsed, loss: 2.2528661e-06\n",
      "step: 152190 train: 0.0793304443359375 elapsed, loss: 1.3466913e-06\n",
      "step: 152200 train: 0.07811951637268066 elapsed, loss: 1.4659004e-06\n",
      "step: 152210 train: 0.08121013641357422 elapsed, loss: 1.4542588e-06\n",
      "step: 152220 train: 0.0780029296875 elapsed, loss: 2.1657877e-06\n",
      "step: 152230 train: 0.08198189735412598 elapsed, loss: 1.3397063e-06\n",
      "step: 152240 train: 0.08379054069519043 elapsed, loss: 1.0849901e-06\n",
      "step: 152250 train: 0.07669734954833984 elapsed, loss: 1.5413374e-06\n",
      "step: 152260 train: 0.08306384086608887 elapsed, loss: 1.3192173e-06\n",
      "step: 152270 train: 0.07955336570739746 elapsed, loss: 1.6251563e-06\n",
      "step: 152280 train: 0.08385467529296875 elapsed, loss: 1.2186347e-06\n",
      "step: 152290 train: 0.09128856658935547 elapsed, loss: 1.3937231e-06\n",
      "step: 152300 train: 0.07602143287658691 elapsed, loss: 2.0689306e-06\n",
      "step: 152310 train: 0.08352875709533691 elapsed, loss: 1.4905805e-06\n",
      "step: 152320 train: 0.0805966854095459 elapsed, loss: 1.9632253e-06\n",
      "step: 152330 train: 0.08368277549743652 elapsed, loss: 1.6703225e-06\n",
      "step: 152340 train: 0.08411073684692383 elapsed, loss: 1.6838292e-06\n",
      "step: 152350 train: 0.08048796653747559 elapsed, loss: 2.3837113e-06\n",
      "step: 152360 train: 0.08140850067138672 elapsed, loss: 1.7313266e-06\n",
      "step: 152370 train: 0.08294534683227539 elapsed, loss: 1.7052496e-06\n",
      "step: 152380 train: 0.08264994621276855 elapsed, loss: 2.0153798e-06\n",
      "step: 152390 train: 0.08246135711669922 elapsed, loss: 1.3876695e-06\n",
      "step: 152400 train: 0.0777900218963623 elapsed, loss: 1.6037359e-06\n",
      "step: 152410 train: 0.0841524600982666 elapsed, loss: 1.8691615e-06\n",
      "step: 152420 train: 0.08446693420410156 elapsed, loss: 1.4235252e-06\n",
      "step: 152430 train: 0.08177971839904785 elapsed, loss: 2.0726557e-06\n",
      "step: 152440 train: 0.08188724517822266 elapsed, loss: 1.8575208e-06\n",
      "step: 152450 train: 0.07899236679077148 elapsed, loss: 1.8319095e-06\n",
      "step: 152460 train: 0.08020877838134766 elapsed, loss: 5.150161e-06\n",
      "step: 152470 train: 0.07674646377563477 elapsed, loss: 1.7946566e-06\n",
      "step: 152480 train: 0.07472038269042969 elapsed, loss: 2.2887216e-06\n",
      "step: 152490 train: 0.08449363708496094 elapsed, loss: 1.445877e-06\n",
      "step: 152500 train: 0.07927227020263672 elapsed, loss: 1.7778927e-06\n",
      "step: 152510 train: 0.08170533180236816 elapsed, loss: 1.8258557e-06\n",
      "step: 152520 train: 0.08716988563537598 elapsed, loss: 1.694074e-06\n",
      "step: 152530 train: 0.07719182968139648 elapsed, loss: 2.2770805e-06\n",
      "step: 152540 train: 0.0803070068359375 elapsed, loss: 1.7289984e-06\n",
      "step: 152550 train: 0.08887362480163574 elapsed, loss: 1.5366808e-06\n",
      "step: 152560 train: 0.07687664031982422 elapsed, loss: 2.7092124e-06\n",
      "step: 152570 train: 0.07467126846313477 elapsed, loss: 2.402343e-06\n",
      "step: 152580 train: 0.08091592788696289 elapsed, loss: 1.6787071e-06\n",
      "step: 152590 train: 0.08300542831420898 elapsed, loss: 2.0004784e-06\n",
      "step: 152600 train: 0.08196067810058594 elapsed, loss: 1.6973315e-06\n",
      "step: 152610 train: 0.08301401138305664 elapsed, loss: 1.6861577e-06\n",
      "step: 152620 train: 0.0756993293762207 elapsed, loss: 2.4344736e-06\n",
      "step: 152630 train: 0.08053970336914062 elapsed, loss: 2.1280691e-06\n",
      "step: 152640 train: 0.09105515480041504 elapsed, loss: 1.8402914e-06\n",
      "step: 152650 train: 0.0843966007232666 elapsed, loss: 1.8919798e-06\n",
      "step: 152660 train: 0.07849359512329102 elapsed, loss: 2.7199233e-06\n",
      "step: 152670 train: 0.07913589477539062 elapsed, loss: 2.9713246e-06\n",
      "step: 152680 train: 0.07904386520385742 elapsed, loss: 0.00019407291\n",
      "step: 152690 train: 0.07895970344543457 elapsed, loss: 6.400968e-05\n",
      "step: 152700 train: 0.0798039436340332 elapsed, loss: 1.3438768e-05\n",
      "step: 152710 train: 0.08124518394470215 elapsed, loss: 1.2297905e-05\n",
      "step: 152720 train: 0.08063173294067383 elapsed, loss: 9.688432e-06\n",
      "step: 152730 train: 0.08335733413696289 elapsed, loss: 8.827921e-06\n",
      "step: 152740 train: 0.08293986320495605 elapsed, loss: 5.4584593e-06\n",
      "step: 152750 train: 0.0828256607055664 elapsed, loss: 5.583253e-06\n",
      "step: 152760 train: 0.07648944854736328 elapsed, loss: 5.116666e-06\n",
      "step: 152770 train: 0.07883429527282715 elapsed, loss: 2.9695152e-06\n",
      "step: 152780 train: 0.07990121841430664 elapsed, loss: 3.2135213e-06\n",
      "step: 152790 train: 0.08092904090881348 elapsed, loss: 3.2954772e-06\n",
      "step: 152800 train: 0.07624244689941406 elapsed, loss: 2.466138e-06\n",
      "step: 152810 train: 0.07886695861816406 elapsed, loss: 2.508979e-06\n",
      "step: 152820 train: 0.08316564559936523 elapsed, loss: 1.8784754e-06\n",
      "step: 152830 train: 0.07607460021972656 elapsed, loss: 2.1066485e-06\n",
      "step: 152840 train: 0.08099579811096191 elapsed, loss: 1.9012928e-06\n",
      "step: 152850 train: 0.08193826675415039 elapsed, loss: 1.8626429e-06\n",
      "step: 152860 train: 0.08437037467956543 elapsed, loss: 1.5874377e-06\n",
      "step: 152870 train: 0.08755993843078613 elapsed, loss: 1.7550753e-06\n",
      "step: 152880 train: 0.0829460620880127 elapsed, loss: 1.8319079e-06\n",
      "step: 152890 train: 0.07729601860046387 elapsed, loss: 1.45327185e-05\n",
      "step: 152900 train: 0.0834503173828125 elapsed, loss: 4.650855e-05\n",
      "step: 152910 train: 0.08557963371276855 elapsed, loss: 9.000792e-05\n",
      "step: 152920 train: 0.0810849666595459 elapsed, loss: 1.8422028e-05\n",
      "step: 152930 train: 0.08292460441589355 elapsed, loss: 7.075193e-06\n",
      "step: 152940 train: 0.08218169212341309 elapsed, loss: 7.3140254e-06\n",
      "step: 152950 train: 0.07630658149719238 elapsed, loss: 6.8172208e-06\n",
      "step: 152960 train: 0.08938455581665039 elapsed, loss: 3.5799933e-06\n",
      "step: 152970 train: 0.08599662780761719 elapsed, loss: 2.9154985e-06\n",
      "step: 152980 train: 0.08605146408081055 elapsed, loss: 2.2421555e-06\n",
      "step: 152990 train: 0.08324885368347168 elapsed, loss: 2.1969863e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 153000 train: 0.0887293815612793 elapsed, loss: 1.9762633e-06\n",
      "step: 153010 train: 0.08574628829956055 elapsed, loss: 1.6749818e-06\n",
      "step: 153020 train: 0.09289789199829102 elapsed, loss: 1.7280668e-06\n",
      "step: 153030 train: 0.08502864837646484 elapsed, loss: 1.4542575e-06\n",
      "step: 153040 train: 0.08505058288574219 elapsed, loss: 1.3480881e-06\n",
      "step: 153050 train: 0.08250260353088379 elapsed, loss: 1.4030362e-06\n",
      "step: 153060 train: 0.07636857032775879 elapsed, loss: 1.6223621e-06\n",
      "step: 153070 train: 0.0827643871307373 elapsed, loss: 1.537612e-06\n",
      "step: 153080 train: 0.0833885669708252 elapsed, loss: 1.3243396e-06\n",
      "step: 153090 train: 0.07749581336975098 elapsed, loss: 1.3834784e-06\n",
      "step: 153100 train: 0.07586145401000977 elapsed, loss: 4.0968316e-06\n",
      "step: 153110 train: 0.08240127563476562 elapsed, loss: 1.7131658e-06\n",
      "step: 153120 train: 0.08786559104919434 elapsed, loss: 1.2437803e-06\n",
      "step: 153130 train: 0.08385348320007324 elapsed, loss: 1.0202632e-06\n",
      "step: 153140 train: 0.07996082305908203 elapsed, loss: 1.293606e-06\n",
      "step: 153150 train: 0.08238959312438965 elapsed, loss: 1.6381946e-06\n",
      "step: 153160 train: 0.08299589157104492 elapsed, loss: 1.5450628e-06\n",
      "step: 153170 train: 0.0799252986907959 elapsed, loss: 3.254488e-06\n",
      "step: 153180 train: 0.07439470291137695 elapsed, loss: 2.074052e-06\n",
      "step: 153190 train: 0.0846872329711914 elapsed, loss: 1.2726514e-06\n",
      "step: 153200 train: 0.08296775817871094 elapsed, loss: 1.2302762e-06\n",
      "step: 153210 train: 0.07959270477294922 elapsed, loss: 1.2358641e-06\n",
      "step: 153220 train: 0.08302831649780273 elapsed, loss: 1.7504187e-06\n",
      "step: 153230 train: 0.0857698917388916 elapsed, loss: 2.0479752e-06\n",
      "step: 153240 train: 0.08269906044006348 elapsed, loss: 1.4686938e-06\n",
      "step: 153250 train: 0.08378052711486816 elapsed, loss: 1.6917456e-06\n",
      "step: 153260 train: 0.08247613906860352 elapsed, loss: 1.4314414e-06\n",
      "step: 153270 train: 0.08101224899291992 elapsed, loss: 2.813661e-06\n",
      "step: 153280 train: 0.08360934257507324 elapsed, loss: 5.2957905e-05\n",
      "step: 153290 train: 0.07824540138244629 elapsed, loss: 2.7689013e-05\n",
      "step: 153300 train: 0.08449506759643555 elapsed, loss: 9.16318e-06\n",
      "step: 153310 train: 0.08538579940795898 elapsed, loss: 9.287288e-06\n",
      "step: 153320 train: 0.08401679992675781 elapsed, loss: 7.1297823e-06\n",
      "step: 153330 train: 0.08323073387145996 elapsed, loss: 4.9131686e-06\n",
      "step: 153340 train: 0.08177852630615234 elapsed, loss: 3.648913e-06\n",
      "step: 153350 train: 0.08213996887207031 elapsed, loss: 3.0859023e-06\n",
      "step: 153360 train: 0.07343482971191406 elapsed, loss: 3.919925e-06\n",
      "step: 153370 train: 0.08619236946105957 elapsed, loss: 1.9571712e-06\n",
      "step: 153380 train: 0.07923769950866699 elapsed, loss: 2.2328425e-06\n",
      "step: 153390 train: 0.0810856819152832 elapsed, loss: 1.7443651e-06\n",
      "step: 153400 train: 0.08181977272033691 elapsed, loss: 1.5706737e-06\n",
      "step: 153410 train: 0.08356022834777832 elapsed, loss: 1.5906974e-06\n",
      "step: 153420 train: 0.08026790618896484 elapsed, loss: 1.892445e-06\n",
      "step: 153430 train: 0.07947278022766113 elapsed, loss: 1.7075782e-06\n",
      "step: 153440 train: 0.0826272964477539 elapsed, loss: 1.3951199e-06\n",
      "step: 153450 train: 0.08316326141357422 elapsed, loss: 1.5781244e-06\n",
      "step: 153460 train: 0.08283424377441406 elapsed, loss: 1.4286475e-06\n",
      "step: 153470 train: 0.07884788513183594 elapsed, loss: 1.3271335e-06\n",
      "step: 153480 train: 0.07937407493591309 elapsed, loss: 1.947859e-06\n",
      "step: 153490 train: 0.07444167137145996 elapsed, loss: 2.0954728e-06\n",
      "step: 153500 train: 0.08248090744018555 elapsed, loss: 1.1497168e-06\n",
      "step: 153510 train: 0.08130764961242676 elapsed, loss: 1.9147967e-06\n",
      "step: 153520 train: 0.0861356258392334 elapsed, loss: 1.3425004e-06\n",
      "step: 153530 train: 0.09357762336730957 elapsed, loss: 1.3299275e-06\n",
      "step: 153540 train: 0.0838613510131836 elapsed, loss: 1.4021049e-06\n",
      "step: 153550 train: 0.0805807113647461 elapsed, loss: 1.7737019e-06\n",
      "step: 153560 train: 0.08222126960754395 elapsed, loss: 1.958569e-06\n",
      "step: 153570 train: 0.0822443962097168 elapsed, loss: 1.973004e-06\n",
      "step: 153580 train: 0.07557868957519531 elapsed, loss: 2.1345886e-06\n",
      "step: 153590 train: 0.08020758628845215 elapsed, loss: 1.9757936e-06\n",
      "step: 153600 train: 0.08569049835205078 elapsed, loss: 1.2787048e-06\n",
      "step: 153610 train: 0.08729910850524902 elapsed, loss: 1.8170081e-06\n",
      "step: 153620 train: 0.08158016204833984 elapsed, loss: 1.7164257e-06\n",
      "step: 153630 train: 0.08391261100769043 elapsed, loss: 1.389532e-06\n",
      "step: 153640 train: 0.0822138786315918 elapsed, loss: 1.5660174e-06\n",
      "step: 153650 train: 0.07733654975891113 elapsed, loss: 2.6319121e-06\n",
      "step: 153660 train: 0.08251047134399414 elapsed, loss: 2.4056804e-05\n",
      "step: 153670 train: 0.08963584899902344 elapsed, loss: 5.2154952e-05\n",
      "step: 153680 train: 0.08583450317382812 elapsed, loss: 2.349681e-05\n",
      "step: 153690 train: 0.08051609992980957 elapsed, loss: 2.1323853e-05\n",
      "step: 153700 train: 0.08792972564697266 elapsed, loss: 1.1888399e-05\n",
      "step: 153710 train: 0.08476781845092773 elapsed, loss: 8.501523e-06\n",
      "step: 153720 train: 0.08293509483337402 elapsed, loss: 1.2712091e-05\n",
      "step: 153730 train: 0.0877995491027832 elapsed, loss: 7.817442e-06\n",
      "step: 153740 train: 0.08591079711914062 elapsed, loss: 4.4530952e-06\n",
      "step: 153750 train: 0.08797192573547363 elapsed, loss: 3.468237e-06\n",
      "step: 153760 train: 0.08094930648803711 elapsed, loss: 0.0006491621\n",
      "step: 153770 train: 0.0767831802368164 elapsed, loss: 0.00068973325\n",
      "step: 153780 train: 0.07981514930725098 elapsed, loss: 3.2569067e-05\n",
      "step: 153790 train: 0.07676482200622559 elapsed, loss: 1.5566136e-05\n",
      "step: 153800 train: 0.08236193656921387 elapsed, loss: 1.3328818e-05\n",
      "step: 153810 train: 0.08056259155273438 elapsed, loss: 1.03996845e-05\n",
      "step: 153820 train: 0.07522273063659668 elapsed, loss: 7.388111e-06\n",
      "step: 153830 train: 0.08232998847961426 elapsed, loss: 4.426067e-06\n",
      "step: 153840 train: 0.0868234634399414 elapsed, loss: 6.1799356e-06\n",
      "step: 153850 train: 0.08171200752258301 elapsed, loss: 4.8200413e-06\n",
      "step: 153860 train: 0.08881092071533203 elapsed, loss: 2.2738182e-06\n",
      "step: 153870 train: 0.0831153392791748 elapsed, loss: 1.9110712e-06\n",
      "step: 153880 train: 0.07945704460144043 elapsed, loss: 2.3818504e-06\n",
      "step: 153890 train: 0.08573198318481445 elapsed, loss: 1.708509e-06\n",
      "step: 153900 train: 0.07903647422790527 elapsed, loss: 1.8603146e-06\n",
      "step: 153910 train: 0.0877370834350586 elapsed, loss: 1.7257385e-06\n",
      "step: 153920 train: 0.07911849021911621 elapsed, loss: 1.9189833e-06\n",
      "step: 153930 train: 0.08122634887695312 elapsed, loss: 4.054031e-06\n",
      "step: 153940 train: 0.08390069007873535 elapsed, loss: 2.1322587e-06\n",
      "step: 153950 train: 0.08566045761108398 elapsed, loss: 1.3047817e-06\n",
      "step: 153960 train: 0.0806739330291748 elapsed, loss: 1.3085065e-06\n",
      "step: 153970 train: 0.08234143257141113 elapsed, loss: 1.3187516e-06\n",
      "step: 153980 train: 0.08141565322875977 elapsed, loss: 1.2693915e-06\n",
      "step: 153990 train: 0.08229541778564453 elapsed, loss: 1.1734655e-06\n",
      "step: 154000 train: 0.08271312713623047 elapsed, loss: 8.9453476e-07\n",
      "step: 154010 train: 0.0812540054321289 elapsed, loss: 1.0388896e-06\n",
      "step: 154020 train: 0.08082437515258789 elapsed, loss: 1.5292302e-06\n",
      "step: 154030 train: 0.08709931373596191 elapsed, loss: 1.1846414e-06\n",
      "step: 154040 train: 0.0804738998413086 elapsed, loss: 8.484344e-07\n",
      "step: 154050 train: 0.07872223854064941 elapsed, loss: 1.0807992e-06\n",
      "step: 154060 train: 0.07966184616088867 elapsed, loss: 1.1497169e-06\n",
      "step: 154070 train: 0.08642029762268066 elapsed, loss: 1.0333017e-06\n",
      "step: 154080 train: 0.07966208457946777 elapsed, loss: 1.1445948e-06\n",
      "step: 154090 train: 0.07360649108886719 elapsed, loss: 1.5092069e-06\n",
      "step: 154100 train: 0.08319306373596191 elapsed, loss: 1.1553047e-06\n",
      "step: 154110 train: 0.08392953872680664 elapsed, loss: 1.1045474e-06\n",
      "step: 154120 train: 0.07521271705627441 elapsed, loss: 1.7085092e-06\n",
      "step: 154130 train: 0.07559084892272949 elapsed, loss: 1.8021071e-06\n",
      "step: 154140 train: 0.08084249496459961 elapsed, loss: 1.8961698e-06\n",
      "step: 154150 train: 0.07627367973327637 elapsed, loss: 1.4435487e-06\n",
      "step: 154160 train: 0.0787200927734375 elapsed, loss: 1.4612438e-06\n",
      "step: 154170 train: 0.08511686325073242 elapsed, loss: 1.1925576e-06\n",
      "step: 154180 train: 0.08365392684936523 elapsed, loss: 1.3341183e-06\n",
      "step: 154190 train: 0.08000016212463379 elapsed, loss: 1.4589153e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 154200 train: 0.08285403251647949 elapsed, loss: 1.8654349e-06\n",
      "step: 154210 train: 0.08398056030273438 elapsed, loss: 1.1492511e-06\n",
      "step: 154220 train: 0.08379483222961426 elapsed, loss: 1.4109523e-06\n",
      "step: 154230 train: 0.0874028205871582 elapsed, loss: 1.4435484e-06\n",
      "step: 154240 train: 0.08520746231079102 elapsed, loss: 1.0812648e-06\n",
      "step: 154250 train: 0.07947659492492676 elapsed, loss: 1.34576e-06\n",
      "step: 154260 train: 0.0829014778137207 elapsed, loss: 2.1397104e-06\n",
      "step: 154270 train: 0.08097481727600098 elapsed, loss: 1.3392407e-06\n",
      "step: 154280 train: 0.07888340950012207 elapsed, loss: 4.136918e-06\n",
      "step: 154290 train: 0.07643938064575195 elapsed, loss: 1.7005926e-06\n",
      "step: 154300 train: 0.0820913314819336 elapsed, loss: 1.2973313e-06\n",
      "step: 154310 train: 0.07773065567016602 elapsed, loss: 2.0945415e-06\n",
      "step: 154320 train: 0.08235001564025879 elapsed, loss: 1.5245736e-06\n",
      "step: 154330 train: 0.07847261428833008 elapsed, loss: 1.5320243e-06\n",
      "step: 154340 train: 0.08422541618347168 elapsed, loss: 2.074517e-06\n",
      "step: 154350 train: 0.08109116554260254 elapsed, loss: 1.5120008e-06\n",
      "step: 154360 train: 0.08447647094726562 elapsed, loss: 1.4491367e-06\n",
      "step: 154370 train: 0.08294558525085449 elapsed, loss: 2.03447e-06\n",
      "step: 154380 train: 0.08022761344909668 elapsed, loss: 3.6763845e-06\n",
      "step: 154390 train: 0.0813300609588623 elapsed, loss: 1.6447141e-06\n",
      "step: 154400 train: 0.08680844306945801 elapsed, loss: 1.4831298e-06\n",
      "step: 154410 train: 0.08388423919677734 elapsed, loss: 1.8589175e-06\n",
      "step: 154420 train: 0.08770036697387695 elapsed, loss: 1.6642717e-06\n",
      "step: 154430 train: 0.08728933334350586 elapsed, loss: 1.8058324e-06\n",
      "step: 154440 train: 0.0849313735961914 elapsed, loss: 1.5040846e-06\n",
      "step: 154450 train: 0.08046126365661621 elapsed, loss: 1.8323751e-06\n",
      "step: 154460 train: 0.07798433303833008 elapsed, loss: 2.4931464e-06\n",
      "step: 154470 train: 0.07995939254760742 elapsed, loss: 2.5513505e-06\n",
      "step: 154480 train: 0.08481860160827637 elapsed, loss: 1.3923258e-06\n",
      "step: 154490 train: 0.08429837226867676 elapsed, loss: 2.0433188e-06\n",
      "step: 154500 train: 0.0858149528503418 elapsed, loss: 1.5101382e-06\n",
      "step: 154510 train: 0.082733154296875 elapsed, loss: 2.0596171e-06\n",
      "step: 154520 train: 0.08138608932495117 elapsed, loss: 4.3403766e-06\n",
      "step: 154530 train: 0.08334779739379883 elapsed, loss: 1.1882583e-05\n",
      "step: 154540 train: 0.07931327819824219 elapsed, loss: 6.28538e-05\n",
      "step: 154550 train: 0.08613228797912598 elapsed, loss: 9.501253e-06\n",
      "step: 154560 train: 0.0820307731628418 elapsed, loss: 5.6824174e-06\n",
      "step: 154570 train: 0.07496833801269531 elapsed, loss: 4.6891855e-06\n",
      "step: 154580 train: 0.0862283706665039 elapsed, loss: 2.2696297e-06\n",
      "step: 154590 train: 0.08494067192077637 elapsed, loss: 2.3795249e-06\n",
      "step: 154600 train: 0.08164763450622559 elapsed, loss: 2.6160806e-06\n",
      "step: 154610 train: 0.0796656608581543 elapsed, loss: 1.7858083e-06\n",
      "step: 154620 train: 0.07862138748168945 elapsed, loss: 1.6144461e-06\n",
      "step: 154630 train: 0.08124899864196777 elapsed, loss: 1.3387748e-06\n",
      "step: 154640 train: 0.08635258674621582 elapsed, loss: 1.9171239e-06\n",
      "step: 154650 train: 0.0756673812866211 elapsed, loss: 2.1648561e-06\n",
      "step: 154660 train: 0.08398556709289551 elapsed, loss: 1.2968657e-06\n",
      "step: 154670 train: 0.08313179016113281 elapsed, loss: 1.7597321e-06\n",
      "step: 154680 train: 0.08422613143920898 elapsed, loss: 1.994424e-06\n",
      "step: 154690 train: 0.0818471908569336 elapsed, loss: 1.4947713e-06\n",
      "step: 154700 train: 0.08143115043640137 elapsed, loss: 1.4668316e-06\n",
      "step: 154710 train: 0.08127832412719727 elapsed, loss: 1.5189856e-06\n",
      "step: 154720 train: 0.08001852035522461 elapsed, loss: 1.5497185e-06\n",
      "step: 154730 train: 0.0824272632598877 elapsed, loss: 1.2801019e-06\n",
      "step: 154740 train: 0.07677388191223145 elapsed, loss: 1.855658e-06\n",
      "step: 154750 train: 0.08651423454284668 elapsed, loss: 1.1268996e-06\n",
      "step: 154760 train: 0.08529448509216309 elapsed, loss: 1.1804505e-06\n",
      "step: 154770 train: 0.0744173526763916 elapsed, loss: 1.7895345e-06\n",
      "step: 154780 train: 0.0827629566192627 elapsed, loss: 1.6833638e-06\n",
      "step: 154790 train: 0.08054256439208984 elapsed, loss: 1.6507674e-06\n",
      "step: 154800 train: 0.08147072792053223 elapsed, loss: 1.4440141e-06\n",
      "step: 154810 train: 0.07922840118408203 elapsed, loss: 2.0735865e-06\n",
      "step: 154820 train: 0.08287739753723145 elapsed, loss: 1.4738166e-06\n",
      "step: 154830 train: 0.08465981483459473 elapsed, loss: 0.0006896362\n",
      "step: 154840 train: 0.08002352714538574 elapsed, loss: 4.7060486e-05\n",
      "step: 154850 train: 0.08486032485961914 elapsed, loss: 1.2428309e-05\n",
      "step: 154860 train: 0.08711957931518555 elapsed, loss: 1.3524685e-05\n",
      "step: 154870 train: 0.08369827270507812 elapsed, loss: 1.0036563e-05\n",
      "step: 154880 train: 0.0822908878326416 elapsed, loss: 7.7870845e-06\n",
      "step: 154890 train: 0.08341789245605469 elapsed, loss: 4.0577606e-06\n",
      "step: 154900 train: 0.08107280731201172 elapsed, loss: 4.636574e-06\n",
      "step: 154910 train: 0.07991814613342285 elapsed, loss: 3.5157314e-06\n",
      "step: 154920 train: 0.07756590843200684 elapsed, loss: 4.631451e-06\n",
      "step: 154930 train: 0.07736754417419434 elapsed, loss: 3.2801086e-06\n",
      "step: 154940 train: 0.08266615867614746 elapsed, loss: 2.3860441e-06\n",
      "step: 154950 train: 0.08705687522888184 elapsed, loss: 5.4179764e-05\n",
      "step: 154960 train: 0.08481240272521973 elapsed, loss: 3.7206084e-06\n",
      "step: 154970 train: 0.07508254051208496 elapsed, loss: 4.878713e-06\n",
      "step: 154980 train: 0.08295154571533203 elapsed, loss: 0.0002703691\n",
      "step: 154990 train: 0.07988500595092773 elapsed, loss: 0.00022873792\n",
      "step: 155000 train: 0.08963656425476074 elapsed, loss: 0.00013504951\n",
      "step: 155010 train: 0.08268523216247559 elapsed, loss: 8.4282045e-05\n",
      "step: 155020 train: 0.08167457580566406 elapsed, loss: 2.2615475e-05\n",
      "step: 155030 train: 0.07871031761169434 elapsed, loss: 2.166085e-05\n",
      "step: 155040 train: 0.0873715877532959 elapsed, loss: 1.3856688e-05\n",
      "step: 155050 train: 0.09374761581420898 elapsed, loss: 6.100595e-06\n",
      "step: 155060 train: 0.0833890438079834 elapsed, loss: 7.198135e-06\n",
      "step: 155070 train: 0.08528709411621094 elapsed, loss: 4.5685792e-06\n",
      "step: 155080 train: 0.0827629566192627 elapsed, loss: 3.7238524e-06\n",
      "step: 155090 train: 0.07975387573242188 elapsed, loss: 3.274985e-06\n",
      "step: 155100 train: 0.08158493041992188 elapsed, loss: 3.3532165e-06\n",
      "step: 155110 train: 0.07931971549987793 elapsed, loss: 2.5033899e-06\n",
      "step: 155120 train: 0.08003687858581543 elapsed, loss: 2.0498378e-06\n",
      "step: 155130 train: 0.08662676811218262 elapsed, loss: 1.3047818e-06\n",
      "step: 155140 train: 0.07535815238952637 elapsed, loss: 2.0605466e-06\n",
      "step: 155150 train: 0.0849919319152832 elapsed, loss: 1.3546073e-06\n",
      "step: 155160 train: 0.0825355052947998 elapsed, loss: 1.6912797e-06\n",
      "step: 155170 train: 0.0820608139038086 elapsed, loss: 1.0505311e-06\n",
      "step: 155180 train: 0.0845944881439209 elapsed, loss: 1.602338e-06\n",
      "step: 155190 train: 0.07723355293273926 elapsed, loss: 3.6572842e-06\n",
      "step: 155200 train: 0.08101987838745117 elapsed, loss: 1.2014052e-06\n",
      "step: 155210 train: 0.07847976684570312 elapsed, loss: 1.1967484e-06\n",
      "step: 155220 train: 0.08702945709228516 elapsed, loss: 1.1078074e-06\n",
      "step: 155230 train: 0.08028936386108398 elapsed, loss: 1.1497167e-06\n",
      "step: 155240 train: 0.08655905723571777 elapsed, loss: 8.5774764e-07\n",
      "step: 155250 train: 0.08100628852844238 elapsed, loss: 1.0482029e-06\n",
      "step: 155260 train: 0.07978630065917969 elapsed, loss: 1.0873184e-06\n",
      "step: 155270 train: 0.08959031105041504 elapsed, loss: 1.2856897e-06\n",
      "step: 155280 train: 0.08152198791503906 elapsed, loss: 1.1525108e-06\n",
      "step: 155290 train: 0.08408617973327637 elapsed, loss: 1.4514649e-06\n",
      "step: 155300 train: 0.07975888252258301 elapsed, loss: 1.3005908e-06\n",
      "step: 155310 train: 0.08300995826721191 elapsed, loss: 1.2177031e-06\n",
      "step: 155320 train: 0.07848095893859863 elapsed, loss: 1.6493705e-06\n",
      "step: 155330 train: 0.0822751522064209 elapsed, loss: 5.9590297e-06\n",
      "step: 155340 train: 0.07793378829956055 elapsed, loss: 2.0787074e-06\n",
      "step: 155350 train: 0.08208322525024414 elapsed, loss: 1.911537e-06\n",
      "step: 155360 train: 0.0836789608001709 elapsed, loss: 1.1748625e-06\n",
      "step: 155370 train: 0.09123682975769043 elapsed, loss: 1.1951382e-05\n",
      "step: 155380 train: 0.08272337913513184 elapsed, loss: 2.7315582e-06\n",
      "step: 155390 train: 0.08316874504089355 elapsed, loss: 1.5282988e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 155400 train: 0.07874107360839844 elapsed, loss: 1.844482e-06\n",
      "step: 155410 train: 0.07971048355102539 elapsed, loss: 1.3099042e-06\n",
      "step: 155420 train: 0.0837554931640625 elapsed, loss: 1.3560044e-06\n",
      "step: 155430 train: 0.08308625221252441 elapsed, loss: 1.3271333e-06\n",
      "step: 155440 train: 0.08068609237670898 elapsed, loss: 1.4728853e-06\n",
      "step: 155450 train: 0.07910323143005371 elapsed, loss: 2.8051377e-06\n",
      "step: 155460 train: 0.07789134979248047 elapsed, loss: 1.5227108e-06\n",
      "step: 155470 train: 0.08298492431640625 elapsed, loss: 1.3252709e-06\n",
      "step: 155480 train: 0.08151960372924805 elapsed, loss: 1.3960514e-06\n",
      "step: 155490 train: 0.08072066307067871 elapsed, loss: 1.5506507e-06\n",
      "step: 155500 train: 0.08299827575683594 elapsed, loss: 1.2218943e-06\n",
      "step: 155510 train: 0.08233261108398438 elapsed, loss: 1.62795e-06\n",
      "step: 155520 train: 0.07848477363586426 elapsed, loss: 1.5604289e-06\n",
      "step: 155530 train: 0.08468127250671387 elapsed, loss: 1.1613585e-06\n",
      "step: 155540 train: 0.08511471748352051 elapsed, loss: 1.3024535e-06\n",
      "step: 155550 train: 0.08341836929321289 elapsed, loss: 1.3997767e-06\n",
      "step: 155560 train: 0.08915591239929199 elapsed, loss: 1.6395918e-06\n",
      "step: 155570 train: 0.08771848678588867 elapsed, loss: 1.3345841e-06\n",
      "step: 155580 train: 0.07980489730834961 elapsed, loss: 1.2554219e-06\n",
      "step: 155590 train: 0.08256840705871582 elapsed, loss: 1.0509968e-06\n",
      "step: 155600 train: 0.07598185539245605 elapsed, loss: 1.7224793e-06\n",
      "step: 155610 train: 0.08350133895874023 elapsed, loss: 1.3713714e-06\n",
      "step: 155620 train: 0.07883739471435547 elapsed, loss: 1.7709078e-06\n",
      "step: 155630 train: 0.07464790344238281 elapsed, loss: 1.9306294e-06\n",
      "step: 155640 train: 0.07994365692138672 elapsed, loss: 1.8756809e-06\n",
      "step: 155650 train: 0.08250069618225098 elapsed, loss: 1.6042015e-06\n",
      "step: 155660 train: 0.08019542694091797 elapsed, loss: 1.5334208e-06\n",
      "step: 155670 train: 0.07793807983398438 elapsed, loss: 1.975332e-06\n",
      "step: 155680 train: 0.0826568603515625 elapsed, loss: 1.7518158e-06\n",
      "step: 155690 train: 0.07713103294372559 elapsed, loss: 2.1485582e-06\n",
      "step: 155700 train: 0.08223152160644531 elapsed, loss: 1.5185201e-06\n",
      "step: 155710 train: 0.08028340339660645 elapsed, loss: 1.7397088e-06\n",
      "step: 155720 train: 0.08213925361633301 elapsed, loss: 1.9199192e-06\n",
      "step: 155730 train: 0.08355712890625 elapsed, loss: 2.0125851e-06\n",
      "step: 155740 train: 0.0849912166595459 elapsed, loss: 1.7019902e-06\n",
      "step: 155750 train: 0.08162736892700195 elapsed, loss: 1.6936082e-06\n",
      "step: 155760 train: 0.08472609519958496 elapsed, loss: 1.4966339e-06\n",
      "step: 155770 train: 0.08980107307434082 elapsed, loss: 1.1897635e-06\n",
      "step: 155780 train: 0.08604788780212402 elapsed, loss: 1.7858091e-06\n",
      "step: 155790 train: 0.08292603492736816 elapsed, loss: 1.9064149e-06\n",
      "step: 155800 train: 0.08194899559020996 elapsed, loss: 1.6223618e-06\n",
      "step: 155810 train: 0.08761048316955566 elapsed, loss: 1.639126e-06\n",
      "step: 155820 train: 0.07545852661132812 elapsed, loss: 2.0870914e-06\n",
      "step: 155830 train: 0.08315157890319824 elapsed, loss: 2.238896e-06\n",
      "step: 155840 train: 0.07925653457641602 elapsed, loss: 2.123878e-06\n",
      "step: 155850 train: 0.07731485366821289 elapsed, loss: 1.8826665e-06\n",
      "step: 155860 train: 0.07823610305786133 elapsed, loss: 2.4731228e-06\n",
      "step: 155870 train: 0.07880854606628418 elapsed, loss: 2.3343569e-06\n",
      "step: 155880 train: 0.08150267601013184 elapsed, loss: 1.8016415e-06\n",
      "step: 155890 train: 0.08309435844421387 elapsed, loss: 2.4125875e-06\n",
      "step: 155900 train: 0.07752513885498047 elapsed, loss: 2.6943117e-06\n",
      "step: 155910 train: 0.07763934135437012 elapsed, loss: 2.173704e-06\n",
      "step: 155920 train: 0.07738447189331055 elapsed, loss: 1.8482074e-06\n",
      "step: 155930 train: 0.08502936363220215 elapsed, loss: 1.5343526e-06\n",
      "step: 155940 train: 0.07921552658081055 elapsed, loss: 2.4042056e-06\n",
      "step: 155950 train: 0.08202028274536133 elapsed, loss: 1.6787071e-06\n",
      "step: 155960 train: 0.07323503494262695 elapsed, loss: 2.1159622e-06\n",
      "step: 155970 train: 0.08098363876342773 elapsed, loss: 1.5492536e-06\n",
      "step: 155980 train: 0.07748961448669434 elapsed, loss: 1.6549585e-06\n",
      "step: 155990 train: 0.08670425415039062 elapsed, loss: 1.5175887e-06\n",
      "step: 156000 train: 0.07801079750061035 elapsed, loss: 2.5737045e-06\n",
      "step: 156010 train: 0.09101724624633789 elapsed, loss: 3.155845e-05\n",
      "step: 156020 train: 0.08092665672302246 elapsed, loss: 0.0004899534\n",
      "step: 156030 train: 0.08184552192687988 elapsed, loss: 6.861166e-05\n",
      "step: 156040 train: 0.08144807815551758 elapsed, loss: 5.323309e-05\n",
      "step: 156050 train: 0.07840800285339355 elapsed, loss: 3.1182903e-05\n",
      "step: 156060 train: 0.0763547420501709 elapsed, loss: 1.7796052e-05\n",
      "step: 156070 train: 0.08473873138427734 elapsed, loss: 1.02882295e-05\n",
      "step: 156080 train: 0.08643937110900879 elapsed, loss: 7.897989e-06\n",
      "step: 156090 train: 0.08423829078674316 elapsed, loss: 7.1571562e-06\n",
      "step: 156100 train: 0.08539509773254395 elapsed, loss: 3.6288864e-06\n",
      "step: 156110 train: 0.07913756370544434 elapsed, loss: 3.969276e-06\n",
      "step: 156120 train: 0.08505702018737793 elapsed, loss: 2.5690488e-06\n",
      "step: 156130 train: 0.08121919631958008 elapsed, loss: 2.2142158e-06\n",
      "step: 156140 train: 0.08127784729003906 elapsed, loss: 2.0754492e-06\n",
      "step: 156150 train: 0.07660484313964844 elapsed, loss: 2.5643922e-06\n",
      "step: 156160 train: 0.08003449440002441 elapsed, loss: 1.6884858e-06\n",
      "step: 156170 train: 0.07845425605773926 elapsed, loss: 1.7071122e-06\n",
      "step: 156180 train: 0.07868361473083496 elapsed, loss: 1.5697425e-06\n",
      "step: 156190 train: 0.08169412612915039 elapsed, loss: 1.2814988e-06\n",
      "step: 156200 train: 0.0776054859161377 elapsed, loss: 1.344363e-06\n",
      "step: 156210 train: 0.08258771896362305 elapsed, loss: 1.4062958e-06\n",
      "step: 156220 train: 0.08045125007629395 elapsed, loss: 1.531093e-06\n",
      "step: 156230 train: 0.08978152275085449 elapsed, loss: 1.2549561e-06\n",
      "step: 156240 train: 0.08532238006591797 elapsed, loss: 1.4114181e-06\n",
      "step: 156250 train: 0.08494329452514648 elapsed, loss: 1.3667147e-06\n",
      "step: 156260 train: 0.08167529106140137 elapsed, loss: 1.1580987e-06\n",
      "step: 156270 train: 0.08176922798156738 elapsed, loss: 1.4454113e-06\n",
      "step: 156280 train: 0.08162736892700195 elapsed, loss: 1.5753294e-06\n",
      "step: 156290 train: 0.08481574058532715 elapsed, loss: 1.4840571e-06\n",
      "step: 156300 train: 0.0812830924987793 elapsed, loss: 1.3587985e-06\n",
      "step: 156310 train: 0.08310890197753906 elapsed, loss: 1.3872037e-06\n",
      "step: 156320 train: 0.07923150062561035 elapsed, loss: 1.6549587e-06\n",
      "step: 156330 train: 0.08287191390991211 elapsed, loss: 1.3178203e-06\n",
      "step: 156340 train: 0.08286809921264648 elapsed, loss: 1.0910437e-06\n",
      "step: 156350 train: 0.07966756820678711 elapsed, loss: 1.4551902e-06\n",
      "step: 156360 train: 0.08112597465515137 elapsed, loss: 1.7802213e-06\n",
      "step: 156370 train: 0.07550883293151855 elapsed, loss: 2.35624e-06\n",
      "step: 156380 train: 0.0795431137084961 elapsed, loss: 1.8817351e-06\n",
      "step: 156390 train: 0.07947492599487305 elapsed, loss: 1.4179375e-06\n",
      "step: 156400 train: 0.08207178115844727 elapsed, loss: 1.6274845e-06\n",
      "step: 156410 train: 0.0854804515838623 elapsed, loss: 1.4579841e-06\n",
      "step: 156420 train: 0.08860564231872559 elapsed, loss: 1.9511174e-06\n",
      "step: 156430 train: 0.0863037109375 elapsed, loss: 1.5734665e-06\n",
      "step: 156440 train: 0.08086299896240234 elapsed, loss: 1.76532e-06\n",
      "step: 156450 train: 0.07876873016357422 elapsed, loss: 1.6926771e-06\n",
      "step: 156460 train: 0.08177638053894043 elapsed, loss: 2.0675336e-06\n",
      "step: 156470 train: 0.08016657829284668 elapsed, loss: 2.272888e-06\n",
      "step: 156480 train: 0.07929873466491699 elapsed, loss: 2.513169e-06\n",
      "step: 156490 train: 0.08087158203125 elapsed, loss: 1.784412e-06\n",
      "step: 156500 train: 0.08440613746643066 elapsed, loss: 1.8728875e-06\n",
      "step: 156510 train: 0.07657480239868164 elapsed, loss: 2.4433211e-06\n",
      "step: 156520 train: 0.08673310279846191 elapsed, loss: 2.4731225e-06\n",
      "step: 156530 train: 0.08737707138061523 elapsed, loss: 1.3536762e-06\n",
      "step: 156540 train: 0.08547520637512207 elapsed, loss: 1.6884859e-06\n",
      "step: 156550 train: 0.08255553245544434 elapsed, loss: 1.7974505e-06\n",
      "step: 156560 train: 0.08603048324584961 elapsed, loss: 2.0456455e-06\n",
      "step: 156570 train: 0.08241963386535645 elapsed, loss: 1.5734679e-06\n",
      "step: 156580 train: 0.08132648468017578 elapsed, loss: 2.388373e-06\n",
      "step: 156590 train: 0.0852956771850586 elapsed, loss: 1.7690434e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 156600 train: 0.08832883834838867 elapsed, loss: 1.8244588e-06\n",
      "step: 156610 train: 0.0844578742980957 elapsed, loss: 1.8994293e-06\n",
      "step: 156620 train: 0.0844266414642334 elapsed, loss: 5.7159186e-06\n",
      "step: 156630 train: 0.08228683471679688 elapsed, loss: 2.4344736e-06\n",
      "step: 156640 train: 0.07813405990600586 elapsed, loss: 2.2519348e-06\n",
      "step: 156650 train: 0.08516049385070801 elapsed, loss: 1.613049e-06\n",
      "step: 156660 train: 0.0885934829711914 elapsed, loss: 4.4861617e-06\n",
      "step: 156670 train: 0.07775402069091797 elapsed, loss: 0.00021692253\n",
      "step: 156680 train: 0.09029316902160645 elapsed, loss: 4.9332117e-05\n",
      "step: 156690 train: 0.08169841766357422 elapsed, loss: 3.1437747e-05\n",
      "step: 156700 train: 0.08956623077392578 elapsed, loss: 1.3352567e-05\n",
      "step: 156710 train: 0.08457612991333008 elapsed, loss: 1.096706e-05\n",
      "step: 156720 train: 0.0789787769317627 elapsed, loss: 8.38742e-06\n",
      "step: 156730 train: 0.08443737030029297 elapsed, loss: 8.702675e-06\n",
      "step: 156740 train: 0.08557510375976562 elapsed, loss: 4.9927994e-06\n",
      "step: 156750 train: 0.08085322380065918 elapsed, loss: 6.803726e-06\n",
      "step: 156760 train: 0.08107519149780273 elapsed, loss: 4.0363384e-06\n",
      "step: 156770 train: 0.08145856857299805 elapsed, loss: 3.3788265e-06\n",
      "step: 156780 train: 0.08388066291809082 elapsed, loss: 2.7213193e-06\n",
      "step: 156790 train: 0.0791018009185791 elapsed, loss: 4.164379e-06\n",
      "step: 156800 train: 0.0832815170288086 elapsed, loss: 2.5047875e-06\n",
      "step: 156810 train: 0.08394241333007812 elapsed, loss: 1.5958196e-06\n",
      "step: 156820 train: 0.08257699012756348 elapsed, loss: 1.4663658e-06\n",
      "step: 156830 train: 0.08485007286071777 elapsed, loss: 1.3560043e-06\n",
      "step: 156840 train: 0.08985471725463867 elapsed, loss: 1.1101357e-06\n",
      "step: 156850 train: 0.08333325386047363 elapsed, loss: 1.5399403e-06\n",
      "step: 156860 train: 0.075408935546875 elapsed, loss: 1.3614957e-05\n",
      "step: 156870 train: 0.08187365531921387 elapsed, loss: 2.6007732e-05\n",
      "step: 156880 train: 0.07839274406433105 elapsed, loss: 1.765068e-05\n",
      "step: 156890 train: 0.08494997024536133 elapsed, loss: 1.1486489e-05\n",
      "step: 156900 train: 0.08072447776794434 elapsed, loss: 1.0450819e-05\n",
      "step: 156910 train: 0.0822603702545166 elapsed, loss: 8.141556e-06\n",
      "step: 156920 train: 0.08344054222106934 elapsed, loss: 7.6149263e-06\n",
      "step: 156930 train: 0.07737493515014648 elapsed, loss: 4.2957045e-06\n",
      "step: 156940 train: 0.0765838623046875 elapsed, loss: 3.446814e-06\n",
      "step: 156950 train: 0.09040641784667969 elapsed, loss: 2.5848717e-06\n",
      "step: 156960 train: 0.0822606086730957 elapsed, loss: 2.2859263e-06\n",
      "step: 156970 train: 0.0772862434387207 elapsed, loss: 2.7790595e-06\n",
      "step: 156980 train: 0.0772407054901123 elapsed, loss: 2.372074e-06\n",
      "step: 156990 train: 0.07900834083557129 elapsed, loss: 2.1965209e-06\n",
      "step: 157000 train: 0.08141160011291504 elapsed, loss: 1.7280538e-06\n",
      "step: 157010 train: 0.08124089241027832 elapsed, loss: 1.2791704e-06\n",
      "step: 157020 train: 0.08453845977783203 elapsed, loss: 1.1827788e-06\n",
      "step: 157030 train: 0.07687759399414062 elapsed, loss: 1.3192173e-06\n",
      "step: 157040 train: 0.08285093307495117 elapsed, loss: 1.0533249e-06\n",
      "step: 157050 train: 0.08342099189758301 elapsed, loss: 1.219566e-06\n",
      "step: 157060 train: 0.07980179786682129 elapsed, loss: 1.3741653e-06\n",
      "step: 157070 train: 0.0807504653930664 elapsed, loss: 1.3015222e-06\n",
      "step: 157080 train: 0.08068037033081055 elapsed, loss: 1.8468102e-06\n",
      "step: 157090 train: 0.08437323570251465 elapsed, loss: 1.3341185e-06\n",
      "step: 157100 train: 0.0855104923248291 elapsed, loss: 1.2391238e-06\n",
      "step: 157110 train: 0.08069419860839844 elapsed, loss: 1.1799847e-06\n",
      "step: 157120 train: 0.08628177642822266 elapsed, loss: 1.0910436e-06\n",
      "step: 157130 train: 0.08210325241088867 elapsed, loss: 2.8728999e-05\n",
      "step: 157140 train: 0.07860088348388672 elapsed, loss: 0.0005232462\n",
      "step: 157150 train: 0.08083868026733398 elapsed, loss: 3.2811175e-05\n",
      "step: 157160 train: 0.0764777660369873 elapsed, loss: 2.9010116e-05\n",
      "step: 157170 train: 0.08447051048278809 elapsed, loss: 1.1895141e-05\n",
      "step: 157180 train: 0.0805668830871582 elapsed, loss: 1.3451761e-05\n",
      "step: 157190 train: 0.08990478515625 elapsed, loss: 7.5007197e-06\n",
      "step: 157200 train: 0.0850975513458252 elapsed, loss: 5.0179406e-06\n",
      "step: 157210 train: 0.08621573448181152 elapsed, loss: 4.9983546e-06\n",
      "step: 157220 train: 0.09003758430480957 elapsed, loss: 3.2847606e-06\n",
      "step: 157230 train: 0.08174276351928711 elapsed, loss: 2.5196878e-06\n",
      "step: 157240 train: 0.08176064491271973 elapsed, loss: 2.3422717e-06\n",
      "step: 157250 train: 0.08362698554992676 elapsed, loss: 2.134122e-06\n",
      "step: 157260 train: 0.08256053924560547 elapsed, loss: 2.220268e-06\n",
      "step: 157270 train: 0.08179688453674316 elapsed, loss: 3.481734e-06\n",
      "step: 157280 train: 0.08913230895996094 elapsed, loss: 1.5171227e-06\n",
      "step: 157290 train: 0.08113813400268555 elapsed, loss: 1.338775e-06\n",
      "step: 157300 train: 0.07889294624328613 elapsed, loss: 1.3606609e-06\n",
      "step: 157310 train: 0.0806264877319336 elapsed, loss: 1.3448284e-06\n",
      "step: 157320 train: 0.08476901054382324 elapsed, loss: 1.2163064e-06\n",
      "step: 157330 train: 0.08527541160583496 elapsed, loss: 1.4253878e-06\n",
      "step: 157340 train: 0.07980775833129883 elapsed, loss: 1.6954699e-06\n",
      "step: 157350 train: 0.0842447280883789 elapsed, loss: 1.2842927e-06\n",
      "step: 157360 train: 0.07678794860839844 elapsed, loss: 1.6167741e-06\n",
      "step: 157370 train: 0.08382821083068848 elapsed, loss: 1.2200317e-06\n",
      "step: 157380 train: 0.0753941535949707 elapsed, loss: 1.8356345e-06\n",
      "step: 157390 train: 0.0841209888458252 elapsed, loss: 1.2917432e-06\n",
      "step: 157400 train: 0.08024430274963379 elapsed, loss: 1.7425024e-06\n",
      "step: 157410 train: 0.07939529418945312 elapsed, loss: 1.5739334e-06\n",
      "step: 157420 train: 0.08188366889953613 elapsed, loss: 1.2908116e-06\n",
      "step: 157430 train: 0.07611513137817383 elapsed, loss: 1.8961703e-06\n",
      "step: 157440 train: 0.08031105995178223 elapsed, loss: 1.0780051e-06\n",
      "step: 157450 train: 0.07705998420715332 elapsed, loss: 1.7322523e-06\n",
      "step: 157460 train: 0.08494281768798828 elapsed, loss: 1.1250369e-06\n",
      "step: 157470 train: 0.08488154411315918 elapsed, loss: 1.1818474e-06\n",
      "step: 157480 train: 0.08298993110656738 elapsed, loss: 1.2922089e-06\n",
      "step: 157490 train: 0.08199286460876465 elapsed, loss: 1.2232913e-06\n",
      "step: 157500 train: 0.08015155792236328 elapsed, loss: 1.5795216e-06\n",
      "step: 157510 train: 0.08691954612731934 elapsed, loss: 1.1189833e-06\n",
      "step: 157520 train: 0.07986116409301758 elapsed, loss: 1.4896489e-06\n",
      "step: 157530 train: 0.08620858192443848 elapsed, loss: 1.4435487e-06\n",
      "step: 157540 train: 0.08929800987243652 elapsed, loss: 1.4957027e-06\n",
      "step: 157550 train: 0.09044122695922852 elapsed, loss: 0.019933196\n",
      "step: 157560 train: 0.08361124992370605 elapsed, loss: 9.2231014e-05\n",
      "step: 157570 train: 0.08123254776000977 elapsed, loss: 6.088638e-05\n",
      "step: 157580 train: 0.08051466941833496 elapsed, loss: 6.509018e-05\n",
      "step: 157590 train: 0.08278512954711914 elapsed, loss: 2.487131e-05\n",
      "step: 157600 train: 0.08979034423828125 elapsed, loss: 1.2536628e-05\n",
      "step: 157610 train: 0.08150196075439453 elapsed, loss: 6.5420427e-06\n",
      "step: 157620 train: 0.07970023155212402 elapsed, loss: 6.729697e-06\n",
      "step: 157630 train: 0.08185768127441406 elapsed, loss: 4.1066523e-06\n",
      "step: 157640 train: 0.08402585983276367 elapsed, loss: 3.807702e-06\n",
      "step: 157650 train: 0.0819406509399414 elapsed, loss: 2.6379657e-06\n",
      "step: 157660 train: 0.08027267456054688 elapsed, loss: 3.71031e-06\n",
      "step: 157670 train: 0.08185315132141113 elapsed, loss: 1.8971017e-06\n",
      "step: 157680 train: 0.07962369918823242 elapsed, loss: 1.8053667e-06\n",
      "step: 157690 train: 0.07925629615783691 elapsed, loss: 2.216544e-06\n",
      "step: 157700 train: 0.07861137390136719 elapsed, loss: 1.4244563e-06\n",
      "step: 157710 train: 0.08192920684814453 elapsed, loss: 1.4989619e-06\n",
      "step: 157720 train: 0.08186173439025879 elapsed, loss: 1.5092064e-06\n",
      "step: 157730 train: 0.08402180671691895 elapsed, loss: 1.6367976e-06\n",
      "step: 157740 train: 0.08690071105957031 elapsed, loss: 1.9152626e-06\n",
      "step: 157750 train: 0.07893514633178711 elapsed, loss: 1.5958194e-06\n",
      "step: 157760 train: 0.08333373069763184 elapsed, loss: 1.3806844e-06\n",
      "step: 157770 train: 0.08453559875488281 elapsed, loss: 1.4863879e-06\n",
      "step: 157780 train: 0.08046412467956543 elapsed, loss: 1.6493705e-06\n",
      "step: 157790 train: 0.080322265625 elapsed, loss: 1.5767276e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 157800 train: 0.08475494384765625 elapsed, loss: 1.6991963e-06\n",
      "step: 157810 train: 0.08103156089782715 elapsed, loss: 2.0437838e-06\n",
      "step: 157820 train: 0.08012700080871582 elapsed, loss: 1.7378452e-06\n",
      "step: 157830 train: 0.07668566703796387 elapsed, loss: 1.5269019e-06\n",
      "step: 157840 train: 0.08122968673706055 elapsed, loss: 3.2195894e-05\n",
      "step: 157850 train: 0.07857060432434082 elapsed, loss: 4.703621e-06\n",
      "step: 157860 train: 0.09309554100036621 elapsed, loss: 2.1317933e-06\n",
      "step: 157870 train: 0.07983851432800293 elapsed, loss: 1.8561236e-06\n",
      "step: 157880 train: 0.07994890213012695 elapsed, loss: 2.0395878e-06\n",
      "step: 157890 train: 0.08053326606750488 elapsed, loss: 2.0209673e-06\n",
      "step: 157900 train: 0.08257389068603516 elapsed, loss: 1.6028046e-06\n",
      "step: 157910 train: 0.07752108573913574 elapsed, loss: 1.6791729e-06\n",
      "step: 157920 train: 0.07761454582214355 elapsed, loss: 2.1951241e-06\n",
      "step: 157930 train: 0.07606053352355957 elapsed, loss: 1.8156113e-06\n",
      "step: 157940 train: 0.07320523262023926 elapsed, loss: 2.0605462e-06\n",
      "step: 157950 train: 0.08484292030334473 elapsed, loss: 1.3350495e-06\n",
      "step: 157960 train: 0.08704829216003418 elapsed, loss: 1.0207289e-06\n",
      "step: 157970 train: 0.0812680721282959 elapsed, loss: 0.016331162\n",
      "step: 157980 train: 0.07892012596130371 elapsed, loss: 6.2442006e-05\n",
      "step: 157990 train: 0.08480715751647949 elapsed, loss: 1.5889496e-05\n",
      "step: 158000 train: 0.07991433143615723 elapsed, loss: 1.1199029e-05\n",
      "step: 158010 train: 0.08332157135009766 elapsed, loss: 1.211373e-05\n",
      "step: 158020 train: 0.08541154861450195 elapsed, loss: 6.484288e-06\n",
      "step: 158030 train: 0.08550834655761719 elapsed, loss: 5.0719536e-06\n",
      "step: 158040 train: 0.07858109474182129 elapsed, loss: 4.313872e-06\n",
      "step: 158050 train: 0.08341121673583984 elapsed, loss: 3.2493758e-06\n",
      "step: 158060 train: 0.07373380661010742 elapsed, loss: 3.1520544e-06\n",
      "step: 158070 train: 0.08194565773010254 elapsed, loss: 2.0684645e-06\n",
      "step: 158080 train: 0.08639883995056152 elapsed, loss: 1.734586e-06\n",
      "step: 158090 train: 0.08273911476135254 elapsed, loss: 1.9790577e-06\n",
      "step: 158100 train: 0.0854034423828125 elapsed, loss: 1.2326044e-06\n",
      "step: 158110 train: 0.07942938804626465 elapsed, loss: 1.8877874e-06\n",
      "step: 158120 train: 0.0835256576538086 elapsed, loss: 1.452861e-06\n",
      "step: 158130 train: 0.07390165328979492 elapsed, loss: 1.9152626e-06\n",
      "step: 158140 train: 0.08263397216796875 elapsed, loss: 1.2298103e-06\n",
      "step: 158150 train: 0.07469630241394043 elapsed, loss: 1.6987303e-06\n",
      "step: 158160 train: 0.08734345436096191 elapsed, loss: 1.2083899e-06\n",
      "step: 158170 train: 0.08403968811035156 elapsed, loss: 1.2405208e-06\n",
      "step: 158180 train: 0.08330345153808594 elapsed, loss: 1.2544904e-06\n",
      "step: 158190 train: 0.08062744140625 elapsed, loss: 1.3657832e-06\n",
      "step: 158200 train: 0.08624792098999023 elapsed, loss: 1.0160722e-06\n",
      "step: 158210 train: 0.07916378974914551 elapsed, loss: 1.3080414e-06\n",
      "step: 158220 train: 0.0782938003540039 elapsed, loss: 1.3490196e-06\n",
      "step: 158230 train: 0.08268165588378906 elapsed, loss: 1.1110671e-06\n",
      "step: 158240 train: 0.08411765098571777 elapsed, loss: 1.7178224e-06\n",
      "step: 158250 train: 0.07629942893981934 elapsed, loss: 1.6768445e-06\n",
      "step: 158260 train: 0.07416558265686035 elapsed, loss: 1.7550753e-06\n",
      "step: 158270 train: 0.08194661140441895 elapsed, loss: 1.1646177e-06\n",
      "step: 158280 train: 0.07892346382141113 elapsed, loss: 1.624225e-06\n",
      "step: 158290 train: 0.08052730560302734 elapsed, loss: 1.6447136e-06\n",
      "step: 158300 train: 0.0847930908203125 elapsed, loss: 2.3865093e-06\n",
      "step: 158310 train: 0.08659958839416504 elapsed, loss: 1.3425e-06\n",
      "step: 158320 train: 0.0769660472869873 elapsed, loss: 1.8589178e-06\n",
      "step: 158330 train: 0.0829155445098877 elapsed, loss: 1.7289985e-06\n",
      "step: 158340 train: 0.08669757843017578 elapsed, loss: 1.2293449e-06\n",
      "step: 158350 train: 0.08450913429260254 elapsed, loss: 1.3657833e-06\n",
      "step: 158360 train: 0.07688021659851074 elapsed, loss: 1.8952392e-06\n",
      "step: 158370 train: 0.07990407943725586 elapsed, loss: 1.7951221e-06\n",
      "step: 158380 train: 0.08010363578796387 elapsed, loss: 1.5539101e-06\n",
      "step: 158390 train: 0.08295941352844238 elapsed, loss: 1.3699744e-06\n",
      "step: 158400 train: 0.07924866676330566 elapsed, loss: 1.3862725e-06\n",
      "step: 158410 train: 0.08587861061096191 elapsed, loss: 1.3550732e-06\n",
      "step: 158420 train: 0.07544136047363281 elapsed, loss: 1.9455306e-06\n",
      "step: 158430 train: 0.08112978935241699 elapsed, loss: 1.5790465e-06\n",
      "step: 158440 train: 0.07771134376525879 elapsed, loss: 2.1182905e-06\n",
      "step: 158450 train: 0.08398938179016113 elapsed, loss: 1.7322573e-06\n",
      "step: 158460 train: 0.08582878112792969 elapsed, loss: 1.4179373e-06\n",
      "step: 158470 train: 0.0843513011932373 elapsed, loss: 1.5408664e-06\n",
      "step: 158480 train: 0.08329653739929199 elapsed, loss: 1.5390092e-06\n",
      "step: 158490 train: 0.07712888717651367 elapsed, loss: 2.390236e-06\n",
      "step: 158500 train: 0.07783126831054688 elapsed, loss: 1.89943e-06\n",
      "step: 158510 train: 0.08215618133544922 elapsed, loss: 1.2731168e-06\n",
      "step: 158520 train: 0.07790994644165039 elapsed, loss: 2.1103742e-06\n",
      "step: 158530 train: 0.08198738098144531 elapsed, loss: 1.5054814e-06\n",
      "step: 158540 train: 0.0780022144317627 elapsed, loss: 1.8882542e-06\n",
      "step: 158550 train: 0.0819542407989502 elapsed, loss: 1.8430853e-06\n",
      "step: 158560 train: 0.0840902328491211 elapsed, loss: 1.591163e-06\n",
      "step: 158570 train: 0.08099889755249023 elapsed, loss: 1.8486734e-06\n",
      "step: 158580 train: 0.07845544815063477 elapsed, loss: 1.650302e-06\n",
      "step: 158590 train: 0.08360052108764648 elapsed, loss: 1.7164253e-06\n",
      "step: 158600 train: 0.08283829689025879 elapsed, loss: 2.5769643e-06\n",
      "step: 158610 train: 0.08219742774963379 elapsed, loss: 2.037731e-06\n",
      "step: 158620 train: 0.08517122268676758 elapsed, loss: 1.5115343e-06\n",
      "step: 158630 train: 0.08115863800048828 elapsed, loss: 5.993223e-06\n",
      "step: 158640 train: 0.07780289649963379 elapsed, loss: 2.81678e-06\n",
      "step: 158650 train: 0.08230423927307129 elapsed, loss: 2.1415735e-06\n",
      "step: 158660 train: 0.08274316787719727 elapsed, loss: 1.6209653e-06\n",
      "step: 158670 train: 0.08420634269714355 elapsed, loss: 1.8528635e-06\n",
      "step: 158680 train: 0.07746267318725586 elapsed, loss: 2.2412248e-06\n",
      "step: 158690 train: 0.0823676586151123 elapsed, loss: 3.7001323e-06\n",
      "step: 158700 train: 0.08021020889282227 elapsed, loss: 1.8519322e-06\n",
      "step: 158710 train: 0.08171796798706055 elapsed, loss: 2.1709102e-06\n",
      "step: 158720 train: 0.07537031173706055 elapsed, loss: 2.1001294e-06\n",
      "step: 158730 train: 0.08079957962036133 elapsed, loss: 3.9776664e-06\n",
      "step: 158740 train: 0.08801698684692383 elapsed, loss: 1.4286475e-06\n",
      "step: 158750 train: 0.08050918579101562 elapsed, loss: 2.188139e-06\n",
      "step: 158760 train: 0.08941173553466797 elapsed, loss: 1.6684626e-06\n",
      "step: 158770 train: 0.08479762077331543 elapsed, loss: 1.6367975e-06\n",
      "step: 158780 train: 0.08048844337463379 elapsed, loss: 2.0735863e-06\n",
      "step: 158790 train: 0.08161091804504395 elapsed, loss: 2.2239956e-06\n",
      "step: 158800 train: 0.0848855972290039 elapsed, loss: 1.5324899e-06\n",
      "step: 158810 train: 0.08032011985778809 elapsed, loss: 2.102458e-06\n",
      "step: 158820 train: 0.07985281944274902 elapsed, loss: 2.526674e-06\n",
      "step: 158830 train: 0.08129000663757324 elapsed, loss: 2.446115e-06\n",
      "step: 158840 train: 0.07731914520263672 elapsed, loss: 2.2919817e-06\n",
      "step: 158850 train: 0.07857060432434082 elapsed, loss: 2.1476271e-06\n",
      "step: 158860 train: 0.08068633079528809 elapsed, loss: 2.0675334e-06\n",
      "step: 158870 train: 0.07835984230041504 elapsed, loss: 2.3432044e-06\n",
      "step: 158880 train: 0.08420753479003906 elapsed, loss: 1.5636891e-06\n",
      "step: 158890 train: 0.08185982704162598 elapsed, loss: 2.3026917e-06\n",
      "step: 158900 train: 0.07524609565734863 elapsed, loss: 2.8661384e-06\n",
      "step: 158910 train: 0.08030915260314941 elapsed, loss: 2.1513524e-06\n",
      "step: 158920 train: 0.0771780014038086 elapsed, loss: 2.245415e-06\n",
      "step: 158930 train: 0.08041048049926758 elapsed, loss: 1.8919784e-06\n",
      "step: 158940 train: 0.08115124702453613 elapsed, loss: 2.328303e-06\n",
      "step: 158950 train: 0.0841984748840332 elapsed, loss: 2.3664873e-06\n",
      "step: 158960 train: 0.0840766429901123 elapsed, loss: 7.728005e-05\n",
      "step: 158970 train: 0.07988476753234863 elapsed, loss: 1.1607424e-05\n",
      "step: 158980 train: 0.08607959747314453 elapsed, loss: 0.002140087\n",
      "step: 158990 train: 0.07894659042358398 elapsed, loss: 9.3360475e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 159000 train: 0.0797121524810791 elapsed, loss: 2.5016288e-05\n",
      "step: 159010 train: 0.08341670036315918 elapsed, loss: 3.059175e-05\n",
      "step: 159020 train: 0.07795953750610352 elapsed, loss: 1.2297445e-05\n",
      "step: 159030 train: 0.07586312294006348 elapsed, loss: 8.206273e-06\n",
      "step: 159040 train: 0.07823848724365234 elapsed, loss: 6.802315e-06\n",
      "step: 159050 train: 0.08748316764831543 elapsed, loss: 4.314326e-06\n",
      "step: 159060 train: 0.07851696014404297 elapsed, loss: 4.0540344e-06\n",
      "step: 159070 train: 0.08336186408996582 elapsed, loss: 4.232839e-06\n",
      "step: 159080 train: 0.07928848266601562 elapsed, loss: 2.6114235e-06\n",
      "step: 159090 train: 0.07989239692687988 elapsed, loss: 2.4167769e-06\n",
      "step: 159100 train: 0.08378958702087402 elapsed, loss: 1.9660192e-06\n",
      "step: 159110 train: 0.08486557006835938 elapsed, loss: 2.5243455e-06\n",
      "step: 159120 train: 0.07928109169006348 elapsed, loss: 1.6898827e-06\n",
      "step: 159130 train: 0.08388972282409668 elapsed, loss: 1.7248074e-06\n",
      "step: 159140 train: 0.09145355224609375 elapsed, loss: 1.2950029e-06\n",
      "step: 159150 train: 0.08347702026367188 elapsed, loss: 1.1958173e-06\n",
      "step: 159160 train: 0.08246016502380371 elapsed, loss: 1.7108375e-06\n",
      "step: 159170 train: 0.07842350006103516 elapsed, loss: 1.4458769e-06\n",
      "step: 159180 train: 0.07854700088500977 elapsed, loss: 1.3723027e-06\n",
      "step: 159190 train: 0.07808375358581543 elapsed, loss: 2.0940756e-06\n",
      "step: 159200 train: 0.07842183113098145 elapsed, loss: 1.1995425e-06\n",
      "step: 159210 train: 0.0784003734588623 elapsed, loss: 1.2516966e-06\n",
      "step: 159220 train: 0.08554768562316895 elapsed, loss: 9.732314e-07\n",
      "step: 159230 train: 0.08265900611877441 elapsed, loss: 1.8295561e-06\n",
      "step: 159240 train: 0.08566904067993164 elapsed, loss: 0.0006540796\n",
      "step: 159250 train: 0.0774998664855957 elapsed, loss: 3.0070958e-05\n",
      "step: 159260 train: 0.08836030960083008 elapsed, loss: 1.0003164e-05\n",
      "step: 159270 train: 0.08079195022583008 elapsed, loss: 1.1920414e-05\n",
      "step: 159280 train: 0.0766291618347168 elapsed, loss: 8.136861e-06\n",
      "step: 159290 train: 0.0803523063659668 elapsed, loss: 4.575568e-06\n",
      "step: 159300 train: 0.08232736587524414 elapsed, loss: 5.1874204e-06\n",
      "step: 159310 train: 0.08083224296569824 elapsed, loss: 3.755068e-06\n",
      "step: 159320 train: 0.07773470878601074 elapsed, loss: 3.404441e-06\n",
      "step: 159330 train: 0.08325529098510742 elapsed, loss: 2.2142156e-06\n",
      "step: 159340 train: 0.08231735229492188 elapsed, loss: 2.2975687e-06\n",
      "step: 159350 train: 0.07992053031921387 elapsed, loss: 2.0568223e-06\n",
      "step: 159360 train: 0.0865316390991211 elapsed, loss: 1.3550729e-06\n",
      "step: 159370 train: 0.0782461166381836 elapsed, loss: 1.759732e-06\n",
      "step: 159380 train: 0.08945322036743164 elapsed, loss: 1.073814e-06\n",
      "step: 159390 train: 0.07810354232788086 elapsed, loss: 1.349485e-06\n",
      "step: 159400 train: 0.08263587951660156 elapsed, loss: 7.01277e-06\n",
      "step: 159410 train: 0.07719230651855469 elapsed, loss: 1.5422686e-06\n",
      "step: 159420 train: 0.0807948112487793 elapsed, loss: 1.7313266e-06\n",
      "step: 159430 train: 0.07149958610534668 elapsed, loss: 2.194658e-06\n",
      "step: 159440 train: 0.07816052436828613 elapsed, loss: 1.7271358e-06\n",
      "step: 159450 train: 0.08136153221130371 elapsed, loss: 1.1725342e-06\n",
      "step: 159460 train: 0.08714556694030762 elapsed, loss: 1.4295789e-06\n",
      "step: 159470 train: 0.07678961753845215 elapsed, loss: 1.533421e-06\n",
      "step: 159480 train: 0.08416271209716797 elapsed, loss: 1.3215442e-06\n",
      "step: 159490 train: 0.08008360862731934 elapsed, loss: 1.6642668e-06\n",
      "step: 159500 train: 0.08223724365234375 elapsed, loss: 1.2749797e-06\n",
      "step: 159510 train: 0.0820012092590332 elapsed, loss: 1.5520476e-06\n",
      "step: 159520 train: 0.08154582977294922 elapsed, loss: 1.1967487e-06\n",
      "step: 159530 train: 0.08482885360717773 elapsed, loss: 1.0072247e-06\n",
      "step: 159540 train: 0.08040761947631836 elapsed, loss: 1.5771927e-06\n",
      "step: 159550 train: 0.08342647552490234 elapsed, loss: 1.3364466e-06\n",
      "step: 159560 train: 0.07524394989013672 elapsed, loss: 1.8901169e-06\n",
      "step: 159570 train: 0.07941198348999023 elapsed, loss: 2.8074585e-06\n",
      "step: 159580 train: 0.0859823226928711 elapsed, loss: 1.4738166e-06\n",
      "step: 159590 train: 0.08903098106384277 elapsed, loss: 1.1501825e-06\n",
      "step: 159600 train: 0.08345460891723633 elapsed, loss: 1.4980308e-06\n",
      "step: 159610 train: 0.07754039764404297 elapsed, loss: 1.7373803e-06\n",
      "step: 159620 train: 0.0856008529663086 elapsed, loss: 1.6214308e-06\n",
      "step: 159630 train: 0.08240890502929688 elapsed, loss: 1.4859238e-06\n",
      "step: 159640 train: 0.08535909652709961 elapsed, loss: 1.9483166e-06\n",
      "step: 159650 train: 0.08428287506103516 elapsed, loss: 2.2319105e-06\n",
      "step: 159660 train: 0.08080172538757324 elapsed, loss: 1.3532106e-06\n",
      "step: 159670 train: 0.08442807197570801 elapsed, loss: 1.2195659e-06\n",
      "step: 159680 train: 0.0823054313659668 elapsed, loss: 1.7564724e-06\n",
      "step: 159690 train: 0.07951092720031738 elapsed, loss: 1.94879e-06\n",
      "step: 159700 train: 0.07993960380554199 elapsed, loss: 1.9581034e-06\n",
      "step: 159710 train: 0.07809090614318848 elapsed, loss: 2.1755668e-06\n",
      "step: 159720 train: 0.08510231971740723 elapsed, loss: 1.2973313e-06\n",
      "step: 159730 train: 0.09293508529663086 elapsed, loss: 1.1445946e-06\n",
      "step: 159740 train: 0.08554744720458984 elapsed, loss: 1.5469253e-06\n",
      "step: 159750 train: 0.08624267578125 elapsed, loss: 1.4319072e-06\n",
      "step: 159760 train: 0.08115100860595703 elapsed, loss: 1.6181712e-06\n",
      "step: 159770 train: 0.08210945129394531 elapsed, loss: 1.6856918e-06\n",
      "step: 159780 train: 0.0864725112915039 elapsed, loss: 1.5739331e-06\n",
      "step: 159790 train: 0.08101153373718262 elapsed, loss: 1.7485562e-06\n",
      "step: 159800 train: 0.07714080810546875 elapsed, loss: 1.9930276e-06\n",
      "step: 159810 train: 0.08244991302490234 elapsed, loss: 2.6659059e-06\n",
      "step: 159820 train: 0.07806038856506348 elapsed, loss: 2.0479758e-06\n",
      "step: 159830 train: 0.07890105247497559 elapsed, loss: 2.1536807e-06\n",
      "step: 159840 train: 0.07971882820129395 elapsed, loss: 1.6624091e-06\n",
      "step: 159850 train: 0.07631301879882812 elapsed, loss: 2.0978014e-06\n",
      "step: 159860 train: 0.0814812183380127 elapsed, loss: 1.5976823e-06\n",
      "step: 159870 train: 0.08483314514160156 elapsed, loss: 1.6712565e-06\n",
      "step: 159880 train: 0.08086800575256348 elapsed, loss: 2.7851152e-06\n",
      "step: 159890 train: 0.08104372024536133 elapsed, loss: 1.6596149e-06\n",
      "step: 159900 train: 0.07898640632629395 elapsed, loss: 2.5173513e-06\n",
      "step: 159910 train: 0.08618545532226562 elapsed, loss: 1.5529788e-06\n",
      "step: 159920 train: 0.08382701873779297 elapsed, loss: 2.3185241e-06\n",
      "step: 159930 train: 0.07542943954467773 elapsed, loss: 2.3920984e-06\n",
      "step: 159940 train: 0.08333826065063477 elapsed, loss: 1.8826663e-06\n",
      "step: 159950 train: 0.08091521263122559 elapsed, loss: 2.2812712e-06\n",
      "step: 159960 train: 0.08432793617248535 elapsed, loss: 1.6428513e-06\n",
      "step: 159970 train: 0.08578634262084961 elapsed, loss: 1.6875548e-06\n",
      "step: 159980 train: 0.07981276512145996 elapsed, loss: 3.1930317e-06\n",
      "step: 159990 train: 0.08174848556518555 elapsed, loss: 1.718754e-06\n",
      "step: 160000 train: 0.08255267143249512 elapsed, loss: 1.7746332e-06\n",
      "step: 160010 train: 0.08402514457702637 elapsed, loss: 1.935286e-06\n",
      "step: 160020 train: 0.09044122695922852 elapsed, loss: 1.6218967e-06\n",
      "step: 160030 train: 0.07975959777832031 elapsed, loss: 1.7252701e-06\n",
      "step: 160040 train: 0.08379840850830078 elapsed, loss: 2.311539e-06\n",
      "step: 160050 train: 0.07238149642944336 elapsed, loss: 3.1664904e-06\n",
      "step: 160060 train: 0.07909989356994629 elapsed, loss: 2.4265573e-06\n",
      "step: 160070 train: 0.08597898483276367 elapsed, loss: 1.6726534e-06\n",
      "step: 160080 train: 0.07234382629394531 elapsed, loss: 2.8558957e-06\n",
      "step: 160090 train: 0.08006930351257324 elapsed, loss: 2.0875557e-06\n",
      "step: 160100 train: 0.0762169361114502 elapsed, loss: 2.0088605e-06\n",
      "step: 160110 train: 0.07584309577941895 elapsed, loss: 3.0375022e-06\n",
      "step: 160120 train: 0.07988381385803223 elapsed, loss: 2.3739315e-06\n",
      "step: 160130 train: 0.07879805564880371 elapsed, loss: 2.5643928e-06\n",
      "step: 160140 train: 0.08310079574584961 elapsed, loss: 2.6845328e-06\n",
      "step: 160150 train: 0.08606600761413574 elapsed, loss: 1.780687e-06\n",
      "step: 160160 train: 0.08381819725036621 elapsed, loss: 1.851467e-06\n",
      "step: 160170 train: 0.08049225807189941 elapsed, loss: 2.6086302e-06\n",
      "step: 160180 train: 0.07652831077575684 elapsed, loss: 2.4703293e-06\n",
      "step: 160190 train: 0.0860602855682373 elapsed, loss: 2.1904675e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 160200 train: 0.0794379711151123 elapsed, loss: 1.912003e-06\n",
      "step: 160210 train: 0.08333873748779297 elapsed, loss: 0.00019899524\n",
      "step: 160220 train: 0.08751201629638672 elapsed, loss: 0.00015009799\n",
      "step: 160230 train: 0.07876944541931152 elapsed, loss: 0.0013002092\n",
      "step: 160240 train: 0.08741164207458496 elapsed, loss: 0.00017158387\n",
      "step: 160250 train: 0.08288979530334473 elapsed, loss: 3.7902857e-05\n",
      "step: 160260 train: 0.09296488761901855 elapsed, loss: 5.1548093e-05\n",
      "step: 160270 train: 0.0779106616973877 elapsed, loss: 1.8266895e-05\n",
      "step: 160280 train: 0.08136606216430664 elapsed, loss: 1.3658563e-05\n",
      "step: 160290 train: 0.07566499710083008 elapsed, loss: 1.1960332e-05\n",
      "step: 160300 train: 0.08191108703613281 elapsed, loss: 9.961803e-06\n",
      "step: 160310 train: 0.08076810836791992 elapsed, loss: 4.0717287e-06\n",
      "step: 160320 train: 0.08401679992675781 elapsed, loss: 3.781621e-06\n",
      "step: 160330 train: 0.08750224113464355 elapsed, loss: 3.0007127e-06\n",
      "step: 160340 train: 0.07939338684082031 elapsed, loss: 2.4686313e-05\n",
      "step: 160350 train: 0.08386969566345215 elapsed, loss: 0.00040085742\n",
      "step: 160360 train: 0.08161258697509766 elapsed, loss: 3.9791463e-05\n",
      "step: 160370 train: 0.08293366432189941 elapsed, loss: 2.2773944e-05\n",
      "step: 160380 train: 0.0812079906463623 elapsed, loss: 2.4035071e-05\n",
      "step: 160390 train: 0.08354640007019043 elapsed, loss: 1.3224548e-05\n",
      "step: 160400 train: 0.08575749397277832 elapsed, loss: 8.068902e-06\n",
      "step: 160410 train: 0.08625340461730957 elapsed, loss: 8.107874e-06\n",
      "step: 160420 train: 0.08074069023132324 elapsed, loss: 4.5820907e-06\n",
      "step: 160430 train: 0.08319354057312012 elapsed, loss: 4.678943e-06\n",
      "step: 160440 train: 0.0824897289276123 elapsed, loss: 4.6747455e-06\n",
      "step: 160450 train: 0.08022332191467285 elapsed, loss: 3.7657865e-06\n",
      "step: 160460 train: 0.08353853225708008 elapsed, loss: 2.6281818e-06\n",
      "step: 160470 train: 0.0878298282623291 elapsed, loss: 1.9287645e-06\n",
      "step: 160480 train: 0.08512306213378906 elapsed, loss: 1.5269015e-06\n",
      "step: 160490 train: 0.08149194717407227 elapsed, loss: 2.126671e-06\n",
      "step: 160500 train: 0.09054970741271973 elapsed, loss: 1.2107182e-06\n",
      "step: 160510 train: 0.08473634719848633 elapsed, loss: 1.2172376e-06\n",
      "step: 160520 train: 0.0816032886505127 elapsed, loss: 1.8626401e-06\n",
      "step: 160530 train: 0.08651590347290039 elapsed, loss: 9.867356e-07\n",
      "step: 160540 train: 0.07865166664123535 elapsed, loss: 1.5008249e-06\n",
      "step: 160550 train: 0.08271527290344238 elapsed, loss: 9.988423e-07\n",
      "step: 160560 train: 0.07564544677734375 elapsed, loss: 1.5078094e-06\n",
      "step: 160570 train: 0.08205628395080566 elapsed, loss: 1.3802187e-06\n",
      "step: 160580 train: 0.08463430404663086 elapsed, loss: 1.3145606e-06\n",
      "step: 160590 train: 0.08095359802246094 elapsed, loss: 1.4221282e-06\n",
      "step: 160600 train: 0.08090066909790039 elapsed, loss: 1.0626384e-06\n",
      "step: 160610 train: 0.07868337631225586 elapsed, loss: 1.3234082e-06\n",
      "step: 160620 train: 0.0802454948425293 elapsed, loss: 1.212581e-06\n",
      "step: 160630 train: 0.0877540111541748 elapsed, loss: 1.5371452e-06\n",
      "step: 160640 train: 0.08751225471496582 elapsed, loss: 1.0617066e-06\n",
      "step: 160650 train: 0.08309435844421387 elapsed, loss: 1.2409855e-06\n",
      "step: 160660 train: 0.08487963676452637 elapsed, loss: 1.0281794e-06\n",
      "step: 160670 train: 0.0801093578338623 elapsed, loss: 1.3727683e-06\n",
      "step: 160680 train: 0.0785369873046875 elapsed, loss: 1.7154939e-06\n",
      "step: 160690 train: 0.07348394393920898 elapsed, loss: 1.9134e-06\n",
      "step: 160700 train: 0.07870340347290039 elapsed, loss: 1.2833614e-06\n",
      "step: 160710 train: 0.08124732971191406 elapsed, loss: 2.171375e-06\n",
      "step: 160720 train: 0.0857248306274414 elapsed, loss: 1.5608939e-06\n",
      "step: 160730 train: 0.08696198463439941 elapsed, loss: 1.1129298e-06\n",
      "step: 160740 train: 0.08213520050048828 elapsed, loss: 1.4416855e-06\n",
      "step: 160750 train: 0.07745146751403809 elapsed, loss: 1.4705569e-06\n",
      "step: 160760 train: 0.09038591384887695 elapsed, loss: 1.2614754e-06\n",
      "step: 160770 train: 0.08020973205566406 elapsed, loss: 1.5948881e-06\n",
      "step: 160780 train: 0.08537673950195312 elapsed, loss: 1.86078e-06\n",
      "step: 160790 train: 0.08008623123168945 elapsed, loss: 1.8938421e-06\n",
      "step: 160800 train: 0.08297538757324219 elapsed, loss: 1.4570528e-06\n",
      "step: 160810 train: 0.08124327659606934 elapsed, loss: 1.4030361e-06\n",
      "step: 160820 train: 0.08541679382324219 elapsed, loss: 1.2340012e-06\n",
      "step: 160830 train: 0.08148670196533203 elapsed, loss: 1.6251563e-06\n",
      "step: 160840 train: 0.07781147956848145 elapsed, loss: 2.307347e-06\n",
      "step: 160850 train: 0.08333110809326172 elapsed, loss: 2.0400591e-06\n",
      "step: 160860 train: 0.08359646797180176 elapsed, loss: 1.5990794e-06\n",
      "step: 160870 train: 0.08477425575256348 elapsed, loss: 1.3178203e-06\n",
      "step: 160880 train: 0.08564972877502441 elapsed, loss: 1.3564702e-06\n",
      "step: 160890 train: 0.08251523971557617 elapsed, loss: 1.3750966e-06\n",
      "step: 160900 train: 0.07891082763671875 elapsed, loss: 1.6749816e-06\n",
      "step: 160910 train: 0.08474349975585938 elapsed, loss: 1.5245737e-06\n",
      "step: 160920 train: 0.08415389060974121 elapsed, loss: 1.4039675e-06\n",
      "step: 160930 train: 0.07758164405822754 elapsed, loss: 2.6486748e-06\n",
      "step: 160940 train: 0.08410763740539551 elapsed, loss: 1.46916e-06\n",
      "step: 160950 train: 0.07608652114868164 elapsed, loss: 1.531093e-06\n",
      "step: 160960 train: 0.07893061637878418 elapsed, loss: 2.5322604e-06\n",
      "step: 160970 train: 0.07614731788635254 elapsed, loss: 2.0880227e-06\n",
      "step: 160980 train: 0.08140230178833008 elapsed, loss: 1.7187535e-06\n",
      "step: 160990 train: 0.07397603988647461 elapsed, loss: 2.434008e-06\n",
      "step: 161000 train: 0.08205556869506836 elapsed, loss: 1.5264362e-06\n",
      "step: 161010 train: 0.0807805061340332 elapsed, loss: 2.6910518e-06\n",
      "step: 161020 train: 0.08444023132324219 elapsed, loss: 1.4612439e-06\n",
      "step: 161030 train: 0.08031010627746582 elapsed, loss: 2.1089772e-06\n",
      "step: 161040 train: 0.0803370475769043 elapsed, loss: 1.8100233e-06\n",
      "step: 161050 train: 0.07868051528930664 elapsed, loss: 2.2323773e-06\n",
      "step: 161060 train: 0.08418679237365723 elapsed, loss: 1.7769615e-06\n",
      "step: 161070 train: 0.08052444458007812 elapsed, loss: 2.057754e-06\n",
      "step: 161080 train: 0.08034610748291016 elapsed, loss: 2.188605e-06\n",
      "step: 161090 train: 0.08638882637023926 elapsed, loss: 1.3322558e-06\n",
      "step: 161100 train: 0.0819249153137207 elapsed, loss: 2.235171e-06\n",
      "step: 161110 train: 0.08331847190856934 elapsed, loss: 2.042853e-06\n",
      "step: 161120 train: 0.08366632461547852 elapsed, loss: 1.7080438e-06\n",
      "step: 161130 train: 0.08106541633605957 elapsed, loss: 2.5643926e-06\n",
      "step: 161140 train: 0.08700704574584961 elapsed, loss: 2.031677e-06\n",
      "step: 161150 train: 0.09004330635070801 elapsed, loss: 1.766251e-06\n",
      "step: 161160 train: 0.08506989479064941 elapsed, loss: 1.6679969e-06\n",
      "step: 161170 train: 0.07623457908630371 elapsed, loss: 2.3925645e-06\n",
      "step: 161180 train: 0.08241558074951172 elapsed, loss: 2.0703274e-06\n",
      "step: 161190 train: 0.08543157577514648 elapsed, loss: 1.7206166e-06\n",
      "step: 161200 train: 0.08485555648803711 elapsed, loss: 2.6202702e-06\n",
      "step: 161210 train: 0.0868837833404541 elapsed, loss: 1.498962e-06\n",
      "step: 161220 train: 0.07697176933288574 elapsed, loss: 2.1518179e-06\n",
      "step: 161230 train: 0.08128690719604492 elapsed, loss: 2.1629937e-06\n",
      "step: 161240 train: 0.08136367797851562 elapsed, loss: 1.8957048e-06\n",
      "step: 161250 train: 0.07635331153869629 elapsed, loss: 3.089191e-06\n",
      "step: 161260 train: 0.07951569557189941 elapsed, loss: 1.817474e-06\n",
      "step: 161270 train: 0.0821373462677002 elapsed, loss: 1.7532104e-06\n",
      "step: 161280 train: 0.0805809497833252 elapsed, loss: 4.627226e-05\n",
      "step: 161290 train: 0.08062124252319336 elapsed, loss: 1.4006729e-05\n",
      "step: 161300 train: 0.07953810691833496 elapsed, loss: 1.6812754e-05\n",
      "step: 161310 train: 0.08466219902038574 elapsed, loss: 9.876601e-06\n",
      "step: 161320 train: 0.08128237724304199 elapsed, loss: 7.115722e-06\n",
      "step: 161330 train: 0.08127188682556152 elapsed, loss: 6.809317e-06\n",
      "step: 161340 train: 0.08077335357666016 elapsed, loss: 4.548565e-06\n",
      "step: 161350 train: 0.08478188514709473 elapsed, loss: 3.7429716e-06\n",
      "step: 161360 train: 0.0817573070526123 elapsed, loss: 3.4365694e-06\n",
      "step: 161370 train: 0.0846552848815918 elapsed, loss: 2.8647419e-06\n",
      "step: 161380 train: 0.0798192024230957 elapsed, loss: 3.3122396e-06\n",
      "step: 161390 train: 0.077911376953125 elapsed, loss: 2.4516853e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 161400 train: 0.07727551460266113 elapsed, loss: 1.9976842e-06\n",
      "step: 161410 train: 0.08350968360900879 elapsed, loss: 1.9404083e-06\n",
      "step: 161420 train: 0.08185434341430664 elapsed, loss: 1.570674e-06\n",
      "step: 161430 train: 0.08219194412231445 elapsed, loss: 1.7797554e-06\n",
      "step: 161440 train: 0.08874034881591797 elapsed, loss: 1.3639207e-06\n",
      "step: 161450 train: 0.08370280265808105 elapsed, loss: 1.8006814e-06\n",
      "step: 161460 train: 0.0865941047668457 elapsed, loss: 1.4477331e-06\n",
      "step: 161470 train: 0.08194494247436523 elapsed, loss: 1.3266679e-06\n",
      "step: 161480 train: 0.07960081100463867 elapsed, loss: 1.5767275e-06\n",
      "step: 161490 train: 0.0875694751739502 elapsed, loss: 1.654017e-06\n",
      "step: 161500 train: 0.0904233455657959 elapsed, loss: 1.573002e-06\n",
      "step: 161510 train: 0.08127307891845703 elapsed, loss: 1.9003605e-06\n",
      "step: 161520 train: 0.08335089683532715 elapsed, loss: 1.2260851e-06\n",
      "step: 161530 train: 0.08161783218383789 elapsed, loss: 1.6386605e-06\n",
      "step: 161540 train: 0.07705211639404297 elapsed, loss: 2.1606652e-06\n",
      "step: 161550 train: 0.0861210823059082 elapsed, loss: 1.2726513e-06\n",
      "step: 161560 train: 0.08481168746948242 elapsed, loss: 1.6768447e-06\n",
      "step: 161570 train: 0.08342242240905762 elapsed, loss: 1.780221e-06\n",
      "step: 161580 train: 0.08007264137268066 elapsed, loss: 1.4910461e-06\n",
      "step: 161590 train: 0.08186125755310059 elapsed, loss: 1.7210823e-06\n",
      "step: 161600 train: 0.077392578125 elapsed, loss: 1.7760303e-06\n",
      "step: 161610 train: 0.09211325645446777 elapsed, loss: 1.2358641e-06\n",
      "step: 161620 train: 0.07935070991516113 elapsed, loss: 1.612118e-06\n",
      "step: 161630 train: 0.08350276947021484 elapsed, loss: 1.6437825e-06\n",
      "step: 161640 train: 0.0784292221069336 elapsed, loss: 1.6945398e-06\n",
      "step: 161650 train: 0.0745081901550293 elapsed, loss: 2.4815045e-06\n",
      "step: 161660 train: 0.08306217193603516 elapsed, loss: 0.00013814063\n",
      "step: 161670 train: 0.07812690734863281 elapsed, loss: 4.940552e-05\n",
      "step: 161680 train: 0.08550047874450684 elapsed, loss: 1.8054157e-05\n",
      "step: 161690 train: 0.08293700218200684 elapsed, loss: 1.5067528e-05\n",
      "step: 161700 train: 0.08020567893981934 elapsed, loss: 9.279585e-06\n",
      "step: 161710 train: 0.08091402053833008 elapsed, loss: 9.959686e-06\n",
      "step: 161720 train: 0.07610630989074707 elapsed, loss: 7.894283e-06\n",
      "step: 161730 train: 0.08211493492126465 elapsed, loss: 5.9390145e-06\n",
      "step: 161740 train: 0.08185815811157227 elapsed, loss: 5.7308694e-06\n",
      "step: 161750 train: 0.08008909225463867 elapsed, loss: 3.1264435e-06\n",
      "step: 161760 train: 0.07920169830322266 elapsed, loss: 3.82726e-06\n",
      "step: 161770 train: 0.08488893508911133 elapsed, loss: 3.0919844e-06\n",
      "step: 161780 train: 0.07930946350097656 elapsed, loss: 2.3883724e-06\n",
      "step: 161790 train: 0.07921028137207031 elapsed, loss: 2.2849963e-06\n",
      "step: 161800 train: 0.07675790786743164 elapsed, loss: 2.6798725e-06\n",
      "step: 161810 train: 0.08591270446777344 elapsed, loss: 1.4500678e-06\n",
      "step: 161820 train: 0.08256268501281738 elapsed, loss: 1.3499509e-06\n",
      "step: 161830 train: 0.07994294166564941 elapsed, loss: 1.3839433e-06\n",
      "step: 161840 train: 0.08519458770751953 elapsed, loss: 3.1636016e-06\n",
      "step: 161850 train: 0.07854008674621582 elapsed, loss: 0.00028408918\n",
      "step: 161860 train: 0.08494973182678223 elapsed, loss: 4.5198973e-05\n",
      "step: 161870 train: 0.08017110824584961 elapsed, loss: 2.8877766e-05\n",
      "step: 161880 train: 0.08132195472717285 elapsed, loss: 1.5363254e-05\n",
      "step: 161890 train: 0.08547782897949219 elapsed, loss: 1.0555941e-05\n",
      "step: 161900 train: 0.07837867736816406 elapsed, loss: 1.01699015e-05\n",
      "step: 161910 train: 0.09316134452819824 elapsed, loss: 4.350649e-06\n",
      "step: 161920 train: 0.08831501007080078 elapsed, loss: 4.6817395e-06\n",
      "step: 161930 train: 0.08733773231506348 elapsed, loss: 3.1245772e-06\n",
      "step: 161940 train: 0.07708621025085449 elapsed, loss: 3.7928003e-06\n",
      "step: 161950 train: 0.08554291725158691 elapsed, loss: 2.2244603e-06\n",
      "step: 161960 train: 0.08155226707458496 elapsed, loss: 2.2142135e-06\n",
      "step: 161970 train: 0.08316755294799805 elapsed, loss: 2.0377302e-06\n",
      "step: 161980 train: 0.0854501724243164 elapsed, loss: 1.5925598e-06\n",
      "step: 161990 train: 0.09056925773620605 elapsed, loss: 1.3629893e-06\n",
      "step: 162000 train: 0.08272671699523926 elapsed, loss: 1.536215e-06\n",
      "step: 162010 train: 0.08391833305358887 elapsed, loss: 1.620498e-06\n",
      "step: 162020 train: 0.08347249031066895 elapsed, loss: 1.5003591e-06\n",
      "step: 162030 train: 0.08019900321960449 elapsed, loss: 1.0984942e-06\n",
      "step: 162040 train: 0.08342671394348145 elapsed, loss: 1.1767252e-06\n",
      "step: 162050 train: 0.08257675170898438 elapsed, loss: 1.2922089e-06\n",
      "step: 162060 train: 0.07505226135253906 elapsed, loss: 1.4766106e-06\n",
      "step: 162070 train: 0.07803201675415039 elapsed, loss: 1.5068786e-06\n",
      "step: 162080 train: 0.08396697044372559 elapsed, loss: 9.862699e-07\n",
      "step: 162090 train: 0.08373379707336426 elapsed, loss: 1.5431999e-06\n",
      "step: 162100 train: 0.08932828903198242 elapsed, loss: 1.0887154e-06\n",
      "step: 162110 train: 0.07938122749328613 elapsed, loss: 1.5483223e-06\n",
      "step: 162120 train: 0.07529783248901367 elapsed, loss: 1.5343524e-06\n",
      "step: 162130 train: 0.08147001266479492 elapsed, loss: 1.2302762e-06\n",
      "step: 162140 train: 0.08638525009155273 elapsed, loss: 1.3862707e-06\n",
      "step: 162150 train: 0.08603358268737793 elapsed, loss: 1.3713709e-06\n",
      "step: 162160 train: 0.08512020111083984 elapsed, loss: 1.1427317e-06\n",
      "step: 162170 train: 0.0760042667388916 elapsed, loss: 1.6116519e-06\n",
      "step: 162180 train: 0.07881283760070801 elapsed, loss: 1.488252e-06\n",
      "step: 162190 train: 0.07697653770446777 elapsed, loss: 1.6903488e-06\n",
      "step: 162200 train: 0.0814046859741211 elapsed, loss: 1.8537955e-06\n",
      "step: 162210 train: 0.08434176445007324 elapsed, loss: 1.6302783e-06\n",
      "step: 162220 train: 0.07882928848266602 elapsed, loss: 1.8719561e-06\n",
      "step: 162230 train: 0.08056211471557617 elapsed, loss: 2.042388e-06\n",
      "step: 162240 train: 0.08066558837890625 elapsed, loss: 1.6181715e-06\n",
      "step: 162250 train: 0.07775592803955078 elapsed, loss: 2.6225994e-06\n",
      "step: 162260 train: 0.08605766296386719 elapsed, loss: 2.4530996e-06\n",
      "step: 162270 train: 0.07343745231628418 elapsed, loss: 1.9492559e-06\n",
      "step: 162280 train: 0.08797454833984375 elapsed, loss: 1.3513479e-06\n",
      "step: 162290 train: 0.07419013977050781 elapsed, loss: 2.0367997e-06\n",
      "step: 162300 train: 0.07947826385498047 elapsed, loss: 1.8766129e-06\n",
      "step: 162310 train: 0.08112215995788574 elapsed, loss: 2.24169e-06\n",
      "step: 162320 train: 0.0856783390045166 elapsed, loss: 1.4607781e-06\n",
      "step: 162330 train: 0.0775003433227539 elapsed, loss: 1.781618e-06\n",
      "step: 162340 train: 0.07923316955566406 elapsed, loss: 1.7262046e-06\n",
      "step: 162350 train: 0.08003020286560059 elapsed, loss: 2.466641e-05\n",
      "step: 162360 train: 0.0800933837890625 elapsed, loss: 0.011956791\n",
      "step: 162370 train: 0.0796205997467041 elapsed, loss: 3.0713847e-05\n",
      "step: 162380 train: 0.08853268623352051 elapsed, loss: 2.7252576e-05\n",
      "step: 162390 train: 0.07631993293762207 elapsed, loss: 2.4481888e-05\n",
      "step: 162400 train: 0.08286452293395996 elapsed, loss: 1.0800846e-05\n",
      "step: 162410 train: 0.08820796012878418 elapsed, loss: 7.730359e-06\n",
      "step: 162420 train: 0.07348823547363281 elapsed, loss: 1.6497906e-05\n",
      "step: 162430 train: 0.08418774604797363 elapsed, loss: 4.6062987e-06\n",
      "step: 162440 train: 0.07811856269836426 elapsed, loss: 4.8274846e-06\n",
      "step: 162450 train: 0.08074665069580078 elapsed, loss: 4.1616013e-06\n",
      "step: 162460 train: 0.08351802825927734 elapsed, loss: 2.70176e-06\n",
      "step: 162470 train: 0.07445359230041504 elapsed, loss: 4.0419072e-06\n",
      "step: 162480 train: 0.08007478713989258 elapsed, loss: 2.351118e-06\n",
      "step: 162490 train: 0.08573102951049805 elapsed, loss: 2.052164e-06\n",
      "step: 162500 train: 0.08355545997619629 elapsed, loss: 1.764854e-06\n",
      "step: 162510 train: 0.08268523216247559 elapsed, loss: 1.4142117e-06\n",
      "step: 162520 train: 0.08391618728637695 elapsed, loss: 1.2656661e-06\n",
      "step: 162530 train: 0.07971787452697754 elapsed, loss: 1.4919772e-06\n",
      "step: 162540 train: 0.08304142951965332 elapsed, loss: 1.6260477e-06\n",
      "step: 162550 train: 0.07761836051940918 elapsed, loss: 1.4356324e-06\n",
      "step: 162560 train: 0.08497214317321777 elapsed, loss: 1.3136294e-06\n",
      "step: 162570 train: 0.08322644233703613 elapsed, loss: 1.4482052e-06\n",
      "step: 162580 train: 0.08325076103210449 elapsed, loss: 1.5790558e-06\n",
      "step: 162590 train: 0.08504462242126465 elapsed, loss: 1.1972143e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 162600 train: 0.08514189720153809 elapsed, loss: 1.3103697e-06\n",
      "step: 162610 train: 0.08316254615783691 elapsed, loss: 1.3904635e-06\n",
      "step: 162620 train: 0.0834801197052002 elapsed, loss: 1.3411034e-06\n",
      "step: 162630 train: 0.08102917671203613 elapsed, loss: 1.3606609e-06\n",
      "step: 162640 train: 0.07862567901611328 elapsed, loss: 1.8486733e-06\n",
      "step: 162650 train: 0.08090972900390625 elapsed, loss: 1.3988453e-06\n",
      "step: 162660 train: 0.07681155204772949 elapsed, loss: 1.85007e-06\n",
      "step: 162670 train: 0.07990455627441406 elapsed, loss: 1.2484364e-06\n",
      "step: 162680 train: 0.07854819297790527 elapsed, loss: 1.8030383e-06\n",
      "step: 162690 train: 0.08087038993835449 elapsed, loss: 1.4724196e-06\n",
      "step: 162700 train: 0.07855677604675293 elapsed, loss: 1.4947714e-06\n",
      "step: 162710 train: 0.0842275619506836 elapsed, loss: 1.4607779e-06\n",
      "step: 162720 train: 0.08298134803771973 elapsed, loss: 1.6414542e-06\n",
      "step: 162730 train: 0.08691573143005371 elapsed, loss: 1.3750966e-06\n",
      "step: 162740 train: 0.07954072952270508 elapsed, loss: 1.4756793e-06\n",
      "step: 162750 train: 0.08302164077758789 elapsed, loss: 1.7597321e-06\n",
      "step: 162760 train: 0.08055925369262695 elapsed, loss: 1.5166574e-06\n",
      "step: 162770 train: 0.08426475524902344 elapsed, loss: 1.6288815e-06\n",
      "step: 162780 train: 0.08228397369384766 elapsed, loss: 2.1452984e-06\n",
      "step: 162790 train: 0.07704806327819824 elapsed, loss: 0.014208786\n",
      "step: 162800 train: 0.0885009765625 elapsed, loss: 2.123036e-05\n",
      "step: 162810 train: 0.0871574878692627 elapsed, loss: 1.6141708e-05\n",
      "step: 162820 train: 0.07716035842895508 elapsed, loss: 1.8023e-05\n",
      "step: 162830 train: 0.08537459373474121 elapsed, loss: 8.623036e-06\n",
      "step: 162840 train: 0.08974790573120117 elapsed, loss: 7.968123e-06\n",
      "step: 162850 train: 0.08655643463134766 elapsed, loss: 4.013521e-06\n",
      "step: 162860 train: 0.0840139389038086 elapsed, loss: 3.1948935e-06\n",
      "step: 162870 train: 0.08277559280395508 elapsed, loss: 4.769734e-06\n",
      "step: 162880 train: 0.07693624496459961 elapsed, loss: 3.9073525e-06\n",
      "step: 162890 train: 0.08208394050598145 elapsed, loss: 2.509907e-06\n",
      "step: 162900 train: 0.07720160484313965 elapsed, loss: 2.544834e-06\n",
      "step: 162910 train: 0.07601356506347656 elapsed, loss: 2.963462e-06\n",
      "step: 162920 train: 0.08174467086791992 elapsed, loss: 1.4887177e-06\n",
      "step: 162930 train: 0.0842735767364502 elapsed, loss: 1.6079241e-06\n",
      "step: 162940 train: 0.08306694030761719 elapsed, loss: 1.521779e-06\n",
      "step: 162950 train: 0.07576298713684082 elapsed, loss: 1.6028036e-06\n",
      "step: 162960 train: 0.07765054702758789 elapsed, loss: 1.476144e-06\n",
      "step: 162970 train: 0.0807504653930664 elapsed, loss: 2.2621775e-06\n",
      "step: 162980 train: 0.07997846603393555 elapsed, loss: 1.8351682e-06\n",
      "step: 162990 train: 0.08333516120910645 elapsed, loss: 1.0915091e-06\n",
      "step: 163000 train: 0.08047294616699219 elapsed, loss: 1.1888324e-06\n",
      "step: 163010 train: 0.08522820472717285 elapsed, loss: 1.4877854e-06\n",
      "step: 163020 train: 0.08313918113708496 elapsed, loss: 1.3629893e-06\n",
      "step: 163030 train: 0.07743477821350098 elapsed, loss: 2.184413e-06\n",
      "step: 163040 train: 0.08169007301330566 elapsed, loss: 1.4458768e-06\n",
      "step: 163050 train: 0.08061981201171875 elapsed, loss: 1.443083e-06\n",
      "step: 163060 train: 0.08548760414123535 elapsed, loss: 1.0775395e-06\n",
      "step: 163070 train: 0.07696914672851562 elapsed, loss: 2.1192213e-06\n",
      "step: 163080 train: 0.07683682441711426 elapsed, loss: 2.0712585e-06\n",
      "step: 163090 train: 0.0791165828704834 elapsed, loss: 1.3895319e-06\n",
      "step: 163100 train: 0.08511114120483398 elapsed, loss: 1.4505335e-06\n",
      "step: 163110 train: 0.07782959938049316 elapsed, loss: 2.3543794e-06\n",
      "step: 163120 train: 0.07627058029174805 elapsed, loss: 2.303623e-06\n",
      "step: 163130 train: 0.08429431915283203 elapsed, loss: 1.5767257e-06\n",
      "step: 163140 train: 0.08080029487609863 elapsed, loss: 1.840757e-06\n",
      "step: 163150 train: 0.0841829776763916 elapsed, loss: 1.6549584e-06\n",
      "step: 163160 train: 0.07722926139831543 elapsed, loss: 5.7918573e-06\n",
      "step: 163170 train: 0.08717131614685059 elapsed, loss: 1.2563532e-06\n",
      "step: 163180 train: 0.08019709587097168 elapsed, loss: 1.5990793e-06\n",
      "step: 163190 train: 0.0803823471069336 elapsed, loss: 1.7150286e-06\n",
      "step: 163200 train: 0.08785772323608398 elapsed, loss: 1.3448287e-06\n",
      "step: 163210 train: 0.08160662651062012 elapsed, loss: 1.5837124e-06\n",
      "step: 163220 train: 0.07970237731933594 elapsed, loss: 1.8682306e-06\n",
      "step: 163230 train: 0.07919716835021973 elapsed, loss: 1.6712565e-06\n",
      "step: 163240 train: 0.07689523696899414 elapsed, loss: 5.830339e-05\n",
      "step: 163250 train: 0.07936310768127441 elapsed, loss: 0.0013080214\n",
      "step: 163260 train: 0.07914447784423828 elapsed, loss: 0.00010221967\n",
      "step: 163270 train: 0.08332371711730957 elapsed, loss: 0.0016801424\n",
      "step: 163280 train: 0.07723069190979004 elapsed, loss: 6.0623024e-05\n",
      "step: 163290 train: 0.08212804794311523 elapsed, loss: 1.1868563e-05\n",
      "step: 163300 train: 0.07860279083251953 elapsed, loss: 9.79416e-06\n",
      "step: 163310 train: 0.07965397834777832 elapsed, loss: 8.699399e-06\n",
      "step: 163320 train: 0.07833290100097656 elapsed, loss: 8.975547e-06\n",
      "step: 163330 train: 0.08183908462524414 elapsed, loss: 7.787197e-06\n",
      "step: 163340 train: 0.07700443267822266 elapsed, loss: 6.048438e-06\n",
      "step: 163350 train: 0.08413887023925781 elapsed, loss: 3.4985012e-06\n",
      "step: 163360 train: 0.08245444297790527 elapsed, loss: 3.8477465e-06\n",
      "step: 163370 train: 0.08582162857055664 elapsed, loss: 2.4163114e-06\n",
      "step: 163380 train: 0.0772707462310791 elapsed, loss: 3.4524012e-06\n",
      "step: 163390 train: 0.08004307746887207 elapsed, loss: 2.013517e-06\n",
      "step: 163400 train: 0.07769012451171875 elapsed, loss: 2.189068e-06\n",
      "step: 163410 train: 0.07998418807983398 elapsed, loss: 1.5138633e-06\n",
      "step: 163420 train: 0.08227181434631348 elapsed, loss: 1.7350521e-06\n",
      "step: 163430 train: 0.0815892219543457 elapsed, loss: 1.2470398e-06\n",
      "step: 163440 train: 0.08667969703674316 elapsed, loss: 1.2647349e-06\n",
      "step: 163450 train: 0.08709120750427246 elapsed, loss: 9.979115e-07\n",
      "step: 163460 train: 0.08080434799194336 elapsed, loss: 1.5092066e-06\n",
      "step: 163470 train: 0.08319211006164551 elapsed, loss: 1.1129297e-06\n",
      "step: 163480 train: 0.07843446731567383 elapsed, loss: 1.544131e-06\n",
      "step: 163490 train: 0.0828254222869873 elapsed, loss: 1.1944203e-06\n",
      "step: 163500 train: 0.08500409126281738 elapsed, loss: 1.4211971e-06\n",
      "step: 163510 train: 0.07843828201293945 elapsed, loss: 1.9655517e-06\n",
      "step: 163520 train: 0.08138394355773926 elapsed, loss: 1.289415e-06\n",
      "step: 163530 train: 0.0839080810546875 elapsed, loss: 1.3802189e-06\n",
      "step: 163540 train: 0.08312129974365234 elapsed, loss: 1.6628744e-06\n",
      "step: 163550 train: 0.07417678833007812 elapsed, loss: 2.5937275e-06\n",
      "step: 163560 train: 0.08006954193115234 elapsed, loss: 2.2631098e-06\n",
      "step: 163570 train: 0.08716654777526855 elapsed, loss: 1.4570527e-06\n",
      "step: 163580 train: 0.08871746063232422 elapsed, loss: 1.2991939e-06\n",
      "step: 163590 train: 0.07907581329345703 elapsed, loss: 1.8849944e-06\n",
      "step: 163600 train: 0.07929301261901855 elapsed, loss: 2.1345877e-06\n",
      "step: 163610 train: 0.07736086845397949 elapsed, loss: 1.748556e-06\n",
      "step: 163620 train: 0.0833132266998291 elapsed, loss: 1.9045524e-06\n",
      "step: 163630 train: 0.08116769790649414 elapsed, loss: 1.5464598e-06\n",
      "step: 163640 train: 0.08478140830993652 elapsed, loss: 1.9595004e-06\n",
      "step: 163650 train: 0.08559370040893555 elapsed, loss: 1.5464598e-06\n",
      "step: 163660 train: 0.08287358283996582 elapsed, loss: 1.3420347e-06\n",
      "step: 163670 train: 0.08113956451416016 elapsed, loss: 1.8938424e-06\n",
      "step: 163680 train: 0.08448672294616699 elapsed, loss: 1.488252e-06\n",
      "step: 163690 train: 0.07976031303405762 elapsed, loss: 1.8202675e-06\n",
      "step: 163700 train: 0.07798981666564941 elapsed, loss: 2.1802232e-06\n",
      "step: 163710 train: 0.08618879318237305 elapsed, loss: 1.3955856e-06\n",
      "step: 163720 train: 0.07976222038269043 elapsed, loss: 1.7094408e-06\n",
      "step: 163730 train: 0.08197760581970215 elapsed, loss: 1.9632257e-06\n",
      "step: 163740 train: 0.08087992668151855 elapsed, loss: 1.6214309e-06\n",
      "step: 163750 train: 0.08859944343566895 elapsed, loss: 1.8910482e-06\n",
      "step: 163760 train: 0.08114504814147949 elapsed, loss: 2.0903506e-06\n",
      "step: 163770 train: 0.07402849197387695 elapsed, loss: 2.0721902e-06\n",
      "step: 163780 train: 0.07461953163146973 elapsed, loss: 2.4978012e-06\n",
      "step: 163790 train: 0.08281660079956055 elapsed, loss: 1.7243419e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 163800 train: 0.0883951187133789 elapsed, loss: 2.474957e-06\n",
      "step: 163810 train: 0.08208155632019043 elapsed, loss: 2.114565e-06\n",
      "step: 163820 train: 0.07964181900024414 elapsed, loss: 1.9790582e-06\n",
      "step: 163830 train: 0.08155941963195801 elapsed, loss: 2.0191044e-06\n",
      "step: 163840 train: 0.07841157913208008 elapsed, loss: 2.0870912e-06\n",
      "step: 163850 train: 0.08080315589904785 elapsed, loss: 2.005135e-06\n",
      "step: 163860 train: 0.07897496223449707 elapsed, loss: 2.057289e-06\n",
      "step: 163870 train: 0.07949090003967285 elapsed, loss: 1.5781246e-06\n",
      "step: 163880 train: 0.07756280899047852 elapsed, loss: 2.2794084e-06\n",
      "step: 163890 train: 0.07883715629577637 elapsed, loss: 1.9818522e-06\n",
      "step: 163900 train: 0.08769559860229492 elapsed, loss: 1.4081586e-06\n",
      "step: 163910 train: 0.08916401863098145 elapsed, loss: 2.3287687e-06\n",
      "step: 163920 train: 0.09201741218566895 elapsed, loss: 1.9394768e-06\n",
      "step: 163930 train: 0.08272600173950195 elapsed, loss: 1.6712568e-06\n",
      "step: 163940 train: 0.07655572891235352 elapsed, loss: 2.3297e-06\n",
      "step: 163950 train: 0.08074760437011719 elapsed, loss: 2.0363327e-06\n",
      "step: 163960 train: 0.08758187294006348 elapsed, loss: 5.9906756e-06\n",
      "step: 163970 train: 0.08279180526733398 elapsed, loss: 2.1513517e-06\n",
      "step: 163980 train: 0.07408285140991211 elapsed, loss: 3.6008948e-06\n",
      "step: 163990 train: 0.07851314544677734 elapsed, loss: 2.1047856e-06\n",
      "step: 164000 train: 0.08053970336914062 elapsed, loss: 2.4028086e-06\n",
      "step: 164010 train: 0.07944893836975098 elapsed, loss: 2.6891894e-06\n",
      "step: 164020 train: 0.07735919952392578 elapsed, loss: 1.8021067e-06\n",
      "step: 164030 train: 0.08535552024841309 elapsed, loss: 1.6032702e-06\n",
      "step: 164040 train: 0.07679533958435059 elapsed, loss: 2.5774311e-06\n",
      "step: 164050 train: 0.0837259292602539 elapsed, loss: 1.5851094e-06\n",
      "step: 164060 train: 0.08270907402038574 elapsed, loss: 1.936217e-06\n",
      "step: 164070 train: 0.0808568000793457 elapsed, loss: 2.755779e-06\n",
      "step: 164080 train: 0.09075522422790527 elapsed, loss: 1.5036189e-06\n",
      "step: 164090 train: 0.08507537841796875 elapsed, loss: 2.067999e-06\n",
      "step: 164100 train: 0.08417940139770508 elapsed, loss: 1.5250394e-06\n",
      "step: 164110 train: 0.08360934257507324 elapsed, loss: 1.4742819e-06\n",
      "step: 164120 train: 0.08154082298278809 elapsed, loss: 2.2528661e-06\n",
      "step: 164130 train: 0.07715249061584473 elapsed, loss: 2.153215e-06\n",
      "step: 164140 train: 0.08271050453186035 elapsed, loss: 1.7741677e-06\n",
      "step: 164150 train: 0.07590103149414062 elapsed, loss: 1.9045525e-06\n",
      "step: 164160 train: 0.08194589614868164 elapsed, loss: 1.4216628e-06\n",
      "step: 164170 train: 0.08276200294494629 elapsed, loss: 1.8919798e-06\n",
      "step: 164180 train: 0.08536767959594727 elapsed, loss: 2.1629933e-06\n",
      "step: 164190 train: 0.07767224311828613 elapsed, loss: 2.083366e-06\n",
      "step: 164200 train: 0.08999824523925781 elapsed, loss: 1.7350521e-06\n",
      "step: 164210 train: 0.08295917510986328 elapsed, loss: 1.8952392e-06\n",
      "step: 164220 train: 0.08018803596496582 elapsed, loss: 2.97324e-06\n",
      "step: 164230 train: 0.08151459693908691 elapsed, loss: 1.6099631e-05\n",
      "step: 164240 train: 0.07565188407897949 elapsed, loss: 0.10518909\n",
      "step: 164250 train: 0.07589840888977051 elapsed, loss: 5.7250712e-05\n",
      "step: 164260 train: 0.082427978515625 elapsed, loss: 7.655884e-05\n",
      "step: 164270 train: 0.08065032958984375 elapsed, loss: 3.2859767e-05\n",
      "step: 164280 train: 0.08433938026428223 elapsed, loss: 1.9891111e-05\n",
      "step: 164290 train: 0.08127236366271973 elapsed, loss: 1.2851506e-05\n",
      "step: 164300 train: 0.08553910255432129 elapsed, loss: 9.045394e-06\n",
      "step: 164310 train: 0.08304786682128906 elapsed, loss: 7.866339e-06\n",
      "step: 164320 train: 0.08074235916137695 elapsed, loss: 6.68639e-06\n",
      "step: 164330 train: 0.08555436134338379 elapsed, loss: 5.4947714e-06\n",
      "step: 164340 train: 0.08125066757202148 elapsed, loss: 3.6517051e-06\n",
      "step: 164350 train: 0.07562565803527832 elapsed, loss: 3.6735914e-06\n",
      "step: 164360 train: 0.08096885681152344 elapsed, loss: 3.9287725e-06\n",
      "step: 164370 train: 0.07883071899414062 elapsed, loss: 2.7660224e-06\n",
      "step: 164380 train: 0.08458447456359863 elapsed, loss: 1.902689e-06\n",
      "step: 164390 train: 0.08218860626220703 elapsed, loss: 1.855658e-06\n",
      "step: 164400 train: 0.07488822937011719 elapsed, loss: 2.4870926e-06\n",
      "step: 164410 train: 0.07903861999511719 elapsed, loss: 1.6898828e-06\n",
      "step: 164420 train: 0.08142852783203125 elapsed, loss: 1.7918625e-06\n",
      "step: 164430 train: 0.08286142349243164 elapsed, loss: 1.4845255e-06\n",
      "step: 164440 train: 0.07976055145263672 elapsed, loss: 1.6489045e-06\n",
      "step: 164450 train: 0.08174681663513184 elapsed, loss: 1.663806e-06\n",
      "step: 164460 train: 0.09023141860961914 elapsed, loss: 1.3625238e-06\n",
      "step: 164470 train: 0.09240126609802246 elapsed, loss: 1.5036188e-06\n",
      "step: 164480 train: 0.07463240623474121 elapsed, loss: 2.1103742e-06\n",
      "step: 164490 train: 0.08236074447631836 elapsed, loss: 1.5208484e-06\n",
      "step: 164500 train: 0.08924388885498047 elapsed, loss: 1.3099032e-06\n",
      "step: 164510 train: 0.08543825149536133 elapsed, loss: 1.4421518e-06\n",
      "step: 164520 train: 0.09019088745117188 elapsed, loss: 1.8207331e-06\n",
      "step: 164530 train: 0.08052301406860352 elapsed, loss: 1.1809162e-06\n",
      "step: 164540 train: 0.0870819091796875 elapsed, loss: 1.8845286e-06\n",
      "step: 164550 train: 0.08098077774047852 elapsed, loss: 1.392326e-06\n",
      "step: 164560 train: 0.07887721061706543 elapsed, loss: 1.7099065e-06\n",
      "step: 164570 train: 0.0790565013885498 elapsed, loss: 2.1341216e-06\n",
      "step: 164580 train: 0.08493232727050781 elapsed, loss: 1.2968658e-06\n",
      "step: 164590 train: 0.08102798461914062 elapsed, loss: 1.7280672e-06\n",
      "step: 164600 train: 0.08573794364929199 elapsed, loss: 1.7243417e-06\n",
      "step: 164610 train: 0.08822751045227051 elapsed, loss: 2.6365683e-06\n",
      "step: 164620 train: 0.0859982967376709 elapsed, loss: 1.5641548e-06\n",
      "step: 164630 train: 0.08279824256896973 elapsed, loss: 1.7341208e-06\n",
      "step: 164640 train: 0.08259415626525879 elapsed, loss: 2.1750975e-06\n",
      "step: 164650 train: 0.08092665672302246 elapsed, loss: 2.1294663e-06\n",
      "step: 164660 train: 0.07617402076721191 elapsed, loss: 2.2789432e-06\n",
      "step: 164670 train: 0.0778965950012207 elapsed, loss: 1.9026897e-06\n",
      "step: 164680 train: 0.08207988739013672 elapsed, loss: 1.9450647e-06\n",
      "step: 164690 train: 0.08075737953186035 elapsed, loss: 0.0001761122\n",
      "step: 164700 train: 0.0856008529663086 elapsed, loss: 6.0218567e-06\n",
      "step: 164710 train: 0.07698798179626465 elapsed, loss: 3.3113088e-06\n",
      "step: 164720 train: 0.08147883415222168 elapsed, loss: 3.0598533e-06\n",
      "step: 164730 train: 0.07868480682373047 elapsed, loss: 2.140642e-06\n",
      "step: 164740 train: 0.08225464820861816 elapsed, loss: 2.6710277e-06\n",
      "step: 164750 train: 0.07586264610290527 elapsed, loss: 2.529002e-06\n",
      "step: 164760 train: 0.08034777641296387 elapsed, loss: 2.39396e-06\n",
      "step: 164770 train: 0.0859227180480957 elapsed, loss: 2.1154965e-06\n",
      "step: 164780 train: 0.07996058464050293 elapsed, loss: 1.659615e-06\n",
      "step: 164790 train: 0.0808720588684082 elapsed, loss: 1.5026876e-06\n",
      "step: 164800 train: 0.08387184143066406 elapsed, loss: 1.4742823e-06\n",
      "step: 164810 train: 0.07996630668640137 elapsed, loss: 2.1015255e-06\n",
      "step: 164820 train: 0.08504867553710938 elapsed, loss: 0.00029187067\n",
      "step: 164830 train: 0.08319354057312012 elapsed, loss: 2.8925915e-05\n",
      "step: 164840 train: 0.08281421661376953 elapsed, loss: 1.0320268e-05\n",
      "step: 164850 train: 0.08417534828186035 elapsed, loss: 8.914082e-06\n",
      "step: 164860 train: 0.07824921607971191 elapsed, loss: 9.853287e-06\n",
      "step: 164870 train: 0.07910037040710449 elapsed, loss: 7.0747283e-06\n",
      "step: 164880 train: 0.08551383018493652 elapsed, loss: 4.8712564e-06\n",
      "step: 164890 train: 0.07955694198608398 elapsed, loss: 9.555274e-06\n",
      "step: 164900 train: 0.0831913948059082 elapsed, loss: 4.176968e-06\n",
      "step: 164910 train: 0.08271217346191406 elapsed, loss: 3.1823217e-06\n",
      "step: 164920 train: 0.0777595043182373 elapsed, loss: 3.4524048e-06\n",
      "step: 164930 train: 0.0836331844329834 elapsed, loss: 2.1578712e-06\n",
      "step: 164940 train: 0.08412432670593262 elapsed, loss: 1.8761468e-06\n",
      "step: 164950 train: 0.08093023300170898 elapsed, loss: 2.1224812e-06\n",
      "step: 164960 train: 0.08865952491760254 elapsed, loss: 1.4910459e-06\n",
      "step: 164970 train: 0.08049154281616211 elapsed, loss: 1.6791728e-06\n",
      "step: 164980 train: 0.08483338356018066 elapsed, loss: 1.3639203e-06\n",
      "step: 164990 train: 0.08003854751586914 elapsed, loss: 1.9087433e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 165000 train: 0.07748174667358398 elapsed, loss: 1.4482051e-06\n",
      "step: 165010 train: 0.07908987998962402 elapsed, loss: 1.976264e-06\n",
      "step: 165020 train: 0.07766580581665039 elapsed, loss: 1.3639208e-06\n",
      "step: 165030 train: 0.08385443687438965 elapsed, loss: 1.2647349e-06\n",
      "step: 165040 train: 0.08066153526306152 elapsed, loss: 1.6265521e-06\n",
      "step: 165050 train: 0.08824729919433594 elapsed, loss: 9.913922e-07\n",
      "step: 165060 train: 0.07853174209594727 elapsed, loss: 1.8523986e-06\n",
      "step: 165070 train: 0.08154749870300293 elapsed, loss: 1.4044322e-06\n",
      "step: 165080 train: 0.0774838924407959 elapsed, loss: 1.5697426e-06\n",
      "step: 165090 train: 0.09171342849731445 elapsed, loss: 1.1119982e-06\n",
      "step: 165100 train: 0.08112788200378418 elapsed, loss: 1.4803359e-06\n",
      "step: 165110 train: 0.07894039154052734 elapsed, loss: 1.7080439e-06\n",
      "step: 165120 train: 0.08751058578491211 elapsed, loss: 1.5227109e-06\n",
      "step: 165130 train: 0.08312630653381348 elapsed, loss: 1.4258535e-06\n",
      "step: 165140 train: 0.09049415588378906 elapsed, loss: 1.1020247e-05\n",
      "step: 165150 train: 0.08731961250305176 elapsed, loss: 0.00019258769\n",
      "step: 165160 train: 0.08029365539550781 elapsed, loss: 1.6092865e-05\n",
      "step: 165170 train: 0.07730603218078613 elapsed, loss: 1.139784e-05\n",
      "step: 165180 train: 0.0868070125579834 elapsed, loss: 2.2628308e-05\n",
      "step: 165190 train: 0.08083462715148926 elapsed, loss: 7.372298e-06\n",
      "step: 165200 train: 0.08118176460266113 elapsed, loss: 3.8919807e-06\n",
      "step: 165210 train: 0.08340692520141602 elapsed, loss: 4.3459845e-06\n",
      "step: 165220 train: 0.08238458633422852 elapsed, loss: 2.2659042e-06\n",
      "step: 165230 train: 0.08115720748901367 elapsed, loss: 2.396754e-06\n",
      "step: 165240 train: 0.08255887031555176 elapsed, loss: 1.8239928e-06\n",
      "step: 165250 train: 0.0812983512878418 elapsed, loss: 2.7231783e-06\n",
      "step: 165260 train: 0.07826375961303711 elapsed, loss: 1.5180542e-06\n",
      "step: 165270 train: 0.08234453201293945 elapsed, loss: 1.5371464e-06\n",
      "step: 165280 train: 0.07687258720397949 elapsed, loss: 1.5995449e-06\n",
      "step: 165290 train: 0.07667756080627441 elapsed, loss: 1.6423857e-06\n",
      "step: 165300 train: 0.08591675758361816 elapsed, loss: 1.1725343e-06\n",
      "step: 165310 train: 0.08371543884277344 elapsed, loss: 1.3224769e-06\n",
      "step: 165320 train: 0.08138179779052734 elapsed, loss: 1.4253876e-06\n",
      "step: 165330 train: 0.0773768424987793 elapsed, loss: 1.8808021e-06\n",
      "step: 165340 train: 0.08361697196960449 elapsed, loss: 1.3359811e-06\n",
      "step: 165350 train: 0.08186101913452148 elapsed, loss: 1.2502994e-06\n",
      "step: 165360 train: 0.07706618309020996 elapsed, loss: 1.5599637e-06\n",
      "step: 165370 train: 0.07269072532653809 elapsed, loss: 2.1439018e-06\n",
      "step: 165380 train: 0.09024357795715332 elapsed, loss: 1.2768419e-06\n",
      "step: 165390 train: 0.07716989517211914 elapsed, loss: 1.8984983e-06\n",
      "step: 165400 train: 0.0780179500579834 elapsed, loss: 4.071255e-06\n",
      "step: 165410 train: 0.0868692398071289 elapsed, loss: 1.2610096e-06\n",
      "step: 165420 train: 0.08316230773925781 elapsed, loss: 1.2642694e-06\n",
      "step: 165430 train: 0.08113956451416016 elapsed, loss: 1.8072294e-06\n",
      "step: 165440 train: 0.08504962921142578 elapsed, loss: 2.3618297e-06\n",
      "step: 165450 train: 0.08261251449584961 elapsed, loss: 1.5180543e-06\n",
      "step: 165460 train: 0.08167219161987305 elapsed, loss: 2.0028065e-06\n",
      "step: 165470 train: 0.07715964317321777 elapsed, loss: 1.590697e-06\n",
      "step: 165480 train: 0.08105134963989258 elapsed, loss: 7.731787e-06\n",
      "step: 165490 train: 0.08003854751586914 elapsed, loss: 2.7175658e-05\n",
      "step: 165500 train: 0.08556890487670898 elapsed, loss: 1.2422279e-05\n",
      "step: 165510 train: 0.07894039154052734 elapsed, loss: 9.706158e-06\n",
      "step: 165520 train: 0.07814359664916992 elapsed, loss: 8.761775e-06\n",
      "step: 165530 train: 0.08297920227050781 elapsed, loss: 4.926182e-06\n",
      "step: 165540 train: 0.08492755889892578 elapsed, loss: 5.640997e-06\n",
      "step: 165550 train: 0.08259987831115723 elapsed, loss: 4.6760615e-06\n",
      "step: 165560 train: 0.07522702217102051 elapsed, loss: 3.0966407e-06\n",
      "step: 165570 train: 0.08520865440368652 elapsed, loss: 2.2952383e-06\n",
      "step: 165580 train: 0.08896255493164062 elapsed, loss: 1.7010587e-06\n",
      "step: 165590 train: 0.08153772354125977 elapsed, loss: 2.0186387e-06\n",
      "step: 165600 train: 0.08343219757080078 elapsed, loss: 0.00035387516\n",
      "step: 165610 train: 0.08490514755249023 elapsed, loss: 1.8209697e-05\n",
      "step: 165620 train: 0.08408069610595703 elapsed, loss: 1.2160061e-05\n",
      "step: 165630 train: 0.08393597602844238 elapsed, loss: 9.764187e-06\n",
      "step: 165640 train: 0.08143067359924316 elapsed, loss: 6.344555e-06\n",
      "step: 165650 train: 0.08045268058776855 elapsed, loss: 7.4807763e-06\n",
      "step: 165660 train: 0.08419013023376465 elapsed, loss: 4.9662344e-06\n",
      "step: 165670 train: 0.08030271530151367 elapsed, loss: 3.7820819e-06\n",
      "step: 165680 train: 0.08447599411010742 elapsed, loss: 2.4069982e-06\n",
      "step: 165690 train: 0.07714128494262695 elapsed, loss: 3.3918677e-06\n",
      "step: 165700 train: 0.08217096328735352 elapsed, loss: 2.1727712e-06\n",
      "step: 165710 train: 0.07862234115600586 elapsed, loss: 1.7583337e-06\n",
      "step: 165720 train: 0.07937407493591309 elapsed, loss: 1.7192192e-06\n",
      "step: 165730 train: 0.08858370780944824 elapsed, loss: 1.337378e-06\n",
      "step: 165740 train: 0.08057594299316406 elapsed, loss: 1.7173568e-06\n",
      "step: 165750 train: 0.07984614372253418 elapsed, loss: 1.2596126e-06\n",
      "step: 165760 train: 0.07691645622253418 elapsed, loss: 1.3513479e-06\n",
      "step: 165770 train: 0.07562136650085449 elapsed, loss: 2.2221316e-06\n",
      "step: 165780 train: 0.07951164245605469 elapsed, loss: 1.2395892e-06\n",
      "step: 165790 train: 0.0765676498413086 elapsed, loss: 1.4076927e-06\n",
      "step: 165800 train: 0.08498573303222656 elapsed, loss: 1.0803333e-06\n",
      "step: 165810 train: 0.08364057540893555 elapsed, loss: 9.51345e-07\n",
      "step: 165820 train: 0.07922101020812988 elapsed, loss: 1.2242222e-06\n",
      "step: 165830 train: 0.07987594604492188 elapsed, loss: 1.3760279e-06\n",
      "step: 165840 train: 0.08167386054992676 elapsed, loss: 1.2083901e-06\n",
      "step: 165850 train: 0.08673930168151855 elapsed, loss: 1.266132e-06\n",
      "step: 165860 train: 0.08402013778686523 elapsed, loss: 1.0021024e-06\n",
      "step: 165870 train: 0.08403468132019043 elapsed, loss: 9.583302e-07\n",
      "step: 165880 train: 0.08054661750793457 elapsed, loss: 0.00071333826\n",
      "step: 165890 train: 0.07887530326843262 elapsed, loss: 9.4388066e-05\n",
      "step: 165900 train: 0.08299469947814941 elapsed, loss: 3.8509446e-05\n",
      "step: 165910 train: 0.082366943359375 elapsed, loss: 5.2752104e-05\n",
      "step: 165920 train: 0.08194088935852051 elapsed, loss: 1.4721391e-05\n",
      "step: 165930 train: 0.07762551307678223 elapsed, loss: 1.3000941e-05\n",
      "step: 165940 train: 0.08337140083312988 elapsed, loss: 8.873478e-06\n",
      "step: 165950 train: 0.07902002334594727 elapsed, loss: 5.254492e-06\n",
      "step: 165960 train: 0.07827448844909668 elapsed, loss: 4.4060685e-06\n",
      "step: 165970 train: 0.0853874683380127 elapsed, loss: 3.2777773e-06\n",
      "step: 165980 train: 0.08085799217224121 elapsed, loss: 2.9429707e-06\n",
      "step: 165990 train: 0.0815422534942627 elapsed, loss: 2.8409918e-06\n",
      "step: 166000 train: 0.07733321189880371 elapsed, loss: 2.4181695e-06\n",
      "step: 166010 train: 0.07860159873962402 elapsed, loss: 1.7187538e-06\n",
      "step: 166020 train: 0.07813382148742676 elapsed, loss: 1.6284157e-06\n",
      "step: 166030 train: 0.0844273567199707 elapsed, loss: 1.2335356e-06\n",
      "step: 166040 train: 0.08785533905029297 elapsed, loss: 1.1594952e-06\n",
      "step: 166050 train: 0.0785055160522461 elapsed, loss: 1.3075758e-06\n",
      "step: 166060 train: 0.08310556411743164 elapsed, loss: 9.336503e-07\n",
      "step: 166070 train: 0.08101081848144531 elapsed, loss: 1.3913941e-06\n",
      "step: 166080 train: 0.08871722221374512 elapsed, loss: 9.872012e-07\n",
      "step: 166090 train: 0.07863068580627441 elapsed, loss: 1.4482052e-06\n",
      "step: 166100 train: 0.07566976547241211 elapsed, loss: 1.3625237e-06\n",
      "step: 166110 train: 0.07978034019470215 elapsed, loss: 1.1124641e-06\n",
      "step: 166120 train: 0.08395695686340332 elapsed, loss: 9.3411586e-07\n",
      "step: 166130 train: 0.08244681358337402 elapsed, loss: 1.4961684e-06\n",
      "step: 166140 train: 0.08435511589050293 elapsed, loss: 1.1962829e-06\n",
      "step: 166150 train: 0.08061599731445312 elapsed, loss: 9.0990164e-07\n",
      "step: 166160 train: 0.07502579689025879 elapsed, loss: 1.8379624e-06\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
