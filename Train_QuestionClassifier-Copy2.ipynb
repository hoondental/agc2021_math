{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./log/question_classifier/20210618_010247\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = 'cuda:' + str(device_id)\n",
    "#device = 'cpu'\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "log_dir = os.path.join('./log/question_classifier', time.strftime('%Y%m%d_%H%M%S', time.localtime(time.time())))\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.parse import *\n",
    "from pkg.words import *\n",
    "from problems import *\n",
    "\n",
    "problems = [P1_1_1, P1_1_2, P1_1_3, P1_1_4, P1_1_5, P1_1_6, P1_1_7, P1_1_8, P1_1_9, P1_1_10, P1_1_11, P1_1_12, \n",
    "            P1_2_1, P1_2_2, P1_3_1, P1_4_1, \n",
    "            P2_1_1, P2_2_2, P2_3_1, \n",
    "            P3_1_1, P3_2_1, P3_2_2, P3_3_1, \n",
    "            P4_1_1, P4_2_1, P4_2_2, P4_3_1, \n",
    "            P5_1_1, P5_2_1, P5_3_1,\n",
    "            P6_1_1, P6_3_1, P6_4_1,\n",
    "            P7_1_1, P7_1_2, P7_3_1,\n",
    "            P8_1_1, P8_2_1, P8_3_1, \n",
    "            P9_1_1, P9_2_1, P9_2_2, P9_3_1, P9_3_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dhlee/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from pkg.trainer_question_classifier import Hyper as hp\n",
    "\n",
    "# dataset\n",
    "hp.add_bos = False\n",
    "hp.add_eos = False\n",
    "\n",
    "hp.batch_size = None\n",
    "hp.batch_type = 'normal'\n",
    "hp.ds_batch_size = 256\n",
    "\n",
    "# train\n",
    "hp.num_workers = 2\n",
    "\n",
    "hp.steps_log = 10\n",
    "hp.steps_eval = 50\n",
    "hp.steps_save = 10000\n",
    "\n",
    "hp.weight_decay = 0.000001\n",
    "hp.initial_lr = 0.0001\n",
    "hp.final_lr = 0.00001\n",
    "hp.lr_decay_factor = 0.99\n",
    "hp.lr_patience = 300\n",
    "hp.ema = 0.99\n",
    "hp.grad_norm_max = 10.0\n",
    "\n",
    "hp.adam_alpha = 2e-4\n",
    "hp.adam_betas = (0.5, 0.9)\n",
    "hp.adam_eps = 1e-6\n",
    "\n",
    "hp.vocab_size = 512\n",
    "hp.add_space_token = False\n",
    "\n",
    "hp.num_problems = len(problems)\n",
    "hp.problems = problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816000 2252800 563200\n"
     ]
    }
   ],
   "source": [
    "from pkg.dataset import ProblemDataset, QuestionDataset, read_questions, write_questions\n",
    "\n",
    "dir_question = 'data/question'\n",
    "_prefix = 'question_'\n",
    "_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "path_questions = [os.path.join(dir_question, _prefix + str(i) + '.txt') for i in _ids]\n",
    "metas = [read_questions(path) for path in path_questions]\n",
    "meta = []\n",
    "for m in metas:\n",
    "    meta += m\n",
    "len_train = int(len(meta) * 0.8)\n",
    "meta_train = meta[:len_train]\n",
    "meta_val = meta[len_train:]\n",
    "print(len(meta), len(meta_train), len(meta_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization/prob_512.model\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "from pkg.vocab import Vocab, CharVocab, SPVocab\n",
    "dir_token = 'tokenization'\n",
    "filename = 'prob'\n",
    "filename += '_' + str(hp.vocab_size)\n",
    "if hp.add_space_token:\n",
    "    filename += '_'\n",
    "filename += '.model'\n",
    "path_model = os.path.join(dir_token, filename)\n",
    "print(path_model)\n",
    "vocab = SPVocab(path_model)\n",
    "print(vocab.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644.2300124168396\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dsTrain = QuestionDataset(meta_train, vocab, batch_size=hp.ds_batch_size)\n",
    "dsVal = QuestionDataset(meta_val, vocab, batch_size=hp.ds_batch_size)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.models.model import QuestionClassifier\n",
    "from pkg.models.config import Config\n",
    "from pkg.models.extractor import AverageExtractor, RNNExtractor\n",
    "from pkg.models.encoders_conv import ConvEncoder, HighwayEncoder\n",
    "from pkg.models.embedding import Embed, Regressor\n",
    "\n",
    "cfg = QuestionClassifier.default_config()\n",
    "cfg.text_embed.num_symbols = hp.vocab_size\n",
    "cfg.encoders[0].dropout_rate=0.5\n",
    "cfg.regressor.num_symbols = hp.num_problems\n",
    "cfg.extractor = RNNExtractor.default_config()\n",
    "model = cfg.create_object().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- hp -------------------\n",
      "\n",
      "__module__ pkg.trainer\n",
      "steps_log 10\n",
      "steps_eval 50\n",
      "steps_save 10000\n",
      "save_model_only True\n",
      "max_lr 0.001\n",
      "base_lr 0.001\n",
      "lr_decay_factor 0.99\n",
      "lr_patience 300\n",
      "scheduler_mode triangular2\n",
      "step_size_up 10\n",
      "ema 0.99\n",
      "max_grad_norm 10.0\n",
      "adam_alpha 0.0002\n",
      "adam_betas (0.5, 0.9)\n",
      "adam_eps 1e-06\n",
      "weight_decay 1e-06\n",
      "__dict__ <attribute '__dict__' of 'Hyper' objects>\n",
      "__weakref__ <attribute '__weakref__' of 'Hyper' objects>\n",
      "__doc__ None\n",
      "add_bos False\n",
      "add_eos False\n",
      "batch_size None\n",
      "batch_type normal\n",
      "ds_batch_size 256\n",
      "num_workers 2\n",
      "initial_lr 0.0001\n",
      "final_lr 1e-05\n",
      "grad_norm_max 10.0\n",
      "vocab_size 512\n",
      "add_space_token False\n",
      "num_problems 44\n",
      "problems [<class 'problems.P1_1_1'>, <class 'problems.P1_1_2'>, <class 'problems.P1_1_3'>, <class 'problems.P1_1_4'>, <class 'problems.P1_1_5'>, <class 'problems.P1_1_6'>, <class 'problems.P1_1_7'>, <class 'problems.P1_1_8'>, <class 'problems.P1_1_9'>, <class 'problems.P1_1_10'>, <class 'problems.P1_1_11'>, <class 'problems.P1_1_12'>, <class 'problems.P1_2_1'>, <class 'problems.P1_2_2'>, <class 'problems.P1_3_1'>, <class 'problems.P1_4_1'>, <class 'problems.P2_1_1'>, <class 'problems.P2_2_2'>, <class 'problems.P2_3_1'>, <class 'problems.P3_1_1'>, <class 'problems.P3_2_1'>, <class 'problems.P3_2_2'>, <class 'problems.P3_3_1'>, <class 'problems.P4_1_1'>, <class 'problems.P4_2_1'>, <class 'problems.P4_2_2'>, <class 'problems.P4_3_1'>, <class 'problems.P5_1_1'>, <class 'problems.P5_2_1'>, <class 'problems.P5_3_1'>, <class 'problems.P6_1_1'>, <class 'problems.P6_3_1'>, <class 'problems.P6_4_1'>, <class 'problems.P7_1_1'>, <class 'problems.P7_1_2'>, <class 'problems.P7_3_1'>, <class 'problems.P8_1_1'>, <class 'problems.P8_2_1'>, <class 'problems.P8_3_1'>, <class 'problems.P9_1_1'>, <class 'problems.P9_2_1'>, <class 'problems.P9_2_2'>, <class 'problems.P9_3_1'>, <class 'problems.P9_3_2'>]\n",
      "--------------------- model cfg -------------------\n",
      "\n",
      "************** model 0 ****************\n",
      "\n",
      "cls : <class 'pkg.models.model.QuestionClassifier'>\n",
      "text_embed : \n",
      "    cls : <class 'pkg.models.embedding.Embed'>\n",
      "    num_symbols : 512\n",
      "    embedding_dim : 256\n",
      "    num_upsample : 1\n",
      "    padding_idx : None\n",
      "encoders : \n",
      "    cls : <class 'pkg.models.config.ConfigList'>\n",
      "    0 : \n",
      "        cls : <class 'pkg.models.encoders_conv.HighwayEncoder'>\n",
      "        in_dim : 256\n",
      "        out_dim : 256\n",
      "        kernel_size : 3\n",
      "        stride : 1\n",
      "        num_blocks : 1\n",
      "        num_layers : 5\n",
      "        dilation_base : 1\n",
      "        dilation_power : 1\n",
      "        dropout_rate : 0.5\n",
      "        padding : same\n",
      "        groups : 1\n",
      "        bias : True\n",
      "        normalization : batch\n",
      "extractor : \n",
      "    cls : <class 'pkg.models.extractor.RNNExtractor'>\n",
      "    in_dim : 256\n",
      "    out_dim : 1024\n",
      "    num_layers : 1\n",
      "    rnn : gru\n",
      "    bidirectional : True\n",
      "regressor : \n",
      "    cls : <class 'pkg.models.embedding.Regressor'>\n",
      "    num_symbols : 44\n",
      "    embedding_dim : 1024\n",
      "    hidden_dims : \n",
      "        cls : <class 'pkg.models.config.ConfigList'>\n",
      "    external_embed : None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pkg.trainer_question_classifier import Trainer_QuestionClassifier\n",
    "\n",
    "trainer = Trainer_QuestionClassifier(model, dsTrain, dsVal, hp=hp, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 10 train: 0.12909269332885742 elapsed, loss: 3.7471013\n",
      "step: 20 train: 0.12599515914916992 elapsed, loss: 3.5824456\n",
      "step: 30 train: 0.1262345314025879 elapsed, loss: 3.28863\n",
      "step: 40 train: 0.12466096878051758 elapsed, loss: 2.9132555\n",
      "step: 50 train: 0.12120389938354492 elapsed, loss: 2.3621979\n",
      "step: 60 train: 0.11808276176452637 elapsed, loss: 1.9671142\n",
      "step: 70 train: 0.12321853637695312 elapsed, loss: 1.7017059\n",
      "step: 80 train: 0.12485313415527344 elapsed, loss: 1.4443362\n",
      "step: 90 train: 0.12475323677062988 elapsed, loss: 1.3768853\n",
      "step: 100 train: 0.12387537956237793 elapsed, loss: 1.2476236\n",
      "step: 110 train: 0.11437439918518066 elapsed, loss: 1.1036048\n",
      "step: 120 train: 0.131760835647583 elapsed, loss: 0.83038163\n",
      "step: 130 train: 0.12621355056762695 elapsed, loss: 0.7873377\n",
      "step: 140 train: 0.1207432746887207 elapsed, loss: 0.69633055\n",
      "step: 150 train: 0.11683416366577148 elapsed, loss: 0.4981973\n",
      "step: 160 train: 0.13048505783081055 elapsed, loss: 0.4476271\n",
      "step: 170 train: 0.1249086856842041 elapsed, loss: 0.40278155\n",
      "step: 180 train: 0.12319040298461914 elapsed, loss: 0.33046794\n",
      "step: 190 train: 0.12567687034606934 elapsed, loss: 0.26147705\n",
      "step: 200 train: 0.12518787384033203 elapsed, loss: 0.2192575\n",
      "step: 210 train: 0.12070059776306152 elapsed, loss: 0.18158115\n",
      "step: 220 train: 0.12390255928039551 elapsed, loss: 0.15418527\n",
      "step: 230 train: 0.1302330493927002 elapsed, loss: 0.1510908\n",
      "step: 240 train: 0.12457633018493652 elapsed, loss: 0.14398909\n",
      "step: 250 train: 0.12392592430114746 elapsed, loss: 0.09396246\n",
      "step: 260 train: 0.12353396415710449 elapsed, loss: 0.08394225\n",
      "step: 270 train: 0.12116408348083496 elapsed, loss: 0.08537139\n",
      "step: 280 train: 0.13120222091674805 elapsed, loss: 0.084047206\n",
      "step: 290 train: 0.13808751106262207 elapsed, loss: 0.08800993\n",
      "step: 300 train: 0.12885355949401855 elapsed, loss: 0.05021497\n",
      "step: 310 train: 0.1331946849822998 elapsed, loss: 0.059511542\n",
      "step: 320 train: 0.13271760940551758 elapsed, loss: 0.046671398\n",
      "step: 330 train: 0.14802789688110352 elapsed, loss: 0.03283283\n",
      "step: 340 train: 0.13340449333190918 elapsed, loss: 0.048279896\n",
      "step: 350 train: 0.1254749298095703 elapsed, loss: 0.060643822\n",
      "step: 360 train: 0.12994027137756348 elapsed, loss: 0.048604857\n",
      "step: 370 train: 0.12258672714233398 elapsed, loss: 0.03134363\n",
      "step: 380 train: 0.13218355178833008 elapsed, loss: 0.03479214\n",
      "step: 390 train: 0.12323808670043945 elapsed, loss: 0.020388748\n",
      "step: 400 train: 0.12396693229675293 elapsed, loss: 0.013957518\n",
      "step: 410 train: 0.13762688636779785 elapsed, loss: 0.009844835\n",
      "step: 420 train: 0.1369800567626953 elapsed, loss: 0.0052444595\n",
      "step: 430 train: 0.14654994010925293 elapsed, loss: 0.004970802\n",
      "step: 440 train: 0.14709949493408203 elapsed, loss: 0.0139154475\n",
      "step: 450 train: 0.14338898658752441 elapsed, loss: 0.012348\n",
      "step: 460 train: 0.14426875114440918 elapsed, loss: 0.00740807\n",
      "step: 470 train: 0.14234066009521484 elapsed, loss: 0.0046947976\n",
      "step: 480 train: 0.12997865676879883 elapsed, loss: 0.0040907618\n",
      "step: 490 train: 0.13062620162963867 elapsed, loss: 0.002106098\n",
      "step: 500 train: 0.12929558753967285 elapsed, loss: 0.002153365\n",
      "step: 510 train: 0.12272286415100098 elapsed, loss: 0.0018135832\n",
      "step: 520 train: 0.12650442123413086 elapsed, loss: 0.0016908905\n",
      "step: 530 train: 0.13744425773620605 elapsed, loss: 0.0011352194\n",
      "step: 540 train: 0.1284184455871582 elapsed, loss: 0.0014988121\n",
      "step: 550 train: 0.12512493133544922 elapsed, loss: 0.002211432\n",
      "step: 560 train: 0.12593650817871094 elapsed, loss: 0.00072725746\n",
      "step: 570 train: 0.12967228889465332 elapsed, loss: 0.000984832\n",
      "step: 580 train: 0.12717843055725098 elapsed, loss: 0.0007620291\n",
      "step: 590 train: 0.12995481491088867 elapsed, loss: 0.0005303592\n",
      "step: 600 train: 0.13473796844482422 elapsed, loss: 0.0005222597\n",
      "step: 610 train: 0.14313840866088867 elapsed, loss: 0.0005462572\n",
      "step: 620 train: 0.1235203742980957 elapsed, loss: 0.0004362992\n",
      "step: 630 train: 0.1312255859375 elapsed, loss: 0.0003905119\n",
      "step: 640 train: 0.1293039321899414 elapsed, loss: 0.0005256818\n",
      "step: 650 train: 0.13039660453796387 elapsed, loss: 0.00023847386\n",
      "step: 660 train: 0.12283992767333984 elapsed, loss: 0.00036149946\n",
      "step: 670 train: 0.12911748886108398 elapsed, loss: 0.00030651875\n",
      "step: 680 train: 0.12053060531616211 elapsed, loss: 0.00030937447\n",
      "step: 690 train: 0.12843942642211914 elapsed, loss: 0.00024221577\n",
      "step: 700 train: 0.12479400634765625 elapsed, loss: 0.0003999987\n",
      "step: 710 train: 0.13463425636291504 elapsed, loss: 0.00033988367\n",
      "step: 720 train: 0.1254270076751709 elapsed, loss: 0.00017941676\n",
      "step: 730 train: 0.1437525749206543 elapsed, loss: 0.00015732361\n",
      "step: 740 train: 0.12764883041381836 elapsed, loss: 0.00012038076\n",
      "step: 750 train: 0.12814712524414062 elapsed, loss: 0.00083818566\n",
      "step: 760 train: 0.12833118438720703 elapsed, loss: 0.00037044985\n",
      "step: 770 train: 0.13789725303649902 elapsed, loss: 0.0001426781\n",
      "step: 780 train: 0.12739109992980957 elapsed, loss: 8.5621155e-05\n",
      "step: 790 train: 0.12124204635620117 elapsed, loss: 7.7636396e-05\n",
      "step: 800 train: 0.11688566207885742 elapsed, loss: 9.768002e-05\n",
      "step: 810 train: 0.12738990783691406 elapsed, loss: 7.5173244e-05\n",
      "step: 820 train: 0.11996984481811523 elapsed, loss: 6.748129e-05\n",
      "step: 830 train: 0.12523674964904785 elapsed, loss: 7.3033996e-05\n",
      "step: 840 train: 0.1390221118927002 elapsed, loss: 0.0008679261\n",
      "step: 850 train: 0.12158679962158203 elapsed, loss: 5.048565e-05\n",
      "step: 860 train: 0.12273955345153809 elapsed, loss: 5.2824435e-05\n",
      "step: 870 train: 0.12828373908996582 elapsed, loss: 4.603906e-05\n",
      "step: 880 train: 0.12586569786071777 elapsed, loss: 5.0948765e-05\n",
      "step: 890 train: 0.12877821922302246 elapsed, loss: 3.294195e-05\n",
      "step: 900 train: 0.1337871551513672 elapsed, loss: 3.163617e-05\n",
      "step: 910 train: 0.13294005393981934 elapsed, loss: 0.00011868101\n",
      "step: 920 train: 0.1274278163909912 elapsed, loss: 5.0918185e-05\n",
      "step: 930 train: 0.12403130531311035 elapsed, loss: 2.4608118e-05\n",
      "step: 940 train: 0.12190628051757812 elapsed, loss: 2.6638536e-05\n",
      "step: 950 train: 0.11864566802978516 elapsed, loss: 3.839363e-05\n",
      "step: 960 train: 0.13223600387573242 elapsed, loss: 2.971825e-05\n",
      "step: 970 train: 0.13877511024475098 elapsed, loss: 3.239148e-05\n",
      "step: 980 train: 0.13199758529663086 elapsed, loss: 0.039773304\n",
      "step: 990 train: 0.12410259246826172 elapsed, loss: 5.7528756e-05\n",
      "step: 1000 train: 0.12470293045043945 elapsed, loss: 4.6217076e-05\n",
      "step: 1010 train: 0.12863421440124512 elapsed, loss: 2.8026727e-05\n",
      "step: 1020 train: 0.1268601417541504 elapsed, loss: 2.8611756e-05\n",
      "step: 1030 train: 0.13873791694641113 elapsed, loss: 2.5224486e-05\n",
      "step: 1040 train: 0.1259756088256836 elapsed, loss: 2.304733e-05\n",
      "step: 1050 train: 0.1381852626800537 elapsed, loss: 1.7663679e-05\n",
      "step: 1060 train: 0.1330552101135254 elapsed, loss: 1.4741566e-05\n",
      "step: 1070 train: 0.13248085975646973 elapsed, loss: 1.4694548e-05\n",
      "step: 1080 train: 0.1233980655670166 elapsed, loss: 2.257125e-05\n",
      "step: 1090 train: 0.11795687675476074 elapsed, loss: 2.8098893e-05\n",
      "step: 1100 train: 0.13627123832702637 elapsed, loss: 1.3769325e-05\n",
      "step: 1110 train: 0.13381743431091309 elapsed, loss: 1.5693544e-05\n",
      "step: 1120 train: 0.1303560733795166 elapsed, loss: 1.4233035e-05\n",
      "step: 1130 train: 0.11774301528930664 elapsed, loss: 3.0935662e-05\n",
      "step: 1140 train: 0.12804293632507324 elapsed, loss: 1.3098989e-05\n",
      "step: 1150 train: 0.13896679878234863 elapsed, loss: 5.050358e-05\n",
      "step: 1160 train: 0.12537765502929688 elapsed, loss: 2.4161243e-05\n",
      "step: 1170 train: 0.12581634521484375 elapsed, loss: 1.6127584e-05\n",
      "step: 1180 train: 0.12546515464782715 elapsed, loss: 1.9679075e-05\n",
      "step: 1190 train: 0.12844538688659668 elapsed, loss: 1.3188504e-05\n",
      "step: 1200 train: 0.11864399909973145 elapsed, loss: 1.236859e-05\n",
      "step: 1210 train: 0.13272786140441895 elapsed, loss: 1.0560385e-05\n",
      "step: 1220 train: 0.12477350234985352 elapsed, loss: 1.0627603e-05\n",
      "step: 1230 train: 0.12073254585266113 elapsed, loss: 9.284592e-06\n",
      "step: 1240 train: 0.12932467460632324 elapsed, loss: 1.6202215e-05\n",
      "step: 1250 train: 0.13602304458618164 elapsed, loss: 1.04059545e-05\n",
      "step: 1260 train: 0.13770389556884766 elapsed, loss: 8.970318e-06\n",
      "step: 1270 train: 0.12383174896240234 elapsed, loss: 2.6683963e-05\n",
      "step: 1280 train: 0.12420082092285156 elapsed, loss: 5.7464964e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1290 train: 0.12659263610839844 elapsed, loss: 9.451736e-06\n",
      "step: 1300 train: 0.12119126319885254 elapsed, loss: 1.2407685e-05\n",
      "step: 1310 train: 0.12518668174743652 elapsed, loss: 8.565613e-06\n",
      "step: 1320 train: 0.1290111541748047 elapsed, loss: 8.79762e-06\n",
      "step: 1330 train: 0.1235804557800293 elapsed, loss: 1.0195535e-05\n",
      "step: 1340 train: 0.12596654891967773 elapsed, loss: 9.072186e-06\n",
      "step: 1350 train: 0.1252121925354004 elapsed, loss: 6.5056624e-06\n",
      "step: 1360 train: 0.12937331199645996 elapsed, loss: 1.05137005e-05\n",
      "step: 1370 train: 0.14623188972473145 elapsed, loss: 5.5185124e-06\n",
      "step: 1380 train: 0.13597321510314941 elapsed, loss: 5.479842e-06\n",
      "step: 1390 train: 0.13138175010681152 elapsed, loss: 5.1855695e-06\n",
      "step: 1400 train: 0.14192962646484375 elapsed, loss: 9.2358705e-06\n",
      "step: 1410 train: 0.1336688995361328 elapsed, loss: 5.7019197e-06\n",
      "step: 1420 train: 0.13150572776794434 elapsed, loss: 4.671831e-05\n",
      "step: 1430 train: 0.1507399082183838 elapsed, loss: 7.2556313e-06\n",
      "step: 1440 train: 0.14441442489624023 elapsed, loss: 9.3495055e-06\n",
      "step: 1450 train: 0.1288917064666748 elapsed, loss: 5.7811358e-06\n",
      "step: 1460 train: 0.12459373474121094 elapsed, loss: 4.7827766e-06\n",
      "step: 1470 train: 0.12582182884216309 elapsed, loss: 6.766343e-06\n",
      "step: 1480 train: 0.12524819374084473 elapsed, loss: 1.7278038e-05\n",
      "step: 1490 train: 0.12713122367858887 elapsed, loss: 5.5920323e-06\n",
      "step: 1500 train: 0.1357414722442627 elapsed, loss: 8.154499e-06\n",
      "step: 1510 train: 0.12421083450317383 elapsed, loss: 9.3067065e-06\n",
      "step: 1520 train: 0.1337432861328125 elapsed, loss: 4.477752e-06\n",
      "step: 1530 train: 0.11641669273376465 elapsed, loss: 5.661894e-06\n",
      "step: 1540 train: 0.11716222763061523 elapsed, loss: 7.201953e-06\n",
      "step: 1550 train: 0.1306617259979248 elapsed, loss: 4.298486e-06\n",
      "step: 1560 train: 0.13985490798950195 elapsed, loss: 3.4244545e-06\n",
      "step: 1570 train: 0.14025115966796875 elapsed, loss: 7.2623175e-06\n",
      "step: 1580 train: 0.11719584465026855 elapsed, loss: 4.843763e-06\n",
      "step: 1590 train: 0.12309527397155762 elapsed, loss: 7.688153e-06\n",
      "step: 1600 train: 0.11809849739074707 elapsed, loss: 4.6202604e-06\n",
      "step: 1610 train: 0.12462306022644043 elapsed, loss: 4.33528e-06\n",
      "step: 1620 train: 0.11995553970336914 elapsed, loss: 4.823751e-06\n",
      "step: 1630 train: 0.13879013061523438 elapsed, loss: 6.31093e-06\n",
      "step: 1640 train: 0.13013815879821777 elapsed, loss: 3.58044e-06\n",
      "step: 1650 train: 0.12493515014648438 elapsed, loss: 7.857048e-06\n",
      "step: 1660 train: 0.1337888240814209 elapsed, loss: 4.2058214e-06\n",
      "step: 1670 train: 0.13071417808532715 elapsed, loss: 4.8344486e-06\n",
      "step: 1680 train: 0.12324929237365723 elapsed, loss: 5.065398e-06\n",
      "step: 1690 train: 0.12707853317260742 elapsed, loss: 4.347391e-06\n",
      "step: 1700 train: 0.11771607398986816 elapsed, loss: 4.658874e-06\n",
      "step: 1710 train: 0.12966299057006836 elapsed, loss: 3.5455214e-06\n",
      "step: 1720 train: 0.1280207633972168 elapsed, loss: 6.0495513e-06\n",
      "step: 1730 train: 0.1290295124053955 elapsed, loss: 5.011329e-06\n",
      "step: 1740 train: 0.13403558731079102 elapsed, loss: 4.222123e-06\n",
      "step: 1750 train: 0.1270768642425537 elapsed, loss: 3.326671e-06\n",
      "step: 1760 train: 0.12378263473510742 elapsed, loss: 4.0703235e-06\n",
      "step: 1770 train: 0.12630891799926758 elapsed, loss: 3.6507558e-06\n",
      "step: 1780 train: 0.12380647659301758 elapsed, loss: 4.1229314e-06\n",
      "step: 1790 train: 0.12421464920043945 elapsed, loss: 4.230041e-06\n",
      "step: 1800 train: 0.14210820198059082 elapsed, loss: 7.535225e-06\n",
      "step: 1810 train: 0.12027764320373535 elapsed, loss: 4.6756672e-06\n",
      "step: 1820 train: 0.1259143352508545 elapsed, loss: 3.655892e-06\n",
      "step: 1830 train: 0.12499332427978516 elapsed, loss: 4.51535e-06\n",
      "step: 1840 train: 0.13830828666687012 elapsed, loss: 4.319845e-06\n",
      "step: 1850 train: 0.13480663299560547 elapsed, loss: 5.9954946e-06\n",
      "step: 1860 train: 0.13438010215759277 elapsed, loss: 3.1320255e-06\n",
      "step: 1870 train: 0.13104009628295898 elapsed, loss: 6.2752933e-06\n",
      "step: 1880 train: 0.125962495803833 elapsed, loss: 4.922385e-06\n",
      "step: 1890 train: 0.13806509971618652 elapsed, loss: 4.306387e-06\n",
      "step: 1900 train: 0.12390875816345215 elapsed, loss: 4.6188543e-06\n",
      "step: 1910 train: 0.12066149711608887 elapsed, loss: 3.4295822e-06\n",
      "step: 1920 train: 0.13592314720153809 elapsed, loss: 3.2861585e-06\n",
      "step: 1930 train: 0.12339401245117188 elapsed, loss: 3.9497104e-06\n",
      "step: 1940 train: 0.13593077659606934 elapsed, loss: 3.4198006e-06\n",
      "step: 1950 train: 0.12149524688720703 elapsed, loss: 4.954063e-06\n",
      "step: 1960 train: 0.12324857711791992 elapsed, loss: 3.5171179e-06\n",
      "step: 1970 train: 0.12374377250671387 elapsed, loss: 3.5916191e-06\n",
      "step: 1980 train: 0.13122272491455078 elapsed, loss: 4.004627e-06\n",
      "step: 1990 train: 0.1327652931213379 elapsed, loss: 3.0826668e-06\n",
      "step: 2000 train: 0.12193989753723145 elapsed, loss: 4.4050917e-06\n",
      "step: 2010 train: 0.12663745880126953 elapsed, loss: 3.572071e-06\n",
      "step: 2020 train: 0.13409876823425293 elapsed, loss: 3.4216655e-06\n",
      "step: 2030 train: 0.12560606002807617 elapsed, loss: 4.2323663e-06\n",
      "step: 2040 train: 0.12370586395263672 elapsed, loss: 3.8155945e-06\n",
      "step: 2050 train: 0.14339542388916016 elapsed, loss: 2.868931e-06\n",
      "step: 2060 train: 0.1375131607055664 elapsed, loss: 4.110355e-06\n",
      "step: 2070 train: 0.12418699264526367 elapsed, loss: 4.405558e-06\n",
      "step: 2080 train: 0.12789511680603027 elapsed, loss: 4.115923e-06\n",
      "step: 2090 train: 0.12762022018432617 elapsed, loss: 3.909664e-06\n",
      "step: 2100 train: 0.12942028045654297 elapsed, loss: 1.2787303e-05\n",
      "step: 2110 train: 0.13569402694702148 elapsed, loss: 3.945038e-06\n",
      "step: 2120 train: 0.1275787353515625 elapsed, loss: 3.6367906e-06\n",
      "step: 2130 train: 0.12166047096252441 elapsed, loss: 3.9106044e-06\n",
      "step: 2140 train: 0.14841437339782715 elapsed, loss: 4.3841665e-06\n",
      "step: 2150 train: 0.1390388011932373 elapsed, loss: 4.1382546e-06\n",
      "step: 2160 train: 0.13181185722351074 elapsed, loss: 3.6205004e-06\n",
      "step: 2170 train: 0.12459373474121094 elapsed, loss: 4.9201294e-06\n",
      "step: 2180 train: 0.12751984596252441 elapsed, loss: 3.3895335e-06\n",
      "step: 2190 train: 0.13031983375549316 elapsed, loss: 4.958977e-05\n",
      "step: 2200 train: 0.12145471572875977 elapsed, loss: 4.1695066e-06\n",
      "step: 2210 train: 0.12819981575012207 elapsed, loss: 3.566483e-06\n",
      "step: 2220 train: 0.12989234924316406 elapsed, loss: 3.9981387e-06\n",
      "step: 2230 train: 0.12819194793701172 elapsed, loss: 3.4188397e-06\n",
      "step: 2240 train: 0.1433863639831543 elapsed, loss: 3.6796366e-06\n",
      "step: 2250 train: 0.13820219039916992 elapsed, loss: 4.0613377e-06\n",
      "step: 2260 train: 0.1275773048400879 elapsed, loss: 3.7909197e-06\n",
      "step: 2270 train: 0.12841296195983887 elapsed, loss: 4.2323663e-06\n",
      "step: 2280 train: 0.12421107292175293 elapsed, loss: 4.6402192e-06\n",
      "step: 2290 train: 0.1307535171508789 elapsed, loss: 4.0139817e-06\n",
      "step: 2300 train: 0.13080954551696777 elapsed, loss: 6.671958e-06\n",
      "step: 2310 train: 0.1283092498779297 elapsed, loss: 1.6570744e-05\n",
      "step: 2320 train: 0.1276710033416748 elapsed, loss: 4.8689208e-06\n",
      "step: 2330 train: 0.1264667510986328 elapsed, loss: 4.8334987e-06\n",
      "step: 2340 train: 0.1295318603515625 elapsed, loss: 3.915259e-06\n",
      "step: 2350 train: 0.12319183349609375 elapsed, loss: 4.366003e-06\n",
      "step: 2360 train: 0.11968231201171875 elapsed, loss: 4.117822e-06\n",
      "step: 2370 train: 0.13360905647277832 elapsed, loss: 0.000110089175\n",
      "step: 2380 train: 0.1282970905303955 elapsed, loss: 4.986242e-06\n",
      "step: 2390 train: 0.13808941841125488 elapsed, loss: 5.3010494e-06\n",
      "step: 2400 train: 0.14939069747924805 elapsed, loss: 3.861245e-06\n",
      "step: 2410 train: 0.13537073135375977 elapsed, loss: 4.4451617e-06\n",
      "step: 2420 train: 0.12481546401977539 elapsed, loss: 4.6584482e-06\n",
      "step: 2430 train: 0.13454866409301758 elapsed, loss: 3.9213096e-06\n",
      "step: 2440 train: 0.12626385688781738 elapsed, loss: 1.6363878e-05\n",
      "step: 2450 train: 0.12383699417114258 elapsed, loss: 5.25027e-06\n",
      "step: 2460 train: 0.1238250732421875 elapsed, loss: 5.3378153e-06\n",
      "step: 2470 train: 0.1317148208618164 elapsed, loss: 3.4109553e-06\n",
      "step: 2480 train: 0.1263580322265625 elapsed, loss: 4.2514566e-06\n",
      "step: 2490 train: 0.13301658630371094 elapsed, loss: 3.4705536e-06\n",
      "step: 2500 train: 0.14275813102722168 elapsed, loss: 5.0056437e-06\n",
      "step: 2510 train: 0.13686323165893555 elapsed, loss: 3.0519302e-06\n",
      "step: 2520 train: 0.12244892120361328 elapsed, loss: 3.7662473e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2530 train: 0.13309884071350098 elapsed, loss: 3.723344e-06\n",
      "step: 2540 train: 0.12346601486206055 elapsed, loss: 3.8179155e-06\n",
      "step: 2550 train: 0.12983322143554688 elapsed, loss: 4.94462e-06\n",
      "step: 2560 train: 0.12224102020263672 elapsed, loss: 3.7709033e-06\n",
      "step: 2570 train: 0.11636543273925781 elapsed, loss: 4.108033e-06\n",
      "step: 2580 train: 0.1274402141571045 elapsed, loss: 4.335217e-06\n",
      "step: 2590 train: 0.11522626876831055 elapsed, loss: 3.936618e-06\n",
      "step: 2600 train: 0.12332844734191895 elapsed, loss: 4.4562594e-06\n",
      "step: 2610 train: 0.12139129638671875 elapsed, loss: 4.8386055e-06\n",
      "step: 2620 train: 0.1410045623779297 elapsed, loss: 2.8265526e-06\n",
      "step: 2630 train: 0.13256525993347168 elapsed, loss: 3.429573e-06\n",
      "step: 2640 train: 0.1389310359954834 elapsed, loss: 3.6991007e-06\n",
      "step: 2650 train: 0.13597989082336426 elapsed, loss: 3.0887163e-06\n",
      "step: 2660 train: 0.12159919738769531 elapsed, loss: 3.616776e-06\n",
      "step: 2670 train: 0.12690091133117676 elapsed, loss: 4.637366e-06\n",
      "step: 2680 train: 0.13047003746032715 elapsed, loss: 3.3764925e-06\n",
      "step: 2690 train: 0.13954925537109375 elapsed, loss: 3.1953514e-06\n",
      "step: 2700 train: 0.11977624893188477 elapsed, loss: 4.5182337e-06\n",
      "step: 2710 train: 0.13447785377502441 elapsed, loss: 2.9951204e-06\n",
      "step: 2720 train: 0.12795662879943848 elapsed, loss: 3.4314353e-06\n",
      "step: 2730 train: 0.12306857109069824 elapsed, loss: 3.7736966e-06\n",
      "step: 2740 train: 0.12659335136413574 elapsed, loss: 3.1827803e-06\n",
      "step: 2750 train: 0.11754989624023438 elapsed, loss: 3.637257e-06\n",
      "step: 2760 train: 0.1228628158569336 elapsed, loss: 4.2793777e-06\n",
      "step: 2770 train: 0.14147233963012695 elapsed, loss: 3.9822535e-06\n",
      "step: 2780 train: 0.14186739921569824 elapsed, loss: 2.905715e-06\n",
      "step: 2790 train: 0.13843774795532227 elapsed, loss: 3.3070958e-06\n",
      "step: 2800 train: 0.13548707962036133 elapsed, loss: 3.1674117e-06\n",
      "step: 2810 train: 0.13259410858154297 elapsed, loss: 2.7147976e-06\n",
      "step: 2820 train: 0.13934588432312012 elapsed, loss: 3.655853e-06\n",
      "step: 2830 train: 0.12583684921264648 elapsed, loss: 4.8334314e-06\n",
      "step: 2840 train: 0.1199178695678711 elapsed, loss: 5.3088397e-06\n",
      "step: 2850 train: 0.11829423904418945 elapsed, loss: 4.6043956e-06\n",
      "step: 2860 train: 0.12213945388793945 elapsed, loss: 3.8929e-06\n",
      "step: 2870 train: 0.13175702095031738 elapsed, loss: 2.7855767e-06\n",
      "step: 2880 train: 0.13170385360717773 elapsed, loss: 3.4901077e-06\n",
      "step: 2890 train: 0.11980843544006348 elapsed, loss: 5.989242e-06\n",
      "step: 2900 train: 0.12472772598266602 elapsed, loss: 3.5450662e-06\n",
      "step: 2910 train: 0.12716317176818848 elapsed, loss: 3.6614597e-06\n",
      "step: 2920 train: 0.1364762783050537 elapsed, loss: 3.680561e-06\n",
      "step: 2930 train: 0.12777352333068848 elapsed, loss: 5.6836666e-06\n",
      "step: 2940 train: 0.13176202774047852 elapsed, loss: 4.698436e-06\n",
      "step: 2950 train: 0.12226271629333496 elapsed, loss: 6.157092e-06\n",
      "step: 2960 train: 0.13005971908569336 elapsed, loss: 4.1289213e-06\n",
      "step: 2970 train: 0.13060998916625977 elapsed, loss: 3.86171e-06\n",
      "step: 2980 train: 0.12496733665466309 elapsed, loss: 4.149003e-06\n",
      "step: 2990 train: 0.12294721603393555 elapsed, loss: 3.739703e-06\n",
      "step: 3000 train: 0.13406610488891602 elapsed, loss: 4.1811268e-06\n",
      "step: 3010 train: 0.1365814208984375 elapsed, loss: 3.7303944e-06\n",
      "step: 3020 train: 0.13782000541687012 elapsed, loss: 2.7790572e-06\n",
      "step: 3030 train: 0.13317084312438965 elapsed, loss: 4.964184e-06\n",
      "step: 3040 train: 0.12431454658508301 elapsed, loss: 3.2945404e-06\n",
      "step: 3050 train: 0.1207578182220459 elapsed, loss: 4.060481e-06\n",
      "step: 3060 train: 0.11973690986633301 elapsed, loss: 4.466579e-06\n",
      "step: 3070 train: 0.13051342964172363 elapsed, loss: 4.8850734e-06\n",
      "step: 3080 train: 0.1360025405883789 elapsed, loss: 6.423671e-06\n",
      "step: 3090 train: 0.13640594482421875 elapsed, loss: 5.148604e-06\n",
      "step: 3100 train: 0.13253402709960938 elapsed, loss: 3.4444824e-06\n",
      "step: 3110 train: 0.14011502265930176 elapsed, loss: 2.956472e-06\n",
      "step: 3120 train: 0.13574981689453125 elapsed, loss: 3.4351644e-06\n",
      "step: 3130 train: 0.14108896255493164 elapsed, loss: 2.6961723e-06\n",
      "step: 3140 train: 0.13032174110412598 elapsed, loss: 4.2193215e-06\n",
      "step: 3150 train: 0.12775135040283203 elapsed, loss: 5.0381336e-06\n",
      "step: 3160 train: 0.12050724029541016 elapsed, loss: 4.076352e-06\n",
      "step: 3170 train: 0.1287534236907959 elapsed, loss: 4.0735777e-06\n",
      "step: 3180 train: 0.133378267288208 elapsed, loss: 3.5916191e-06\n",
      "step: 3190 train: 0.13808751106262207 elapsed, loss: 2.9127004e-06\n",
      "step: 3200 train: 0.12411856651306152 elapsed, loss: 4.6654177e-06\n",
      "step: 3210 train: 0.12755489349365234 elapsed, loss: 5.1673787e-06\n",
      "step: 3220 train: 0.13761377334594727 elapsed, loss: 3.8593844e-06\n",
      "step: 3230 train: 0.13268136978149414 elapsed, loss: 3.750417e-06\n",
      "step: 3240 train: 0.12677693367004395 elapsed, loss: 5.0033814e-06\n",
      "step: 3250 train: 0.12547612190246582 elapsed, loss: 4.782752e-06\n",
      "step: 3260 train: 0.13212990760803223 elapsed, loss: 3.6661374e-06\n",
      "step: 3270 train: 0.13040804862976074 elapsed, loss: 1.0670795e-05\n",
      "step: 3280 train: 0.11690473556518555 elapsed, loss: 4.278938e-06\n",
      "step: 3290 train: 0.1436152458190918 elapsed, loss: 3.317355e-06\n",
      "step: 3300 train: 0.14648652076721191 elapsed, loss: 3.32899e-06\n",
      "step: 3310 train: 0.12850117683410645 elapsed, loss: 3.3192214e-06\n",
      "step: 3320 train: 0.12406325340270996 elapsed, loss: 3.5031521e-06\n",
      "step: 3330 train: 0.1377577781677246 elapsed, loss: 3.71363e-06\n",
      "step: 3340 train: 0.12492561340332031 elapsed, loss: 3.881268e-06\n",
      "step: 3350 train: 0.1434192657470703 elapsed, loss: 3.4067525e-06\n",
      "step: 3360 train: 0.1419849395751953 elapsed, loss: 2.9508828e-06\n",
      "step: 3370 train: 0.1296987533569336 elapsed, loss: 6.455911e-06\n",
      "step: 3380 train: 0.12087631225585938 elapsed, loss: 6.8739464e-06\n",
      "step: 3390 train: 0.12206006050109863 elapsed, loss: 3.4747518e-06\n",
      "step: 3400 train: 0.12154555320739746 elapsed, loss: 5.154294e-06\n",
      "step: 3410 train: 0.12402486801147461 elapsed, loss: 3.7238733e-06\n",
      "step: 3420 train: 0.1254105567932129 elapsed, loss: 7.1610416e-06\n",
      "step: 3430 train: 0.12319064140319824 elapsed, loss: 4.2016154e-06\n",
      "step: 3440 train: 0.1276094913482666 elapsed, loss: 3.2163043e-06\n",
      "step: 3450 train: 0.13074254989624023 elapsed, loss: 2.7478595e-06\n",
      "step: 3460 train: 0.1285552978515625 elapsed, loss: 3.3820766e-06\n",
      "step: 3470 train: 0.1281747817993164 elapsed, loss: 3.2512282e-06\n",
      "step: 3480 train: 0.12035727500915527 elapsed, loss: 3.3764973e-06\n",
      "step: 3490 train: 0.13318514823913574 elapsed, loss: 2.996052e-06\n",
      "step: 3500 train: 0.13640451431274414 elapsed, loss: 3.5702135e-06\n",
      "step: 3510 train: 0.15118789672851562 elapsed, loss: 2.6211994e-06\n",
      "step: 3520 train: 0.120574951171875 elapsed, loss: 5.579328e-06\n",
      "step: 3530 train: 0.13238096237182617 elapsed, loss: 3.2856915e-06\n",
      "step: 3540 train: 0.12044620513916016 elapsed, loss: 4.0488985e-06\n",
      "step: 3550 train: 0.13074111938476562 elapsed, loss: 3.024459e-06\n",
      "step: 3560 train: 0.12552118301391602 elapsed, loss: 1.794389e-05\n",
      "step: 3570 train: 0.13094305992126465 elapsed, loss: 3.321069e-06\n",
      "step: 3580 train: 0.12905502319335938 elapsed, loss: 5.1189563e-06\n",
      "step: 3590 train: 0.1374964714050293 elapsed, loss: 4.232703e-06\n",
      "step: 3600 train: 0.13280272483825684 elapsed, loss: 3.864041e-06\n",
      "step: 3610 train: 0.128554105758667 elapsed, loss: 2.9783573e-06\n",
      "step: 3620 train: 0.11732006072998047 elapsed, loss: 5.2120304e-06\n",
      "step: 3630 train: 0.1341691017150879 elapsed, loss: 3.389999e-06\n",
      "step: 3640 train: 0.13096046447753906 elapsed, loss: 4.525711e-06\n",
      "step: 3650 train: 0.12797307968139648 elapsed, loss: 2.8568215e-06\n",
      "step: 3660 train: 0.14114975929260254 elapsed, loss: 3.2223586e-06\n",
      "step: 3670 train: 0.1255645751953125 elapsed, loss: 4.5974307e-06\n",
      "step: 3680 train: 0.11984777450561523 elapsed, loss: 9.4329866e-05\n",
      "step: 3690 train: 0.1316993236541748 elapsed, loss: 5.3804592e-06\n",
      "step: 3700 train: 0.1398775577545166 elapsed, loss: 3.9525157e-06\n",
      "step: 3710 train: 0.13126468658447266 elapsed, loss: 5.7101643e-06\n",
      "step: 3720 train: 0.12355852127075195 elapsed, loss: 4.196969e-06\n",
      "step: 3730 train: 0.14366984367370605 elapsed, loss: 3.1157201e-06\n",
      "step: 3740 train: 0.1260218620300293 elapsed, loss: 5.333368e-06\n",
      "step: 3750 train: 0.12925386428833008 elapsed, loss: 4.2528545e-06\n",
      "step: 3760 train: 0.12498807907104492 elapsed, loss: 1.058198e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3770 train: 0.12884235382080078 elapsed, loss: 6.7197043e-06\n",
      "step: 3780 train: 0.1383051872253418 elapsed, loss: 4.083816e-06\n",
      "step: 3790 train: 0.12803030014038086 elapsed, loss: 6.0088555e-06\n",
      "step: 3800 train: 0.13739514350891113 elapsed, loss: 5.835433e-06\n",
      "step: 3810 train: 0.1329340934753418 elapsed, loss: 3.4915129e-06\n",
      "step: 3820 train: 0.13950324058532715 elapsed, loss: 3.4924465e-06\n",
      "step: 3830 train: 0.12243509292602539 elapsed, loss: 6.3645884e-06\n",
      "step: 3840 train: 0.13419842720031738 elapsed, loss: 4.9875634e-06\n",
      "step: 3850 train: 0.12719488143920898 elapsed, loss: 4.2379124e-06\n",
      "step: 3860 train: 0.13112521171569824 elapsed, loss: 2.9615958e-06\n",
      "step: 3870 train: 0.12811541557312012 elapsed, loss: 3.5948535e-06\n",
      "step: 3880 train: 0.11988997459411621 elapsed, loss: 5.5370083e-06\n",
      "step: 3890 train: 0.13707828521728516 elapsed, loss: 2.6100245e-06\n",
      "step: 3900 train: 0.11949777603149414 elapsed, loss: 4.919192e-06\n",
      "step: 3910 train: 0.13315701484680176 elapsed, loss: 4.862294e-05\n",
      "step: 3920 train: 0.1330547332763672 elapsed, loss: 5.1608076e-06\n",
      "step: 3930 train: 0.124847412109375 elapsed, loss: 4.084289e-06\n",
      "step: 3940 train: 0.12750959396362305 elapsed, loss: 4.5951206e-06\n",
      "step: 3950 train: 0.1304793357849121 elapsed, loss: 3.6447188e-06\n",
      "step: 3960 train: 0.13014745712280273 elapsed, loss: 3.6363285e-06\n",
      "step: 3970 train: 0.12944912910461426 elapsed, loss: 4.023295e-06\n",
      "step: 3980 train: 0.13343548774719238 elapsed, loss: 3.4486711e-06\n",
      "step: 3990 train: 0.1287531852722168 elapsed, loss: 3.987422e-06\n",
      "step: 4000 train: 0.13727641105651855 elapsed, loss: 3.2088576e-06\n",
      "step: 4010 train: 0.13099312782287598 elapsed, loss: 4.7443186e-06\n",
      "step: 4020 train: 0.1302032470703125 elapsed, loss: 2.6891876e-06\n",
      "step: 4030 train: 0.1332545280456543 elapsed, loss: 3.35833e-06\n",
      "step: 4040 train: 0.12614011764526367 elapsed, loss: 3.8416865e-06\n",
      "step: 4050 train: 0.12717604637145996 elapsed, loss: 5.716865e-06\n",
      "step: 4060 train: 0.11990165710449219 elapsed, loss: 5.4853763e-06\n",
      "step: 4070 train: 0.14191651344299316 elapsed, loss: 3.1175832e-06\n",
      "step: 4080 train: 0.12637066841125488 elapsed, loss: 4.2062966e-06\n",
      "step: 4090 train: 0.13336730003356934 elapsed, loss: 4.7767e-06\n",
      "step: 4100 train: 0.12173056602478027 elapsed, loss: 3.7303957e-06\n",
      "step: 4110 train: 0.13264131546020508 elapsed, loss: 2.6090943e-06\n",
      "step: 4120 train: 0.14131426811218262 elapsed, loss: 3.6120873e-06\n",
      "step: 4130 train: 0.1234285831451416 elapsed, loss: 3.6884885e-06\n",
      "step: 4140 train: 0.1424560546875 elapsed, loss: 3.3108377e-06\n",
      "step: 4150 train: 0.12795233726501465 elapsed, loss: 3.72479e-06\n",
      "step: 4160 train: 0.13144755363464355 elapsed, loss: 3.9501274e-06\n",
      "step: 4170 train: 0.12932443618774414 elapsed, loss: 4.3980517e-06\n",
      "step: 4180 train: 0.13486409187316895 elapsed, loss: 3.1459917e-06\n",
      "step: 4190 train: 0.12276268005371094 elapsed, loss: 3.6116558e-06\n",
      "step: 4200 train: 0.13151955604553223 elapsed, loss: 2.4118735e-05\n",
      "step: 4210 train: 0.13351750373840332 elapsed, loss: 3.2582204e-06\n",
      "step: 4220 train: 0.1286768913269043 elapsed, loss: 5.0435247e-06\n",
      "step: 4230 train: 0.1295909881591797 elapsed, loss: 4.4479525e-06\n",
      "step: 4240 train: 0.1327199935913086 elapsed, loss: 4.4637673e-06\n",
      "step: 4250 train: 0.14550042152404785 elapsed, loss: 3.2936105e-06\n",
      "step: 4260 train: 0.1214592456817627 elapsed, loss: 3.819799e-06\n",
      "step: 4270 train: 0.13295769691467285 elapsed, loss: 3.6130136e-06\n",
      "step: 4280 train: 0.13251328468322754 elapsed, loss: 6.3429397e-06\n",
      "step: 4290 train: 0.12489748001098633 elapsed, loss: 3.3117692e-06\n",
      "step: 4300 train: 0.11973452568054199 elapsed, loss: 3.9110673e-06\n",
      "step: 4310 train: 0.12925195693969727 elapsed, loss: 3.1916281e-06\n",
      "step: 4320 train: 0.12769317626953125 elapsed, loss: 3.7276031e-06\n",
      "step: 4330 train: 0.12595653533935547 elapsed, loss: 3.3224778e-06\n",
      "step: 4340 train: 0.12732505798339844 elapsed, loss: 5.9384142e-06\n",
      "step: 4350 train: 0.12763428688049316 elapsed, loss: 3.1017607e-06\n",
      "step: 4360 train: 0.13199162483215332 elapsed, loss: 3.204659e-06\n",
      "step: 4370 train: 0.1398305892944336 elapsed, loss: 3.530151e-06\n",
      "step: 4380 train: 0.141066312789917 elapsed, loss: 8.52837e-06\n",
      "step: 4390 train: 0.14056396484375 elapsed, loss: 7.246813e-06\n",
      "step: 4400 train: 0.1370382308959961 elapsed, loss: 5.40331e-06\n",
      "step: 4410 train: 0.12744617462158203 elapsed, loss: 3.7629916e-06\n",
      "step: 4420 train: 0.13222575187683105 elapsed, loss: 5.931479e-06\n",
      "step: 4430 train: 0.13304924964904785 elapsed, loss: 2.8684656e-06\n",
      "step: 4440 train: 0.12334609031677246 elapsed, loss: 4.2309684e-06\n",
      "step: 4450 train: 0.1398169994354248 elapsed, loss: 3.6149122e-06\n",
      "step: 4460 train: 0.126417875289917 elapsed, loss: 3.681943e-06\n",
      "step: 4470 train: 0.12032842636108398 elapsed, loss: 5.198499e-06\n",
      "step: 4480 train: 0.12436294555664062 elapsed, loss: 3.4491313e-06\n",
      "step: 4490 train: 0.12085819244384766 elapsed, loss: 4.1471453e-06\n",
      "step: 4500 train: 0.11749863624572754 elapsed, loss: 4.7873655e-06\n",
      "step: 4510 train: 0.12897348403930664 elapsed, loss: 2.9695134e-06\n",
      "step: 4520 train: 0.12686824798583984 elapsed, loss: 5.0691324e-06\n",
      "step: 4530 train: 0.11987113952636719 elapsed, loss: 3.8030391e-06\n",
      "step: 4540 train: 0.13005685806274414 elapsed, loss: 3.135288e-06\n",
      "step: 4550 train: 0.12736845016479492 elapsed, loss: 3.7955838e-06\n",
      "step: 4560 train: 0.13047385215759277 elapsed, loss: 4.321767e-06\n",
      "step: 4570 train: 0.13456368446350098 elapsed, loss: 4.4283197e-06\n",
      "step: 4580 train: 0.12218904495239258 elapsed, loss: 3.7639236e-06\n",
      "step: 4590 train: 0.13350868225097656 elapsed, loss: 3.5794933e-06\n",
      "step: 4600 train: 0.12897467613220215 elapsed, loss: 5.1280945e-06\n",
      "step: 4610 train: 0.13537025451660156 elapsed, loss: 3.6237532e-06\n",
      "step: 4620 train: 0.11913895606994629 elapsed, loss: 3.7983773e-06\n",
      "step: 4630 train: 0.13177824020385742 elapsed, loss: 3.1334234e-06\n",
      "step: 4640 train: 0.13544368743896484 elapsed, loss: 2.8563545e-06\n",
      "step: 4650 train: 0.13091325759887695 elapsed, loss: 3.275451e-06\n",
      "step: 4660 train: 0.13936161994934082 elapsed, loss: 2.9685802e-06\n",
      "step: 4670 train: 0.13148975372314453 elapsed, loss: 4.323173e-06\n",
      "step: 4680 train: 0.1307239532470703 elapsed, loss: 1.4506228e-05\n",
      "step: 4690 train: 0.1299586296081543 elapsed, loss: 0.0004475381\n",
      "step: 4700 train: 0.12403607368469238 elapsed, loss: 8.125559e-06\n",
      "step: 4710 train: 0.1354813575744629 elapsed, loss: 5.5654887e-06\n",
      "step: 4720 train: 0.11587119102478027 elapsed, loss: 1.0695109e-05\n",
      "step: 4730 train: 0.1285855770111084 elapsed, loss: 5.1268585e-06\n",
      "step: 4740 train: 0.12599468231201172 elapsed, loss: 4.75204e-06\n",
      "step: 4750 train: 0.1379702091217041 elapsed, loss: 3.8160247e-06\n",
      "step: 4760 train: 0.13230443000793457 elapsed, loss: 4.02376e-06\n",
      "step: 4770 train: 0.12198400497436523 elapsed, loss: 5.3773965e-06\n",
      "step: 4780 train: 0.14459586143493652 elapsed, loss: 5.187133e-06\n",
      "step: 4790 train: 0.12015080451965332 elapsed, loss: 4.6100095e-06\n",
      "step: 4800 train: 0.1228492259979248 elapsed, loss: 3.5585665e-06\n",
      "step: 4810 train: 0.12303948402404785 elapsed, loss: 9.765343e-06\n",
      "step: 4820 train: 0.12435746192932129 elapsed, loss: 3.8030373e-06\n",
      "step: 4830 train: 0.12370085716247559 elapsed, loss: 5.4974084e-06\n",
      "step: 4840 train: 0.12522149085998535 elapsed, loss: 4.424207e-06\n",
      "step: 4850 train: 0.12037062644958496 elapsed, loss: 6.3984753e-06\n",
      "step: 4860 train: 0.14271903038024902 elapsed, loss: 5.8708415e-06\n",
      "step: 4870 train: 0.1314239501953125 elapsed, loss: 6.087515e-06\n",
      "step: 4880 train: 0.13970422744750977 elapsed, loss: 3.231213e-06\n",
      "step: 4890 train: 0.13110852241516113 elapsed, loss: 2.7744009e-06\n",
      "step: 4900 train: 0.13954639434814453 elapsed, loss: 3.251222e-06\n",
      "step: 4910 train: 0.13724660873413086 elapsed, loss: 6.0592383e-06\n",
      "step: 4920 train: 0.13794445991516113 elapsed, loss: 3.5958174e-06\n",
      "step: 4930 train: 0.12564849853515625 elapsed, loss: 3.894304e-06\n",
      "step: 4940 train: 0.12636399269104004 elapsed, loss: 2.9606654e-06\n",
      "step: 4950 train: 0.1322464942932129 elapsed, loss: 6.0790944e-06\n",
      "step: 4960 train: 0.13322782516479492 elapsed, loss: 3.582735e-06\n",
      "step: 4970 train: 0.12565302848815918 elapsed, loss: 5.9180106e-06\n",
      "step: 4980 train: 0.12424063682556152 elapsed, loss: 4.960164e-06\n",
      "step: 4990 train: 0.12794733047485352 elapsed, loss: 3.9231763e-06\n",
      "step: 5000 train: 0.13224577903747559 elapsed, loss: 4.4377775e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5010 train: 0.12532329559326172 elapsed, loss: 4.2426027e-06\n",
      "step: 5020 train: 0.1294100284576416 elapsed, loss: 3.7913946e-06\n",
      "step: 5030 train: 0.13141441345214844 elapsed, loss: 3.0803385e-06\n",
      "step: 5040 train: 0.12769579887390137 elapsed, loss: 3.245647e-06\n",
      "step: 5050 train: 0.1407604217529297 elapsed, loss: 2.874517e-06\n",
      "step: 5060 train: 0.1333770751953125 elapsed, loss: 9.9915305e-06\n",
      "step: 5070 train: 0.1313464641571045 elapsed, loss: 3.1455083e-06\n",
      "step: 5080 train: 0.11784172058105469 elapsed, loss: 8.084118e-06\n",
      "step: 5090 train: 0.11809325218200684 elapsed, loss: 3.9497127e-06\n",
      "step: 5100 train: 0.12310457229614258 elapsed, loss: 6.8985814e-06\n",
      "step: 5110 train: 0.12281489372253418 elapsed, loss: 4.403172e-06\n",
      "step: 5120 train: 0.12765026092529297 elapsed, loss: 3.382987e-06\n",
      "step: 5130 train: 0.12347245216369629 elapsed, loss: 3.8127619e-06\n",
      "step: 5140 train: 0.1273946762084961 elapsed, loss: 0.00021448395\n",
      "step: 5150 train: 0.11408615112304688 elapsed, loss: 9.163311e-06\n",
      "step: 5160 train: 0.12600135803222656 elapsed, loss: 4.3329537e-06\n",
      "step: 5170 train: 0.13146114349365234 elapsed, loss: 3.7848733e-06\n",
      "step: 5180 train: 0.13710689544677734 elapsed, loss: 5.768922e-06\n",
      "step: 5190 train: 0.13634276390075684 elapsed, loss: 3.980438e-06\n",
      "step: 5200 train: 0.13128232955932617 elapsed, loss: 4.176949e-06\n",
      "step: 5210 train: 0.13760018348693848 elapsed, loss: 3.3923284e-06\n",
      "step: 5220 train: 0.12323451042175293 elapsed, loss: 4.7441263e-06\n",
      "step: 5230 train: 0.14757871627807617 elapsed, loss: 4.7427184e-06\n",
      "step: 5240 train: 0.13443231582641602 elapsed, loss: 4.5466486e-06\n",
      "step: 5250 train: 0.1316378116607666 elapsed, loss: 3.6777733e-06\n",
      "step: 5260 train: 0.11929774284362793 elapsed, loss: 3.2768478e-06\n",
      "step: 5270 train: 0.1338808536529541 elapsed, loss: 3.0510023e-06\n",
      "step: 5280 train: 0.13611459732055664 elapsed, loss: 4.123839e-06\n",
      "step: 5290 train: 0.1203160285949707 elapsed, loss: 3.5813887e-06\n",
      "step: 5300 train: 0.13069915771484375 elapsed, loss: 4.7444055e-06\n",
      "step: 5310 train: 0.11848568916320801 elapsed, loss: 5.7140824e-06\n",
      "step: 5320 train: 0.12794852256774902 elapsed, loss: 4.550866e-06\n",
      "step: 5330 train: 0.12781476974487305 elapsed, loss: 4.067978e-06\n",
      "step: 5340 train: 0.1314692497253418 elapsed, loss: 2.5755653e-06\n",
      "step: 5350 train: 0.1293165683746338 elapsed, loss: 4.508394e-06\n",
      "step: 5360 train: 0.12938165664672852 elapsed, loss: 3.2903313e-06\n",
      "step: 5370 train: 0.12756133079528809 elapsed, loss: 2.7743993e-06\n",
      "step: 5380 train: 0.12538719177246094 elapsed, loss: 3.57766e-06\n",
      "step: 5390 train: 0.13126349449157715 elapsed, loss: 3.329469e-06\n",
      "step: 5400 train: 0.12847399711608887 elapsed, loss: 1.8645385e-05\n",
      "step: 5410 train: 0.12897348403930664 elapsed, loss: 3.3886063e-06\n",
      "step: 5420 train: 0.1288158893585205 elapsed, loss: 3.7979169e-06\n",
      "step: 5430 train: 0.12816691398620605 elapsed, loss: 4.309668e-06\n",
      "step: 5440 train: 0.13411521911621094 elapsed, loss: 2.650531e-06\n",
      "step: 5450 train: 0.13452625274658203 elapsed, loss: 3.7192165e-06\n",
      "step: 5460 train: 0.1304919719696045 elapsed, loss: 4.311978e-06\n",
      "step: 5470 train: 0.1287391185760498 elapsed, loss: 5.4550333e-06\n",
      "step: 5480 train: 0.12741613388061523 elapsed, loss: 4.146229e-06\n",
      "step: 5490 train: 0.13431024551391602 elapsed, loss: 2.8139812e-06\n",
      "step: 5500 train: 0.12819266319274902 elapsed, loss: 3.6475037e-06\n",
      "step: 5510 train: 0.1301741600036621 elapsed, loss: 3.9790584e-06\n",
      "step: 5520 train: 0.1282196044921875 elapsed, loss: 4.4777753e-06\n",
      "step: 5530 train: 0.1342172622680664 elapsed, loss: 3.471956e-06\n",
      "step: 5540 train: 0.12942194938659668 elapsed, loss: 5.1580896e-06\n",
      "step: 5550 train: 0.12549757957458496 elapsed, loss: 4.1299263e-06\n",
      "step: 5560 train: 0.12682390213012695 elapsed, loss: 3.6405231e-06\n",
      "step: 5570 train: 0.12609457969665527 elapsed, loss: 3.2889557e-06\n",
      "step: 5580 train: 0.12741923332214355 elapsed, loss: 4.1559197e-06\n",
      "step: 5590 train: 0.14026927947998047 elapsed, loss: 3.479867e-06\n",
      "step: 5600 train: 0.1353929042816162 elapsed, loss: 3.7806797e-06\n",
      "step: 5610 train: 0.11841225624084473 elapsed, loss: 5.4370166e-06\n",
      "step: 5620 train: 0.13567471504211426 elapsed, loss: 2.8228314e-06\n",
      "step: 5630 train: 0.14359307289123535 elapsed, loss: 4.0711784e-06\n",
      "step: 5640 train: 0.12910795211791992 elapsed, loss: 4.7660196e-06\n",
      "step: 5650 train: 0.13106513023376465 elapsed, loss: 3.481265e-06\n",
      "step: 5660 train: 0.12007856369018555 elapsed, loss: 3.886385e-06\n",
      "step: 5670 train: 0.13054323196411133 elapsed, loss: 3.7620562e-06\n",
      "step: 5680 train: 0.12243509292602539 elapsed, loss: 4.6923587e-06\n",
      "step: 5690 train: 0.12252020835876465 elapsed, loss: 7.3849137e-06\n",
      "step: 5700 train: 0.13414549827575684 elapsed, loss: 3.1655509e-06\n",
      "step: 5710 train: 0.12394380569458008 elapsed, loss: 3.4803343e-06\n",
      "step: 5720 train: 0.1234273910522461 elapsed, loss: 5.382059e-06\n",
      "step: 5730 train: 0.1293504238128662 elapsed, loss: 4.4521553e-06\n",
      "step: 5740 train: 0.11685299873352051 elapsed, loss: 5.2577307e-06\n",
      "step: 5750 train: 0.11985063552856445 elapsed, loss: 8.218147e-06\n",
      "step: 5760 train: 0.12726664543151855 elapsed, loss: 3.8970848e-06\n",
      "step: 5770 train: 0.12140297889709473 elapsed, loss: 1.0019656e-05\n",
      "step: 5780 train: 0.12697148323059082 elapsed, loss: 3.895707e-06\n",
      "step: 5790 train: 0.1274433135986328 elapsed, loss: 3.2195708e-06\n",
      "step: 5800 train: 0.12273120880126953 elapsed, loss: 3.3941674e-06\n",
      "step: 5810 train: 0.13380718231201172 elapsed, loss: 2.639826e-06\n",
      "step: 5820 train: 0.12772369384765625 elapsed, loss: 3.7150303e-06\n",
      "step: 5830 train: 0.13627982139587402 elapsed, loss: 3.1525167e-06\n",
      "step: 5840 train: 0.13197803497314453 elapsed, loss: 3.7504126e-06\n",
      "step: 5850 train: 0.12372374534606934 elapsed, loss: 4.9004398e-06\n",
      "step: 5860 train: 0.13347077369689941 elapsed, loss: 3.3867414e-06\n",
      "step: 5870 train: 0.13414978981018066 elapsed, loss: 3.6172403e-06\n",
      "step: 5880 train: 0.12447547912597656 elapsed, loss: 3.2540281e-06\n",
      "step: 5890 train: 0.12863492965698242 elapsed, loss: 3.3341098e-06\n",
      "step: 5900 train: 0.1290907859802246 elapsed, loss: 3.5729945e-06\n",
      "step: 5910 train: 0.13190126419067383 elapsed, loss: 4.0502946e-06\n",
      "step: 5920 train: 0.1365969181060791 elapsed, loss: 3.0775404e-06\n",
      "step: 5930 train: 0.12205243110656738 elapsed, loss: 6.317331e-06\n",
      "step: 5940 train: 0.12885618209838867 elapsed, loss: 5.1618135e-06\n",
      "step: 5950 train: 0.12244296073913574 elapsed, loss: 4.330158e-06\n",
      "step: 5960 train: 0.12338113784790039 elapsed, loss: 5.396931e-06\n",
      "step: 5970 train: 0.12455320358276367 elapsed, loss: 4.0139657e-06\n",
      "step: 5980 train: 0.12950348854064941 elapsed, loss: 4.208159e-06\n",
      "step: 5990 train: 0.13305163383483887 elapsed, loss: 3.2042035e-06\n",
      "step: 6000 train: 0.1201481819152832 elapsed, loss: 5.775821e-06\n",
      "step: 6010 train: 0.13512015342712402 elapsed, loss: 3.2260903e-06\n",
      "step: 6020 train: 0.1171572208404541 elapsed, loss: 5.617212e-06\n",
      "step: 6030 train: 0.1333627700805664 elapsed, loss: 4.079603e-06\n",
      "step: 6040 train: 0.11707782745361328 elapsed, loss: 4.2668344e-06\n",
      "step: 6050 train: 0.13891363143920898 elapsed, loss: 3.4118796e-06\n",
      "step: 6060 train: 0.1310262680053711 elapsed, loss: 4.4833623e-06\n",
      "step: 6070 train: 0.13153386116027832 elapsed, loss: 3.511073e-06\n",
      "step: 6080 train: 0.12040472030639648 elapsed, loss: 6.4127908e-06\n",
      "step: 6090 train: 0.1335456371307373 elapsed, loss: 4.743475e-06\n",
      "step: 6100 train: 0.12160468101501465 elapsed, loss: 4.6295163e-06\n",
      "step: 6110 train: 0.12343811988830566 elapsed, loss: 4.11083e-06\n",
      "step: 6120 train: 0.12407469749450684 elapsed, loss: 3.923182e-06\n",
      "step: 6130 train: 0.13309216499328613 elapsed, loss: 3.725274e-06\n",
      "step: 6140 train: 0.1346426010131836 elapsed, loss: 3.5324833e-06\n",
      "step: 6150 train: 0.12231206893920898 elapsed, loss: 3.7643883e-06\n",
      "step: 6160 train: 0.12199521064758301 elapsed, loss: 5.5152186e-06\n",
      "step: 6170 train: 0.13670086860656738 elapsed, loss: 3.841683e-06\n",
      "step: 6180 train: 0.1396040916442871 elapsed, loss: 4.4321214e-06\n",
      "step: 6190 train: 0.12999391555786133 elapsed, loss: 3.80257e-06\n",
      "step: 6200 train: 0.13059425354003906 elapsed, loss: 3.979056e-06\n",
      "step: 6210 train: 0.13054275512695312 elapsed, loss: 5.173878e-06\n",
      "step: 6220 train: 0.13812756538391113 elapsed, loss: 2.8321444e-06\n",
      "step: 6230 train: 0.13533568382263184 elapsed, loss: 2.8163092e-06\n",
      "step: 6240 train: 0.12875723838806152 elapsed, loss: 3.264738e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6250 train: 0.1209864616394043 elapsed, loss: 4.0130526e-06\n",
      "step: 6260 train: 0.12523102760314941 elapsed, loss: 5.595773e-06\n",
      "step: 6270 train: 0.12950754165649414 elapsed, loss: 4.559728e-06\n",
      "step: 6280 train: 0.1314835548400879 elapsed, loss: 3.968344e-06\n",
      "step: 6290 train: 0.12331962585449219 elapsed, loss: 4.0367963e-06\n",
      "step: 6300 train: 0.12863945960998535 elapsed, loss: 4.217919e-06\n",
      "step: 6310 train: 0.12598586082458496 elapsed, loss: 3.850998e-06\n",
      "step: 6320 train: 0.13181829452514648 elapsed, loss: 4.8156608e-06\n",
      "step: 6330 train: 0.1302807331085205 elapsed, loss: 3.6176377e-06\n",
      "step: 6340 train: 0.1348884105682373 elapsed, loss: 5.0253316e-06\n",
      "step: 6350 train: 0.12647080421447754 elapsed, loss: 4.3161813e-06\n",
      "step: 6360 train: 0.1273348331451416 elapsed, loss: 4.444697e-06\n",
      "step: 6370 train: 0.1264350414276123 elapsed, loss: 3.653559e-06\n",
      "step: 6380 train: 0.12397289276123047 elapsed, loss: 5.4914726e-06\n",
      "step: 6390 train: 0.14492273330688477 elapsed, loss: 2.8614725e-06\n",
      "step: 6400 train: 0.13339662551879883 elapsed, loss: 3.225143e-06\n",
      "step: 6410 train: 0.12893176078796387 elapsed, loss: 3.8337657e-06\n",
      "step: 6420 train: 0.1470324993133545 elapsed, loss: 4.2401903e-06\n",
      "step: 6430 train: 0.12604928016662598 elapsed, loss: 4.8684433e-06\n",
      "step: 6440 train: 0.11758947372436523 elapsed, loss: 3.241459e-06\n",
      "step: 6450 train: 0.12528729438781738 elapsed, loss: 4.670096e-06\n",
      "step: 6460 train: 0.13256359100341797 elapsed, loss: 9.409392e-06\n",
      "step: 6470 train: 0.13262724876403809 elapsed, loss: 3.7154869e-06\n",
      "step: 6480 train: 0.12545228004455566 elapsed, loss: 3.776961e-06\n",
      "step: 6490 train: 0.12463140487670898 elapsed, loss: 3.3364531e-06\n",
      "step: 6500 train: 0.13116002082824707 elapsed, loss: 3.3844135e-06\n",
      "step: 6510 train: 0.12912368774414062 elapsed, loss: 3.1725397e-06\n",
      "step: 6520 train: 0.12770509719848633 elapsed, loss: 4.1583153e-06\n",
      "step: 6530 train: 0.1370387077331543 elapsed, loss: 3.3015033e-06\n",
      "step: 6540 train: 0.12591290473937988 elapsed, loss: 4.486149e-06\n",
      "step: 6550 train: 0.1456918716430664 elapsed, loss: 4.4715775e-06\n",
      "step: 6560 train: 0.13401079177856445 elapsed, loss: 3.0160754e-06\n",
      "step: 6570 train: 0.13148975372314453 elapsed, loss: 3.027721e-06\n",
      "step: 6580 train: 0.12962961196899414 elapsed, loss: 3.357871e-06\n",
      "step: 6590 train: 0.12486553192138672 elapsed, loss: 9.485263e-05\n",
      "step: 6600 train: 0.12558317184448242 elapsed, loss: 7.1647173e-06\n",
      "step: 6610 train: 0.1349811553955078 elapsed, loss: 4.8237243e-06\n",
      "step: 6620 train: 0.12874984741210938 elapsed, loss: 3.7778902e-06\n",
      "step: 6630 train: 0.12000584602355957 elapsed, loss: 4.733416e-06\n",
      "step: 6640 train: 0.12649798393249512 elapsed, loss: 4.012574e-06\n",
      "step: 6650 train: 0.1172189712524414 elapsed, loss: 4.725044e-06\n",
      "step: 6660 train: 0.12835192680358887 elapsed, loss: 4.002778e-06\n",
      "step: 6670 train: 0.13466715812683105 elapsed, loss: 4.9782875e-06\n",
      "step: 6680 train: 0.12887287139892578 elapsed, loss: 2.8768472e-06\n",
      "step: 6690 train: 0.1293776035308838 elapsed, loss: 5.532268e-06\n",
      "step: 6700 train: 0.1296083927154541 elapsed, loss: 4.0027917e-06\n",
      "step: 6710 train: 0.13340330123901367 elapsed, loss: 3.7452955e-06\n",
      "step: 6720 train: 0.13827753067016602 elapsed, loss: 3.8100202e-06\n",
      "step: 6730 train: 0.13780879974365234 elapsed, loss: 4.409777e-06\n",
      "step: 6740 train: 0.1344003677368164 elapsed, loss: 2.8773134e-06\n",
      "step: 6750 train: 0.12323951721191406 elapsed, loss: 5.0849726e-06\n",
      "step: 6760 train: 0.12207865715026855 elapsed, loss: 4.628174e-06\n",
      "step: 6770 train: 0.13059639930725098 elapsed, loss: 3.545511e-06\n",
      "step: 6780 train: 0.12197637557983398 elapsed, loss: 4.928692e-06\n",
      "step: 6790 train: 0.12366533279418945 elapsed, loss: 4.50059e-06\n",
      "step: 6800 train: 0.12862181663513184 elapsed, loss: 3.8765793e-06\n",
      "step: 6810 train: 0.12326788902282715 elapsed, loss: 6.1182072e-06\n",
      "step: 6820 train: 0.1354973316192627 elapsed, loss: 2.8121194e-06\n",
      "step: 6830 train: 0.12793493270874023 elapsed, loss: 3.3690376e-06\n",
      "step: 6840 train: 0.1396045684814453 elapsed, loss: 4.074466e-06\n",
      "step: 6850 train: 0.12757515907287598 elapsed, loss: 3.5911662e-06\n",
      "step: 6860 train: 0.1297285556793213 elapsed, loss: 4.258337e-06\n",
      "step: 6870 train: 0.13600993156433105 elapsed, loss: 4.192254e-06\n",
      "step: 6880 train: 0.13341069221496582 elapsed, loss: 4.2766096e-06\n",
      "step: 6890 train: 0.13289189338684082 elapsed, loss: 3.8053593e-06\n",
      "step: 6900 train: 0.13020777702331543 elapsed, loss: 3.6521656e-06\n",
      "step: 6910 train: 0.12487268447875977 elapsed, loss: 5.298709e-06\n",
      "step: 6920 train: 0.1239163875579834 elapsed, loss: 1.1400274e-05\n",
      "step: 6930 train: 0.12575960159301758 elapsed, loss: 3.973456e-06\n",
      "step: 6940 train: 0.12728381156921387 elapsed, loss: 5.263778e-06\n",
      "step: 6950 train: 0.13009858131408691 elapsed, loss: 4.8926236e-06\n",
      "step: 6960 train: 0.13023161888122559 elapsed, loss: 4.0414207e-06\n",
      "step: 6970 train: 0.11487293243408203 elapsed, loss: 4.5993156e-06\n",
      "step: 6980 train: 0.12238407135009766 elapsed, loss: 5.76608e-06\n",
      "step: 6990 train: 0.13524293899536133 elapsed, loss: 3.8137425e-06\n",
      "step: 7000 train: 0.12266039848327637 elapsed, loss: 8.8852175e-06\n",
      "step: 7010 train: 0.13343095779418945 elapsed, loss: 2.8000131e-06\n",
      "step: 7020 train: 0.12630414962768555 elapsed, loss: 4.926629e-06\n",
      "step: 7030 train: 0.12274360656738281 elapsed, loss: 4.2379565e-06\n",
      "step: 7040 train: 0.1303565502166748 elapsed, loss: 3.404904e-06\n",
      "step: 7050 train: 0.12965011596679688 elapsed, loss: 3.7350542e-06\n",
      "step: 7060 train: 0.1341414451599121 elapsed, loss: 3.0924443e-06\n",
      "step: 7070 train: 0.13994359970092773 elapsed, loss: 4.011161e-06\n",
      "step: 7080 train: 0.12971854209899902 elapsed, loss: 3.4887144e-06\n",
      "step: 7090 train: 0.12960553169250488 elapsed, loss: 3.4468135e-06\n",
      "step: 7100 train: 0.12803173065185547 elapsed, loss: 3.9241077e-06\n",
      "step: 7110 train: 0.13258886337280273 elapsed, loss: 2.6859261e-06\n",
      "step: 7120 train: 0.12910771369934082 elapsed, loss: 4.2347015e-06\n",
      "step: 7130 train: 0.12996721267700195 elapsed, loss: 4.314794e-06\n",
      "step: 7140 train: 0.13292860984802246 elapsed, loss: 3.977607e-06\n",
      "step: 7150 train: 0.13022065162658691 elapsed, loss: 3.6461136e-06\n",
      "step: 7160 train: 0.12991762161254883 elapsed, loss: 5.8393525e-06\n",
      "step: 7170 train: 0.12815022468566895 elapsed, loss: 3.6624026e-06\n",
      "step: 7180 train: 0.12390995025634766 elapsed, loss: 3.891975e-06\n",
      "step: 7190 train: 0.13440942764282227 elapsed, loss: 3.739113e-06\n",
      "step: 7200 train: 0.13163518905639648 elapsed, loss: 2.7124665e-06\n",
      "step: 7210 train: 0.1340351104736328 elapsed, loss: 3.4062887e-06\n",
      "step: 7220 train: 0.13264846801757812 elapsed, loss: 6.4715855e-06\n",
      "step: 7230 train: 0.15187454223632812 elapsed, loss: 2.9424832e-06\n",
      "step: 7240 train: 0.1339735984802246 elapsed, loss: 3.554845e-06\n",
      "step: 7250 train: 0.14380311965942383 elapsed, loss: 3.7741293e-06\n",
      "step: 7260 train: 0.11877083778381348 elapsed, loss: 4.6705527e-06\n",
      "step: 7270 train: 0.12394976615905762 elapsed, loss: 3.8794087e-06\n",
      "step: 7280 train: 0.12942218780517578 elapsed, loss: 2.892209e-06\n",
      "step: 7290 train: 0.13198328018188477 elapsed, loss: 3.4761383e-06\n",
      "step: 7300 train: 0.13176608085632324 elapsed, loss: 4.422814e-06\n",
      "step: 7310 train: 0.12367129325866699 elapsed, loss: 3.864504e-06\n",
      "step: 7320 train: 0.12479758262634277 elapsed, loss: 3.726207e-06\n",
      "step: 7330 train: 0.12348103523254395 elapsed, loss: 3.6503072e-06\n",
      "step: 7340 train: 0.1349656581878662 elapsed, loss: 3.0551923e-06\n",
      "step: 7350 train: 0.12814664840698242 elapsed, loss: 3.8370317e-06\n",
      "step: 7360 train: 0.12997651100158691 elapsed, loss: 3.2838311e-06\n",
      "step: 7370 train: 0.1372387409210205 elapsed, loss: 3.3946376e-06\n",
      "step: 7380 train: 0.11966252326965332 elapsed, loss: 3.9501865e-06\n",
      "step: 7390 train: 0.1210625171661377 elapsed, loss: 4.3427344e-06\n",
      "step: 7400 train: 0.13012123107910156 elapsed, loss: 3.59442e-06\n",
      "step: 7410 train: 0.13320708274841309 elapsed, loss: 4.312433e-06\n",
      "step: 7420 train: 0.13529706001281738 elapsed, loss: 3.0295816e-06\n",
      "step: 7430 train: 0.12368416786193848 elapsed, loss: 3.67545e-06\n",
      "step: 7440 train: 0.11876606941223145 elapsed, loss: 4.4959215e-06\n",
      "step: 7450 train: 0.13111019134521484 elapsed, loss: 3.2344642e-06\n",
      "step: 7460 train: 0.1436910629272461 elapsed, loss: 4.398598e-06\n",
      "step: 7470 train: 0.1306312084197998 elapsed, loss: 5.423957e-06\n",
      "step: 7480 train: 0.12399029731750488 elapsed, loss: 4.178808e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7490 train: 0.12461972236633301 elapsed, loss: 4.4143776e-06\n",
      "step: 7500 train: 0.12805390357971191 elapsed, loss: 2.886161e-06\n",
      "step: 7510 train: 0.13401126861572266 elapsed, loss: 4.43959e-06\n",
      "step: 7520 train: 0.14012742042541504 elapsed, loss: 3.607455e-06\n",
      "step: 7530 train: 0.13975191116333008 elapsed, loss: 4.7184644e-06\n",
      "step: 7540 train: 0.15017247200012207 elapsed, loss: 3.0109559e-06\n",
      "step: 7550 train: 0.12871384620666504 elapsed, loss: 3.3322465e-06\n",
      "step: 7560 train: 0.12892436981201172 elapsed, loss: 3.4076963e-06\n",
      "step: 7570 train: 0.12942051887512207 elapsed, loss: 3.1203858e-06\n",
      "step: 7580 train: 0.13714146614074707 elapsed, loss: 2.9890716e-06\n",
      "step: 7590 train: 0.12842035293579102 elapsed, loss: 4.079166e-06\n",
      "step: 7600 train: 0.13274192810058594 elapsed, loss: 4.289177e-06\n",
      "step: 7610 train: 0.12801241874694824 elapsed, loss: 4.1629846e-06\n",
      "step: 7620 train: 0.12789654731750488 elapsed, loss: 3.0598508e-06\n",
      "step: 7630 train: 0.13498330116271973 elapsed, loss: 4.5408688e-06\n",
      "step: 7640 train: 0.13225936889648438 elapsed, loss: 3.9194333e-06\n",
      "step: 7650 train: 0.1337871551513672 elapsed, loss: 5.063078e-06\n",
      "step: 7660 train: 0.12459182739257812 elapsed, loss: 4.3124633e-06\n",
      "step: 7670 train: 0.1391892433166504 elapsed, loss: 2.7636925e-06\n",
      "step: 7680 train: 0.11872005462646484 elapsed, loss: 4.1406392e-06\n",
      "step: 7690 train: 0.13880324363708496 elapsed, loss: 3.1054728e-06\n",
      "step: 7700 train: 0.12518000602722168 elapsed, loss: 4.8479305e-06\n",
      "step: 7710 train: 0.12135457992553711 elapsed, loss: 5.1147513e-06\n",
      "step: 7720 train: 0.12784910202026367 elapsed, loss: 6.8483528e-06\n",
      "step: 7730 train: 0.12615203857421875 elapsed, loss: 4.206274e-06\n",
      "step: 7740 train: 0.1376805305480957 elapsed, loss: 3.5846283e-06\n",
      "step: 7750 train: 0.11865782737731934 elapsed, loss: 8.258579e-06\n",
      "step: 7760 train: 0.12212705612182617 elapsed, loss: 5.4388356e-06\n",
      "step: 7770 train: 0.1238851547241211 elapsed, loss: 4.5061793e-06\n",
      "step: 7780 train: 0.12937188148498535 elapsed, loss: 4.13366e-06\n",
      "step: 7790 train: 0.13285350799560547 elapsed, loss: 3.6600343e-06\n",
      "step: 7800 train: 0.1356055736541748 elapsed, loss: 1.1170568e-05\n",
      "step: 7810 train: 0.1262073516845703 elapsed, loss: 5.126427e-06\n",
      "step: 7820 train: 0.1248629093170166 elapsed, loss: 4.726895e-06\n",
      "step: 7830 train: 0.12180161476135254 elapsed, loss: 4.3823184e-06\n",
      "step: 7840 train: 0.1244964599609375 elapsed, loss: 4.7067306e-06\n",
      "step: 7850 train: 0.11771535873413086 elapsed, loss: 3.8905864e-06\n",
      "step: 7860 train: 0.1392078399658203 elapsed, loss: 3.0845272e-06\n",
      "step: 7870 train: 0.11886024475097656 elapsed, loss: 5.023917e-06\n",
      "step: 7880 train: 0.1395411491394043 elapsed, loss: 3.6279425e-06\n",
      "step: 7890 train: 0.1328732967376709 elapsed, loss: 5.2432474e-06\n",
      "step: 7900 train: 0.12578630447387695 elapsed, loss: 4.8809943e-06\n",
      "step: 7910 train: 0.12640786170959473 elapsed, loss: 2.9997825e-06\n",
      "step: 7920 train: 0.12405920028686523 elapsed, loss: 6.2223667e-06\n",
      "step: 7930 train: 0.1185615062713623 elapsed, loss: 4.490354e-06\n",
      "step: 7940 train: 0.12725448608398438 elapsed, loss: 3.1432005e-06\n",
      "step: 7950 train: 0.13513398170471191 elapsed, loss: 3.8523913e-06\n",
      "step: 7960 train: 0.12794041633605957 elapsed, loss: 5.430965e-06\n",
      "step: 7970 train: 0.13490772247314453 elapsed, loss: 3.9110228e-06\n",
      "step: 7980 train: 0.12398529052734375 elapsed, loss: 4.835368e-06\n",
      "step: 7990 train: 0.1273951530456543 elapsed, loss: 5.077988e-06\n",
      "step: 8000 train: 0.12365984916687012 elapsed, loss: 3.6456468e-06\n",
      "step: 8010 train: 0.1289522647857666 elapsed, loss: 3.2037392e-06\n",
      "step: 8020 train: 0.12847423553466797 elapsed, loss: 2.9676517e-06\n",
      "step: 8030 train: 0.13168859481811523 elapsed, loss: 2.939244e-06\n",
      "step: 8040 train: 0.1429755687713623 elapsed, loss: 9.773516e-06\n",
      "step: 8050 train: 0.12844038009643555 elapsed, loss: 0.0023015204\n",
      "step: 8060 train: 0.12914490699768066 elapsed, loss: 0.016146604\n",
      "step: 8070 train: 0.1278553009033203 elapsed, loss: 1.0122745e-05\n",
      "step: 8080 train: 0.12141275405883789 elapsed, loss: 2.3037872e-05\n",
      "step: 8090 train: 0.12645387649536133 elapsed, loss: 8.834738e-06\n",
      "step: 8100 train: 0.13437533378601074 elapsed, loss: 1.3880521e-05\n",
      "step: 8110 train: 0.13680458068847656 elapsed, loss: 7.858365e-06\n",
      "step: 8120 train: 0.13483548164367676 elapsed, loss: 6.0679845e-06\n",
      "step: 8130 train: 0.13192486763000488 elapsed, loss: 6.953982e-06\n",
      "step: 8140 train: 0.12810015678405762 elapsed, loss: 7.087003e-06\n",
      "step: 8150 train: 0.13270211219787598 elapsed, loss: 5.4346538e-06\n",
      "step: 8160 train: 0.13662052154541016 elapsed, loss: 4.4838043e-06\n",
      "step: 8170 train: 0.14299392700195312 elapsed, loss: 3.944131e-06\n",
      "step: 8180 train: 0.12698626518249512 elapsed, loss: 3.946463e-06\n",
      "step: 8190 train: 0.11833047866821289 elapsed, loss: 6.103308e-06\n",
      "step: 8200 train: 0.12900209426879883 elapsed, loss: 3.2256219e-06\n",
      "step: 8210 train: 0.1161661148071289 elapsed, loss: 1.2145907e-05\n",
      "step: 8220 train: 0.12971162796020508 elapsed, loss: 2.939708e-06\n",
      "step: 8230 train: 0.13044452667236328 elapsed, loss: 5.1435154e-06\n",
      "step: 8240 train: 0.12362194061279297 elapsed, loss: 6.900211e-06\n",
      "step: 8250 train: 0.12257933616638184 elapsed, loss: 5.680985e-06\n",
      "step: 8260 train: 0.1447453498840332 elapsed, loss: 3.040756e-06\n",
      "step: 8270 train: 0.135772705078125 elapsed, loss: 4.262134e-06\n",
      "step: 8280 train: 0.13914847373962402 elapsed, loss: 2.9513524e-06\n",
      "step: 8290 train: 0.12174582481384277 elapsed, loss: 3.6256238e-06\n",
      "step: 8300 train: 0.1259615421295166 elapsed, loss: 3.7699742e-06\n",
      "step: 8310 train: 0.13114380836486816 elapsed, loss: 5.407515e-06\n",
      "step: 8320 train: 0.13434720039367676 elapsed, loss: 3.2079306e-06\n",
      "step: 8330 train: 0.11425042152404785 elapsed, loss: 4.1541493e-06\n",
      "step: 8340 train: 0.11575841903686523 elapsed, loss: 4.939234e-06\n",
      "step: 8350 train: 0.12236714363098145 elapsed, loss: 4.0433156e-06\n",
      "step: 8360 train: 0.11808466911315918 elapsed, loss: 7.5125045e-06\n",
      "step: 8370 train: 0.1410226821899414 elapsed, loss: 2.4321407e-06\n",
      "step: 8380 train: 0.12380862236022949 elapsed, loss: 3.97997e-06\n",
      "step: 8390 train: 0.1350574493408203 elapsed, loss: 2.5364516e-06\n",
      "step: 8400 train: 0.13722705841064453 elapsed, loss: 2.4805709e-06\n",
      "step: 8410 train: 0.13335275650024414 elapsed, loss: 3.3294614e-06\n",
      "step: 8420 train: 0.12609219551086426 elapsed, loss: 4.5717957e-06\n",
      "step: 8430 train: 0.12543964385986328 elapsed, loss: 3.9189877e-06\n",
      "step: 8440 train: 0.12827825546264648 elapsed, loss: 6.5057384e-06\n",
      "step: 8450 train: 0.1297142505645752 elapsed, loss: 3.7843638e-06\n",
      "step: 8460 train: 0.1322181224822998 elapsed, loss: 4.412932e-06\n",
      "step: 8470 train: 0.13566136360168457 elapsed, loss: 3.678238e-06\n",
      "step: 8480 train: 0.13878369331359863 elapsed, loss: 2.9718408e-06\n",
      "step: 8490 train: 0.12438249588012695 elapsed, loss: 3.130166e-06\n",
      "step: 8500 train: 0.11985611915588379 elapsed, loss: 3.8244643e-06\n",
      "step: 8510 train: 0.1307837963104248 elapsed, loss: 4.1853355e-06\n",
      "step: 8520 train: 0.12599802017211914 elapsed, loss: 4.343643e-06\n",
      "step: 8530 train: 0.13595032691955566 elapsed, loss: 2.7646233e-06\n",
      "step: 8540 train: 0.12167763710021973 elapsed, loss: 4.106181e-06\n",
      "step: 8550 train: 0.13593602180480957 elapsed, loss: 2.7115293e-06\n",
      "step: 8560 train: 0.12778592109680176 elapsed, loss: 3.944553e-06\n",
      "step: 8570 train: 0.1288764476776123 elapsed, loss: 3.0645028e-06\n",
      "step: 8580 train: 0.12052345275878906 elapsed, loss: 3.4114205e-06\n",
      "step: 8590 train: 0.12482070922851562 elapsed, loss: 3.4491427e-06\n",
      "step: 8600 train: 0.13330292701721191 elapsed, loss: 3.921735e-06\n",
      "step: 8610 train: 0.12065649032592773 elapsed, loss: 6.3329644e-06\n",
      "step: 8620 train: 0.1406557559967041 elapsed, loss: 2.612816e-06\n",
      "step: 8630 train: 0.14231467247009277 elapsed, loss: 2.5546078e-06\n",
      "step: 8640 train: 0.13759326934814453 elapsed, loss: 3.5995447e-06\n",
      "step: 8650 train: 0.12488913536071777 elapsed, loss: 3.4668344e-06\n",
      "step: 8660 train: 0.12206721305847168 elapsed, loss: 3.6628785e-06\n",
      "step: 8670 train: 0.12179088592529297 elapsed, loss: 5.7523725e-06\n",
      "step: 8680 train: 0.13877010345458984 elapsed, loss: 4.9061678e-06\n",
      "step: 8690 train: 0.1402122974395752 elapsed, loss: 4.727649e-06\n",
      "step: 8700 train: 0.14305853843688965 elapsed, loss: 2.7050169e-06\n",
      "step: 8710 train: 0.1297588348388672 elapsed, loss: 3.6232946e-06\n",
      "step: 8720 train: 0.11693310737609863 elapsed, loss: 5.0714516e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8730 train: 0.12124896049499512 elapsed, loss: 3.528768e-06\n",
      "step: 8740 train: 0.12334537506103516 elapsed, loss: 3.3546112e-06\n",
      "step: 8750 train: 0.1322770118713379 elapsed, loss: 3.1189877e-06\n",
      "step: 8760 train: 0.12697625160217285 elapsed, loss: 4.1141e-06\n",
      "step: 8770 train: 0.12799310684204102 elapsed, loss: 4.8501256e-06\n",
      "step: 8780 train: 0.1343832015991211 elapsed, loss: 3.7839268e-06\n",
      "step: 8790 train: 0.12162995338439941 elapsed, loss: 4.174627e-06\n",
      "step: 8800 train: 0.1290416717529297 elapsed, loss: 3.4658997e-06\n",
      "step: 8810 train: 0.12396001815795898 elapsed, loss: 5.1067072e-06\n",
      "step: 8820 train: 0.13094115257263184 elapsed, loss: 3.3448312e-06\n",
      "step: 8830 train: 0.13490843772888184 elapsed, loss: 3.1930235e-06\n",
      "step: 8840 train: 0.12249946594238281 elapsed, loss: 5.4230313e-06\n",
      "step: 8850 train: 0.1411454677581787 elapsed, loss: 4.1056983e-06\n",
      "step: 8860 train: 0.13191699981689453 elapsed, loss: 4.6253635e-06\n",
      "step: 8870 train: 0.1292719841003418 elapsed, loss: 4.931607e-06\n",
      "step: 8880 train: 0.1360785961151123 elapsed, loss: 3.6880097e-06\n",
      "step: 8890 train: 0.13351058959960938 elapsed, loss: 2.8801032e-06\n",
      "step: 8900 train: 0.11851263046264648 elapsed, loss: 4.0922105e-06\n",
      "step: 8910 train: 0.12498211860656738 elapsed, loss: 3.3676508e-06\n",
      "step: 8920 train: 0.12666010856628418 elapsed, loss: 4.268697e-06\n",
      "step: 8930 train: 0.1291201114654541 elapsed, loss: 3.93991e-06\n",
      "step: 8940 train: 0.11412930488586426 elapsed, loss: 5.55481e-06\n",
      "step: 8950 train: 0.1145327091217041 elapsed, loss: 5.0342333e-06\n",
      "step: 8960 train: 0.12317705154418945 elapsed, loss: 3.505952e-06\n",
      "step: 8970 train: 0.1400156021118164 elapsed, loss: 2.7385472e-06\n",
      "step: 8980 train: 0.11538195610046387 elapsed, loss: 5.213031e-06\n",
      "step: 8990 train: 0.12455868721008301 elapsed, loss: 3.8286526e-06\n",
      "step: 9000 train: 0.1280958652496338 elapsed, loss: 4.616979e-06\n",
      "step: 9010 train: 0.12605857849121094 elapsed, loss: 4.091279e-06\n",
      "step: 9020 train: 0.13526487350463867 elapsed, loss: 3.212583e-06\n",
      "step: 9030 train: 0.13353204727172852 elapsed, loss: 2.816776e-06\n",
      "step: 9040 train: 0.1329786777496338 elapsed, loss: 2.6989644e-06\n",
      "step: 9050 train: 0.13524508476257324 elapsed, loss: 4.3781233e-06\n",
      "step: 9060 train: 0.13438105583190918 elapsed, loss: 2.6039722e-06\n",
      "step: 9070 train: 0.1301419734954834 elapsed, loss: 3.5702042e-06\n",
      "step: 9080 train: 0.1307222843170166 elapsed, loss: 2.9620624e-06\n",
      "step: 9090 train: 0.12914085388183594 elapsed, loss: 2.991399e-06\n",
      "step: 9100 train: 0.12015342712402344 elapsed, loss: 4.4991925e-06\n",
      "step: 9110 train: 0.1263129711151123 elapsed, loss: 4.8065194e-06\n",
      "step: 9120 train: 0.1461319923400879 elapsed, loss: 3.0519313e-06\n",
      "step: 9130 train: 0.12035179138183594 elapsed, loss: 4.7166523e-06\n",
      "step: 9140 train: 0.1306438446044922 elapsed, loss: 3.794633e-06\n",
      "step: 9150 train: 0.13397479057312012 elapsed, loss: 1.05855715e-05\n",
      "step: 9160 train: 0.1311354637145996 elapsed, loss: 4.3114733e-06\n",
      "step: 9170 train: 0.13712644577026367 elapsed, loss: 2.771607e-06\n",
      "step: 9180 train: 0.12886452674865723 elapsed, loss: 3.1925606e-06\n",
      "step: 9190 train: 0.13199615478515625 elapsed, loss: 4.429813e-06\n",
      "step: 9200 train: 0.14371323585510254 elapsed, loss: 2.8549584e-06\n",
      "step: 9210 train: 0.12609243392944336 elapsed, loss: 4.3785853e-06\n",
      "step: 9220 train: 0.12402582168579102 elapsed, loss: 4.02609e-06\n",
      "step: 9230 train: 0.11842727661132812 elapsed, loss: 5.4900365e-06\n",
      "step: 9240 train: 0.11530828475952148 elapsed, loss: 7.722429e-06\n",
      "step: 9250 train: 0.12702059745788574 elapsed, loss: 4.0372684e-06\n",
      "step: 9260 train: 0.13056683540344238 elapsed, loss: 4.1560006e-06\n",
      "step: 9270 train: 0.11687588691711426 elapsed, loss: 4.2221304e-06\n",
      "step: 9280 train: 0.1365807056427002 elapsed, loss: 4.543367e-06\n",
      "step: 9290 train: 0.1317613124847412 elapsed, loss: 3.5706728e-06\n",
      "step: 9300 train: 0.12349677085876465 elapsed, loss: 4.5490233e-06\n",
      "step: 9310 train: 0.12755918502807617 elapsed, loss: 3.7043224e-06\n",
      "step: 9320 train: 0.12947773933410645 elapsed, loss: 3.1897687e-06\n",
      "step: 9330 train: 0.12479305267333984 elapsed, loss: 4.0391287e-06\n",
      "step: 9340 train: 0.12851428985595703 elapsed, loss: 3.8323715e-06\n",
      "step: 9350 train: 0.1287519931793213 elapsed, loss: 3.7047653e-06\n",
      "step: 9360 train: 0.12611651420593262 elapsed, loss: 9.627777e-06\n",
      "step: 9370 train: 0.12368273735046387 elapsed, loss: 3.993488e-06\n",
      "step: 9380 train: 0.1243276596069336 elapsed, loss: 3.7220102e-06\n",
      "step: 9390 train: 0.12601780891418457 elapsed, loss: 3.9515844e-06\n",
      "step: 9400 train: 0.12313294410705566 elapsed, loss: 6.226264e-06\n",
      "step: 9410 train: 0.12763571739196777 elapsed, loss: 4.2225593e-06\n",
      "step: 9420 train: 0.11963534355163574 elapsed, loss: 5.1189736e-06\n",
      "step: 9430 train: 0.1206512451171875 elapsed, loss: 5.064938e-06\n",
      "step: 9440 train: 0.1305854320526123 elapsed, loss: 3.1203867e-06\n",
      "step: 9450 train: 0.14171051979064941 elapsed, loss: 3.7192099e-06\n",
      "step: 9460 train: 0.13933491706848145 elapsed, loss: 3.932495e-06\n",
      "step: 9470 train: 0.14217233657836914 elapsed, loss: 3.1436616e-06\n",
      "step: 9480 train: 0.1322948932647705 elapsed, loss: 3.7555208e-06\n",
      "step: 9490 train: 0.12632346153259277 elapsed, loss: 4.189526e-06\n",
      "step: 9500 train: 0.12478089332580566 elapsed, loss: 4.303132e-06\n",
      "step: 9510 train: 0.11618447303771973 elapsed, loss: 5.1496972e-06\n",
      "step: 9520 train: 0.11728239059448242 elapsed, loss: 5.2270193e-06\n",
      "step: 9530 train: 0.12749767303466797 elapsed, loss: 4.000941e-06\n",
      "step: 9540 train: 0.13173770904541016 elapsed, loss: 3.409064e-06\n",
      "step: 9550 train: 0.13808131217956543 elapsed, loss: 2.9830146e-06\n",
      "step: 9560 train: 0.11864352226257324 elapsed, loss: 5.0603016e-06\n",
      "step: 9570 train: 0.12363958358764648 elapsed, loss: 4.151353e-06\n",
      "step: 9580 train: 0.12433791160583496 elapsed, loss: 4.709203e-06\n",
      "step: 9590 train: 0.13584184646606445 elapsed, loss: 3.518504e-06\n",
      "step: 9600 train: 0.14405274391174316 elapsed, loss: 6.4604255e-06\n",
      "step: 9610 train: 0.14261245727539062 elapsed, loss: 2.7948913e-06\n",
      "step: 9620 train: 0.13760685920715332 elapsed, loss: 3.4747497e-06\n",
      "step: 9630 train: 0.1310558319091797 elapsed, loss: 2.8377312e-06\n",
      "step: 9640 train: 0.13008451461791992 elapsed, loss: 4.970886e-06\n",
      "step: 9650 train: 0.13587093353271484 elapsed, loss: 3.4653908e-06\n",
      "step: 9660 train: 0.11525130271911621 elapsed, loss: 5.84029e-06\n",
      "step: 9670 train: 0.12299728393554688 elapsed, loss: 4.1094336e-06\n",
      "step: 9680 train: 0.13972854614257812 elapsed, loss: 5.4201764e-06\n",
      "step: 9690 train: 0.13051629066467285 elapsed, loss: 7.636647e-06\n",
      "step: 9700 train: 0.13650155067443848 elapsed, loss: 2.7827787e-06\n",
      "step: 9710 train: 0.12267494201660156 elapsed, loss: 6.983112e-06\n",
      "step: 9720 train: 0.13125848770141602 elapsed, loss: 3.195823e-06\n",
      "step: 9730 train: 0.12732195854187012 elapsed, loss: 3.4663558e-06\n",
      "step: 9740 train: 0.13207244873046875 elapsed, loss: 2.8344675e-06\n",
      "step: 9750 train: 0.1313786506652832 elapsed, loss: 3.7746145e-06\n",
      "step: 9760 train: 0.14127588272094727 elapsed, loss: 3.6176893e-06\n",
      "step: 9770 train: 0.12433171272277832 elapsed, loss: 6.266266e-06\n",
      "step: 9780 train: 0.12392497062683105 elapsed, loss: 4.9778805e-06\n",
      "step: 9790 train: 0.12660956382751465 elapsed, loss: 3.195354e-06\n",
      "step: 9800 train: 0.1338820457458496 elapsed, loss: 2.8544828e-06\n",
      "step: 9810 train: 0.12690258026123047 elapsed, loss: 4.3716036e-06\n",
      "step: 9820 train: 0.14207816123962402 elapsed, loss: 3.5841786e-06\n",
      "step: 9830 train: 0.13705873489379883 elapsed, loss: 2.7334227e-06\n",
      "step: 9840 train: 0.13034439086914062 elapsed, loss: 4.306407e-06\n",
      "step: 9850 train: 0.12358617782592773 elapsed, loss: 3.5692813e-06\n",
      "step: 9860 train: 0.1440122127532959 elapsed, loss: 2.7995443e-06\n",
      "step: 9870 train: 0.13333940505981445 elapsed, loss: 3.906405e-06\n",
      "step: 9880 train: 0.11766409873962402 elapsed, loss: 4.9620567e-06\n",
      "step: 9890 train: 0.12915349006652832 elapsed, loss: 3.900805e-06\n",
      "step: 9900 train: 0.13237714767456055 elapsed, loss: 3.2083915e-06\n",
      "step: 9910 train: 0.1367778778076172 elapsed, loss: 2.5411082e-06\n",
      "step: 9920 train: 0.12172102928161621 elapsed, loss: 4.6370305e-06\n",
      "step: 9930 train: 0.12479662895202637 elapsed, loss: 4.604434e-06\n",
      "step: 9940 train: 0.11840200424194336 elapsed, loss: 5.366676e-06\n",
      "step: 9950 train: 0.12318253517150879 elapsed, loss: 4.9233786e-06\n",
      "step: 9960 train: 0.1247565746307373 elapsed, loss: 3.6214342e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9970 train: 0.1165320873260498 elapsed, loss: 5.3345657e-06\n",
      "step: 9980 train: 0.13518786430358887 elapsed, loss: 3.5692728e-06\n",
      "step: 9990 train: 0.13233017921447754 elapsed, loss: 3.5725272e-06\n",
      "step: 10000 train: 0.13027048110961914 elapsed, loss: 5.311262e-06\n",
      "step: 10010 train: 0.14054298400878906 elapsed, loss: 3.111536e-06\n",
      "step: 10020 train: 0.1333014965057373 elapsed, loss: 3.6861388e-06\n",
      "step: 10030 train: 0.11685919761657715 elapsed, loss: 5.2437863e-06\n",
      "step: 10040 train: 0.13116240501403809 elapsed, loss: 4.048891e-06\n",
      "step: 10050 train: 0.1291215419769287 elapsed, loss: 3.9301603e-06\n",
      "step: 10060 train: 0.1311037540435791 elapsed, loss: 4.7338713e-06\n",
      "step: 10070 train: 0.13548541069030762 elapsed, loss: 3.800205e-06\n",
      "step: 10080 train: 0.1339893341064453 elapsed, loss: 3.358331e-06\n",
      "step: 10090 train: 0.12928438186645508 elapsed, loss: 3.7071145e-06\n",
      "step: 10100 train: 0.13136053085327148 elapsed, loss: 4.273346e-06\n",
      "step: 10110 train: 0.12568402290344238 elapsed, loss: 6.04188e-06\n",
      "step: 10120 train: 0.12316441535949707 elapsed, loss: 4.783182e-06\n",
      "step: 10130 train: 0.12579035758972168 elapsed, loss: 4.4940534e-06\n",
      "step: 10140 train: 0.1329338550567627 elapsed, loss: 5.4522097e-06\n",
      "step: 10150 train: 0.13257622718811035 elapsed, loss: 3.3411084e-06\n",
      "step: 10160 train: 0.12958407402038574 elapsed, loss: 3.9380784e-06\n",
      "step: 10170 train: 0.1282496452331543 elapsed, loss: 4.930841e-06\n",
      "step: 10180 train: 0.12628984451293945 elapsed, loss: 3.3746342e-06\n",
      "step: 10190 train: 0.13039851188659668 elapsed, loss: 4.0842965e-06\n",
      "step: 10200 train: 0.13299965858459473 elapsed, loss: 3.1408695e-06\n",
      "step: 10210 train: 0.13248372077941895 elapsed, loss: 6.871396e-06\n",
      "step: 10220 train: 0.1159980297088623 elapsed, loss: 6.437606e-06\n",
      "step: 10230 train: 0.13404369354248047 elapsed, loss: 2.7557753e-06\n",
      "step: 10240 train: 0.13514494895935059 elapsed, loss: 4.19977e-06\n",
      "step: 10250 train: 0.12427377700805664 elapsed, loss: 3.901293e-06\n",
      "step: 10260 train: 0.127488374710083 elapsed, loss: 3.7080442e-06\n",
      "step: 10270 train: 0.12725234031677246 elapsed, loss: 3.6270224e-06\n",
      "step: 10280 train: 0.14027762413024902 elapsed, loss: 2.661247e-06\n",
      "step: 10290 train: 0.12592649459838867 elapsed, loss: 4.283134e-06\n",
      "step: 10300 train: 0.12702131271362305 elapsed, loss: 4.9010396e-06\n",
      "step: 10310 train: 0.1459484100341797 elapsed, loss: 3.5627545e-06\n",
      "step: 10320 train: 0.13119006156921387 elapsed, loss: 3.017941e-06\n",
      "step: 10330 train: 0.14189720153808594 elapsed, loss: 4.9846863e-06\n",
      "step: 10340 train: 0.13022232055664062 elapsed, loss: 3.051467e-06\n",
      "step: 10350 train: 0.13525891304016113 elapsed, loss: 5.6501904e-06\n",
      "step: 10360 train: 0.14333748817443848 elapsed, loss: 4.131566e-05\n",
      "step: 10370 train: 0.12837958335876465 elapsed, loss: 3.4200966e-05\n",
      "step: 10380 train: 0.12737083435058594 elapsed, loss: 7.0779342e-06\n",
      "step: 10390 train: 0.11709904670715332 elapsed, loss: 6.3478337e-06\n",
      "step: 10400 train: 0.12522220611572266 elapsed, loss: 5.4746442e-06\n",
      "step: 10410 train: 0.13284754753112793 elapsed, loss: 4.395336e-06\n",
      "step: 10420 train: 0.14040899276733398 elapsed, loss: 4.055425e-06\n",
      "step: 10430 train: 0.12691569328308105 elapsed, loss: 4.266836e-06\n",
      "step: 10440 train: 0.12010598182678223 elapsed, loss: 7.4257055e-06\n",
      "step: 10450 train: 0.12929153442382812 elapsed, loss: 3.6395709e-06\n",
      "step: 10460 train: 0.11907100677490234 elapsed, loss: 4.6100195e-06\n",
      "step: 10470 train: 0.12168574333190918 elapsed, loss: 3.722947e-06\n",
      "step: 10480 train: 0.12788605690002441 elapsed, loss: 3.3611307e-06\n",
      "step: 10490 train: 0.12152814865112305 elapsed, loss: 4.469387e-06\n",
      "step: 10500 train: 0.11976981163024902 elapsed, loss: 5.0891676e-06\n",
      "step: 10510 train: 0.1297144889831543 elapsed, loss: 3.2391222e-06\n",
      "step: 10520 train: 0.12456107139587402 elapsed, loss: 3.4379632e-06\n",
      "step: 10530 train: 0.12207794189453125 elapsed, loss: 3.8030407e-06\n",
      "step: 10540 train: 0.12678861618041992 elapsed, loss: 2.8186378e-06\n",
      "step: 10550 train: 0.12775015830993652 elapsed, loss: 3.3792871e-06\n",
      "step: 10560 train: 0.1363673210144043 elapsed, loss: 3.4011773e-06\n",
      "step: 10570 train: 0.13099074363708496 elapsed, loss: 4.5257375e-06\n",
      "step: 10580 train: 0.1416482925415039 elapsed, loss: 2.8009217e-06\n",
      "step: 10590 train: 0.13412261009216309 elapsed, loss: 2.4982658e-06\n",
      "step: 10600 train: 0.12296009063720703 elapsed, loss: 4.7939366e-06\n",
      "step: 10610 train: 0.13956165313720703 elapsed, loss: 4.6553287e-06\n",
      "step: 10620 train: 0.13324189186096191 elapsed, loss: 4.9284813e-06\n",
      "step: 10630 train: 0.1281139850616455 elapsed, loss: 4.073121e-06\n",
      "step: 10640 train: 0.13150525093078613 elapsed, loss: 2.5974523e-06\n",
      "step: 10650 train: 0.12712526321411133 elapsed, loss: 4.0386544e-06\n",
      "step: 10660 train: 0.11750984191894531 elapsed, loss: 4.262175e-06\n",
      "step: 10670 train: 0.11954808235168457 elapsed, loss: 7.2292596e-06\n",
      "step: 10680 train: 0.13808631896972656 elapsed, loss: 1.979154e-05\n",
      "step: 10690 train: 0.12276625633239746 elapsed, loss: 1.9490566e-05\n",
      "step: 10700 train: 0.12937140464782715 elapsed, loss: 1.3785134e-05\n",
      "step: 10710 train: 0.14556288719177246 elapsed, loss: 1.51573395e-05\n",
      "step: 10720 train: 0.1280367374420166 elapsed, loss: 1.0622576e-05\n",
      "step: 10730 train: 0.11992955207824707 elapsed, loss: 7.802956e-06\n",
      "step: 10740 train: 0.1343851089477539 elapsed, loss: 4.045596e-06\n",
      "step: 10750 train: 0.12359166145324707 elapsed, loss: 4.316191e-06\n",
      "step: 10760 train: 0.12824797630310059 elapsed, loss: 1.9564648e-05\n",
      "step: 10770 train: 0.12631750106811523 elapsed, loss: 7.1216373e-06\n",
      "step: 10780 train: 0.14099407196044922 elapsed, loss: 2.767418e-06\n",
      "step: 10790 train: 0.13434791564941406 elapsed, loss: 5.4407046e-06\n",
      "step: 10800 train: 0.11956620216369629 elapsed, loss: 4.4074604e-06\n",
      "step: 10810 train: 0.12746644020080566 elapsed, loss: 4.0451796e-06\n",
      "step: 10820 train: 0.11858272552490234 elapsed, loss: 4.2468123e-06\n",
      "step: 10830 train: 0.13393926620483398 elapsed, loss: 3.239038e-05\n",
      "step: 10840 train: 0.11729264259338379 elapsed, loss: 5.480788e-06\n",
      "step: 10850 train: 0.12518692016601562 elapsed, loss: 4.447915e-06\n",
      "step: 10860 train: 0.12712526321411133 elapsed, loss: 3.3634456e-06\n",
      "step: 10870 train: 0.12606072425842285 elapsed, loss: 4.2570014e-06\n",
      "step: 10880 train: 0.1276540756225586 elapsed, loss: 3.5855744e-06\n",
      "step: 10890 train: 0.13269710540771484 elapsed, loss: 2.6416883e-06\n",
      "step: 10900 train: 0.11839413642883301 elapsed, loss: 5.011866e-06\n",
      "step: 10910 train: 0.12272858619689941 elapsed, loss: 3.43843e-06\n",
      "step: 10920 train: 0.12523412704467773 elapsed, loss: 3.976209e-06\n",
      "step: 10930 train: 0.12828564643859863 elapsed, loss: 2.8037307e-06\n",
      "step: 10940 train: 0.13158702850341797 elapsed, loss: 3.4072173e-06\n",
      "step: 10950 train: 0.125715970993042 elapsed, loss: 3.665651e-06\n",
      "step: 10960 train: 0.12691760063171387 elapsed, loss: 2.508511e-06\n",
      "step: 10970 train: 0.1321098804473877 elapsed, loss: 3.02353e-06\n",
      "step: 10980 train: 0.1249227523803711 elapsed, loss: 3.223763e-06\n",
      "step: 10990 train: 0.12312483787536621 elapsed, loss: 3.1390134e-06\n",
      "step: 11000 train: 0.12132644653320312 elapsed, loss: 4.418166e-06\n",
      "step: 11010 train: 0.13474130630493164 elapsed, loss: 2.8870857e-06\n",
      "step: 11020 train: 0.13512158393859863 elapsed, loss: 2.9466892e-06\n",
      "step: 11030 train: 0.13003849983215332 elapsed, loss: 3.6679862e-06\n",
      "step: 11040 train: 0.13209199905395508 elapsed, loss: 3.046348e-06\n",
      "step: 11050 train: 0.11837410926818848 elapsed, loss: 5.1748334e-06\n",
      "step: 11060 train: 0.12047338485717773 elapsed, loss: 4.019099e-06\n",
      "step: 11070 train: 0.12722325325012207 elapsed, loss: 3.710375e-06\n",
      "step: 11080 train: 0.1251201629638672 elapsed, loss: 4.067998e-06\n",
      "step: 11090 train: 0.13622760772705078 elapsed, loss: 2.386975e-06\n",
      "step: 11100 train: 0.13150906562805176 elapsed, loss: 4.620378e-06\n",
      "step: 11110 train: 0.135148286819458 elapsed, loss: 2.6686985e-06\n",
      "step: 11120 train: 0.13153553009033203 elapsed, loss: 4.649103e-06\n",
      "step: 11130 train: 0.13841962814331055 elapsed, loss: 5.7861944e-06\n",
      "step: 11140 train: 0.12938690185546875 elapsed, loss: 2.9238781e-06\n",
      "step: 11150 train: 0.12852096557617188 elapsed, loss: 5.7027e-06\n",
      "step: 11160 train: 0.12350630760192871 elapsed, loss: 4.251934e-06\n",
      "step: 11170 train: 0.1250603199005127 elapsed, loss: 3.3955907e-06\n",
      "step: 11180 train: 0.12121033668518066 elapsed, loss: 3.887787e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 11190 train: 0.1319746971130371 elapsed, loss: 2.5415648e-06\n",
      "step: 11200 train: 0.13038301467895508 elapsed, loss: 7.086877e-06\n",
      "step: 11210 train: 0.1253354549407959 elapsed, loss: 2.9480912e-06\n",
      "step: 11220 train: 0.12652325630187988 elapsed, loss: 4.199306e-06\n",
      "step: 11230 train: 0.13299894332885742 elapsed, loss: 2.8973332e-06\n",
      "step: 11240 train: 0.12624001502990723 elapsed, loss: 3.1171162e-06\n",
      "step: 11250 train: 0.13596558570861816 elapsed, loss: 4.105611e-06\n",
      "step: 11260 train: 0.12788963317871094 elapsed, loss: 3.9194565e-06\n",
      "step: 11270 train: 0.1282792091369629 elapsed, loss: 7.323092e-06\n",
      "step: 11280 train: 0.13808917999267578 elapsed, loss: 2.5546121e-06\n",
      "step: 11290 train: 0.1257784366607666 elapsed, loss: 4.39023e-06\n",
      "step: 11300 train: 0.12737703323364258 elapsed, loss: 4.35484e-06\n",
      "step: 11310 train: 0.1275646686553955 elapsed, loss: 3.7415768e-06\n",
      "step: 11320 train: 0.1318671703338623 elapsed, loss: 2.6142152e-06\n",
      "step: 11330 train: 0.11878037452697754 elapsed, loss: 4.173232e-06\n",
      "step: 11340 train: 0.1303730010986328 elapsed, loss: 3.0537922e-06\n",
      "step: 11350 train: 0.12242984771728516 elapsed, loss: 3.933422e-06\n",
      "step: 11360 train: 0.12385153770446777 elapsed, loss: 3.872893e-06\n",
      "step: 11370 train: 0.126495361328125 elapsed, loss: 4.9094097e-06\n",
      "step: 11380 train: 0.13029932975769043 elapsed, loss: 3.7457673e-06\n",
      "step: 11390 train: 0.12517476081848145 elapsed, loss: 3.8747517e-06\n",
      "step: 11400 train: 0.12099337577819824 elapsed, loss: 4.659824e-06\n",
      "step: 11410 train: 0.12041258811950684 elapsed, loss: 7.342948e-06\n",
      "step: 11420 train: 0.13129496574401855 elapsed, loss: 3.9779294e-05\n",
      "step: 11430 train: 0.13223981857299805 elapsed, loss: 1.7816605e-05\n",
      "step: 11440 train: 0.12923669815063477 elapsed, loss: 1.0689507e-05\n",
      "step: 11450 train: 0.12684226036071777 elapsed, loss: 1.3019793e-05\n",
      "step: 11460 train: 0.1314864158630371 elapsed, loss: 8.601109e-06\n",
      "step: 11470 train: 0.1317737102508545 elapsed, loss: 6.950364e-06\n",
      "step: 11480 train: 0.13098406791687012 elapsed, loss: 6.710564e-06\n",
      "step: 11490 train: 0.12163639068603516 elapsed, loss: 1.0041176e-05\n",
      "step: 11500 train: 0.12634968757629395 elapsed, loss: 7.682716e-06\n",
      "step: 11510 train: 0.1258561611175537 elapsed, loss: 7.641332e-06\n",
      "step: 11520 train: 0.12169027328491211 elapsed, loss: 7.943426e-06\n",
      "step: 11530 train: 0.12646865844726562 elapsed, loss: 5.2240953e-06\n",
      "step: 11540 train: 0.12009668350219727 elapsed, loss: 7.3807487e-06\n",
      "step: 11550 train: 0.126572847366333 elapsed, loss: 4.5420297e-06\n",
      "step: 11560 train: 0.12613844871520996 elapsed, loss: 6.742603e-06\n",
      "step: 11570 train: 0.12793517112731934 elapsed, loss: 2.645876e-06\n",
      "step: 11580 train: 0.11961889266967773 elapsed, loss: 4.739938e-06\n",
      "step: 11590 train: 0.11435341835021973 elapsed, loss: 4.893609e-06\n",
      "step: 11600 train: 0.12242460250854492 elapsed, loss: 3.974389e-06\n",
      "step: 11610 train: 0.12297391891479492 elapsed, loss: 3.998618e-06\n",
      "step: 11620 train: 0.12967276573181152 elapsed, loss: 2.2705601e-06\n",
      "step: 11630 train: 0.13440203666687012 elapsed, loss: 2.6216553e-06\n",
      "step: 11640 train: 0.13347172737121582 elapsed, loss: 4.4925882e-06\n",
      "step: 11650 train: 0.1280207633972168 elapsed, loss: 2.8111904e-06\n",
      "step: 11660 train: 0.12416625022888184 elapsed, loss: 3.1292316e-06\n",
      "step: 11670 train: 0.13345122337341309 elapsed, loss: 2.7399342e-06\n",
      "step: 11680 train: 0.13198280334472656 elapsed, loss: 3.6144345e-06\n",
      "step: 11690 train: 0.12393474578857422 elapsed, loss: 2.924344e-06\n",
      "step: 11700 train: 0.14026951789855957 elapsed, loss: 2.5983784e-06\n",
      "step: 11710 train: 0.12491464614868164 elapsed, loss: 5.4331626e-06\n",
      "step: 11720 train: 0.11954092979431152 elapsed, loss: 1.8045408e-05\n",
      "step: 11730 train: 0.11599063873291016 elapsed, loss: 5.004411e-06\n",
      "step: 11740 train: 0.12351822853088379 elapsed, loss: 4.917758e-06\n",
      "step: 11750 train: 0.13304400444030762 elapsed, loss: 3.7462123e-06\n",
      "step: 11760 train: 0.12392807006835938 elapsed, loss: 3.951577e-06\n",
      "step: 11770 train: 0.1378931999206543 elapsed, loss: 2.2840632e-06\n",
      "step: 11780 train: 0.1388998031616211 elapsed, loss: 2.7050185e-06\n",
      "step: 11790 train: 0.11640143394470215 elapsed, loss: 4.10339e-06\n",
      "step: 11800 train: 0.1206965446472168 elapsed, loss: 5.048591e-06\n",
      "step: 11810 train: 0.1211857795715332 elapsed, loss: 3.8500702e-06\n",
      "step: 11820 train: 0.12305068969726562 elapsed, loss: 3.4225973e-06\n",
      "step: 11830 train: 0.12838053703308105 elapsed, loss: 5.3526164e-06\n",
      "step: 11840 train: 0.1190643310546875 elapsed, loss: 3.8132864e-06\n",
      "step: 11850 train: 0.11489462852478027 elapsed, loss: 4.24914e-06\n",
      "step: 11860 train: 0.1344597339630127 elapsed, loss: 2.7175927e-06\n",
      "step: 11870 train: 0.12920784950256348 elapsed, loss: 4.0172404e-06\n",
      "step: 11880 train: 0.12494921684265137 elapsed, loss: 3.7192074e-06\n",
      "step: 11890 train: 0.12127828598022461 elapsed, loss: 4.264974e-06\n",
      "step: 11900 train: 0.1331801414489746 elapsed, loss: 2.8968652e-06\n",
      "step: 11910 train: 0.12165594100952148 elapsed, loss: 4.09687e-06\n",
      "step: 11920 train: 0.12268304824829102 elapsed, loss: 3.921777e-06\n",
      "step: 11930 train: 0.14606261253356934 elapsed, loss: 2.300826e-06\n",
      "step: 11940 train: 0.12482142448425293 elapsed, loss: 4.064721e-06\n",
      "step: 11950 train: 0.12366175651550293 elapsed, loss: 3.5459982e-06\n",
      "step: 11960 train: 0.1318671703338623 elapsed, loss: 2.7874346e-06\n",
      "step: 11970 train: 0.13055133819580078 elapsed, loss: 3.1078139e-06\n",
      "step: 11980 train: 0.14002442359924316 elapsed, loss: 2.4451797e-06\n",
      "step: 11990 train: 0.11518168449401855 elapsed, loss: 5.2209425e-06\n",
      "step: 12000 train: 0.12390279769897461 elapsed, loss: 4.1853355e-06\n",
      "step: 12010 train: 0.13334965705871582 elapsed, loss: 1.9841777e-06\n",
      "step: 12020 train: 0.11715841293334961 elapsed, loss: 5.1575876e-06\n",
      "step: 12030 train: 0.1245274543762207 elapsed, loss: 2.8540308e-06\n",
      "step: 12040 train: 0.129563570022583 elapsed, loss: 3.0319125e-06\n",
      "step: 12050 train: 0.12648820877075195 elapsed, loss: 3.967415e-06\n",
      "step: 12060 train: 0.13503623008728027 elapsed, loss: 2.8982602e-06\n",
      "step: 12070 train: 0.13664722442626953 elapsed, loss: 3.0179401e-06\n",
      "step: 12080 train: 0.12057256698608398 elapsed, loss: 5.2231644e-06\n",
      "step: 12090 train: 0.12405920028686523 elapsed, loss: 3.904548e-06\n",
      "step: 12100 train: 0.1290287971496582 elapsed, loss: 3.1082773e-06\n",
      "step: 12110 train: 0.13518571853637695 elapsed, loss: 3.4258546e-06\n",
      "step: 12120 train: 0.13422846794128418 elapsed, loss: 4.2407573e-06\n",
      "step: 12130 train: 0.13208317756652832 elapsed, loss: 2.7241074e-06\n",
      "step: 12140 train: 0.13419032096862793 elapsed, loss: 3.944601e-06\n",
      "step: 12150 train: 0.13136553764343262 elapsed, loss: 4.430268e-06\n",
      "step: 12160 train: 0.13621211051940918 elapsed, loss: 3.954836e-06\n",
      "step: 12170 train: 0.12404322624206543 elapsed, loss: 3.3033903e-06\n",
      "step: 12180 train: 0.1216738224029541 elapsed, loss: 3.9171287e-06\n",
      "step: 12190 train: 0.1300523281097412 elapsed, loss: 2.5872073e-06\n",
      "step: 12200 train: 0.1300351619720459 elapsed, loss: 4.1490066e-06\n",
      "step: 12210 train: 0.12802958488464355 elapsed, loss: 3.7550767e-06\n",
      "step: 12220 train: 0.1265411376953125 elapsed, loss: 4.608157e-06\n",
      "step: 12230 train: 0.12889695167541504 elapsed, loss: 5.4577267e-06\n",
      "step: 12240 train: 0.12249636650085449 elapsed, loss: 4.5960464e-06\n",
      "step: 12250 train: 0.13771748542785645 elapsed, loss: 3.0267865e-06\n",
      "step: 12260 train: 0.12674331665039062 elapsed, loss: 5.1650663e-06\n",
      "step: 12270 train: 0.13729500770568848 elapsed, loss: 3.6902381e-06\n",
      "step: 12280 train: 0.1227726936340332 elapsed, loss: 3.5543812e-06\n",
      "step: 12290 train: 0.13696861267089844 elapsed, loss: 4.0511304e-06\n",
      "step: 12300 train: 0.12506604194641113 elapsed, loss: 3.3769632e-06\n",
      "step: 12310 train: 0.11678886413574219 elapsed, loss: 4.4591548e-06\n",
      "step: 12320 train: 0.12653279304504395 elapsed, loss: 2.8125867e-06\n",
      "step: 12330 train: 0.11805057525634766 elapsed, loss: 4.673819e-06\n",
      "step: 12340 train: 0.12579917907714844 elapsed, loss: 4.593251e-06\n",
      "step: 12350 train: 0.12749171257019043 elapsed, loss: 4.7231433e-06\n",
      "step: 12360 train: 0.1372840404510498 elapsed, loss: 3.7378009e-06\n",
      "step: 12370 train: 0.12908339500427246 elapsed, loss: 3.2773112e-06\n",
      "step: 12380 train: 0.13055777549743652 elapsed, loss: 3.156227e-06\n",
      "step: 12390 train: 0.12732362747192383 elapsed, loss: 5.4477023e-06\n",
      "step: 12400 train: 0.12518930435180664 elapsed, loss: 3.8333083e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 12410 train: 0.12452554702758789 elapsed, loss: 4.0400555e-06\n",
      "step: 12420 train: 0.12212443351745605 elapsed, loss: 3.4901198e-06\n",
      "step: 12430 train: 0.13513875007629395 elapsed, loss: 3.939014e-06\n",
      "step: 12440 train: 0.1423046588897705 elapsed, loss: 3.5236421e-06\n",
      "step: 12450 train: 0.13080334663391113 elapsed, loss: 1.0061901e-05\n",
      "step: 12460 train: 0.13533878326416016 elapsed, loss: 4.3972384e-05\n",
      "step: 12470 train: 0.12096834182739258 elapsed, loss: 1.8648083e-05\n",
      "step: 12480 train: 0.1313614845275879 elapsed, loss: 1.1633817e-05\n",
      "step: 12490 train: 0.13187074661254883 elapsed, loss: 1.2277974e-05\n",
      "step: 12500 train: 0.12788772583007812 elapsed, loss: 1.2136381e-05\n",
      "step: 12510 train: 0.1332411766052246 elapsed, loss: 9.700963e-06\n",
      "step: 12520 train: 0.1355299949645996 elapsed, loss: 6.442347e-06\n",
      "step: 12530 train: 0.12427711486816406 elapsed, loss: 9.612906e-06\n",
      "step: 12540 train: 0.1182105541229248 elapsed, loss: 6.9089224e-06\n",
      "step: 12550 train: 0.13083791732788086 elapsed, loss: 4.539699e-06\n",
      "step: 12560 train: 0.13554668426513672 elapsed, loss: 2.7459976e-06\n",
      "step: 12570 train: 0.12166261672973633 elapsed, loss: 5.4588913e-06\n",
      "step: 12580 train: 0.12574076652526855 elapsed, loss: 2.987208e-06\n",
      "step: 12590 train: 0.1322484016418457 elapsed, loss: 3.295932e-06\n",
      "step: 12600 train: 0.13161444664001465 elapsed, loss: 5.980819e-06\n",
      "step: 12610 train: 0.13273358345031738 elapsed, loss: 4.5005804e-06\n",
      "step: 12620 train: 0.1368114948272705 elapsed, loss: 4.093071e-06\n",
      "step: 12630 train: 0.1292438507080078 elapsed, loss: 2.4321423e-06\n",
      "step: 12640 train: 0.13188743591308594 elapsed, loss: 3.7750528e-06\n",
      "step: 12650 train: 0.12473535537719727 elapsed, loss: 3.5781172e-06\n",
      "step: 12660 train: 0.13632988929748535 elapsed, loss: 4.7342364e-06\n",
      "step: 12670 train: 0.13171935081481934 elapsed, loss: 2.61375e-06\n",
      "step: 12680 train: 0.12320280075073242 elapsed, loss: 3.1888403e-06\n",
      "step: 12690 train: 0.1274256706237793 elapsed, loss: 3.2083872e-06\n",
      "step: 12700 train: 0.13562703132629395 elapsed, loss: 2.7953545e-06\n",
      "step: 12710 train: 0.11752796173095703 elapsed, loss: 3.818874e-06\n",
      "step: 12720 train: 0.11869144439697266 elapsed, loss: 3.702456e-06\n",
      "step: 12730 train: 0.1334218978881836 elapsed, loss: 2.8791655e-06\n",
      "step: 12740 train: 0.12285733222961426 elapsed, loss: 3.433306e-06\n",
      "step: 12750 train: 0.12460064888000488 elapsed, loss: 3.5934877e-06\n",
      "step: 12760 train: 0.12713146209716797 elapsed, loss: 2.855887e-06\n",
      "step: 12770 train: 0.12859630584716797 elapsed, loss: 3.5287587e-06\n",
      "step: 12780 train: 0.11668777465820312 elapsed, loss: 4.4228245e-06\n",
      "step: 12790 train: 0.13188433647155762 elapsed, loss: 2.5066488e-06\n",
      "step: 12800 train: 0.13627839088439941 elapsed, loss: 2.7455144e-06\n",
      "step: 12810 train: 0.13250732421875 elapsed, loss: 2.8181685e-06\n",
      "step: 12820 train: 0.13216257095336914 elapsed, loss: 3.0258534e-06\n",
      "step: 12830 train: 0.14107632637023926 elapsed, loss: 2.5671843e-06\n",
      "step: 12840 train: 0.13042330741882324 elapsed, loss: 3.3634583e-06\n",
      "step: 12850 train: 0.12250542640686035 elapsed, loss: 3.7541386e-06\n",
      "step: 12860 train: 0.13312697410583496 elapsed, loss: 4.318492e-06\n",
      "step: 12870 train: 0.12643074989318848 elapsed, loss: 4.740861e-06\n",
      "step: 12880 train: 0.1378772258758545 elapsed, loss: 3.0197955e-06\n",
      "step: 12890 train: 0.1304914951324463 elapsed, loss: 2.8000136e-06\n",
      "step: 12900 train: 0.1194601058959961 elapsed, loss: 3.87382e-06\n",
      "step: 12910 train: 0.12830138206481934 elapsed, loss: 4.142017e-06\n",
      "step: 12920 train: 0.13774847984313965 elapsed, loss: 3.077078e-06\n",
      "step: 12930 train: 0.14123082160949707 elapsed, loss: 2.0647385e-06\n",
      "step: 12940 train: 0.12444472312927246 elapsed, loss: 2.8973368e-06\n",
      "step: 12950 train: 0.12740015983581543 elapsed, loss: 3.4933391e-06\n",
      "step: 12960 train: 0.1268928050994873 elapsed, loss: 3.2959415e-06\n",
      "step: 12970 train: 0.14006638526916504 elapsed, loss: 2.2295774e-06\n",
      "step: 12980 train: 0.12307071685791016 elapsed, loss: 4.5178044e-06\n",
      "step: 12990 train: 0.13068056106567383 elapsed, loss: 3.4584564e-06\n",
      "step: 13000 train: 0.13203048706054688 elapsed, loss: 4.3519017e-06\n",
      "step: 13010 train: 0.12501764297485352 elapsed, loss: 3.5641567e-06\n",
      "step: 13020 train: 0.11160063743591309 elapsed, loss: 4.271027e-06\n",
      "step: 13030 train: 0.1308908462524414 elapsed, loss: 2.668232e-06\n",
      "step: 13040 train: 0.13821864128112793 elapsed, loss: 3.145521e-06\n",
      "step: 13050 train: 0.12324023246765137 elapsed, loss: 4.3203827e-06\n",
      "step: 13060 train: 0.12890005111694336 elapsed, loss: 3.7043121e-06\n",
      "step: 13070 train: 0.12992215156555176 elapsed, loss: 4.009307e-06\n",
      "step: 13080 train: 0.13643145561218262 elapsed, loss: 3.829996e-06\n",
      "step: 13090 train: 0.13163399696350098 elapsed, loss: 3.9129345e-06\n",
      "step: 13100 train: 0.1192178726196289 elapsed, loss: 3.8752187e-06\n",
      "step: 13110 train: 0.1383521556854248 elapsed, loss: 2.8344728e-06\n",
      "step: 13120 train: 0.13081073760986328 elapsed, loss: 4.6908563e-06\n",
      "step: 13130 train: 0.12037873268127441 elapsed, loss: 4.7972167e-06\n",
      "step: 13140 train: 0.12122344970703125 elapsed, loss: 5.5059063e-06\n",
      "step: 13150 train: 0.1306447982788086 elapsed, loss: 2.9979185e-06\n",
      "step: 13160 train: 0.12936902046203613 elapsed, loss: 3.437493e-06\n",
      "step: 13170 train: 0.12317395210266113 elapsed, loss: 3.3755632e-06\n",
      "step: 13180 train: 0.1369616985321045 elapsed, loss: 3.579387e-06\n",
      "step: 13190 train: 0.1342942714691162 elapsed, loss: 7.252345e-05\n",
      "step: 13200 train: 0.13268828392028809 elapsed, loss: 3.4491327e-06\n",
      "step: 13210 train: 0.1288456916809082 elapsed, loss: 3.819791e-06\n",
      "step: 13220 train: 0.13546133041381836 elapsed, loss: 3.154376e-06\n",
      "step: 13230 train: 0.13955044746398926 elapsed, loss: 2.7157294e-06\n",
      "step: 13240 train: 0.14313673973083496 elapsed, loss: 2.459617e-06\n",
      "step: 13250 train: 0.12173652648925781 elapsed, loss: 5.214769e-06\n",
      "step: 13260 train: 0.1277608871459961 elapsed, loss: 4.464721e-06\n",
      "step: 13270 train: 0.12499880790710449 elapsed, loss: 3.5795165e-06\n",
      "step: 13280 train: 0.13097524642944336 elapsed, loss: 3.5161968e-06\n",
      "step: 13290 train: 0.1405026912689209 elapsed, loss: 2.5136328e-06\n",
      "step: 13300 train: 0.1409158706665039 elapsed, loss: 3.311296e-06\n",
      "step: 13310 train: 0.13491463661193848 elapsed, loss: 2.8461154e-06\n",
      "step: 13320 train: 0.13004088401794434 elapsed, loss: 5.592527e-06\n",
      "step: 13330 train: 0.1321854591369629 elapsed, loss: 4.520589e-06\n",
      "step: 13340 train: 0.12464427947998047 elapsed, loss: 4.137361e-06\n",
      "step: 13350 train: 0.12254929542541504 elapsed, loss: 3.946e-06\n",
      "step: 13360 train: 0.13509821891784668 elapsed, loss: 2.1317933e-06\n",
      "step: 13370 train: 0.1457076072692871 elapsed, loss: 3.4300474e-06\n",
      "step: 13380 train: 0.13341736793518066 elapsed, loss: 4.6807754e-06\n",
      "step: 13390 train: 0.12664484977722168 elapsed, loss: 6.4012197e-06\n",
      "step: 13400 train: 0.12278318405151367 elapsed, loss: 5.2935757e-06\n",
      "step: 13410 train: 0.12598037719726562 elapsed, loss: 8.555615e-06\n",
      "step: 13420 train: 0.1226966381072998 elapsed, loss: 6.1171704e-06\n",
      "step: 13430 train: 0.12362480163574219 elapsed, loss: 3.302457e-06\n",
      "step: 13440 train: 0.12776875495910645 elapsed, loss: 3.7471407e-06\n",
      "step: 13450 train: 0.12854456901550293 elapsed, loss: 3.3001302e-06\n",
      "step: 13460 train: 0.13566946983337402 elapsed, loss: 3.3955894e-06\n",
      "step: 13470 train: 0.1302628517150879 elapsed, loss: 3.0561234e-06\n",
      "step: 13480 train: 0.14530372619628906 elapsed, loss: 2.4037374e-06\n",
      "step: 13490 train: 0.13257360458374023 elapsed, loss: 4.1307776e-06\n",
      "step: 13500 train: 0.12270164489746094 elapsed, loss: 4.2593874e-06\n",
      "step: 13510 train: 0.1273179054260254 elapsed, loss: 4.0214277e-06\n",
      "step: 13520 train: 0.12889575958251953 elapsed, loss: 3.0170088e-06\n",
      "step: 13530 train: 0.12816572189331055 elapsed, loss: 3.0412255e-06\n",
      "step: 13540 train: 0.1269550323486328 elapsed, loss: 3.7476107e-06\n",
      "step: 13550 train: 0.13594579696655273 elapsed, loss: 2.959734e-06\n",
      "step: 13560 train: 0.11842966079711914 elapsed, loss: 3.7257432e-06\n",
      "step: 13570 train: 0.12104010581970215 elapsed, loss: 3.6964057e-06\n",
      "step: 13580 train: 0.12168741226196289 elapsed, loss: 1.4425415e-05\n",
      "step: 13590 train: 0.12854576110839844 elapsed, loss: 1.1372266e-05\n",
      "step: 13600 train: 0.12483716011047363 elapsed, loss: 9.886694e-06\n",
      "step: 13610 train: 0.1190040111541748 elapsed, loss: 9.313504e-06\n",
      "step: 13620 train: 0.13538670539855957 elapsed, loss: 5.193944e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13630 train: 0.1314702033996582 elapsed, loss: 5.809933e-06\n",
      "step: 13640 train: 0.12422680854797363 elapsed, loss: 7.804885e-06\n",
      "step: 13650 train: 0.13389086723327637 elapsed, loss: 4.323645e-06\n",
      "step: 13660 train: 0.12641048431396484 elapsed, loss: 4.166719e-06\n",
      "step: 13670 train: 0.12693476676940918 elapsed, loss: 4.705005e-06\n",
      "step: 13680 train: 0.13488364219665527 elapsed, loss: 2.7604256e-06\n",
      "step: 13690 train: 0.13445281982421875 elapsed, loss: 3.3345646e-06\n",
      "step: 13700 train: 0.12802648544311523 elapsed, loss: 2.8274876e-06\n",
      "step: 13710 train: 0.12315058708190918 elapsed, loss: 4.814906e-06\n",
      "step: 13720 train: 0.13676071166992188 elapsed, loss: 3.81142e-06\n",
      "step: 13730 train: 0.12573862075805664 elapsed, loss: 2.65566e-06\n",
      "step: 13740 train: 0.1369609832763672 elapsed, loss: 5.467733e-06\n",
      "step: 13750 train: 0.1347179412841797 elapsed, loss: 5.9359604e-06\n",
      "step: 13760 train: 0.1284012794494629 elapsed, loss: 3.2721914e-06\n",
      "step: 13770 train: 0.13871097564697266 elapsed, loss: 2.867067e-06\n",
      "step: 13780 train: 0.1308145523071289 elapsed, loss: 2.582086e-06\n",
      "step: 13790 train: 0.12325096130371094 elapsed, loss: 3.1259701e-06\n",
      "step: 13800 train: 0.12696456909179688 elapsed, loss: 3.0002466e-06\n",
      "step: 13810 train: 0.1345987319946289 elapsed, loss: 4.1904364e-06\n",
      "step: 13820 train: 0.12771058082580566 elapsed, loss: 4.6961086e-06\n",
      "step: 13830 train: 0.12081432342529297 elapsed, loss: 3.8412227e-06\n",
      "step: 13840 train: 0.12991690635681152 elapsed, loss: 2.857288e-06\n",
      "step: 13850 train: 0.12865567207336426 elapsed, loss: 5.106686e-06\n",
      "step: 13860 train: 0.13318753242492676 elapsed, loss: 3.954845e-06\n",
      "step: 13870 train: 0.1326732635498047 elapsed, loss: 2.675216e-06\n",
      "step: 13880 train: 0.12953448295593262 elapsed, loss: 3.2069947e-06\n",
      "step: 13890 train: 0.1349954605102539 elapsed, loss: 2.387439e-06\n",
      "step: 13900 train: 0.13646745681762695 elapsed, loss: 2.798151e-06\n",
      "step: 13910 train: 0.12541675567626953 elapsed, loss: 4.7552558e-06\n",
      "step: 13920 train: 0.13765430450439453 elapsed, loss: 4.9395476e-06\n",
      "step: 13930 train: 0.11945891380310059 elapsed, loss: 3.7187579e-06\n",
      "step: 13940 train: 0.13548946380615234 elapsed, loss: 3.2060416e-06\n",
      "step: 13950 train: 0.135634183883667 elapsed, loss: 2.4232936e-06\n",
      "step: 13960 train: 0.1274864673614502 elapsed, loss: 3.1888312e-06\n",
      "step: 13970 train: 0.13434314727783203 elapsed, loss: 4.7254825e-06\n",
      "step: 13980 train: 0.12996506690979004 elapsed, loss: 3.6232886e-06\n",
      "step: 13990 train: 0.13312220573425293 elapsed, loss: 5.2602904e-06\n",
      "step: 14000 train: 0.12388920783996582 elapsed, loss: 2.7976826e-06\n",
      "step: 14010 train: 0.12546944618225098 elapsed, loss: 4.623047e-06\n",
      "step: 14020 train: 0.1274721622467041 elapsed, loss: 2.9257405e-06\n",
      "step: 14030 train: 0.13138532638549805 elapsed, loss: 3.0724254e-06\n",
      "step: 14040 train: 0.12539172172546387 elapsed, loss: 2.8945365e-06\n",
      "step: 14050 train: 0.13245892524719238 elapsed, loss: 3.801494e-06\n",
      "step: 14060 train: 0.12786078453063965 elapsed, loss: 5.2032683e-06\n",
      "step: 14070 train: 0.13932275772094727 elapsed, loss: 3.0589183e-06\n",
      "step: 14080 train: 0.1298823356628418 elapsed, loss: 3.0640417e-06\n",
      "step: 14090 train: 0.12041425704956055 elapsed, loss: 4.8535594e-06\n",
      "step: 14100 train: 0.12489438056945801 elapsed, loss: 5.1347997e-06\n",
      "step: 14110 train: 0.1203162670135498 elapsed, loss: 6.288595e-06\n",
      "step: 14120 train: 0.13782978057861328 elapsed, loss: 3.2596172e-06\n",
      "step: 14130 train: 0.12188124656677246 elapsed, loss: 5.558543e-06\n",
      "step: 14140 train: 0.12727880477905273 elapsed, loss: 4.503858e-06\n",
      "step: 14150 train: 0.12956857681274414 elapsed, loss: 3.2102594e-06\n",
      "step: 14160 train: 0.12453079223632812 elapsed, loss: 3.280563e-06\n",
      "step: 14170 train: 0.12291860580444336 elapsed, loss: 3.7518193e-06\n",
      "step: 14180 train: 0.1277451515197754 elapsed, loss: 3.9691595e-06\n",
      "step: 14190 train: 0.13074707984924316 elapsed, loss: 2.9220168e-06\n",
      "step: 14200 train: 0.13005852699279785 elapsed, loss: 2.848906e-06\n",
      "step: 14210 train: 0.1345813274383545 elapsed, loss: 3.0775445e-06\n",
      "step: 14220 train: 0.12622499465942383 elapsed, loss: 3.7168916e-06\n",
      "step: 14230 train: 0.125227689743042 elapsed, loss: 4.1508874e-06\n",
      "step: 14240 train: 0.14096784591674805 elapsed, loss: 2.9862763e-06\n",
      "step: 14250 train: 0.12944602966308594 elapsed, loss: 4.4990948e-06\n",
      "step: 14260 train: 0.12679004669189453 elapsed, loss: 6.8134177e-06\n",
      "step: 14270 train: 0.12514662742614746 elapsed, loss: 4.4097706e-06\n",
      "step: 14280 train: 0.12328100204467773 elapsed, loss: 4.0856935e-06\n",
      "step: 14290 train: 0.13162565231323242 elapsed, loss: 3.20048e-06\n",
      "step: 14300 train: 0.13597679138183594 elapsed, loss: 4.410724e-06\n",
      "step: 14310 train: 0.12699365615844727 elapsed, loss: 1.213592e-05\n",
      "step: 14320 train: 0.13787579536437988 elapsed, loss: 3.373702e-06\n",
      "step: 14330 train: 0.12771272659301758 elapsed, loss: 4.5522693e-06\n",
      "step: 14340 train: 0.12639141082763672 elapsed, loss: 2.9876733e-06\n",
      "step: 14350 train: 0.12898874282836914 elapsed, loss: 4.070319e-06\n",
      "step: 14360 train: 0.1250002384185791 elapsed, loss: 4.121549e-06\n",
      "step: 14370 train: 0.12848997116088867 elapsed, loss: 4.104321e-06\n",
      "step: 14380 train: 0.12371110916137695 elapsed, loss: 7.2859993e-06\n",
      "step: 14390 train: 0.13079309463500977 elapsed, loss: 4.022826e-06\n",
      "step: 14400 train: 0.12204718589782715 elapsed, loss: 5.03558e-06\n",
      "step: 14410 train: 0.12089776992797852 elapsed, loss: 4.852058e-06\n",
      "step: 14420 train: 0.12274026870727539 elapsed, loss: 4.7501144e-06\n",
      "step: 14430 train: 0.14502358436584473 elapsed, loss: 3.5427292e-06\n",
      "step: 14440 train: 0.13109254837036133 elapsed, loss: 3.5241035e-06\n",
      "step: 14450 train: 0.13668131828308105 elapsed, loss: 2.6780087e-06\n",
      "step: 14460 train: 0.13668179512023926 elapsed, loss: 3.4770824e-06\n",
      "step: 14470 train: 0.14104962348937988 elapsed, loss: 3.618164e-06\n",
      "step: 14480 train: 0.1300046443939209 elapsed, loss: 3.4463483e-06\n",
      "step: 14490 train: 0.11979126930236816 elapsed, loss: 3.2642765e-06\n",
      "step: 14500 train: 0.12884187698364258 elapsed, loss: 3.1571494e-06\n",
      "step: 14510 train: 0.13665175437927246 elapsed, loss: 3.069622e-06\n",
      "step: 14520 train: 0.12850713729858398 elapsed, loss: 3.2516923e-06\n",
      "step: 14530 train: 0.12428736686706543 elapsed, loss: 6.064997e-06\n",
      "step: 14540 train: 0.1349034309387207 elapsed, loss: 2.8512286e-06\n",
      "step: 14550 train: 0.14304351806640625 elapsed, loss: 2.7287674e-06\n",
      "step: 14560 train: 0.12344098091125488 elapsed, loss: 3.2568232e-06\n",
      "step: 14570 train: 0.1298215389251709 elapsed, loss: 2.6367134e-05\n",
      "step: 14580 train: 0.12665748596191406 elapsed, loss: 0.00015469608\n",
      "step: 14590 train: 0.12163782119750977 elapsed, loss: 3.0510055e-05\n",
      "step: 14600 train: 0.13178753852844238 elapsed, loss: 9.143425e-06\n",
      "step: 14610 train: 0.1351945400238037 elapsed, loss: 1.7597547e-05\n",
      "step: 14620 train: 0.12393450736999512 elapsed, loss: 1.4904269e-05\n",
      "step: 14630 train: 0.1185603141784668 elapsed, loss: 1.00520065e-05\n",
      "step: 14640 train: 0.13602495193481445 elapsed, loss: 4.981147e-06\n",
      "step: 14650 train: 0.13890790939331055 elapsed, loss: 4.536901e-06\n",
      "step: 14660 train: 0.12885189056396484 elapsed, loss: 4.058219e-06\n",
      "step: 14670 train: 0.13063979148864746 elapsed, loss: 5.247486e-06\n",
      "step: 14680 train: 0.1346292495727539 elapsed, loss: 3.7601835e-06\n",
      "step: 14690 train: 0.12499809265136719 elapsed, loss: 4.1420362e-06\n",
      "step: 14700 train: 0.1396315097808838 elapsed, loss: 3.7825528e-06\n",
      "step: 14710 train: 0.13857364654541016 elapsed, loss: 4.2225965e-06\n",
      "step: 14720 train: 0.13386750221252441 elapsed, loss: 4.0963914e-06\n",
      "step: 14730 train: 0.1374211311340332 elapsed, loss: 4.855344e-06\n",
      "step: 14740 train: 0.12236452102661133 elapsed, loss: 5.108257e-06\n",
      "step: 14750 train: 0.11906647682189941 elapsed, loss: 3.8114158e-06\n",
      "step: 14760 train: 0.13068389892578125 elapsed, loss: 2.4484411e-06\n",
      "step: 14770 train: 0.12146186828613281 elapsed, loss: 5.288798e-06\n",
      "step: 14780 train: 0.13287138938903809 elapsed, loss: 2.836788e-06\n",
      "step: 14790 train: 0.12703347206115723 elapsed, loss: 3.9338693e-06\n",
      "step: 14800 train: 0.12350130081176758 elapsed, loss: 3.2968646e-06\n",
      "step: 14810 train: 0.12240076065063477 elapsed, loss: 4.0528e-05\n",
      "step: 14820 train: 0.12713861465454102 elapsed, loss: 8.887173e-06\n",
      "step: 14830 train: 0.1277925968170166 elapsed, loss: 1.0272668e-05\n",
      "step: 14840 train: 0.12593746185302734 elapsed, loss: 1.2336278e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 14850 train: 0.1333613395690918 elapsed, loss: 4.474513e-06\n",
      "step: 14860 train: 0.12513518333435059 elapsed, loss: 5.328986e-06\n",
      "step: 14870 train: 0.12147998809814453 elapsed, loss: 4.202576e-06\n",
      "step: 14880 train: 0.12488436698913574 elapsed, loss: 6.5992017e-06\n",
      "step: 14890 train: 0.12908363342285156 elapsed, loss: 4.192789e-06\n",
      "step: 14900 train: 0.12238121032714844 elapsed, loss: 5.331318e-06\n",
      "step: 14910 train: 0.1348583698272705 elapsed, loss: 3.775097e-06\n",
      "step: 14920 train: 0.12348294258117676 elapsed, loss: 4.1057174e-06\n",
      "step: 14930 train: 0.13350558280944824 elapsed, loss: 3.4291015e-06\n",
      "step: 14940 train: 0.13979125022888184 elapsed, loss: 2.3520497e-06\n",
      "step: 14950 train: 0.1320033073425293 elapsed, loss: 2.7539131e-06\n",
      "step: 14960 train: 0.13049864768981934 elapsed, loss: 3.1976829e-06\n",
      "step: 14970 train: 0.13298702239990234 elapsed, loss: 2.7385438e-06\n",
      "step: 14980 train: 0.1291806697845459 elapsed, loss: 2.5704421e-06\n",
      "step: 14990 train: 0.1246633529663086 elapsed, loss: 2.845182e-06\n",
      "step: 15000 train: 0.12945890426635742 elapsed, loss: 3.958565e-06\n",
      "step: 15010 train: 0.12984371185302734 elapsed, loss: 3.048204e-06\n",
      "step: 15020 train: 0.12721943855285645 elapsed, loss: 1.4918293e-05\n",
      "step: 15030 train: 0.1243581771850586 elapsed, loss: 6.8134655e-06\n",
      "step: 15040 train: 0.1236426830291748 elapsed, loss: 5.7061507e-06\n",
      "step: 15050 train: 0.1266014575958252 elapsed, loss: 5.3261356e-06\n",
      "step: 15060 train: 0.1399991512298584 elapsed, loss: 3.0239912e-06\n",
      "step: 15070 train: 0.13242459297180176 elapsed, loss: 3.3215433e-06\n",
      "step: 15080 train: 0.1281449794769287 elapsed, loss: 2.4964038e-06\n",
      "step: 15090 train: 0.13377857208251953 elapsed, loss: 2.2957047e-06\n",
      "step: 15100 train: 0.14293837547302246 elapsed, loss: 4.180233e-06\n",
      "step: 15110 train: 0.1310582160949707 elapsed, loss: 4.4605463e-06\n",
      "step: 15120 train: 0.13431859016418457 elapsed, loss: 4.3929613e-06\n",
      "step: 15130 train: 0.13646721839904785 elapsed, loss: 7.943305e-06\n",
      "step: 15140 train: 0.1253981590270996 elapsed, loss: 4.4693834e-06\n",
      "step: 15150 train: 0.12454605102539062 elapsed, loss: 5.845871e-06\n",
      "step: 15160 train: 0.1349947452545166 elapsed, loss: 3.6954737e-06\n",
      "step: 15170 train: 0.13248538970947266 elapsed, loss: 3.3122383e-06\n",
      "step: 15180 train: 0.14178133010864258 elapsed, loss: 2.9573966e-06\n",
      "step: 15190 train: 0.12156009674072266 elapsed, loss: 4.446102e-06\n",
      "step: 15200 train: 0.12281441688537598 elapsed, loss: 2.5900013e-06\n",
      "step: 15210 train: 0.13213872909545898 elapsed, loss: 2.876373e-06\n",
      "step: 15220 train: 0.12192773818969727 elapsed, loss: 5.2395744e-06\n",
      "step: 15230 train: 0.11822628974914551 elapsed, loss: 3.7643806e-06\n",
      "step: 15240 train: 0.13924479484558105 elapsed, loss: 2.191398e-06\n",
      "step: 15250 train: 0.13476848602294922 elapsed, loss: 2.9364319e-06\n",
      "step: 15260 train: 0.12732863426208496 elapsed, loss: 4.6061014e-06\n",
      "step: 15270 train: 0.1231684684753418 elapsed, loss: 3.4076954e-06\n",
      "step: 15280 train: 0.1383678913116455 elapsed, loss: 2.1685792e-06\n",
      "step: 15290 train: 0.1228642463684082 elapsed, loss: 2.9965217e-06\n",
      "step: 15300 train: 0.11809730529785156 elapsed, loss: 3.248909e-06\n",
      "step: 15310 train: 0.12550044059753418 elapsed, loss: 3.7932373e-06\n",
      "step: 15320 train: 0.1260979175567627 elapsed, loss: 2.9979165e-06\n",
      "step: 15330 train: 0.13163447380065918 elapsed, loss: 2.7431875e-06\n",
      "step: 15340 train: 0.12587332725524902 elapsed, loss: 3.563672e-06\n",
      "step: 15350 train: 0.1436324119567871 elapsed, loss: 2.454024e-06\n",
      "step: 15360 train: 0.1338508129119873 elapsed, loss: 3.2092807e-06\n",
      "step: 15370 train: 0.1265544891357422 elapsed, loss: 3.1888335e-06\n",
      "step: 15380 train: 0.12429022789001465 elapsed, loss: 5.222288e-06\n",
      "step: 15390 train: 0.1313788890838623 elapsed, loss: 6.18018e-06\n",
      "step: 15400 train: 0.11382603645324707 elapsed, loss: 4.5518154e-06\n",
      "step: 15410 train: 0.134080171585083 elapsed, loss: 3.1851046e-06\n",
      "step: 15420 train: 0.1314232349395752 elapsed, loss: 4.079589e-06\n",
      "step: 15430 train: 0.1333622932434082 elapsed, loss: 2.601177e-06\n",
      "step: 15440 train: 0.12986230850219727 elapsed, loss: 2.5047768e-06\n",
      "step: 15450 train: 0.12328910827636719 elapsed, loss: 3.2768312e-06\n",
      "step: 15460 train: 0.1387031078338623 elapsed, loss: 3.968349e-06\n",
      "step: 15470 train: 0.12749361991882324 elapsed, loss: 3.2791745e-06\n",
      "step: 15480 train: 0.1355440616607666 elapsed, loss: 2.264039e-06\n",
      "step: 15490 train: 0.12548398971557617 elapsed, loss: 4.0977984e-06\n",
      "step: 15500 train: 0.12534666061401367 elapsed, loss: 3.0891874e-06\n",
      "step: 15510 train: 0.11669111251831055 elapsed, loss: 4.6440146e-06\n",
      "step: 15520 train: 0.12777471542358398 elapsed, loss: 2.689186e-06\n",
      "step: 15530 train: 0.13340282440185547 elapsed, loss: 2.9443659e-06\n",
      "step: 15540 train: 0.13815832138061523 elapsed, loss: 2.5327186e-06\n",
      "step: 15550 train: 0.1425321102142334 elapsed, loss: 2.2449476e-06\n",
      "step: 15560 train: 0.1346731185913086 elapsed, loss: 2.4712576e-06\n",
      "step: 15570 train: 0.13242769241333008 elapsed, loss: 2.8051213e-06\n",
      "step: 15580 train: 0.14074110984802246 elapsed, loss: 2.4586664e-06\n",
      "step: 15590 train: 0.1239924430847168 elapsed, loss: 3.8179332e-06\n",
      "step: 15600 train: 0.1352221965789795 elapsed, loss: 2.7804542e-06\n",
      "step: 15610 train: 0.12677550315856934 elapsed, loss: 3.1278355e-06\n",
      "step: 15620 train: 0.12590622901916504 elapsed, loss: 4.147605e-06\n",
      "step: 15630 train: 0.14276337623596191 elapsed, loss: 2.407929e-06\n",
      "step: 15640 train: 0.137007474899292 elapsed, loss: 2.2645067e-06\n",
      "step: 15650 train: 0.13873624801635742 elapsed, loss: 3.3490185e-06\n",
      "step: 15660 train: 0.13154029846191406 elapsed, loss: 2.4372662e-06\n",
      "step: 15670 train: 0.14130020141601562 elapsed, loss: 2.664945e-06\n",
      "step: 15680 train: 0.1404893398284912 elapsed, loss: 6.9942316e-06\n",
      "step: 15690 train: 0.12680768966674805 elapsed, loss: 4.3543787e-06\n",
      "step: 15700 train: 0.12913274765014648 elapsed, loss: 2.4791748e-06\n",
      "step: 15710 train: 0.1381359100341797 elapsed, loss: 2.9941916e-06\n",
      "step: 15720 train: 0.12654924392700195 elapsed, loss: 4.8344573e-06\n",
      "step: 15730 train: 0.14775633811950684 elapsed, loss: 3.0193007e-06\n",
      "step: 15740 train: 0.12970185279846191 elapsed, loss: 6.749919e-05\n",
      "step: 15750 train: 0.13689875602722168 elapsed, loss: 4.639678e-06\n",
      "step: 15760 train: 0.12540698051452637 elapsed, loss: 3.53296e-06\n",
      "step: 15770 train: 0.12332463264465332 elapsed, loss: 4.5499355e-06\n",
      "step: 15780 train: 0.13938355445861816 elapsed, loss: 2.349256e-06\n",
      "step: 15790 train: 0.12122750282287598 elapsed, loss: 4.9898727e-06\n",
      "step: 15800 train: 0.12461638450622559 elapsed, loss: 3.3736947e-06\n",
      "step: 15810 train: 0.13071894645690918 elapsed, loss: 2.6198036e-06\n",
      "step: 15820 train: 0.1331164836883545 elapsed, loss: 4.27518e-06\n",
      "step: 15830 train: 0.1381676197052002 elapsed, loss: 4.427871e-06\n",
      "step: 15840 train: 0.13874173164367676 elapsed, loss: 2.9234106e-06\n",
      "step: 15850 train: 0.1288139820098877 elapsed, loss: 3.5576354e-06\n",
      "step: 15860 train: 0.147383451461792 elapsed, loss: 2.8911757e-06\n",
      "step: 15870 train: 0.1300032138824463 elapsed, loss: 2.9052521e-06\n",
      "step: 15880 train: 0.13440632820129395 elapsed, loss: 2.6458797e-06\n",
      "step: 15890 train: 0.11649632453918457 elapsed, loss: 5.1688075e-06\n",
      "step: 15900 train: 0.1317129135131836 elapsed, loss: 3.0216665e-06\n",
      "step: 15910 train: 0.12173748016357422 elapsed, loss: 4.984829e-06\n",
      "step: 15920 train: 0.12290310859680176 elapsed, loss: 5.3508534e-06\n",
      "step: 15930 train: 0.11688709259033203 elapsed, loss: 5.0891454e-06\n",
      "step: 15940 train: 0.12277626991271973 elapsed, loss: 3.831448e-06\n",
      "step: 15950 train: 0.1324620246887207 elapsed, loss: 3.2903351e-06\n",
      "step: 15960 train: 0.13211607933044434 elapsed, loss: 3.2675148e-06\n",
      "step: 15970 train: 0.13640832901000977 elapsed, loss: 2.8931345e-06\n",
      "step: 15980 train: 0.13538146018981934 elapsed, loss: 3.4458762e-06\n",
      "step: 15990 train: 0.13364291191101074 elapsed, loss: 2.4661358e-06\n",
      "step: 16000 train: 0.13780951499938965 elapsed, loss: 2.0228288e-06\n",
      "step: 16010 train: 0.12569451332092285 elapsed, loss: 4.3888394e-06\n",
      "step: 16020 train: 0.13313794136047363 elapsed, loss: 2.6845262e-06\n",
      "step: 16030 train: 0.11544060707092285 elapsed, loss: 7.2465436e-06\n",
      "step: 16040 train: 0.13278865814208984 elapsed, loss: 3.2037246e-06\n",
      "step: 16050 train: 0.12240147590637207 elapsed, loss: 7.171941e-06\n",
      "step: 16060 train: 0.13118815422058105 elapsed, loss: 2.8642671e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 16070 train: 0.1332399845123291 elapsed, loss: 6.04131e-06\n",
      "step: 16080 train: 0.12677621841430664 elapsed, loss: 4.5709107e-06\n",
      "step: 16090 train: 0.1390669345855713 elapsed, loss: 2.680802e-06\n",
      "step: 16100 train: 0.13014554977416992 elapsed, loss: 4.3967557e-06\n",
      "step: 16110 train: 0.12256050109863281 elapsed, loss: 4.492666e-06\n",
      "step: 16120 train: 0.1292872428894043 elapsed, loss: 4.1010617e-06\n",
      "step: 16130 train: 0.12881183624267578 elapsed, loss: 3.6824044e-06\n",
      "step: 16140 train: 0.12306594848632812 elapsed, loss: 3.6130534e-06\n",
      "step: 16150 train: 0.1322178840637207 elapsed, loss: 3.8570142e-06\n",
      "step: 16160 train: 0.12994885444641113 elapsed, loss: 3.4174766e-06\n",
      "step: 16170 train: 0.14168334007263184 elapsed, loss: 3.8225717e-06\n",
      "step: 16180 train: 0.13044214248657227 elapsed, loss: 4.146695e-06\n",
      "step: 16190 train: 0.12543916702270508 elapsed, loss: 3.4421587e-06\n",
      "step: 16200 train: 0.12930059432983398 elapsed, loss: 2.9913936e-06\n",
      "step: 16210 train: 0.13195037841796875 elapsed, loss: 3.490586e-06\n",
      "step: 16220 train: 0.12797093391418457 elapsed, loss: 3.7657858e-06\n",
      "step: 16230 train: 0.1358962059020996 elapsed, loss: 2.2062989e-06\n",
      "step: 16240 train: 0.12522387504577637 elapsed, loss: 2.612353e-06\n",
      "step: 16250 train: 0.1286463737487793 elapsed, loss: 3.7588047e-06\n",
      "step: 16260 train: 0.12730813026428223 elapsed, loss: 6.192695e-06\n",
      "step: 16270 train: 0.11690950393676758 elapsed, loss: 4.9196856e-06\n",
      "step: 16280 train: 0.12295341491699219 elapsed, loss: 4.69707e-06\n",
      "step: 16290 train: 0.12819862365722656 elapsed, loss: 3.2931418e-06\n",
      "step: 16300 train: 0.12262439727783203 elapsed, loss: 4.925269e-06\n",
      "step: 16310 train: 0.13136601448059082 elapsed, loss: 6.3086945e-06\n",
      "step: 16320 train: 0.1354234218597412 elapsed, loss: 2.6975665e-06\n",
      "step: 16330 train: 0.13011932373046875 elapsed, loss: 3.0794085e-06\n",
      "step: 16340 train: 0.12003564834594727 elapsed, loss: 6.23047e-06\n",
      "step: 16350 train: 0.13143491744995117 elapsed, loss: 3.2139765e-06\n",
      "step: 16360 train: 0.14364027976989746 elapsed, loss: 2.6770788e-06\n",
      "step: 16370 train: 0.1353754997253418 elapsed, loss: 3.7010527e-06\n",
      "step: 16380 train: 0.1353132724761963 elapsed, loss: 3.5781293e-06\n",
      "step: 16390 train: 0.14171838760375977 elapsed, loss: 3.090579e-06\n",
      "step: 16400 train: 0.11968231201171875 elapsed, loss: 4.982985e-06\n",
      "step: 16410 train: 0.1347212791442871 elapsed, loss: 2.6938412e-06\n",
      "step: 16420 train: 0.13405632972717285 elapsed, loss: 4.6327077e-06\n",
      "step: 16430 train: 0.12238550186157227 elapsed, loss: 0.000118081065\n",
      "step: 16440 train: 0.12939238548278809 elapsed, loss: 4.5140505e-06\n",
      "step: 16450 train: 0.13174891471862793 elapsed, loss: 3.7965183e-06\n",
      "step: 16460 train: 0.1338043212890625 elapsed, loss: 4.363227e-06\n",
      "step: 16470 train: 0.1296372413635254 elapsed, loss: 5.683355e-06\n",
      "step: 16480 train: 0.1309657096862793 elapsed, loss: 3.3979093e-06\n",
      "step: 16490 train: 0.11924099922180176 elapsed, loss: 1.2080532e-05\n",
      "step: 16500 train: 0.12354373931884766 elapsed, loss: 5.212385e-06\n",
      "step: 16510 train: 0.12017941474914551 elapsed, loss: 5.5427076e-06\n",
      "step: 16520 train: 0.1311028003692627 elapsed, loss: 3.7583031e-06\n",
      "step: 16530 train: 0.12435674667358398 elapsed, loss: 4.5438965e-06\n",
      "step: 16540 train: 0.12795662879943848 elapsed, loss: 3.4104849e-06\n",
      "step: 16550 train: 0.12408661842346191 elapsed, loss: 3.2302823e-06\n",
      "step: 16560 train: 0.12150025367736816 elapsed, loss: 4.2044394e-06\n",
      "step: 16570 train: 0.126265287399292 elapsed, loss: 4.4767927e-05\n",
      "step: 16580 train: 0.11962127685546875 elapsed, loss: 7.135235e-06\n",
      "step: 16590 train: 0.12770438194274902 elapsed, loss: 4.1853355e-06\n",
      "step: 16600 train: 0.12101435661315918 elapsed, loss: 3.9133997e-06\n",
      "step: 16610 train: 0.1237325668334961 elapsed, loss: 4.4726494e-06\n",
      "step: 16620 train: 0.12018561363220215 elapsed, loss: 4.506182e-06\n",
      "step: 16630 train: 0.13417530059814453 elapsed, loss: 2.670085e-06\n",
      "step: 16640 train: 0.11981058120727539 elapsed, loss: 4.2612432e-06\n",
      "step: 16650 train: 0.12771105766296387 elapsed, loss: 3.118988e-06\n",
      "step: 16660 train: 0.1284325122833252 elapsed, loss: 2.8554264e-06\n",
      "step: 16670 train: 0.13156938552856445 elapsed, loss: 4.0768095e-06\n",
      "step: 16680 train: 0.13227200508117676 elapsed, loss: 3.2950047e-06\n",
      "step: 16690 train: 0.1374974250793457 elapsed, loss: 2.9285359e-06\n",
      "step: 16700 train: 0.12817144393920898 elapsed, loss: 3.3248111e-06\n",
      "step: 16710 train: 0.13792729377746582 elapsed, loss: 2.4749827e-06\n",
      "step: 16720 train: 0.12946438789367676 elapsed, loss: 7.602484e-06\n",
      "step: 16730 train: 0.13026213645935059 elapsed, loss: 2.8829e-06\n",
      "step: 16740 train: 0.13273143768310547 elapsed, loss: 3.2638068e-06\n",
      "step: 16750 train: 0.12834692001342773 elapsed, loss: 3.5091923e-06\n",
      "step: 16760 train: 0.1285865306854248 elapsed, loss: 3.3303945e-06\n",
      "step: 16770 train: 0.12921833992004395 elapsed, loss: 5.1725387e-06\n",
      "step: 16780 train: 0.14048552513122559 elapsed, loss: 2.348791e-06\n",
      "step: 16790 train: 0.12911725044250488 elapsed, loss: 2.8004788e-06\n",
      "step: 16800 train: 0.12831687927246094 elapsed, loss: 2.4088604e-06\n",
      "step: 16810 train: 0.12113809585571289 elapsed, loss: 3.957174e-06\n",
      "step: 16820 train: 0.12125301361083984 elapsed, loss: 1.5618145e-05\n",
      "step: 16830 train: 0.12403297424316406 elapsed, loss: 3.4137502e-06\n",
      "step: 16840 train: 0.13256192207336426 elapsed, loss: 3.5865123e-06\n",
      "step: 16850 train: 0.12974047660827637 elapsed, loss: 3.0849976e-06\n",
      "step: 16860 train: 0.13186907768249512 elapsed, loss: 2.6496025e-06\n",
      "step: 16870 train: 0.11824893951416016 elapsed, loss: 5.7140087e-06\n",
      "step: 16880 train: 0.13895583152770996 elapsed, loss: 2.55461e-06\n",
      "step: 16890 train: 0.14527153968811035 elapsed, loss: 3.5165256e-06\n",
      "step: 16900 train: 0.1317155361175537 elapsed, loss: 3.1795175e-06\n",
      "step: 16910 train: 0.1317903995513916 elapsed, loss: 2.8610084e-06\n",
      "step: 16920 train: 0.12605547904968262 elapsed, loss: 4.1490184e-06\n",
      "step: 16930 train: 0.14561033248901367 elapsed, loss: 2.3529753e-06\n",
      "step: 16940 train: 0.12936663627624512 elapsed, loss: 4.6313253e-06\n",
      "step: 16950 train: 0.1447739601135254 elapsed, loss: 1.1895485e-05\n",
      "step: 16960 train: 0.13663506507873535 elapsed, loss: 2.5341224e-06\n",
      "step: 16970 train: 0.12522578239440918 elapsed, loss: 3.9841802e-06\n",
      "step: 16980 train: 0.1271193027496338 elapsed, loss: 5.771909e-06\n",
      "step: 16990 train: 0.12800121307373047 elapsed, loss: 4.902743e-06\n",
      "step: 17000 train: 0.12071633338928223 elapsed, loss: 4.2686975e-06\n",
      "step: 17010 train: 0.11795377731323242 elapsed, loss: 4.789288e-06\n",
      "step: 17020 train: 0.12362241744995117 elapsed, loss: 3.2992007e-06\n",
      "step: 17030 train: 0.12963247299194336 elapsed, loss: 3.7224404e-06\n",
      "step: 17040 train: 0.1227729320526123 elapsed, loss: 5.3285103e-06\n",
      "step: 17050 train: 0.1239175796508789 elapsed, loss: 3.8244616e-06\n",
      "step: 17060 train: 0.12864017486572266 elapsed, loss: 3.885461e-06\n",
      "step: 17070 train: 0.12346768379211426 elapsed, loss: 3.4109578e-06\n",
      "step: 17080 train: 0.12528419494628906 elapsed, loss: 3.11014e-06\n",
      "step: 17090 train: 0.12915492057800293 elapsed, loss: 3.8532344e-06\n",
      "step: 17100 train: 0.12867236137390137 elapsed, loss: 2.6165435e-06\n",
      "step: 17110 train: 0.1372523307800293 elapsed, loss: 3.1851125e-06\n",
      "step: 17120 train: 0.13671875 elapsed, loss: 3.93994e-06\n",
      "step: 17130 train: 0.13640189170837402 elapsed, loss: 2.9313167e-06\n",
      "step: 17140 train: 0.13215947151184082 elapsed, loss: 2.5136328e-06\n",
      "step: 17150 train: 0.13935494422912598 elapsed, loss: 2.4563547e-06\n",
      "step: 17160 train: 0.13089251518249512 elapsed, loss: 2.8870923e-06\n",
      "step: 17170 train: 0.13226819038391113 elapsed, loss: 5.4908724e-06\n",
      "step: 17180 train: 0.12714886665344238 elapsed, loss: 3.5971962e-06\n",
      "step: 17190 train: 0.12731027603149414 elapsed, loss: 0.00010076934\n",
      "step: 17200 train: 0.13190245628356934 elapsed, loss: 3.918951e-06\n",
      "step: 17210 train: 0.13414883613586426 elapsed, loss: 3.909143e-06\n",
      "step: 17220 train: 0.13699889183044434 elapsed, loss: 3.015608e-06\n",
      "step: 17230 train: 0.13042831420898438 elapsed, loss: 3.3359825e-06\n",
      "step: 17240 train: 0.1314103603363037 elapsed, loss: 3.5580988e-06\n",
      "step: 17250 train: 0.138139009475708 elapsed, loss: 2.1159603e-06\n",
      "step: 17260 train: 0.12304496765136719 elapsed, loss: 4.4731178e-06\n",
      "step: 17270 train: 0.11684226989746094 elapsed, loss: 4.29942e-06\n",
      "step: 17280 train: 0.12799072265625 elapsed, loss: 4.4409912e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 17290 train: 0.13950705528259277 elapsed, loss: 2.3879015e-06\n",
      "step: 17300 train: 0.1285853385925293 elapsed, loss: 4.4344747e-06\n",
      "step: 17310 train: 0.12522482872009277 elapsed, loss: 3.8770627e-06\n",
      "step: 17320 train: 0.12923598289489746 elapsed, loss: 4.358487e-06\n",
      "step: 17330 train: 0.13033175468444824 elapsed, loss: 2.8582208e-06\n",
      "step: 17340 train: 0.1263108253479004 elapsed, loss: 2.8088564e-06\n",
      "step: 17350 train: 0.13905596733093262 elapsed, loss: 3.7005593e-06\n",
      "step: 17360 train: 0.13161826133728027 elapsed, loss: 3.1492186e-06\n",
      "step: 17370 train: 0.12505292892456055 elapsed, loss: 2.7012925e-06\n",
      "step: 17380 train: 0.12811589241027832 elapsed, loss: 4.061945e-06\n",
      "step: 17390 train: 0.1415235996246338 elapsed, loss: 2.47638e-06\n",
      "step: 17400 train: 0.14004278182983398 elapsed, loss: 2.935516e-06\n",
      "step: 17410 train: 0.13404226303100586 elapsed, loss: 1.0267599e-05\n",
      "step: 17420 train: 0.12916994094848633 elapsed, loss: 7.3926167e-06\n",
      "step: 17430 train: 0.13043522834777832 elapsed, loss: 4.9951086e-06\n",
      "step: 17440 train: 0.12409806251525879 elapsed, loss: 4.820932e-06\n",
      "step: 17450 train: 0.13019776344299316 elapsed, loss: 3.5753314e-06\n",
      "step: 17460 train: 0.14477205276489258 elapsed, loss: 2.9834832e-06\n",
      "step: 17470 train: 0.12301230430603027 elapsed, loss: 3.8756816e-06\n",
      "step: 17480 train: 0.11971688270568848 elapsed, loss: 4.3185246e-06\n",
      "step: 17490 train: 0.13115358352661133 elapsed, loss: 3.4481973e-06\n",
      "step: 17500 train: 0.13325238227844238 elapsed, loss: 2.7227152e-06\n",
      "step: 17510 train: 0.12877845764160156 elapsed, loss: 3.3522683e-06\n",
      "step: 17520 train: 0.12786197662353516 elapsed, loss: 3.2405246e-06\n",
      "step: 17530 train: 0.13100719451904297 elapsed, loss: 4.181077e-06\n",
      "step: 17540 train: 0.12420535087585449 elapsed, loss: 3.5725348e-06\n",
      "step: 17550 train: 0.12781310081481934 elapsed, loss: 2.5229456e-06\n",
      "step: 17560 train: 0.11837005615234375 elapsed, loss: 4.328764e-06\n",
      "step: 17570 train: 0.12050032615661621 elapsed, loss: 3.5078142e-06\n",
      "step: 17580 train: 0.12479257583618164 elapsed, loss: 2.6728899e-06\n",
      "step: 17590 train: 0.1244816780090332 elapsed, loss: 4.463797e-06\n",
      "step: 17600 train: 0.11902737617492676 elapsed, loss: 4.454944e-06\n",
      "step: 17610 train: 0.12212896347045898 elapsed, loss: 3.4989637e-06\n",
      "step: 17620 train: 0.12930560111999512 elapsed, loss: 2.6309551e-06\n",
      "step: 17630 train: 0.130842924118042 elapsed, loss: 2.421431e-06\n",
      "step: 17640 train: 0.12510967254638672 elapsed, loss: 3.3755582e-06\n",
      "step: 17650 train: 0.14389729499816895 elapsed, loss: 3.1185207e-06\n",
      "step: 17660 train: 0.14399313926696777 elapsed, loss: 2.6188732e-06\n",
      "step: 17670 train: 0.14003300666809082 elapsed, loss: 2.598382e-06\n",
      "step: 17680 train: 0.12002062797546387 elapsed, loss: 6.359351e-06\n",
      "step: 17690 train: 0.14092397689819336 elapsed, loss: 3.501296e-06\n",
      "step: 17700 train: 0.13553619384765625 elapsed, loss: 3.3704455e-06\n",
      "step: 17710 train: 0.12937450408935547 elapsed, loss: 4.1082653e-06\n",
      "step: 17720 train: 0.13350391387939453 elapsed, loss: 2.6048967e-06\n",
      "step: 17730 train: 0.1282508373260498 elapsed, loss: 4.3366795e-06\n",
      "step: 17740 train: 0.13202524185180664 elapsed, loss: 3.3383085e-06\n",
      "step: 17750 train: 0.13857245445251465 elapsed, loss: 2.4442484e-06\n",
      "step: 17760 train: 0.1276240348815918 elapsed, loss: 3.474274e-06\n",
      "step: 17770 train: 0.13898587226867676 elapsed, loss: 2.795357e-06\n",
      "step: 17780 train: 0.11905670166015625 elapsed, loss: 4.1913895e-06\n",
      "step: 17790 train: 0.1196751594543457 elapsed, loss: 4.2468137e-06\n",
      "step: 17800 train: 0.126786470413208 elapsed, loss: 9.641608e-06\n",
      "step: 17810 train: 0.13186192512512207 elapsed, loss: 3.382082e-06\n",
      "step: 17820 train: 0.13932490348815918 elapsed, loss: 2.7724718e-06\n",
      "step: 17830 train: 0.12796974182128906 elapsed, loss: 2.5862778e-06\n",
      "step: 17840 train: 0.12459373474121094 elapsed, loss: 4.1592048e-06\n",
      "step: 17850 train: 0.13849806785583496 elapsed, loss: 3.211189e-06\n",
      "step: 17860 train: 0.1382462978363037 elapsed, loss: 2.2938402e-06\n",
      "step: 17870 train: 0.13537955284118652 elapsed, loss: 2.4535623e-06\n",
      "step: 17880 train: 0.12755084037780762 elapsed, loss: 7.768826e-06\n",
      "step: 17890 train: 0.13493704795837402 elapsed, loss: 2.7920942e-06\n",
      "step: 17900 train: 0.13713836669921875 elapsed, loss: 2.5685822e-06\n",
      "step: 17910 train: 0.127593994140625 elapsed, loss: 4.8926786e-06\n",
      "step: 17920 train: 0.12597203254699707 elapsed, loss: 4.1103767e-06\n",
      "step: 17930 train: 0.13039183616638184 elapsed, loss: 6.259149e-06\n",
      "step: 17940 train: 0.12284207344055176 elapsed, loss: 4.103853e-06\n",
      "step: 17950 train: 0.13002848625183105 elapsed, loss: 3.0053573e-06\n",
      "step: 17960 train: 0.13552451133728027 elapsed, loss: 2.4465771e-06\n",
      "step: 17970 train: 0.13163161277770996 elapsed, loss: 3.2409907e-06\n",
      "step: 17980 train: 0.13247036933898926 elapsed, loss: 2.2738159e-06\n",
      "step: 17990 train: 0.129622220993042 elapsed, loss: 3.846788e-06\n",
      "step: 18000 train: 0.13037562370300293 elapsed, loss: 3.6009155e-06\n",
      "step: 18010 train: 0.12379908561706543 elapsed, loss: 4.037734e-06\n",
      "step: 18020 train: 0.1201474666595459 elapsed, loss: 4.6961595e-06\n",
      "step: 18030 train: 0.12707138061523438 elapsed, loss: 4.5294314e-06\n",
      "step: 18040 train: 0.12441658973693848 elapsed, loss: 4.09547e-06\n",
      "step: 18050 train: 0.12903356552124023 elapsed, loss: 5.0708695e-06\n",
      "step: 18060 train: 0.13817095756530762 elapsed, loss: 2.9806888e-06\n",
      "step: 18070 train: 0.12668943405151367 elapsed, loss: 3.9259685e-06\n",
      "step: 18080 train: 0.12318539619445801 elapsed, loss: 5.4692457e-05\n",
      "step: 18090 train: 0.12338590621948242 elapsed, loss: 0.22836491\n",
      "step: 18100 train: 0.12353277206420898 elapsed, loss: 4.474147e-05\n",
      "step: 18110 train: 0.1311938762664795 elapsed, loss: 2.3723323e-05\n",
      "step: 18120 train: 0.12785720825195312 elapsed, loss: 1.9600613e-05\n",
      "step: 18130 train: 0.12129402160644531 elapsed, loss: 1.6663273e-05\n",
      "step: 18140 train: 0.13502907752990723 elapsed, loss: 1.3153947e-05\n",
      "step: 18150 train: 0.12443876266479492 elapsed, loss: 1.3088778e-05\n",
      "step: 18160 train: 0.12121057510375977 elapsed, loss: 1.1171403e-05\n",
      "step: 18170 train: 0.131514310836792 elapsed, loss: 8.134974e-06\n",
      "step: 18180 train: 0.13481569290161133 elapsed, loss: 3.431442e-06\n",
      "step: 18190 train: 0.13558602333068848 elapsed, loss: 3.1972186e-06\n",
      "step: 18200 train: 0.13254928588867188 elapsed, loss: 4.6686164e-06\n",
      "step: 18210 train: 0.12590479850769043 elapsed, loss: 4.304089e-06\n",
      "step: 18220 train: 0.13909173011779785 elapsed, loss: 2.9811522e-06\n",
      "step: 18230 train: 0.12286734580993652 elapsed, loss: 4.2384304e-06\n",
      "step: 18240 train: 0.12468981742858887 elapsed, loss: 3.5278358e-06\n",
      "step: 18250 train: 0.12489581108093262 elapsed, loss: 4.0298064e-06\n",
      "step: 18260 train: 0.11953353881835938 elapsed, loss: 4.757625e-06\n",
      "step: 18270 train: 0.13228511810302734 elapsed, loss: 4.0596096e-06\n",
      "step: 18280 train: 0.13099288940429688 elapsed, loss: 3.0076849e-06\n",
      "step: 18290 train: 0.12801647186279297 elapsed, loss: 3.358318e-06\n",
      "step: 18300 train: 0.12499833106994629 elapsed, loss: 3.544603e-06\n",
      "step: 18310 train: 0.13849830627441406 elapsed, loss: 4.807848e-05\n",
      "step: 18320 train: 0.13486313819885254 elapsed, loss: 8.485115e-06\n",
      "step: 18330 train: 0.12502408027648926 elapsed, loss: 1.13483175e-05\n",
      "step: 18340 train: 0.13071250915527344 elapsed, loss: 1.21178855e-05\n",
      "step: 18350 train: 0.1279895305633545 elapsed, loss: 5.880324e-06\n",
      "step: 18360 train: 0.13556742668151855 elapsed, loss: 5.7536e-06\n",
      "step: 18370 train: 0.12284517288208008 elapsed, loss: 4.7604285e-06\n",
      "step: 18380 train: 0.13317036628723145 elapsed, loss: 3.8640273e-06\n",
      "step: 18390 train: 0.1269674301147461 elapsed, loss: 3.276382e-06\n",
      "step: 18400 train: 0.1232309341430664 elapsed, loss: 4.0866203e-06\n",
      "step: 18410 train: 0.12845659255981445 elapsed, loss: 2.9029238e-06\n",
      "step: 18420 train: 0.14226102828979492 elapsed, loss: 3.0063002e-06\n",
      "step: 18430 train: 0.13623714447021484 elapsed, loss: 4.4428234e-06\n",
      "step: 18440 train: 0.1201481819152832 elapsed, loss: 6.9954476e-06\n",
      "step: 18450 train: 0.12834644317626953 elapsed, loss: 2.818636e-06\n",
      "step: 18460 train: 0.13070368766784668 elapsed, loss: 2.9317882e-06\n",
      "step: 18470 train: 0.12057757377624512 elapsed, loss: 4.669044e-06\n",
      "step: 18480 train: 0.1392982006072998 elapsed, loss: 2.3203847e-06\n",
      "step: 18490 train: 0.13718795776367188 elapsed, loss: 2.359965e-06\n",
      "step: 18500 train: 0.13260364532470703 elapsed, loss: 2.1154942e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18510 train: 0.12746143341064453 elapsed, loss: 2.3217817e-06\n",
      "step: 18520 train: 0.13259363174438477 elapsed, loss: 2.963454e-06\n",
      "step: 18530 train: 0.12438130378723145 elapsed, loss: 2.6384307e-06\n",
      "step: 18540 train: 0.1382763385772705 elapsed, loss: 3.0295785e-06\n",
      "step: 18550 train: 0.11713457107543945 elapsed, loss: 4.180685e-06\n",
      "step: 18560 train: 0.1201019287109375 elapsed, loss: 3.2088617e-06\n",
      "step: 18570 train: 0.12337064743041992 elapsed, loss: 3.8044366e-06\n",
      "step: 18580 train: 0.13785624504089355 elapsed, loss: 2.954107e-06\n",
      "step: 18590 train: 0.12687349319458008 elapsed, loss: 2.659376e-06\n",
      "step: 18600 train: 0.13573789596557617 elapsed, loss: 2.5336483e-06\n",
      "step: 18610 train: 0.1287236213684082 elapsed, loss: 3.639594e-06\n",
      "step: 18620 train: 0.11653375625610352 elapsed, loss: 3.86636e-06\n",
      "step: 18630 train: 0.12712860107421875 elapsed, loss: 3.3420406e-06\n",
      "step: 18640 train: 0.13600516319274902 elapsed, loss: 3.0025708e-06\n",
      "step: 18650 train: 0.12643647193908691 elapsed, loss: 2.6253865e-06\n",
      "step: 18660 train: 0.13105177879333496 elapsed, loss: 2.880572e-06\n",
      "step: 18670 train: 0.1358792781829834 elapsed, loss: 3.3750994e-06\n",
      "step: 18680 train: 0.14139485359191895 elapsed, loss: 3.540868e-06\n",
      "step: 18690 train: 0.13104939460754395 elapsed, loss: 2.8367945e-06\n",
      "step: 18700 train: 0.13379120826721191 elapsed, loss: 2.155541e-06\n",
      "step: 18710 train: 0.13914871215820312 elapsed, loss: 1.956238e-06\n",
      "step: 18720 train: 0.12949275970458984 elapsed, loss: 2.8116535e-06\n",
      "step: 18730 train: 0.13088059425354004 elapsed, loss: 2.840527e-06\n",
      "step: 18740 train: 0.14159512519836426 elapsed, loss: 2.1867404e-06\n",
      "step: 18750 train: 0.12365460395812988 elapsed, loss: 3.1813865e-06\n",
      "step: 18760 train: 0.12636351585388184 elapsed, loss: 4.5555043e-06\n",
      "step: 18770 train: 0.13947248458862305 elapsed, loss: 2.3981497e-06\n",
      "step: 18780 train: 0.14211583137512207 elapsed, loss: 2.2258569e-06\n",
      "step: 18790 train: 0.11922717094421387 elapsed, loss: 4.4605476e-06\n",
      "step: 18800 train: 0.12435030937194824 elapsed, loss: 3.7271386e-06\n",
      "step: 18810 train: 0.12308526039123535 elapsed, loss: 2.9751018e-06\n",
      "step: 18820 train: 0.12983226776123047 elapsed, loss: 2.9466964e-06\n",
      "step: 18830 train: 0.12482023239135742 elapsed, loss: 3.2363348e-06\n",
      "step: 18840 train: 0.1232149600982666 elapsed, loss: 3.7182929e-06\n",
      "step: 18850 train: 0.1402437686920166 elapsed, loss: 2.4898795e-06\n",
      "step: 18860 train: 0.137923002243042 elapsed, loss: 2.6863886e-06\n",
      "step: 18870 train: 0.14316773414611816 elapsed, loss: 2.640751e-06\n",
      "step: 18880 train: 0.14294052124023438 elapsed, loss: 2.2826616e-06\n",
      "step: 18890 train: 0.13133859634399414 elapsed, loss: 5.0691333e-06\n",
      "step: 18900 train: 0.13727736473083496 elapsed, loss: 2.700351e-06\n",
      "step: 18910 train: 0.14122438430786133 elapsed, loss: 2.7669457e-06\n",
      "step: 18920 train: 0.12019515037536621 elapsed, loss: 4.7231774e-06\n",
      "step: 18930 train: 0.13503575325012207 elapsed, loss: 3.3886038e-06\n",
      "step: 18940 train: 0.13019943237304688 elapsed, loss: 4.8702245e-06\n",
      "step: 18950 train: 0.12930083274841309 elapsed, loss: 2.357172e-06\n",
      "step: 18960 train: 0.1253359317779541 elapsed, loss: 3.954368e-06\n",
      "step: 18970 train: 0.12158560752868652 elapsed, loss: 4.954121e-06\n",
      "step: 18980 train: 0.1352527141571045 elapsed, loss: 3.7108416e-06\n",
      "step: 18990 train: 0.13087081909179688 elapsed, loss: 2.8982438e-06\n",
      "step: 19000 train: 0.1283717155456543 elapsed, loss: 8.520891e-06\n",
      "step: 19010 train: 0.13457679748535156 elapsed, loss: 3.7951258e-06\n",
      "step: 19020 train: 0.13391447067260742 elapsed, loss: 4.893603e-06\n",
      "step: 19030 train: 0.1229703426361084 elapsed, loss: 2.7907004e-06\n",
      "step: 19040 train: 0.11792445182800293 elapsed, loss: 5.562287e-06\n",
      "step: 19050 train: 0.1264195442199707 elapsed, loss: 0.014006809\n",
      "step: 19060 train: 0.1347672939300537 elapsed, loss: 2.1888654e-05\n",
      "step: 19070 train: 0.12869691848754883 elapsed, loss: 1.3457238e-05\n",
      "step: 19080 train: 0.12291264533996582 elapsed, loss: 1.4049946e-05\n",
      "step: 19090 train: 0.12023186683654785 elapsed, loss: 1.1865215e-05\n",
      "step: 19100 train: 0.1253650188446045 elapsed, loss: 6.1043133e-06\n",
      "step: 19110 train: 0.13594627380371094 elapsed, loss: 6.486118e-06\n",
      "step: 19120 train: 0.14273667335510254 elapsed, loss: 4.2267757e-06\n",
      "step: 19130 train: 0.12557363510131836 elapsed, loss: 7.916938e-06\n",
      "step: 19140 train: 0.1169748306274414 elapsed, loss: 7.458879e-06\n",
      "step: 19150 train: 0.12696146965026855 elapsed, loss: 4.8526135e-06\n",
      "step: 19160 train: 0.12320852279663086 elapsed, loss: 4.0973237e-06\n",
      "step: 19170 train: 0.12158370018005371 elapsed, loss: 9.460795e-06\n",
      "step: 19180 train: 0.12426209449768066 elapsed, loss: 4.071698e-06\n",
      "step: 19190 train: 0.13130927085876465 elapsed, loss: 3.4882569e-06\n",
      "step: 19200 train: 0.1298229694366455 elapsed, loss: 3.416081e-06\n",
      "step: 19210 train: 0.12436270713806152 elapsed, loss: 4.2319034e-06\n",
      "step: 19220 train: 0.13919925689697266 elapsed, loss: 2.5466927e-06\n",
      "step: 19230 train: 0.13360166549682617 elapsed, loss: 3.2051312e-06\n",
      "step: 19240 train: 0.13595175743103027 elapsed, loss: 2.900129e-06\n",
      "step: 19250 train: 0.14048051834106445 elapsed, loss: 4.766925e-06\n",
      "step: 19260 train: 0.11770153045654297 elapsed, loss: 5.880732e-06\n",
      "step: 19270 train: 0.135512113571167 elapsed, loss: 3.799314e-06\n",
      "step: 19280 train: 0.1376953125 elapsed, loss: 2.0977939e-06\n",
      "step: 19290 train: 0.12483620643615723 elapsed, loss: 3.3299077e-06\n",
      "step: 19300 train: 0.12526607513427734 elapsed, loss: 4.5997754e-06\n",
      "step: 19310 train: 0.13376879692077637 elapsed, loss: 5.5859177e-06\n",
      "step: 19320 train: 0.12323212623596191 elapsed, loss: 6.573954e-06\n",
      "step: 19330 train: 0.1310741901397705 elapsed, loss: 2.5345853e-06\n",
      "step: 19340 train: 0.1314389705657959 elapsed, loss: 2.792097e-06\n",
      "step: 19350 train: 0.12063312530517578 elapsed, loss: 3.245181e-06\n",
      "step: 19360 train: 0.11996793746948242 elapsed, loss: 3.6461124e-06\n",
      "step: 19370 train: 0.12321639060974121 elapsed, loss: 3.9250363e-06\n",
      "step: 19380 train: 0.1388258934020996 elapsed, loss: 2.5145634e-06\n",
      "step: 19390 train: 0.11816644668579102 elapsed, loss: 3.6042065e-06\n",
      "step: 19400 train: 0.12444090843200684 elapsed, loss: 3.891052e-06\n",
      "step: 19410 train: 0.13709282875061035 elapsed, loss: 4.508503e-06\n",
      "step: 19420 train: 0.14170026779174805 elapsed, loss: 2.8572754e-06\n",
      "step: 19430 train: 0.1206965446472168 elapsed, loss: 3.1534428e-06\n",
      "step: 19440 train: 0.12405204772949219 elapsed, loss: 3.824912e-06\n",
      "step: 19450 train: 0.12462997436523438 elapsed, loss: 3.7927941e-06\n",
      "step: 19460 train: 0.1295337677001953 elapsed, loss: 3.5366734e-06\n",
      "step: 19470 train: 0.1225881576538086 elapsed, loss: 4.200706e-06\n",
      "step: 19480 train: 0.12986493110656738 elapsed, loss: 2.4554224e-06\n",
      "step: 19490 train: 0.13421154022216797 elapsed, loss: 2.9150297e-06\n",
      "step: 19500 train: 0.12161922454833984 elapsed, loss: 4.2335974e-05\n",
      "step: 19510 train: 0.11774301528930664 elapsed, loss: 7.279508e-06\n",
      "step: 19520 train: 0.11894965171813965 elapsed, loss: 5.719189e-06\n",
      "step: 19530 train: 0.13034820556640625 elapsed, loss: 3.9399265e-06\n",
      "step: 19540 train: 0.12540411949157715 elapsed, loss: 3.7466898e-06\n",
      "step: 19550 train: 0.12714552879333496 elapsed, loss: 3.7806853e-06\n",
      "step: 19560 train: 0.13638782501220703 elapsed, loss: 2.264505e-06\n",
      "step: 19570 train: 0.13011646270751953 elapsed, loss: 2.63424e-06\n",
      "step: 19580 train: 0.1281719207763672 elapsed, loss: 2.7129302e-06\n",
      "step: 19590 train: 0.13060212135314941 elapsed, loss: 4.11177e-06\n",
      "step: 19600 train: 0.13275933265686035 elapsed, loss: 2.7362184e-06\n",
      "step: 19610 train: 0.1319561004638672 elapsed, loss: 3.9222496e-06\n",
      "step: 19620 train: 0.12853789329528809 elapsed, loss: 4.2300126e-06\n",
      "step: 19630 train: 0.12845134735107422 elapsed, loss: 3.2432554e-06\n",
      "step: 19640 train: 0.13829493522644043 elapsed, loss: 3.3569245e-06\n",
      "step: 19650 train: 0.11695170402526855 elapsed, loss: 5.022122e-06\n",
      "step: 19660 train: 0.12606239318847656 elapsed, loss: 4.353438e-06\n",
      "step: 19670 train: 0.12375807762145996 elapsed, loss: 4.7385265e-06\n",
      "step: 19680 train: 0.12093377113342285 elapsed, loss: 3.4700925e-06\n",
      "step: 19690 train: 0.12696385383605957 elapsed, loss: 3.3178185e-06\n",
      "step: 19700 train: 0.13013815879821777 elapsed, loss: 3.2344658e-06\n",
      "step: 19710 train: 0.1392381191253662 elapsed, loss: 2.0223627e-06\n",
      "step: 19720 train: 0.11934328079223633 elapsed, loss: 3.7369177e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 19730 train: 0.12377452850341797 elapsed, loss: 4.3133996e-06\n",
      "step: 19740 train: 0.12265515327453613 elapsed, loss: 3.4030422e-06\n",
      "step: 19750 train: 0.13022589683532715 elapsed, loss: 3.7774266e-06\n",
      "step: 19760 train: 0.1234443187713623 elapsed, loss: 3.9678835e-06\n",
      "step: 19770 train: 0.12787866592407227 elapsed, loss: 8.537166e-05\n",
      "step: 19780 train: 0.13578557968139648 elapsed, loss: 8.6471355e-06\n",
      "step: 19790 train: 0.12415194511413574 elapsed, loss: 1.8778246e-05\n",
      "step: 19800 train: 0.13921117782592773 elapsed, loss: 5.653076e-06\n",
      "step: 19810 train: 0.14004087448120117 elapsed, loss: 6.668549e-06\n",
      "step: 19820 train: 0.1397557258605957 elapsed, loss: 6.8437357e-06\n",
      "step: 19830 train: 0.1282520294189453 elapsed, loss: 5.682854e-06\n",
      "step: 19840 train: 0.12550139427185059 elapsed, loss: 5.4291104e-06\n",
      "step: 19850 train: 0.1333625316619873 elapsed, loss: 8.581332e-06\n",
      "step: 19860 train: 0.1275010108947754 elapsed, loss: 3.1390143e-06\n",
      "step: 19870 train: 0.13003063201904297 elapsed, loss: 3.4249215e-06\n",
      "step: 19880 train: 0.14000272750854492 elapsed, loss: 2.5178251e-06\n",
      "step: 19890 train: 0.11979794502258301 elapsed, loss: 4.0977934e-06\n",
      "step: 19900 train: 0.12518596649169922 elapsed, loss: 5.629764e-06\n",
      "step: 19910 train: 0.1337885856628418 elapsed, loss: 3.4728835e-06\n",
      "step: 19920 train: 0.13076210021972656 elapsed, loss: 3.7178197e-06\n",
      "step: 19930 train: 0.12469172477722168 elapsed, loss: 2.4791689e-06\n",
      "step: 19940 train: 0.12391090393066406 elapsed, loss: 2.7553099e-06\n",
      "step: 19950 train: 0.1291806697845459 elapsed, loss: 2.7180577e-06\n",
      "step: 19960 train: 0.13003802299499512 elapsed, loss: 5.9802796e-06\n",
      "step: 19970 train: 0.1280522346496582 elapsed, loss: 2.3553093e-06\n",
      "step: 19980 train: 0.12416291236877441 elapsed, loss: 3.1427376e-06\n",
      "step: 19990 train: 0.12642931938171387 elapsed, loss: 3.252168e-06\n",
      "step: 20000 train: 0.1330571174621582 elapsed, loss: 2.1876713e-06\n",
      "step: 20010 train: 0.12416696548461914 elapsed, loss: 2.8284112e-06\n",
      "step: 20020 train: 0.12610363960266113 elapsed, loss: 3.0947756e-06\n",
      "step: 20030 train: 0.15207910537719727 elapsed, loss: 2.0707917e-06\n",
      "step: 20040 train: 0.12484860420227051 elapsed, loss: 2.657057e-06\n",
      "step: 20050 train: 0.11848759651184082 elapsed, loss: 4.360894e-06\n",
      "step: 20060 train: 0.12466239929199219 elapsed, loss: 3.0477436e-06\n",
      "step: 20070 train: 0.12493681907653809 elapsed, loss: 4.1574017e-06\n",
      "step: 20080 train: 0.13775944709777832 elapsed, loss: 2.3706743e-06\n",
      "step: 20090 train: 0.12523102760314941 elapsed, loss: 2.652855e-06\n",
      "step: 20100 train: 0.13144564628601074 elapsed, loss: 2.5224801e-06\n",
      "step: 20110 train: 0.12747740745544434 elapsed, loss: 2.3809189e-06\n",
      "step: 20120 train: 0.122802734375 elapsed, loss: 5.425857e-06\n",
      "step: 20130 train: 0.13516688346862793 elapsed, loss: 2.7869733e-06\n",
      "step: 20140 train: 0.13051939010620117 elapsed, loss: 3.302922e-06\n",
      "step: 20150 train: 0.13174676895141602 elapsed, loss: 2.4102546e-06\n",
      "step: 20160 train: 0.11872529983520508 elapsed, loss: 4.3064156e-06\n",
      "step: 20170 train: 0.13727164268493652 elapsed, loss: 2.3818477e-06\n",
      "step: 20180 train: 0.141998291015625 elapsed, loss: 3.1278391e-06\n",
      "step: 20190 train: 0.14601922035217285 elapsed, loss: 2.5480927e-06\n",
      "step: 20200 train: 0.13265204429626465 elapsed, loss: 3.1506543e-06\n",
      "step: 20210 train: 0.12601208686828613 elapsed, loss: 9.611543e-06\n",
      "step: 20220 train: 0.1349332332611084 elapsed, loss: 2.6551934e-06\n",
      "step: 20230 train: 0.12164878845214844 elapsed, loss: 3.7192174e-06\n",
      "step: 20240 train: 0.12798666954040527 elapsed, loss: 3.2079295e-06\n",
      "step: 20250 train: 0.12372183799743652 elapsed, loss: 4.3292193e-06\n",
      "step: 20260 train: 0.1201467514038086 elapsed, loss: 4.011188e-06\n",
      "step: 20270 train: 0.1333177089691162 elapsed, loss: 3.1296913e-06\n",
      "step: 20280 train: 0.12520051002502441 elapsed, loss: 3.6810334e-06\n",
      "step: 20290 train: 0.12997746467590332 elapsed, loss: 3.6326082e-06\n",
      "step: 20300 train: 0.13284659385681152 elapsed, loss: 2.9638328e-05\n",
      "step: 20310 train: 0.12746262550354004 elapsed, loss: 6.3306295e-05\n",
      "step: 20320 train: 0.13244223594665527 elapsed, loss: 8.011904e-06\n",
      "step: 20330 train: 0.13103938102722168 elapsed, loss: 3.932489e-06\n",
      "step: 20340 train: 0.13575243949890137 elapsed, loss: 6.2230383e-06\n",
      "step: 20350 train: 0.13782167434692383 elapsed, loss: 3.4109523e-06\n",
      "step: 20360 train: 0.12491035461425781 elapsed, loss: 5.4499687e-06\n",
      "step: 20370 train: 0.13168787956237793 elapsed, loss: 4.267764e-06\n",
      "step: 20380 train: 0.1288003921508789 elapsed, loss: 3.2209618e-06\n",
      "step: 20390 train: 0.1255810260772705 elapsed, loss: 4.0568198e-06\n",
      "step: 20400 train: 0.13580584526062012 elapsed, loss: 3.3695135e-06\n",
      "step: 20410 train: 0.1332097053527832 elapsed, loss: 3.1944257e-06\n",
      "step: 20420 train: 0.12145543098449707 elapsed, loss: 4.0200193e-06\n",
      "step: 20430 train: 0.13843917846679688 elapsed, loss: 1.8323738e-06\n",
      "step: 20440 train: 0.1275498867034912 elapsed, loss: 3.5790588e-06\n",
      "step: 20450 train: 0.12823176383972168 elapsed, loss: 3.6745168e-06\n",
      "step: 20460 train: 0.1342787742614746 elapsed, loss: 2.3790544e-06\n",
      "step: 20470 train: 0.1354837417602539 elapsed, loss: 3.2880212e-06\n",
      "step: 20480 train: 0.13386917114257812 elapsed, loss: 2.59233e-06\n",
      "step: 20490 train: 0.1325058937072754 elapsed, loss: 2.3408725e-06\n",
      "step: 20500 train: 0.12263822555541992 elapsed, loss: 6.2220433e-06\n",
      "step: 20510 train: 0.1315617561340332 elapsed, loss: 3.1050151e-06\n",
      "step: 20520 train: 0.13120031356811523 elapsed, loss: 2.633305e-06\n",
      "step: 20530 train: 0.11939358711242676 elapsed, loss: 7.9319825e-06\n",
      "step: 20540 train: 0.12887835502624512 elapsed, loss: 4.8451348e-06\n",
      "step: 20550 train: 0.12095355987548828 elapsed, loss: 4.805575e-06\n",
      "step: 20560 train: 0.13074660301208496 elapsed, loss: 2.657056e-06\n",
      "step: 20570 train: 0.1311044692993164 elapsed, loss: 2.4698622e-06\n",
      "step: 20580 train: 0.12179374694824219 elapsed, loss: 3.7876698e-06\n",
      "step: 20590 train: 0.13537955284118652 elapsed, loss: 3.3583376e-06\n",
      "step: 20600 train: 0.13008570671081543 elapsed, loss: 3.9869665e-06\n",
      "step: 20610 train: 0.13505053520202637 elapsed, loss: 3.3159438e-06\n",
      "step: 20620 train: 0.12782502174377441 elapsed, loss: 2.3245761e-06\n",
      "step: 20630 train: 0.13467001914978027 elapsed, loss: 8.69931e-06\n",
      "step: 20640 train: 0.12879323959350586 elapsed, loss: 5.9962103e-06\n",
      "step: 20650 train: 0.13143301010131836 elapsed, loss: 5.4733414e-06\n",
      "step: 20660 train: 0.1317307949066162 elapsed, loss: 3.4542632e-06\n",
      "step: 20670 train: 0.11507892608642578 elapsed, loss: 5.957162e-06\n",
      "step: 20680 train: 0.1335465908050537 elapsed, loss: 3.3811282e-06\n",
      "step: 20690 train: 0.12778711318969727 elapsed, loss: 4.4288695e-06\n",
      "step: 20700 train: 0.1263720989227295 elapsed, loss: 5.4621414e-06\n",
      "step: 20710 train: 0.1208646297454834 elapsed, loss: 4.280335e-06\n",
      "step: 20720 train: 0.1385195255279541 elapsed, loss: 2.5965155e-06\n",
      "step: 20730 train: 0.13307929039001465 elapsed, loss: 2.530845e-06\n",
      "step: 20740 train: 0.12730145454406738 elapsed, loss: 3.2889511e-06\n",
      "step: 20750 train: 0.13236021995544434 elapsed, loss: 3.363899e-06\n",
      "step: 20760 train: 0.1272144317626953 elapsed, loss: 2.4200353e-06\n",
      "step: 20770 train: 0.12209343910217285 elapsed, loss: 3.5366852e-06\n",
      "step: 20780 train: 0.1283113956451416 elapsed, loss: 2.3273692e-06\n",
      "step: 20790 train: 0.1299440860748291 elapsed, loss: 2.559731e-06\n",
      "step: 20800 train: 0.1303396224975586 elapsed, loss: 2.8363338e-06\n",
      "step: 20810 train: 0.1325838565826416 elapsed, loss: 3.1217835e-06\n",
      "step: 20820 train: 0.13118553161621094 elapsed, loss: 2.137846e-06\n",
      "step: 20830 train: 0.13180136680603027 elapsed, loss: 2.2556567e-06\n",
      "step: 20840 train: 0.140045166015625 elapsed, loss: 4.671917e-06\n",
      "step: 20850 train: 0.1200418472290039 elapsed, loss: 4.1187564e-06\n",
      "step: 20860 train: 0.12078213691711426 elapsed, loss: 4.395353e-06\n",
      "step: 20870 train: 0.12932777404785156 elapsed, loss: 3.044951e-06\n",
      "step: 20880 train: 0.12783026695251465 elapsed, loss: 3.1012883e-06\n",
      "step: 20890 train: 0.12836980819702148 elapsed, loss: 3.0281842e-06\n",
      "step: 20900 train: 0.11687254905700684 elapsed, loss: 3.7047769e-06\n",
      "step: 20910 train: 0.12293124198913574 elapsed, loss: 3.5553135e-06\n",
      "step: 20920 train: 0.12789058685302734 elapsed, loss: 2.7613632e-06\n",
      "step: 20930 train: 0.13813138008117676 elapsed, loss: 2.806999e-06\n",
      "step: 20940 train: 0.12540984153747559 elapsed, loss: 3.8174776e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20950 train: 0.12639856338500977 elapsed, loss: 4.4894105e-06\n",
      "step: 20960 train: 0.11904191970825195 elapsed, loss: 3.5883695e-06\n",
      "step: 20970 train: 0.12751531600952148 elapsed, loss: 8.522356e-06\n",
      "step: 20980 train: 0.14287185668945312 elapsed, loss: 3.179524e-06\n",
      "step: 20990 train: 0.1326770782470703 elapsed, loss: 4.2898873e-06\n",
      "step: 21000 train: 0.11817193031311035 elapsed, loss: 3.9697325e-06\n",
      "step: 21010 train: 0.1259918212890625 elapsed, loss: 3.4910217e-06\n",
      "step: 21020 train: 0.12582778930664062 elapsed, loss: 5.8602905e-06\n",
      "step: 21030 train: 0.12317943572998047 elapsed, loss: 5.1067354e-06\n",
      "step: 21040 train: 0.12694406509399414 elapsed, loss: 3.0659032e-06\n",
      "step: 21050 train: 0.1272447109222412 elapsed, loss: 2.4223648e-06\n",
      "step: 21060 train: 0.12135052680969238 elapsed, loss: 4.6132795e-06\n",
      "step: 21070 train: 0.12302565574645996 elapsed, loss: 4.76317e-06\n",
      "step: 21080 train: 0.12965989112854004 elapsed, loss: 2.715729e-06\n",
      "step: 21090 train: 0.1338815689086914 elapsed, loss: 2.9047878e-06\n",
      "step: 21100 train: 0.1466197967529297 elapsed, loss: 2.1248088e-06\n",
      "step: 21110 train: 0.12392830848693848 elapsed, loss: 4.7804183e-06\n",
      "step: 21120 train: 0.11929988861083984 elapsed, loss: 4.656589e-06\n",
      "step: 21130 train: 0.11914348602294922 elapsed, loss: 4.6002388e-06\n",
      "step: 21140 train: 0.11470198631286621 elapsed, loss: 4.8409884e-06\n",
      "step: 21150 train: 0.12251162528991699 elapsed, loss: 4.327834e-06\n",
      "step: 21160 train: 0.1220390796661377 elapsed, loss: 4.098725e-06\n",
      "step: 21170 train: 0.12335991859436035 elapsed, loss: 3.8933763e-06\n",
      "step: 21180 train: 0.13372206687927246 elapsed, loss: 3.6284155e-06\n",
      "step: 21190 train: 0.13592314720153809 elapsed, loss: 2.2835982e-06\n",
      "step: 21200 train: 0.1242055892944336 elapsed, loss: 4.0716086e-06\n",
      "step: 21210 train: 0.13230133056640625 elapsed, loss: 2.906642e-06\n",
      "step: 21220 train: 0.13686013221740723 elapsed, loss: 4.8365628e-06\n",
      "step: 21230 train: 0.1380305290222168 elapsed, loss: 4.1322537e-06\n",
      "step: 21240 train: 0.12308096885681152 elapsed, loss: 4.9066166e-06\n",
      "step: 21250 train: 0.12109637260437012 elapsed, loss: 4.179761e-06\n",
      "step: 21260 train: 0.1283106803894043 elapsed, loss: 3.4388936e-06\n",
      "step: 21270 train: 0.12270855903625488 elapsed, loss: 3.2268625e-05\n",
      "step: 21280 train: 0.12019610404968262 elapsed, loss: 2.3017226e-05\n",
      "step: 21290 train: 0.12119865417480469 elapsed, loss: 3.491559e-05\n",
      "step: 21300 train: 0.12109875679016113 elapsed, loss: 2.9216944e-05\n",
      "step: 21310 train: 0.12336897850036621 elapsed, loss: 1.570921e-05\n",
      "step: 21320 train: 0.13897109031677246 elapsed, loss: 1.0835041e-05\n",
      "step: 21330 train: 0.12811732292175293 elapsed, loss: 7.2376633e-06\n",
      "step: 21340 train: 0.12479662895202637 elapsed, loss: 7.244109e-06\n",
      "step: 21350 train: 0.12834572792053223 elapsed, loss: 1.2795339e-05\n",
      "step: 21360 train: 0.13886451721191406 elapsed, loss: 3.7890707e-06\n",
      "step: 21370 train: 0.12803077697753906 elapsed, loss: 4.9587898e-06\n",
      "step: 21380 train: 0.12430262565612793 elapsed, loss: 4.4516955e-06\n",
      "step: 21390 train: 0.13225460052490234 elapsed, loss: 4.0316763e-06\n",
      "step: 21400 train: 0.12727689743041992 elapsed, loss: 5.45658e-06\n",
      "step: 21410 train: 0.13734197616577148 elapsed, loss: 3.6619265e-06\n",
      "step: 21420 train: 0.12740826606750488 elapsed, loss: 3.4952286e-06\n",
      "step: 21430 train: 0.12052059173583984 elapsed, loss: 4.4372646e-06\n",
      "step: 21440 train: 0.12021064758300781 elapsed, loss: 3.7937293e-06\n",
      "step: 21450 train: 0.13403820991516113 elapsed, loss: 2.0936093e-06\n",
      "step: 21460 train: 0.12801504135131836 elapsed, loss: 2.9029227e-06\n",
      "step: 21470 train: 0.1166539192199707 elapsed, loss: 4.7054828e-06\n",
      "step: 21480 train: 0.13144183158874512 elapsed, loss: 2.4633418e-06\n",
      "step: 21490 train: 0.11955809593200684 elapsed, loss: 3.7941948e-06\n",
      "step: 21500 train: 0.13160300254821777 elapsed, loss: 2.7646138e-06\n",
      "step: 21510 train: 0.13721060752868652 elapsed, loss: 2.7976832e-06\n",
      "step: 21520 train: 0.13019752502441406 elapsed, loss: 3.5250391e-06\n",
      "step: 21530 train: 0.12765097618103027 elapsed, loss: 4.698864e-06\n",
      "step: 21540 train: 0.12804245948791504 elapsed, loss: 2.9643893e-06\n",
      "step: 21550 train: 0.12088561058044434 elapsed, loss: 3.2358626e-06\n",
      "step: 21560 train: 0.1310899257659912 elapsed, loss: 3.5249932e-06\n",
      "step: 21570 train: 0.13132810592651367 elapsed, loss: 2.4959377e-06\n",
      "step: 21580 train: 0.12448930740356445 elapsed, loss: 2.7594983e-06\n",
      "step: 21590 train: 0.13243699073791504 elapsed, loss: 3.476146e-06\n",
      "step: 21600 train: 0.12607288360595703 elapsed, loss: 3.7192221e-06\n",
      "step: 21610 train: 0.12760376930236816 elapsed, loss: 2.9471619e-06\n",
      "step: 21620 train: 0.1373298168182373 elapsed, loss: 5.728073e-06\n",
      "step: 21630 train: 0.1276857852935791 elapsed, loss: 3.8882526e-06\n",
      "step: 21640 train: 0.1337723731994629 elapsed, loss: 3.7485397e-06\n",
      "step: 21650 train: 0.13235688209533691 elapsed, loss: 3.4444165e-06\n",
      "step: 21660 train: 0.11577486991882324 elapsed, loss: 3.9785946e-06\n",
      "step: 21670 train: 0.1458570957183838 elapsed, loss: 2.5867414e-06\n",
      "step: 21680 train: 0.12506604194641113 elapsed, loss: 2.8800944e-06\n",
      "step: 21690 train: 0.12955617904663086 elapsed, loss: 2.7380781e-06\n",
      "step: 21700 train: 0.13346529006958008 elapsed, loss: 2.6235261e-06\n",
      "step: 21710 train: 0.12854290008544922 elapsed, loss: 3.96368e-06\n",
      "step: 21720 train: 0.13553118705749512 elapsed, loss: 2.5578706e-06\n",
      "step: 21730 train: 0.12278509140014648 elapsed, loss: 2.9238713e-06\n",
      "step: 21740 train: 0.14371156692504883 elapsed, loss: 2.708744e-06\n",
      "step: 21750 train: 0.12401580810546875 elapsed, loss: 3.2829023e-06\n",
      "step: 21760 train: 0.12385177612304688 elapsed, loss: 4.4321414e-06\n",
      "step: 21770 train: 0.11677956581115723 elapsed, loss: 4.363222e-06\n",
      "step: 21780 train: 0.12927770614624023 elapsed, loss: 3.268931e-06\n",
      "step: 21790 train: 0.1347811222076416 elapsed, loss: 2.2798736e-06\n",
      "step: 21800 train: 0.12966370582580566 elapsed, loss: 3.866834e-06\n",
      "step: 21810 train: 0.1278858184814453 elapsed, loss: 2.8349386e-06\n",
      "step: 21820 train: 0.12629413604736328 elapsed, loss: 3.192562e-06\n",
      "step: 21830 train: 0.1273329257965088 elapsed, loss: 2.1741687e-06\n",
      "step: 21840 train: 0.1361701488494873 elapsed, loss: 2.2770787e-06\n",
      "step: 21850 train: 0.12938261032104492 elapsed, loss: 2.9653224e-06\n",
      "step: 21860 train: 0.13884973526000977 elapsed, loss: 2.981155e-06\n",
      "step: 21870 train: 0.13274455070495605 elapsed, loss: 3.3401425e-06\n",
      "step: 21880 train: 0.13466596603393555 elapsed, loss: 3.44588e-06\n",
      "step: 21890 train: 0.12909197807312012 elapsed, loss: 4.2100173e-06\n",
      "step: 21900 train: 0.12382960319519043 elapsed, loss: 3.076148e-06\n",
      "step: 21910 train: 0.1317734718322754 elapsed, loss: 2.509907e-06\n",
      "step: 21920 train: 0.12412619590759277 elapsed, loss: 4.159271e-06\n",
      "step: 21930 train: 0.12744641304016113 elapsed, loss: 4.2812517e-06\n",
      "step: 21940 train: 0.13228368759155273 elapsed, loss: 3.8109583e-06\n",
      "step: 21950 train: 0.12675833702087402 elapsed, loss: 3.2796288e-06\n",
      "step: 21960 train: 0.12813711166381836 elapsed, loss: 3.586044e-06\n",
      "step: 21970 train: 0.12968111038208008 elapsed, loss: 3.7988493e-06\n",
      "step: 21980 train: 0.12707066535949707 elapsed, loss: 0.00013397908\n",
      "step: 21990 train: 0.1406552791595459 elapsed, loss: 2.8321334e-05\n",
      "step: 22000 train: 0.12414145469665527 elapsed, loss: 5.532655e-05\n",
      "step: 22010 train: 0.12155675888061523 elapsed, loss: 9.3051e-06\n",
      "step: 22020 train: 0.14437580108642578 elapsed, loss: 9.662963e-06\n",
      "step: 22030 train: 0.13597679138183594 elapsed, loss: 2.2806755e-05\n",
      "step: 22040 train: 0.13747191429138184 elapsed, loss: 5.128266e-06\n",
      "step: 22050 train: 0.12985730171203613 elapsed, loss: 5.546449e-06\n",
      "step: 22060 train: 0.13672184944152832 elapsed, loss: 4.1541434e-06\n",
      "step: 22070 train: 0.13310956954956055 elapsed, loss: 4.99465e-06\n",
      "step: 22080 train: 0.12808775901794434 elapsed, loss: 5.6284152e-06\n",
      "step: 22090 train: 0.13035964965820312 elapsed, loss: 3.2335415e-06\n",
      "step: 22100 train: 0.13235163688659668 elapsed, loss: 2.9634548e-06\n",
      "step: 22110 train: 0.1260535717010498 elapsed, loss: 5.0382073e-06\n",
      "step: 22120 train: 0.12813878059387207 elapsed, loss: 4.067038e-06\n",
      "step: 22130 train: 0.12259864807128906 elapsed, loss: 4.7613503e-06\n",
      "step: 22140 train: 0.1211709976196289 elapsed, loss: 5.4188376e-06\n",
      "step: 22150 train: 0.12926602363586426 elapsed, loss: 3.2996627e-06\n",
      "step: 22160 train: 0.13014817237854004 elapsed, loss: 3.7527213e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 22170 train: 0.12443017959594727 elapsed, loss: 3.843081e-06\n",
      "step: 22180 train: 0.136002779006958 elapsed, loss: 2.713401e-06\n",
      "step: 22190 train: 0.13521218299865723 elapsed, loss: 1.9902322e-06\n",
      "step: 22200 train: 0.13218045234680176 elapsed, loss: 2.7888377e-06\n",
      "step: 22210 train: 0.1366877555847168 elapsed, loss: 2.444716e-06\n",
      "step: 22220 train: 0.12526774406433105 elapsed, loss: 3.3262063e-06\n",
      "step: 22230 train: 0.13918757438659668 elapsed, loss: 2.8107231e-06\n",
      "step: 22240 train: 0.14220118522644043 elapsed, loss: 3.761256e-06\n",
      "step: 22250 train: 0.13246941566467285 elapsed, loss: 3.7820619e-06\n",
      "step: 22260 train: 0.12231588363647461 elapsed, loss: 5.8560327e-06\n",
      "step: 22270 train: 0.1351149082183838 elapsed, loss: 3.0561255e-06\n",
      "step: 22280 train: 0.12111210823059082 elapsed, loss: 4.813985e-06\n",
      "step: 22290 train: 0.1284945011138916 elapsed, loss: 4.0929735e-06\n",
      "step: 22300 train: 0.12015748023986816 elapsed, loss: 4.7245603e-06\n",
      "step: 22310 train: 0.1253342628479004 elapsed, loss: 3.6479787e-06\n",
      "step: 22320 train: 0.12684202194213867 elapsed, loss: 3.1948853e-06\n",
      "step: 22330 train: 0.13227200508117676 elapsed, loss: 2.2398262e-06\n",
      "step: 22340 train: 0.1321120262145996 elapsed, loss: 4.3324817e-06\n",
      "step: 22350 train: 0.13547301292419434 elapsed, loss: 2.7315577e-06\n",
      "step: 22360 train: 0.12714171409606934 elapsed, loss: 3.8048997e-06\n",
      "step: 22370 train: 0.13790392875671387 elapsed, loss: 2.8237316e-06\n",
      "step: 22380 train: 0.12934041023254395 elapsed, loss: 5.5943683e-06\n",
      "step: 22390 train: 0.13692855834960938 elapsed, loss: 2.0121183e-06\n",
      "step: 22400 train: 0.12984490394592285 elapsed, loss: 3.056584e-06\n",
      "step: 22410 train: 0.12624192237854004 elapsed, loss: 3.6279503e-06\n",
      "step: 22420 train: 0.14195775985717773 elapsed, loss: 2.2519332e-06\n",
      "step: 22430 train: 0.12739038467407227 elapsed, loss: 3.5529818e-06\n",
      "step: 22440 train: 0.1292896270751953 elapsed, loss: 2.8624115e-06\n",
      "step: 22450 train: 0.1316981315612793 elapsed, loss: 2.396753e-06\n",
      "step: 22460 train: 0.1262037754058838 elapsed, loss: 3.4910397e-06\n",
      "step: 22470 train: 0.12373137474060059 elapsed, loss: 6.257954e-06\n",
      "step: 22480 train: 0.1270887851715088 elapsed, loss: 4.3427244e-06\n",
      "step: 22490 train: 0.12462544441223145 elapsed, loss: 3.1553102e-06\n",
      "step: 22500 train: 0.14136624336242676 elapsed, loss: 2.833076e-06\n",
      "step: 22510 train: 0.12410306930541992 elapsed, loss: 4.1248095e-06\n",
      "step: 22520 train: 0.13373875617980957 elapsed, loss: 2.7343317e-06\n",
      "step: 22530 train: 0.12795162200927734 elapsed, loss: 2.7515835e-06\n",
      "step: 22540 train: 0.12762951850891113 elapsed, loss: 4.641197e-06\n",
      "step: 22550 train: 0.1249234676361084 elapsed, loss: 4.287768e-06\n",
      "step: 22560 train: 0.125136137008667 elapsed, loss: 3.8612434e-06\n",
      "step: 22570 train: 0.13544893264770508 elapsed, loss: 2.9262048e-06\n",
      "step: 22580 train: 0.11791038513183594 elapsed, loss: 5.5049795e-06\n",
      "step: 22590 train: 0.12473011016845703 elapsed, loss: 3.556192e-06\n",
      "step: 22600 train: 0.13279271125793457 elapsed, loss: 2.1876726e-06\n",
      "step: 22610 train: 0.13082337379455566 elapsed, loss: 3.6321453e-06\n",
      "step: 22620 train: 0.13457489013671875 elapsed, loss: 2.6263097e-06\n",
      "step: 22630 train: 0.12732934951782227 elapsed, loss: 2.549954e-06\n",
      "step: 22640 train: 0.12416434288024902 elapsed, loss: 4.9066457e-06\n",
      "step: 22650 train: 0.1344451904296875 elapsed, loss: 2.598839e-06\n",
      "step: 22660 train: 0.13358497619628906 elapsed, loss: 3.0221327e-06\n",
      "step: 22670 train: 0.125901460647583 elapsed, loss: 3.1213165e-06\n",
      "step: 22680 train: 0.12804198265075684 elapsed, loss: 4.1182902e-06\n",
      "step: 22690 train: 0.13013577461242676 elapsed, loss: 2.6486737e-06\n",
      "step: 22700 train: 0.12890148162841797 elapsed, loss: 3.161363e-06\n",
      "step: 22710 train: 0.13727426528930664 elapsed, loss: 2.4689277e-06\n",
      "step: 22720 train: 0.123199462890625 elapsed, loss: 4.1019675e-06\n",
      "step: 22730 train: 0.1297893524169922 elapsed, loss: 2.5462302e-06\n",
      "step: 22740 train: 0.1343545913696289 elapsed, loss: 3.3830195e-06\n",
      "step: 22750 train: 0.12836480140686035 elapsed, loss: 2.0167754e-06\n",
      "step: 22760 train: 0.12400245666503906 elapsed, loss: 3.1520494e-06\n",
      "step: 22770 train: 0.12402796745300293 elapsed, loss: 5.104551e-06\n",
      "step: 22780 train: 0.13375306129455566 elapsed, loss: 2.282663e-06\n",
      "step: 22790 train: 0.1270921230316162 elapsed, loss: 3.3420208e-06\n",
      "step: 22800 train: 0.12857890129089355 elapsed, loss: 3.6647407e-06\n",
      "step: 22810 train: 0.1312093734741211 elapsed, loss: 2.3753314e-06\n",
      "step: 22820 train: 0.13628911972045898 elapsed, loss: 1.0373249e-05\n",
      "step: 22830 train: 0.13266873359680176 elapsed, loss: 6.394607e-06\n",
      "step: 22840 train: 0.13748621940612793 elapsed, loss: 3.1241125e-06\n",
      "step: 22850 train: 0.1263904571533203 elapsed, loss: 3.7764848e-06\n",
      "step: 22860 train: 0.1295478343963623 elapsed, loss: 9.196774e-06\n",
      "step: 22870 train: 0.13238000869750977 elapsed, loss: 2.7348203e-06\n",
      "step: 22880 train: 0.12540531158447266 elapsed, loss: 4.0419222e-06\n",
      "step: 22890 train: 0.14133334159851074 elapsed, loss: 2.704553e-06\n",
      "step: 22900 train: 0.13358664512634277 elapsed, loss: 3.174871e-06\n",
      "step: 22910 train: 0.13636493682861328 elapsed, loss: 2.1052508e-06\n",
      "step: 22920 train: 0.12971949577331543 elapsed, loss: 2.7427218e-06\n",
      "step: 22930 train: 0.12717151641845703 elapsed, loss: 4.863338e-06\n",
      "step: 22940 train: 0.13193035125732422 elapsed, loss: 3.716199e-06\n",
      "step: 22950 train: 0.11713075637817383 elapsed, loss: 8.058897e-06\n",
      "step: 22960 train: 0.1269974708557129 elapsed, loss: 3.9879096e-06\n",
      "step: 22970 train: 0.1329479217529297 elapsed, loss: 2.904318e-06\n",
      "step: 22980 train: 0.13204503059387207 elapsed, loss: 8.395104e-06\n",
      "step: 22990 train: 0.13595080375671387 elapsed, loss: 4.01584e-06\n",
      "step: 23000 train: 0.12075686454772949 elapsed, loss: 4.5606644e-06\n",
      "step: 23010 train: 0.13411951065063477 elapsed, loss: 3.64565e-06\n",
      "step: 23020 train: 0.12329649925231934 elapsed, loss: 4.3921e-06\n",
      "step: 23030 train: 0.12050366401672363 elapsed, loss: 9.90893e-06\n",
      "step: 23040 train: 0.12534546852111816 elapsed, loss: 3.6829033e-06\n",
      "step: 23050 train: 0.1342146396636963 elapsed, loss: 2.5140907e-06\n",
      "step: 23060 train: 0.12351202964782715 elapsed, loss: 3.8780126e-06\n",
      "step: 23070 train: 0.13274002075195312 elapsed, loss: 3.7471573e-06\n",
      "step: 23080 train: 0.1266312599182129 elapsed, loss: 2.585808e-06\n",
      "step: 23090 train: 0.13099050521850586 elapsed, loss: 3.8875432e-06\n",
      "step: 23100 train: 0.13759589195251465 elapsed, loss: 2.6263233e-06\n",
      "step: 23110 train: 0.12710094451904297 elapsed, loss: 3.3699562e-06\n",
      "step: 23120 train: 0.13123226165771484 elapsed, loss: 2.6295822e-06\n",
      "step: 23130 train: 0.12876033782958984 elapsed, loss: 5.911628e-05\n",
      "step: 23140 train: 0.1277141571044922 elapsed, loss: 1.0971478e-05\n",
      "step: 23150 train: 0.12364006042480469 elapsed, loss: 7.449853e-06\n",
      "step: 23160 train: 0.1205904483795166 elapsed, loss: 5.3392364e-06\n",
      "step: 23170 train: 0.13035893440246582 elapsed, loss: 4.8442453e-06\n",
      "step: 23180 train: 0.12807369232177734 elapsed, loss: 4.296171e-06\n",
      "step: 23190 train: 0.12088680267333984 elapsed, loss: 4.284996e-06\n",
      "step: 23200 train: 0.13381552696228027 elapsed, loss: 3.3979145e-06\n",
      "step: 23210 train: 0.1250898838043213 elapsed, loss: 3.2917503e-06\n",
      "step: 23220 train: 0.13251471519470215 elapsed, loss: 2.5448335e-06\n",
      "step: 23230 train: 0.1279311180114746 elapsed, loss: 3.208391e-06\n",
      "step: 23240 train: 0.12343239784240723 elapsed, loss: 3.0989647e-06\n",
      "step: 23250 train: 0.12706637382507324 elapsed, loss: 4.078235e-06\n",
      "step: 23260 train: 0.14197874069213867 elapsed, loss: 0.13594511\n",
      "step: 23270 train: 0.13108015060424805 elapsed, loss: 9.863251e-05\n",
      "step: 23280 train: 0.1288137435913086 elapsed, loss: 4.9776536e-05\n",
      "step: 23290 train: 0.1213524341583252 elapsed, loss: 3.732201e-05\n",
      "step: 23300 train: 0.1454465389251709 elapsed, loss: 1.406099e-05\n",
      "step: 23310 train: 0.12354707717895508 elapsed, loss: 0.015070249\n",
      "step: 23320 train: 0.12590622901916504 elapsed, loss: 1.6672224e-05\n",
      "step: 23330 train: 0.12792181968688965 elapsed, loss: 1.6041573e-05\n",
      "step: 23340 train: 0.1314849853515625 elapsed, loss: 2.4991947e-05\n",
      "step: 23350 train: 0.13609695434570312 elapsed, loss: 9.4879615e-06\n",
      "step: 23360 train: 0.14499950408935547 elapsed, loss: 1.8847972e-05\n",
      "step: 23370 train: 0.12168145179748535 elapsed, loss: 1.4474618e-05\n",
      "step: 23380 train: 0.1270737648010254 elapsed, loss: 8.241983e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 23390 train: 0.13208246231079102 elapsed, loss: 4.8293314e-06\n",
      "step: 23400 train: 0.13295459747314453 elapsed, loss: 5.9170743e-06\n",
      "step: 23410 train: 0.11647558212280273 elapsed, loss: 8.351679e-06\n",
      "step: 23420 train: 0.1334385871887207 elapsed, loss: 3.528758e-06\n",
      "step: 23430 train: 0.13074421882629395 elapsed, loss: 3.9837064e-06\n",
      "step: 23440 train: 0.12277984619140625 elapsed, loss: 3.4281743e-06\n",
      "step: 23450 train: 0.12248086929321289 elapsed, loss: 2.8163113e-06\n",
      "step: 23460 train: 0.12889528274536133 elapsed, loss: 3.902688e-06\n",
      "step: 23470 train: 0.14282798767089844 elapsed, loss: 2.7194171e-06\n",
      "step: 23480 train: 0.13448882102966309 elapsed, loss: 1.9506497e-06\n",
      "step: 23490 train: 0.1397538185119629 elapsed, loss: 2.299893e-06\n",
      "step: 23500 train: 0.13463211059570312 elapsed, loss: 1.7760281e-06\n",
      "step: 23510 train: 0.11342859268188477 elapsed, loss: 3.9753318e-06\n",
      "step: 23520 train: 0.12703371047973633 elapsed, loss: 2.2421534e-06\n",
      "step: 23530 train: 0.12500452995300293 elapsed, loss: 2.8940572e-06\n",
      "step: 23540 train: 0.13195252418518066 elapsed, loss: 2.1276014e-06\n",
      "step: 23550 train: 0.13475990295410156 elapsed, loss: 2.445181e-06\n",
      "step: 23560 train: 0.1293315887451172 elapsed, loss: 2.2919771e-06\n",
      "step: 23570 train: 0.12664365768432617 elapsed, loss: 2.1741664e-06\n",
      "step: 23580 train: 0.12988042831420898 elapsed, loss: 2.3864764e-06\n",
      "step: 23590 train: 0.131331205368042 elapsed, loss: 2.2784752e-06\n",
      "step: 23600 train: 0.12492704391479492 elapsed, loss: 2.0042025e-06\n",
      "step: 23610 train: 0.13962173461914062 elapsed, loss: 1.7154938e-06\n",
      "step: 23620 train: 0.14323019981384277 elapsed, loss: 2.131791e-06\n",
      "step: 23630 train: 0.12424159049987793 elapsed, loss: 2.6132848e-06\n",
      "step: 23640 train: 0.15149736404418945 elapsed, loss: 1.4775413e-06\n",
      "step: 23650 train: 0.12821030616760254 elapsed, loss: 2.4256094e-06\n",
      "step: 23660 train: 0.11939668655395508 elapsed, loss: 3.3941903e-06\n",
      "step: 23670 train: 0.12968158721923828 elapsed, loss: 2.0610125e-06\n",
      "step: 23680 train: 0.12479114532470703 elapsed, loss: 4.35293e-06\n",
      "step: 23690 train: 0.127821683883667 elapsed, loss: 2.9234134e-06\n",
      "step: 23700 train: 0.12573933601379395 elapsed, loss: 3.6083916e-06\n",
      "step: 23710 train: 0.13747549057006836 elapsed, loss: 1.7289973e-06\n",
      "step: 23720 train: 0.14529919624328613 elapsed, loss: 2.1923283e-06\n",
      "step: 23730 train: 0.12297487258911133 elapsed, loss: 5.404818e-06\n",
      "step: 23740 train: 0.12359070777893066 elapsed, loss: 3.1632273e-06\n",
      "step: 23750 train: 0.12558531761169434 elapsed, loss: 3.6661218e-06\n",
      "step: 23760 train: 0.12413907051086426 elapsed, loss: 2.5266718e-06\n",
      "step: 23770 train: 0.1267697811126709 elapsed, loss: 2.8344723e-06\n",
      "step: 23780 train: 0.12976574897766113 elapsed, loss: 2.2523996e-06\n",
      "step: 23790 train: 0.1299135684967041 elapsed, loss: 3.330818e-06\n",
      "step: 23800 train: 0.1251850128173828 elapsed, loss: 3.050537e-06\n",
      "step: 23810 train: 0.12920308113098145 elapsed, loss: 2.8195707e-06\n",
      "step: 23820 train: 0.13413596153259277 elapsed, loss: 2.316658e-06\n",
      "step: 23830 train: 0.12436032295227051 elapsed, loss: 3.075213e-06\n",
      "step: 23840 train: 0.11974620819091797 elapsed, loss: 4.4321296e-06\n",
      "step: 23850 train: 0.12226033210754395 elapsed, loss: 3.03331e-06\n",
      "step: 23860 train: 0.12784051895141602 elapsed, loss: 2.8949817e-06\n",
      "step: 23870 train: 0.12620830535888672 elapsed, loss: 3.6367994e-06\n",
      "step: 23880 train: 0.13272643089294434 elapsed, loss: 3.7094435e-06\n",
      "step: 23890 train: 0.1409015655517578 elapsed, loss: 0.00027634017\n",
      "step: 23900 train: 0.13750147819519043 elapsed, loss: 2.8980883e-05\n",
      "step: 23910 train: 0.14476990699768066 elapsed, loss: 1.6480422e-05\n",
      "step: 23920 train: 0.11625385284423828 elapsed, loss: 3.1664167e-05\n",
      "step: 23930 train: 0.13511013984680176 elapsed, loss: 1.045057e-05\n",
      "step: 23940 train: 0.12445926666259766 elapsed, loss: 8.302173e-06\n",
      "step: 23950 train: 0.12004899978637695 elapsed, loss: 9.750886e-06\n",
      "step: 23960 train: 0.12480497360229492 elapsed, loss: 6.1899145e-06\n",
      "step: 23970 train: 0.14016056060791016 elapsed, loss: 5.020716e-06\n",
      "step: 23980 train: 0.13436007499694824 elapsed, loss: 3.027248e-06\n",
      "step: 23990 train: 0.12310242652893066 elapsed, loss: 5.2958676e-06\n",
      "step: 24000 train: 0.13171720504760742 elapsed, loss: 3.691264e-06\n",
      "step: 24010 train: 0.1245272159576416 elapsed, loss: 3.7536788e-06\n",
      "step: 24020 train: 0.1313033103942871 elapsed, loss: 3.6628744e-06\n",
      "step: 24030 train: 0.1284017562866211 elapsed, loss: 3.027716e-06\n",
      "step: 24040 train: 0.12974786758422852 elapsed, loss: 3.358704e-06\n",
      "step: 24050 train: 0.12156105041503906 elapsed, loss: 4.6230407e-06\n",
      "step: 24060 train: 0.1311490535736084 elapsed, loss: 2.5140976e-06\n",
      "step: 24070 train: 0.11977505683898926 elapsed, loss: 3.2959363e-06\n",
      "step: 24080 train: 0.12616562843322754 elapsed, loss: 3.6340086e-06\n",
      "step: 24090 train: 0.1215212345123291 elapsed, loss: 3.6917338e-06\n",
      "step: 24100 train: 0.13031864166259766 elapsed, loss: 2.0982657e-06\n",
      "step: 24110 train: 0.1232602596282959 elapsed, loss: 2.8991858e-06\n",
      "step: 24120 train: 0.13185715675354004 elapsed, loss: 1.5962846e-06\n",
      "step: 24130 train: 0.12904047966003418 elapsed, loss: 3.06171e-06\n",
      "step: 24140 train: 0.11867237091064453 elapsed, loss: 3.8039632e-06\n",
      "step: 24150 train: 0.12331628799438477 elapsed, loss: 2.797217e-06\n",
      "step: 24160 train: 0.1289961338043213 elapsed, loss: 2.305485e-06\n",
      "step: 24170 train: 0.13179588317871094 elapsed, loss: 2.5541444e-06\n",
      "step: 24180 train: 0.12492990493774414 elapsed, loss: 2.4386622e-06\n",
      "step: 24190 train: 0.13010716438293457 elapsed, loss: 2.5671848e-06\n",
      "step: 24200 train: 0.1269066333770752 elapsed, loss: 3.8863936e-06\n",
      "step: 24210 train: 0.1326894760131836 elapsed, loss: 2.6072244e-06\n",
      "step: 24220 train: 0.140716552734375 elapsed, loss: 2.6388939e-06\n",
      "step: 24230 train: 0.12398695945739746 elapsed, loss: 4.0013697e-06\n",
      "step: 24240 train: 0.13241147994995117 elapsed, loss: 3.6274018e-06\n",
      "step: 24250 train: 0.13134336471557617 elapsed, loss: 3.7834861e-06\n",
      "step: 24260 train: 0.1384718418121338 elapsed, loss: 3.3806855e-06\n",
      "step: 24270 train: 0.1444246768951416 elapsed, loss: 2.539707e-06\n",
      "step: 24280 train: 0.1333177089691162 elapsed, loss: 2.8149086e-06\n",
      "step: 24290 train: 0.12897038459777832 elapsed, loss: 2.6966386e-06\n",
      "step: 24300 train: 0.1299140453338623 elapsed, loss: 2.4582164e-06\n",
      "step: 24310 train: 0.1318526268005371 elapsed, loss: 2.390233e-06\n",
      "step: 24320 train: 0.1242372989654541 elapsed, loss: 3.0840633e-06\n",
      "step: 24330 train: 0.13210487365722656 elapsed, loss: 2.5550771e-06\n",
      "step: 24340 train: 0.13326144218444824 elapsed, loss: 3.3485471e-06\n",
      "step: 24350 train: 0.12212920188903809 elapsed, loss: 4.4600765e-06\n",
      "step: 24360 train: 0.13012027740478516 elapsed, loss: 3.081689e-06\n",
      "step: 24370 train: 0.12082433700561523 elapsed, loss: 3.4966386e-06\n",
      "step: 24380 train: 0.12979984283447266 elapsed, loss: 2.6882549e-06\n",
      "step: 24390 train: 0.13286805152893066 elapsed, loss: 3.20281e-06\n",
      "step: 24400 train: 0.13464903831481934 elapsed, loss: 2.564856e-06\n",
      "step: 24410 train: 0.12091684341430664 elapsed, loss: 0.0008232844\n",
      "step: 24420 train: 0.1263446807861328 elapsed, loss: 8.276549e-06\n",
      "step: 24430 train: 0.12886476516723633 elapsed, loss: 8.774316e-06\n",
      "step: 24440 train: 0.12808942794799805 elapsed, loss: 8.508442e-06\n",
      "step: 24450 train: 0.1258711814880371 elapsed, loss: 5.5128517e-06\n",
      "step: 24460 train: 0.13194775581359863 elapsed, loss: 3.625607e-06\n",
      "step: 24470 train: 0.12702274322509766 elapsed, loss: 7.0119227e-06\n",
      "step: 24480 train: 0.12652206420898438 elapsed, loss: 5.7312604e-06\n",
      "step: 24490 train: 0.13502883911132812 elapsed, loss: 4.2398287e-06\n",
      "step: 24500 train: 0.12807273864746094 elapsed, loss: 5.7340776e-06\n",
      "step: 24510 train: 0.1300182342529297 elapsed, loss: 2.6794087e-06\n",
      "step: 24520 train: 0.12447094917297363 elapsed, loss: 2.5909328e-06\n",
      "step: 24530 train: 0.12644290924072266 elapsed, loss: 3.8593826e-06\n",
      "step: 24540 train: 0.11999082565307617 elapsed, loss: 4.1913736e-06\n",
      "step: 24550 train: 0.13983154296875 elapsed, loss: 2.6313928e-06\n",
      "step: 24560 train: 0.14168190956115723 elapsed, loss: 3.600807e-06\n",
      "step: 24570 train: 0.13617610931396484 elapsed, loss: 2.0349364e-06\n",
      "step: 24580 train: 0.12368011474609375 elapsed, loss: 3.4714922e-06\n",
      "step: 24590 train: 0.12242650985717773 elapsed, loss: 8.576915e-06\n",
      "step: 24600 train: 0.12381267547607422 elapsed, loss: 0.00023288123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24610 train: 0.13222193717956543 elapsed, loss: 1.7599164e-05\n",
      "step: 24620 train: 0.12923288345336914 elapsed, loss: 1.756594e-05\n",
      "step: 24630 train: 0.13535261154174805 elapsed, loss: 1.17882155e-05\n",
      "step: 24640 train: 0.12169218063354492 elapsed, loss: 1.607197e-05\n",
      "step: 24650 train: 0.12199687957763672 elapsed, loss: 1.6030193e-05\n",
      "step: 24660 train: 0.12694907188415527 elapsed, loss: 6.555941e-06\n",
      "step: 24670 train: 0.13147616386413574 elapsed, loss: 6.2602785e-06\n",
      "step: 24680 train: 0.1363966464996338 elapsed, loss: 1.38413325e-05\n",
      "step: 24690 train: 0.13175535202026367 elapsed, loss: 5.335508e-06\n",
      "step: 24700 train: 0.13522887229919434 elapsed, loss: 4.289634e-06\n",
      "step: 24710 train: 0.13372325897216797 elapsed, loss: 3.8887124e-06\n",
      "step: 24720 train: 0.13815712928771973 elapsed, loss: 2.923413e-06\n",
      "step: 24730 train: 0.13426995277404785 elapsed, loss: 5.773649e-06\n",
      "step: 24740 train: 0.13656020164489746 elapsed, loss: 2.8558864e-06\n",
      "step: 24750 train: 0.13977289199829102 elapsed, loss: 5.181765e-06\n",
      "step: 24760 train: 0.13072514533996582 elapsed, loss: 3.1371487e-06\n",
      "step: 24770 train: 0.1370089054107666 elapsed, loss: 4.027017e-06\n",
      "step: 24780 train: 0.13476777076721191 elapsed, loss: 2.604437e-06\n",
      "step: 24790 train: 0.14082789421081543 elapsed, loss: 4.3515474e-06\n",
      "step: 24800 train: 0.12309932708740234 elapsed, loss: 2.1992537e-05\n",
      "step: 24810 train: 0.12294411659240723 elapsed, loss: 0.012158254\n",
      "step: 24820 train: 0.12350654602050781 elapsed, loss: 3.6577112e-05\n",
      "step: 24830 train: 0.12451553344726562 elapsed, loss: 1.735669e-05\n",
      "step: 24840 train: 0.11608552932739258 elapsed, loss: 1.9804334e-05\n",
      "step: 24850 train: 0.12741780281066895 elapsed, loss: 8.033033e-06\n",
      "step: 24860 train: 0.13501596450805664 elapsed, loss: 5.605556e-06\n",
      "step: 24870 train: 0.12976884841918945 elapsed, loss: 6.215105e-06\n",
      "step: 24880 train: 0.13416099548339844 elapsed, loss: 4.403718e-06\n",
      "step: 24890 train: 0.12756800651550293 elapsed, loss: 3.8570465e-06\n",
      "step: 24900 train: 0.14323163032531738 elapsed, loss: 2.5327236e-06\n",
      "step: 24910 train: 0.12502717971801758 elapsed, loss: 3.5585588e-06\n",
      "step: 24920 train: 0.12781429290771484 elapsed, loss: 3.7196846e-06\n",
      "step: 24930 train: 0.1312248706817627 elapsed, loss: 3.2475077e-06\n",
      "step: 24940 train: 0.12265610694885254 elapsed, loss: 3.3876672e-06\n",
      "step: 24950 train: 0.12412023544311523 elapsed, loss: 5.179949e-06\n",
      "step: 24960 train: 0.12987947463989258 elapsed, loss: 2.3408727e-06\n",
      "step: 24970 train: 0.12900304794311523 elapsed, loss: 2.8582201e-06\n",
      "step: 24980 train: 0.1251816749572754 elapsed, loss: 3.3773977e-06\n",
      "step: 24990 train: 0.11596298217773438 elapsed, loss: 2.942506e-06\n",
      "step: 25000 train: 0.13690590858459473 elapsed, loss: 2.3757962e-06\n",
      "step: 25010 train: 0.12949490547180176 elapsed, loss: 2.758531e-06\n",
      "step: 25020 train: 0.12233185768127441 elapsed, loss: 2.8758984e-06\n",
      "step: 25030 train: 0.12316417694091797 elapsed, loss: 2.1988487e-06\n",
      "step: 25040 train: 0.13131189346313477 elapsed, loss: 2.5359793e-06\n",
      "step: 25050 train: 0.1316232681274414 elapsed, loss: 2.4340038e-06\n",
      "step: 25060 train: 0.13007712364196777 elapsed, loss: 4.994744e-06\n",
      "step: 25070 train: 0.1403510570526123 elapsed, loss: 1.4770758e-06\n",
      "step: 25080 train: 0.12687444686889648 elapsed, loss: 2.0326074e-06\n",
      "step: 25090 train: 0.11441469192504883 elapsed, loss: 4.936427e-06\n",
      "step: 25100 train: 0.1238863468170166 elapsed, loss: 2.7250308e-06\n",
      "step: 25110 train: 0.12745070457458496 elapsed, loss: 1.9189865e-06\n",
      "step: 25120 train: 0.1200704574584961 elapsed, loss: 3.4127675e-06\n",
      "step: 25130 train: 0.12303376197814941 elapsed, loss: 3.1613536e-06\n",
      "step: 25140 train: 0.12248468399047852 elapsed, loss: 2.8353945e-06\n",
      "step: 25150 train: 0.11886811256408691 elapsed, loss: 3.1692794e-06\n",
      "step: 25160 train: 0.1193697452545166 elapsed, loss: 3.2386647e-06\n",
      "step: 25170 train: 0.14665842056274414 elapsed, loss: 2.2035047e-06\n",
      "step: 25180 train: 0.12679266929626465 elapsed, loss: 3.565086e-06\n",
      "step: 25190 train: 0.1306462287902832 elapsed, loss: 2.277075e-06\n",
      "step: 25200 train: 0.12726235389709473 elapsed, loss: 2.7995407e-06\n",
      "step: 25210 train: 0.12329316139221191 elapsed, loss: 3.518984e-06\n",
      "step: 25220 train: 0.12392401695251465 elapsed, loss: 3.3033882e-06\n",
      "step: 25230 train: 0.12458419799804688 elapsed, loss: 2.4745182e-06\n",
      "step: 25240 train: 0.12224149703979492 elapsed, loss: 3.2978026e-06\n",
      "step: 25250 train: 0.12473320960998535 elapsed, loss: 3.5469293e-06\n",
      "step: 25260 train: 0.1360635757446289 elapsed, loss: 1.7345853e-06\n",
      "step: 25270 train: 0.12408924102783203 elapsed, loss: 2.356707e-06\n",
      "step: 25280 train: 0.136199951171875 elapsed, loss: 1.7518146e-06\n",
      "step: 25290 train: 0.13197064399719238 elapsed, loss: 3.4416912e-06\n",
      "step: 25300 train: 0.13705801963806152 elapsed, loss: 2.1504197e-06\n",
      "step: 25310 train: 0.12785100936889648 elapsed, loss: 2.1201517e-06\n",
      "step: 25320 train: 0.13227295875549316 elapsed, loss: 2.9397083e-06\n",
      "step: 25330 train: 0.13594532012939453 elapsed, loss: 2.0498362e-06\n",
      "step: 25340 train: 0.12614655494689941 elapsed, loss: 2.6421553e-06\n",
      "step: 25350 train: 0.12223935127258301 elapsed, loss: 0.0066297213\n",
      "step: 25360 train: 0.1267993450164795 elapsed, loss: 2.563246e-05\n",
      "step: 25370 train: 0.12058782577514648 elapsed, loss: 3.5440884e-05\n",
      "step: 25380 train: 0.13210082054138184 elapsed, loss: 2.1879547e-05\n",
      "step: 25390 train: 0.1322193145751953 elapsed, loss: 1.0590321e-05\n",
      "step: 25400 train: 0.1259782314300537 elapsed, loss: 1.0147904e-05\n",
      "step: 25410 train: 0.11756205558776855 elapsed, loss: 9.437839e-06\n",
      "step: 25420 train: 0.12282180786132812 elapsed, loss: 7.3326582e-06\n",
      "step: 25430 train: 0.128096342086792 elapsed, loss: 4.518277e-06\n",
      "step: 25440 train: 0.12876057624816895 elapsed, loss: 7.0857195e-06\n",
      "step: 25450 train: 0.12842822074890137 elapsed, loss: 3.4808043e-06\n",
      "step: 25460 train: 0.11845016479492188 elapsed, loss: 4.784176e-06\n",
      "step: 25470 train: 0.13631534576416016 elapsed, loss: 4.135984e-06\n",
      "step: 25480 train: 0.1441655158996582 elapsed, loss: 3.4863904e-06\n",
      "step: 25490 train: 0.1251688003540039 elapsed, loss: 3.1650882e-06\n",
      "step: 25500 train: 0.1327207088470459 elapsed, loss: 3.0752158e-06\n",
      "step: 25510 train: 0.12986469268798828 elapsed, loss: 3.0649712e-06\n",
      "step: 25520 train: 0.14495038986206055 elapsed, loss: 2.3129332e-06\n",
      "step: 25530 train: 0.12502312660217285 elapsed, loss: 3.865861e-06\n",
      "step: 25540 train: 0.1251981258392334 elapsed, loss: 2.8488987e-06\n",
      "step: 25550 train: 0.13351035118103027 elapsed, loss: 5.6485837e-06\n",
      "step: 25560 train: 0.1258711814880371 elapsed, loss: 2.9220082e-06\n",
      "step: 25570 train: 0.13837623596191406 elapsed, loss: 2.1681096e-06\n",
      "step: 25580 train: 0.12515020370483398 elapsed, loss: 2.8391255e-06\n",
      "step: 25590 train: 0.12428832054138184 elapsed, loss: 2.3869738e-06\n",
      "step: 25600 train: 0.13288187980651855 elapsed, loss: 2.8093273e-06\n",
      "step: 25610 train: 0.1471116542816162 elapsed, loss: 1.6204992e-06\n",
      "step: 25620 train: 0.1357724666595459 elapsed, loss: 2.4130495e-06\n",
      "step: 25630 train: 0.11681389808654785 elapsed, loss: 6.5940685e-06\n",
      "step: 25640 train: 0.12552714347839355 elapsed, loss: 3.216309e-06\n",
      "step: 25650 train: 0.12810826301574707 elapsed, loss: 3.1134007e-06\n",
      "step: 25660 train: 0.12801361083984375 elapsed, loss: 2.7986157e-06\n",
      "step: 25670 train: 0.12100791931152344 elapsed, loss: 3.712698e-06\n",
      "step: 25680 train: 0.13202452659606934 elapsed, loss: 3.734569e-06\n",
      "step: 25690 train: 0.13530945777893066 elapsed, loss: 2.0614784e-06\n",
      "step: 25700 train: 0.13077020645141602 elapsed, loss: 3.1375887e-06\n",
      "step: 25710 train: 0.12758660316467285 elapsed, loss: 5.1604093e-06\n",
      "step: 25720 train: 0.13386273384094238 elapsed, loss: 2.0135158e-06\n",
      "step: 25730 train: 0.12128853797912598 elapsed, loss: 2.9471626e-06\n",
      "step: 25740 train: 0.13175368309020996 elapsed, loss: 2.4870915e-06\n",
      "step: 25750 train: 0.1249547004699707 elapsed, loss: 3.7331492e-06\n",
      "step: 25760 train: 0.13124418258666992 elapsed, loss: 2.7832484e-06\n",
      "step: 25770 train: 0.12464356422424316 elapsed, loss: 3.4565865e-06\n",
      "step: 25780 train: 0.13411974906921387 elapsed, loss: 2.080105e-06\n",
      "step: 25790 train: 0.12385296821594238 elapsed, loss: 2.5019922e-06\n",
      "step: 25800 train: 0.13446259498596191 elapsed, loss: 2.3622947e-06\n",
      "step: 25810 train: 0.1402266025543213 elapsed, loss: 3.584635e-06\n",
      "step: 25820 train: 0.12985730171203613 elapsed, loss: 2.5005934e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 25830 train: 0.11722803115844727 elapsed, loss: 4.236569e-06\n",
      "step: 25840 train: 0.1205754280090332 elapsed, loss: 3.033309e-06\n",
      "step: 25850 train: 0.12517118453979492 elapsed, loss: 4.0549457e-06\n",
      "step: 25860 train: 0.1394662857055664 elapsed, loss: 2.3208513e-06\n",
      "step: 25870 train: 0.1349794864654541 elapsed, loss: 2.37673e-06\n",
      "step: 25880 train: 0.1342165470123291 elapsed, loss: 1.9217796e-06\n",
      "step: 25890 train: 0.12512779235839844 elapsed, loss: 3.7490245e-06\n",
      "step: 25900 train: 0.13187670707702637 elapsed, loss: 3.287999e-06\n",
      "step: 25910 train: 0.12855315208435059 elapsed, loss: 2.7227084e-06\n",
      "step: 25920 train: 0.1358935832977295 elapsed, loss: 2.7944238e-06\n",
      "step: 25930 train: 0.12475204467773438 elapsed, loss: 3.3378328e-06\n",
      "step: 25940 train: 0.13240885734558105 elapsed, loss: 3.5497246e-06\n",
      "step: 25950 train: 0.12154316902160645 elapsed, loss: 3.5213184e-06\n",
      "step: 25960 train: 0.1330280303955078 elapsed, loss: 2.9266594e-06\n",
      "step: 25970 train: 0.12639570236206055 elapsed, loss: 2.6258567e-06\n",
      "step: 25980 train: 0.13146615028381348 elapsed, loss: 3.202805e-06\n",
      "step: 25990 train: 0.1417093276977539 elapsed, loss: 2.6407583e-06\n",
      "step: 26000 train: 0.13680195808410645 elapsed, loss: 2.270556e-06\n",
      "step: 26010 train: 0.12631559371948242 elapsed, loss: 2.9983744e-06\n",
      "step: 26020 train: 0.12012028694152832 elapsed, loss: 4.2556603e-06\n",
      "step: 26030 train: 0.11960268020629883 elapsed, loss: 4.5145516e-06\n",
      "step: 26040 train: 0.1322004795074463 elapsed, loss: 1.9990798e-06\n",
      "step: 26050 train: 0.1274425983428955 elapsed, loss: 5.2473997e-06\n",
      "step: 26060 train: 0.13118386268615723 elapsed, loss: 4.699414e-06\n",
      "step: 26070 train: 0.12374711036682129 elapsed, loss: 3.9241136e-06\n",
      "step: 26080 train: 0.13301682472229004 elapsed, loss: 2.4065303e-06\n",
      "step: 26090 train: 0.12789583206176758 elapsed, loss: 2.630041e-06\n",
      "step: 26100 train: 0.12752556800842285 elapsed, loss: 2.9448288e-06\n",
      "step: 26110 train: 0.12908673286437988 elapsed, loss: 6.660699e-06\n",
      "step: 26120 train: 0.12723779678344727 elapsed, loss: 3.8859307e-06\n",
      "step: 26130 train: 0.1257483959197998 elapsed, loss: 3.5157245e-06\n",
      "step: 26140 train: 0.13920903205871582 elapsed, loss: 2.0931413e-06\n",
      "step: 26150 train: 0.12102270126342773 elapsed, loss: 3.9911665e-06\n",
      "step: 26160 train: 0.12366771697998047 elapsed, loss: 4.1657804e-06\n",
      "step: 26170 train: 0.12848186492919922 elapsed, loss: 2.816308e-06\n",
      "step: 26180 train: 0.12404417991638184 elapsed, loss: 3.5380526e-06\n",
      "step: 26190 train: 0.12641072273254395 elapsed, loss: 3.6810343e-06\n",
      "step: 26200 train: 0.12692546844482422 elapsed, loss: 3.7578645e-06\n",
      "step: 26210 train: 0.12454032897949219 elapsed, loss: 3.4938305e-06\n",
      "step: 26220 train: 0.12642502784729004 elapsed, loss: 3.8877915e-06\n",
      "step: 26230 train: 0.12455248832702637 elapsed, loss: 3.767181e-06\n",
      "step: 26240 train: 0.13718318939208984 elapsed, loss: 2.492672e-06\n",
      "step: 26250 train: 0.12637948989868164 elapsed, loss: 4.0931454e-06\n",
      "step: 26260 train: 0.13589072227478027 elapsed, loss: 2.4368003e-06\n",
      "step: 26270 train: 0.13339471817016602 elapsed, loss: 3.833307e-06\n",
      "step: 26280 train: 0.13760900497436523 elapsed, loss: 3.89151e-06\n",
      "step: 26290 train: 0.12991786003112793 elapsed, loss: 3.457987e-06\n",
      "step: 26300 train: 0.1278393268585205 elapsed, loss: 2.9597345e-06\n",
      "step: 26310 train: 0.1312084197998047 elapsed, loss: 2.700363e-06\n",
      "step: 26320 train: 0.12215614318847656 elapsed, loss: 4.081951e-06\n",
      "step: 26330 train: 0.13991618156433105 elapsed, loss: 3.583244e-06\n",
      "step: 26340 train: 0.13740301132202148 elapsed, loss: 3.6326064e-06\n",
      "step: 26350 train: 0.12458419799804688 elapsed, loss: 5.032762e-06\n",
      "step: 26360 train: 0.12510967254638672 elapsed, loss: 5.942544e-06\n",
      "step: 26370 train: 0.1383523941040039 elapsed, loss: 4.167645e-06\n",
      "step: 26380 train: 0.12571954727172852 elapsed, loss: 5.7781485e-06\n",
      "step: 26390 train: 0.12453317642211914 elapsed, loss: 3.8440157e-06\n",
      "step: 26400 train: 0.13661575317382812 elapsed, loss: 2.6444839e-06\n",
      "step: 26410 train: 0.12486529350280762 elapsed, loss: 3.1283043e-06\n",
      "step: 26420 train: 0.12211251258850098 elapsed, loss: 3.8524017e-06\n",
      "step: 26430 train: 0.14650917053222656 elapsed, loss: 1.918987e-06\n",
      "step: 26440 train: 0.13583898544311523 elapsed, loss: 4.871251e-06\n",
      "step: 26450 train: 0.13448095321655273 elapsed, loss: 4.006991e-06\n",
      "step: 26460 train: 0.1231081485748291 elapsed, loss: 3.2386652e-06\n",
      "step: 26470 train: 0.11940598487854004 elapsed, loss: 3.6349365e-06\n",
      "step: 26480 train: 0.1285231113433838 elapsed, loss: 2.9159614e-06\n",
      "step: 26490 train: 0.1302483081817627 elapsed, loss: 2.4377318e-06\n",
      "step: 26500 train: 0.12997937202453613 elapsed, loss: 2.9834769e-06\n",
      "step: 26510 train: 0.13362693786621094 elapsed, loss: 2.1043202e-06\n",
      "step: 26520 train: 0.12551546096801758 elapsed, loss: 4.0209648e-06\n",
      "step: 26530 train: 0.12114763259887695 elapsed, loss: 4.5173556e-06\n",
      "step: 26540 train: 0.12717914581298828 elapsed, loss: 2.644949e-06\n",
      "step: 26550 train: 0.13236474990844727 elapsed, loss: 3.1902348e-06\n",
      "step: 26560 train: 0.13406634330749512 elapsed, loss: 1.949255e-06\n",
      "step: 26570 train: 0.1225578784942627 elapsed, loss: 3.8826693e-06\n",
      "step: 26580 train: 0.1474456787109375 elapsed, loss: 2.3585665e-06\n",
      "step: 26590 train: 0.13358020782470703 elapsed, loss: 4.331553e-06\n",
      "step: 26600 train: 0.13641977310180664 elapsed, loss: 2.6095586e-06\n",
      "step: 26610 train: 0.13260388374328613 elapsed, loss: 4.6198047e-06\n",
      "step: 26620 train: 0.13005781173706055 elapsed, loss: 3.949719e-06\n",
      "step: 26630 train: 0.1271522045135498 elapsed, loss: 3.6754466e-06\n",
      "step: 26640 train: 0.13824820518493652 elapsed, loss: 2.7352794e-06\n",
      "step: 26650 train: 0.12183713912963867 elapsed, loss: 3.5632265e-06\n",
      "step: 26660 train: 0.11725473403930664 elapsed, loss: 5.0891786e-06\n",
      "step: 26670 train: 0.13852882385253906 elapsed, loss: 2.3865089e-06\n",
      "step: 26680 train: 0.12089323997497559 elapsed, loss: 3.8388976e-06\n",
      "step: 26690 train: 0.1245279312133789 elapsed, loss: 3.328071e-06\n",
      "step: 26700 train: 0.12485861778259277 elapsed, loss: 3.2414569e-06\n",
      "step: 26710 train: 0.14165043830871582 elapsed, loss: 2.015844e-06\n",
      "step: 26720 train: 0.12978553771972656 elapsed, loss: 3.7303953e-06\n",
      "step: 26730 train: 0.13569426536560059 elapsed, loss: 4.131785e-06\n",
      "step: 26740 train: 0.1253194808959961 elapsed, loss: 4.2440174e-06\n",
      "step: 26750 train: 0.12929248809814453 elapsed, loss: 2.8381914e-06\n",
      "step: 26760 train: 0.1346123218536377 elapsed, loss: 1.8882533e-06\n",
      "step: 26770 train: 0.13123369216918945 elapsed, loss: 3.2172418e-06\n",
      "step: 26780 train: 0.1286468505859375 elapsed, loss: 3.09757e-06\n",
      "step: 26790 train: 0.12729334831237793 elapsed, loss: 4.8273587e-06\n",
      "step: 26800 train: 0.1285233497619629 elapsed, loss: 3.7001269e-06\n",
      "step: 26810 train: 0.1324002742767334 elapsed, loss: 2.395353e-06\n",
      "step: 26820 train: 0.13176417350769043 elapsed, loss: 3.3667056e-06\n",
      "step: 26830 train: 0.13495492935180664 elapsed, loss: 2.6174714e-06\n",
      "step: 26840 train: 0.12682390213012695 elapsed, loss: 3.2046664e-06\n",
      "step: 26850 train: 0.12258434295654297 elapsed, loss: 5.920801e-06\n",
      "step: 26860 train: 0.1211400032043457 elapsed, loss: 4.3981204e-06\n",
      "step: 26870 train: 0.12681365013122559 elapsed, loss: 3.6735837e-06\n",
      "step: 26880 train: 0.13287615776062012 elapsed, loss: 4.189528e-06\n",
      "step: 26890 train: 0.12943363189697266 elapsed, loss: 3.7569398e-06\n",
      "step: 26900 train: 0.13419795036315918 elapsed, loss: 2.5583354e-06\n",
      "step: 26910 train: 0.12747550010681152 elapsed, loss: 2.9038524e-06\n",
      "step: 26920 train: 0.12014579772949219 elapsed, loss: 3.5399473e-06\n",
      "step: 26930 train: 0.12538480758666992 elapsed, loss: 3.5697383e-06\n",
      "step: 26940 train: 0.14152836799621582 elapsed, loss: 2.6230623e-06\n",
      "step: 26950 train: 0.14046001434326172 elapsed, loss: 3.4100253e-06\n",
      "step: 26960 train: 0.14386844635009766 elapsed, loss: 2.8139839e-06\n",
      "step: 26970 train: 0.13004136085510254 elapsed, loss: 4.8237453e-06\n",
      "step: 26980 train: 0.12871623039245605 elapsed, loss: 3.2428534e-06\n",
      "step: 26990 train: 0.1282658576965332 elapsed, loss: 2.8782442e-06\n",
      "step: 27000 train: 0.1289975643157959 elapsed, loss: 2.6225982e-06\n",
      "step: 27010 train: 0.12833189964294434 elapsed, loss: 2.5671848e-06\n",
      "step: 27020 train: 0.12899065017700195 elapsed, loss: 3.6381985e-06\n",
      "step: 27030 train: 0.12947416305541992 elapsed, loss: 3.2954752e-06\n",
      "step: 27040 train: 0.1290266513824463 elapsed, loss: 3.6046536e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 27050 train: 0.12915372848510742 elapsed, loss: 7.74159e-06\n",
      "step: 27060 train: 0.13004755973815918 elapsed, loss: 4.0447176e-06\n",
      "step: 27070 train: 0.12931275367736816 elapsed, loss: 2.8968702e-06\n",
      "step: 27080 train: 0.1263134479522705 elapsed, loss: 3.4416898e-06\n",
      "step: 27090 train: 0.12458968162536621 elapsed, loss: 4.411654e-06\n",
      "step: 27100 train: 0.1395094394683838 elapsed, loss: 3.772772e-06\n",
      "step: 27110 train: 0.13718843460083008 elapsed, loss: 3.2693538e-06\n",
      "step: 27120 train: 0.14782285690307617 elapsed, loss: 1.7555408e-06\n",
      "step: 27130 train: 0.12661099433898926 elapsed, loss: 3.0584558e-06\n",
      "step: 27140 train: 0.12789082527160645 elapsed, loss: 1.03684815e-05\n",
      "step: 27150 train: 0.12918972969055176 elapsed, loss: 4.1173535e-06\n",
      "step: 27160 train: 0.13112139701843262 elapsed, loss: 4.6657933e-06\n",
      "step: 27170 train: 0.1252450942993164 elapsed, loss: 2.346927e-06\n",
      "step: 27180 train: 0.13814759254455566 elapsed, loss: 2.91218e-06\n",
      "step: 27190 train: 0.13254094123840332 elapsed, loss: 2.0563566e-06\n",
      "step: 27200 train: 0.13536787033081055 elapsed, loss: 2.2752158e-06\n",
      "step: 27210 train: 0.1308274269104004 elapsed, loss: 3.5059415e-06\n",
      "step: 27220 train: 0.1280529499053955 elapsed, loss: 2.8912832e-06\n",
      "step: 27230 train: 0.1354508399963379 elapsed, loss: 0.06221751\n",
      "step: 27240 train: 0.1328902244567871 elapsed, loss: 3.2986245e-05\n",
      "step: 27250 train: 0.12949299812316895 elapsed, loss: 3.1304677e-05\n",
      "step: 27260 train: 0.12775874137878418 elapsed, loss: 1.5601225e-05\n",
      "step: 27270 train: 0.12726640701293945 elapsed, loss: 1.39680415e-05\n",
      "step: 27280 train: 0.12318563461303711 elapsed, loss: 1.1769353e-05\n",
      "step: 27290 train: 0.12794184684753418 elapsed, loss: 6.6475486e-06\n",
      "step: 27300 train: 0.12781214714050293 elapsed, loss: 5.912434e-06\n",
      "step: 27310 train: 0.12896323204040527 elapsed, loss: 5.1343545e-06\n",
      "step: 27320 train: 0.13285017013549805 elapsed, loss: 3.4379555e-06\n",
      "step: 27330 train: 0.14101481437683105 elapsed, loss: 6.3402217e-06\n",
      "step: 27340 train: 0.1300048828125 elapsed, loss: 4.8358656e-06\n",
      "step: 27350 train: 0.12568259239196777 elapsed, loss: 4.2230604e-06\n",
      "step: 27360 train: 0.12869524955749512 elapsed, loss: 4.3650894e-06\n",
      "step: 27370 train: 0.12260007858276367 elapsed, loss: 4.740407e-06\n",
      "step: 27380 train: 0.13228964805603027 elapsed, loss: 2.9839475e-06\n",
      "step: 27390 train: 0.11406135559082031 elapsed, loss: 5.0025683e-06\n",
      "step: 27400 train: 0.1253349781036377 elapsed, loss: 3.1427362e-06\n",
      "step: 27410 train: 0.12848377227783203 elapsed, loss: 2.6239877e-06\n",
      "step: 27420 train: 0.13942670822143555 elapsed, loss: 2.4419219e-06\n",
      "step: 27430 train: 0.13216352462768555 elapsed, loss: 2.365555e-06\n",
      "step: 27440 train: 0.1411149501800537 elapsed, loss: 2.5546096e-06\n",
      "step: 27450 train: 0.13609981536865234 elapsed, loss: 3.4002433e-06\n",
      "step: 27460 train: 0.13182425498962402 elapsed, loss: 2.7404053e-06\n",
      "step: 27470 train: 0.12913250923156738 elapsed, loss: 2.3711405e-06\n",
      "step: 27480 train: 0.1340780258178711 elapsed, loss: 2.9662501e-06\n",
      "step: 27490 train: 0.12374305725097656 elapsed, loss: 3.0924477e-06\n",
      "step: 27500 train: 0.1205906867980957 elapsed, loss: 3.2624134e-06\n",
      "step: 27510 train: 0.12418198585510254 elapsed, loss: 3.675913e-06\n",
      "step: 27520 train: 0.14408183097839355 elapsed, loss: 2.4311914e-06\n",
      "step: 27530 train: 0.12929630279541016 elapsed, loss: 2.9075802e-06\n",
      "step: 27540 train: 0.13018393516540527 elapsed, loss: 3.484997e-06\n",
      "step: 27550 train: 0.12862443923950195 elapsed, loss: 3.3420392e-06\n",
      "step: 27560 train: 0.13214397430419922 elapsed, loss: 3.4104842e-06\n",
      "step: 27570 train: 0.13946986198425293 elapsed, loss: 2.7269057e-06\n",
      "step: 27580 train: 0.1302030086517334 elapsed, loss: 2.6742857e-06\n",
      "step: 27590 train: 0.12895488739013672 elapsed, loss: 2.7194533e-06\n",
      "step: 27600 train: 0.1306285858154297 elapsed, loss: 3.765199e-06\n",
      "step: 27610 train: 0.1165313720703125 elapsed, loss: 4.0745194e-06\n",
      "step: 27620 train: 0.13602733612060547 elapsed, loss: 2.6235286e-06\n",
      "step: 27630 train: 0.13936161994934082 elapsed, loss: 2.1508836e-06\n",
      "step: 27640 train: 0.14303064346313477 elapsed, loss: 2.3320267e-06\n",
      "step: 27650 train: 0.13294053077697754 elapsed, loss: 2.5620598e-06\n",
      "step: 27660 train: 0.12675166130065918 elapsed, loss: 3.9725164e-06\n",
      "step: 27670 train: 0.12187433242797852 elapsed, loss: 4.823284e-06\n",
      "step: 27680 train: 0.1347799301147461 elapsed, loss: 3.4882505e-06\n",
      "step: 27690 train: 0.12049102783203125 elapsed, loss: 5.700501e-06\n",
      "step: 27700 train: 0.12966585159301758 elapsed, loss: 1.9888364e-06\n",
      "step: 27710 train: 0.14033913612365723 elapsed, loss: 2.4735862e-06\n",
      "step: 27720 train: 0.13567352294921875 elapsed, loss: 2.25659e-06\n",
      "step: 27730 train: 0.13170981407165527 elapsed, loss: 2.5494883e-06\n",
      "step: 27740 train: 0.13802886009216309 elapsed, loss: 4.170916e-06\n",
      "step: 27750 train: 0.13750505447387695 elapsed, loss: 3.2391185e-06\n",
      "step: 27760 train: 0.12761163711547852 elapsed, loss: 3.1157247e-06\n",
      "step: 27770 train: 0.14055156707763672 elapsed, loss: 2.922944e-06\n",
      "step: 27780 train: 0.13455677032470703 elapsed, loss: 2.6896478e-06\n",
      "step: 27790 train: 0.1207277774810791 elapsed, loss: 4.3477326e-06\n",
      "step: 27800 train: 0.1346731185913086 elapsed, loss: 2.5094425e-06\n",
      "step: 27810 train: 0.137162446975708 elapsed, loss: 2.2109543e-06\n",
      "step: 27820 train: 0.12580418586730957 elapsed, loss: 2.5602003e-06\n",
      "step: 27830 train: 0.12351346015930176 elapsed, loss: 4.520104e-06\n",
      "step: 27840 train: 0.1329941749572754 elapsed, loss: 4.4083927e-06\n",
      "step: 27850 train: 0.1352555751800537 elapsed, loss: 3.015141e-06\n",
      "step: 27860 train: 0.12439250946044922 elapsed, loss: 2.9927978e-06\n",
      "step: 27870 train: 0.12757372856140137 elapsed, loss: 3.4994343e-06\n",
      "step: 27880 train: 0.1329202651977539 elapsed, loss: 2.5629915e-06\n",
      "step: 27890 train: 0.13246607780456543 elapsed, loss: 2.9937237e-06\n",
      "step: 27900 train: 0.1348583698272705 elapsed, loss: 2.771139e-06\n",
      "step: 27910 train: 0.12930727005004883 elapsed, loss: 3.5594758e-06\n",
      "step: 27920 train: 0.1334400177001953 elapsed, loss: 3.4579903e-06\n",
      "step: 27930 train: 0.12238955497741699 elapsed, loss: 4.3050186e-06\n",
      "step: 27940 train: 0.12825846672058105 elapsed, loss: 4.359017e-06\n",
      "step: 27950 train: 0.1338794231414795 elapsed, loss: 2.9904668e-06\n",
      "step: 27960 train: 0.12290024757385254 elapsed, loss: 3.852861e-06\n",
      "step: 27970 train: 0.14180469512939453 elapsed, loss: 2.840049e-06\n",
      "step: 27980 train: 0.12930846214294434 elapsed, loss: 3.1985994e-06\n",
      "step: 27990 train: 0.13553833961486816 elapsed, loss: 3.4356372e-06\n",
      "step: 28000 train: 0.1314694881439209 elapsed, loss: 4.4428525e-06\n",
      "step: 28010 train: 0.1248331069946289 elapsed, loss: 1.2772481e-05\n",
      "step: 28020 train: 0.1339888572692871 elapsed, loss: 2.9429689e-06\n",
      "step: 28030 train: 0.1278855800628662 elapsed, loss: 3.1851127e-06\n",
      "step: 28040 train: 0.11487150192260742 elapsed, loss: 4.2943097e-06\n",
      "step: 28050 train: 0.1363816261291504 elapsed, loss: 2.3771927e-06\n",
      "step: 28060 train: 0.12676477432250977 elapsed, loss: 6.375295e-06\n",
      "step: 28070 train: 0.13215112686157227 elapsed, loss: 2.4400592e-06\n",
      "step: 28080 train: 0.1332247257232666 elapsed, loss: 2.6076975e-06\n",
      "step: 28090 train: 0.13155770301818848 elapsed, loss: 3.8262424e-06\n",
      "step: 28100 train: 0.12973308563232422 elapsed, loss: 2.4312094e-06\n",
      "step: 28110 train: 0.12742209434509277 elapsed, loss: 2.9140958e-06\n",
      "step: 28120 train: 0.14736294746398926 elapsed, loss: 1.8067626e-06\n",
      "step: 28130 train: 0.1392345428466797 elapsed, loss: 2.4773557e-05\n",
      "step: 28140 train: 0.13191652297973633 elapsed, loss: 3.943656e-06\n",
      "step: 28150 train: 0.1279277801513672 elapsed, loss: 3.4198038e-06\n",
      "step: 28160 train: 0.12503576278686523 elapsed, loss: 5.1143297e-06\n",
      "step: 28170 train: 0.12870240211486816 elapsed, loss: 4.15507e-06\n",
      "step: 28180 train: 0.13040423393249512 elapsed, loss: 3.0589185e-06\n",
      "step: 28190 train: 0.13128089904785156 elapsed, loss: 2.867999e-06\n",
      "step: 28200 train: 0.14069390296936035 elapsed, loss: 3.2922044e-06\n",
      "step: 28210 train: 0.12800168991088867 elapsed, loss: 3.6070014e-06\n",
      "step: 28220 train: 0.12433576583862305 elapsed, loss: 4.088025e-06\n",
      "step: 28230 train: 0.1350562572479248 elapsed, loss: 2.6309597e-06\n",
      "step: 28240 train: 0.11842703819274902 elapsed, loss: 0.0008361981\n",
      "step: 28250 train: 0.1359724998474121 elapsed, loss: 1.3703768e-05\n",
      "step: 28260 train: 0.13811635971069336 elapsed, loss: 5.0466813e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28270 train: 0.13225746154785156 elapsed, loss: 1.7700504e-05\n",
      "step: 28280 train: 0.1358790397644043 elapsed, loss: 3.3179065e-05\n",
      "step: 28290 train: 0.14618611335754395 elapsed, loss: 7.4378686e-06\n",
      "step: 28300 train: 0.13022971153259277 elapsed, loss: 1.33629055e-05\n",
      "step: 28310 train: 0.14152979850769043 elapsed, loss: 5.08496e-06\n",
      "step: 28320 train: 0.13943815231323242 elapsed, loss: 5.61955e-06\n",
      "step: 28330 train: 0.12966275215148926 elapsed, loss: 4.8638003e-06\n",
      "step: 28340 train: 0.13208818435668945 elapsed, loss: 4.2863794e-06\n",
      "step: 28350 train: 0.13868999481201172 elapsed, loss: 3.4388945e-06\n",
      "step: 28360 train: 0.13263773918151855 elapsed, loss: 4.730585e-06\n",
      "step: 28370 train: 0.13650298118591309 elapsed, loss: 4.020965e-06\n",
      "step: 28380 train: 0.14608025550842285 elapsed, loss: 2.123411e-06\n",
      "step: 28390 train: 0.13003778457641602 elapsed, loss: 2.7120013e-06\n",
      "step: 28400 train: 0.1257174015045166 elapsed, loss: 3.4998475e-06\n",
      "step: 28410 train: 0.12846660614013672 elapsed, loss: 2.8419201e-06\n",
      "step: 28420 train: 0.1358199119567871 elapsed, loss: 1.9771903e-06\n",
      "step: 28430 train: 0.13054966926574707 elapsed, loss: 3.43703e-06\n",
      "step: 28440 train: 0.13635754585266113 elapsed, loss: 2.2035015e-06\n",
      "step: 28450 train: 0.12910056114196777 elapsed, loss: 3.64192e-06\n",
      "step: 28460 train: 0.12653875350952148 elapsed, loss: 2.5378486e-06\n",
      "step: 28470 train: 0.13065433502197266 elapsed, loss: 3.9222487e-06\n",
      "step: 28480 train: 0.12115359306335449 elapsed, loss: 2.7101428e-06\n",
      "step: 28490 train: 0.12451744079589844 elapsed, loss: 2.432142e-06\n",
      "step: 28500 train: 0.12863802909851074 elapsed, loss: 4.221665e-06\n",
      "step: 28510 train: 0.13265728950500488 elapsed, loss: 2.1336557e-06\n",
      "step: 28520 train: 0.12361478805541992 elapsed, loss: 3.3341175e-06\n",
      "step: 28530 train: 0.13166141510009766 elapsed, loss: 7.3806277e-06\n",
      "step: 28540 train: 0.12468338012695312 elapsed, loss: 3.274976e-06\n",
      "step: 28550 train: 0.12668728828430176 elapsed, loss: 4.2938627e-05\n",
      "step: 28560 train: 0.12343001365661621 elapsed, loss: 3.85892e-06\n",
      "step: 28570 train: 0.12102103233337402 elapsed, loss: 3.2847634e-06\n",
      "step: 28580 train: 0.11861348152160645 elapsed, loss: 4.287322e-06\n",
      "step: 28590 train: 0.1271042823791504 elapsed, loss: 4.883818e-06\n",
      "step: 28600 train: 0.12517786026000977 elapsed, loss: 4.3725354e-06\n",
      "step: 28610 train: 0.12505006790161133 elapsed, loss: 3.8020903e-06\n",
      "step: 28620 train: 0.13202118873596191 elapsed, loss: 2.8293405e-06\n",
      "step: 28630 train: 0.1254255771636963 elapsed, loss: 3.196746e-06\n",
      "step: 28640 train: 0.12081241607666016 elapsed, loss: 3.4277205e-06\n",
      "step: 28650 train: 0.12754130363464355 elapsed, loss: 2.7157282e-06\n",
      "step: 28660 train: 0.1315300464630127 elapsed, loss: 1.8249227e-06\n",
      "step: 28670 train: 0.12494587898254395 elapsed, loss: 2.7608971e-06\n",
      "step: 28680 train: 0.12343621253967285 elapsed, loss: 3.5306305e-06\n",
      "step: 28690 train: 0.13478636741638184 elapsed, loss: 3.4621721e-06\n",
      "step: 28700 train: 0.1387641429901123 elapsed, loss: 2.8321444e-06\n",
      "step: 28710 train: 0.13029932975769043 elapsed, loss: 2.2756817e-06\n",
      "step: 28720 train: 0.1361992359161377 elapsed, loss: 2.4498377e-06\n",
      "step: 28730 train: 0.12892436981201172 elapsed, loss: 2.1774272e-06\n",
      "step: 28740 train: 0.13057255744934082 elapsed, loss: 2.006996e-06\n",
      "step: 28750 train: 0.13570618629455566 elapsed, loss: 2.1890696e-06\n",
      "step: 28760 train: 0.1354818344116211 elapsed, loss: 2.2579866e-06\n",
      "step: 28770 train: 0.1292555332183838 elapsed, loss: 2.5154955e-06\n",
      "step: 28780 train: 0.12727069854736328 elapsed, loss: 3.5045518e-06\n",
      "step: 28790 train: 0.13625717163085938 elapsed, loss: 2.5504155e-06\n",
      "step: 28800 train: 0.1369171142578125 elapsed, loss: 4.8889347e-06\n",
      "step: 28810 train: 0.1329364776611328 elapsed, loss: 3.8198036e-06\n",
      "step: 28820 train: 0.13500547409057617 elapsed, loss: 3.412818e-06\n",
      "step: 28830 train: 0.13192129135131836 elapsed, loss: 4.3422647e-06\n",
      "step: 28840 train: 0.12927556037902832 elapsed, loss: 3.060316e-06\n",
      "step: 28850 train: 0.1286640167236328 elapsed, loss: 2.5918584e-06\n",
      "step: 28860 train: 0.12302494049072266 elapsed, loss: 0.00016818447\n",
      "step: 28870 train: 0.12957334518432617 elapsed, loss: 1.7122042e-05\n",
      "step: 28880 train: 0.12731122970581055 elapsed, loss: 2.3922845e-05\n",
      "step: 28890 train: 0.12581586837768555 elapsed, loss: 1.2693521e-05\n",
      "step: 28900 train: 0.13087224960327148 elapsed, loss: 7.611561e-06\n",
      "step: 28910 train: 0.13598108291625977 elapsed, loss: 8.237257e-06\n",
      "step: 28920 train: 0.13079357147216797 elapsed, loss: 8.464181e-06\n",
      "step: 28930 train: 0.13399553298950195 elapsed, loss: 5.6349113e-06\n",
      "step: 28940 train: 0.13625454902648926 elapsed, loss: 6.1517208e-06\n",
      "step: 28950 train: 0.13217663764953613 elapsed, loss: 3.07056e-06\n",
      "step: 28960 train: 0.12091350555419922 elapsed, loss: 4.3306227e-06\n",
      "step: 28970 train: 0.13550257682800293 elapsed, loss: 2.741801e-06\n",
      "step: 28980 train: 0.1321556568145752 elapsed, loss: 2.5061831e-06\n",
      "step: 28990 train: 0.13210272789001465 elapsed, loss: 2.831213e-06\n",
      "step: 29000 train: 0.11769819259643555 elapsed, loss: 4.039128e-06\n",
      "step: 29010 train: 0.12044000625610352 elapsed, loss: 4.1564494e-06\n",
      "step: 29020 train: 0.12093949317932129 elapsed, loss: 3.3541437e-06\n",
      "step: 29030 train: 0.13499951362609863 elapsed, loss: 2.5574052e-06\n",
      "step: 29040 train: 0.13440465927124023 elapsed, loss: 3.3988497e-06\n",
      "step: 29050 train: 0.13444137573242188 elapsed, loss: 2.7655533e-06\n",
      "step: 29060 train: 0.14642572402954102 elapsed, loss: 1.8598473e-06\n",
      "step: 29070 train: 0.14042353630065918 elapsed, loss: 2.3734701e-06\n",
      "step: 29080 train: 0.1298539638519287 elapsed, loss: 2.4442443e-06\n",
      "step: 29090 train: 0.12114739418029785 elapsed, loss: 2.8898862e-06\n",
      "step: 29100 train: 0.12848854064941406 elapsed, loss: 2.152282e-06\n",
      "step: 29110 train: 0.12691569328308105 elapsed, loss: 3.40211e-06\n",
      "step: 29120 train: 0.12587451934814453 elapsed, loss: 2.7329486e-06\n",
      "step: 29130 train: 0.12624716758728027 elapsed, loss: 3.4384298e-06\n",
      "step: 29140 train: 0.11447381973266602 elapsed, loss: 4.0358727e-06\n",
      "step: 29150 train: 0.12169957160949707 elapsed, loss: 2.924811e-06\n",
      "step: 29160 train: 0.13115239143371582 elapsed, loss: 2.208161e-06\n",
      "step: 29170 train: 0.13100576400756836 elapsed, loss: 2.5234094e-06\n",
      "step: 29180 train: 0.1309947967529297 elapsed, loss: 4.0787077e-06\n",
      "step: 29190 train: 0.13847136497497559 elapsed, loss: 2.1667151e-06\n",
      "step: 29200 train: 0.12855958938598633 elapsed, loss: 2.5872075e-06\n",
      "step: 29210 train: 0.1330256462097168 elapsed, loss: 2.433074e-06\n",
      "step: 29220 train: 0.12932515144348145 elapsed, loss: 3.302454e-06\n",
      "step: 29230 train: 0.14602947235107422 elapsed, loss: 0.00058396894\n",
      "step: 29240 train: 0.1488780975341797 elapsed, loss: 3.8238562e-05\n",
      "step: 29250 train: 0.12387752532958984 elapsed, loss: 0.00010952099\n",
      "step: 29260 train: 0.12424039840698242 elapsed, loss: 1.928999e-05\n",
      "step: 29270 train: 0.12971806526184082 elapsed, loss: 1.2032355e-05\n",
      "step: 29280 train: 0.13615655899047852 elapsed, loss: 9.762832e-06\n",
      "step: 29290 train: 0.11579227447509766 elapsed, loss: 7.913364e-06\n",
      "step: 29300 train: 0.13655924797058105 elapsed, loss: 4.186276e-06\n",
      "step: 29310 train: 0.12135529518127441 elapsed, loss: 9.197443e-06\n",
      "step: 29320 train: 0.12762999534606934 elapsed, loss: 5.625134e-06\n",
      "step: 29330 train: 0.12673473358154297 elapsed, loss: 5.637706e-06\n",
      "step: 29340 train: 0.12995624542236328 elapsed, loss: 3.373705e-06\n",
      "step: 29350 train: 0.13165640830993652 elapsed, loss: 4.7738486e-06\n",
      "step: 29360 train: 0.13004851341247559 elapsed, loss: 3.5003218e-06\n",
      "step: 29370 train: 0.1323840618133545 elapsed, loss: 2.662643e-06\n",
      "step: 29380 train: 0.1376206874847412 elapsed, loss: 2.0926757e-06\n",
      "step: 29390 train: 0.13192129135131836 elapsed, loss: 3.1185223e-06\n",
      "step: 29400 train: 0.1269669532775879 elapsed, loss: 3.865889e-06\n",
      "step: 29410 train: 0.13071560859680176 elapsed, loss: 1.7103715e-06\n",
      "step: 29420 train: 0.12618780136108398 elapsed, loss: 2.6151472e-06\n",
      "step: 29430 train: 0.13779211044311523 elapsed, loss: 2.6584526e-06\n",
      "step: 29440 train: 0.12374162673950195 elapsed, loss: 2.1532132e-06\n",
      "step: 29450 train: 0.13234400749206543 elapsed, loss: 2.2137483e-06\n",
      "step: 29460 train: 0.1217355728149414 elapsed, loss: 3.3108413e-06\n",
      "step: 29470 train: 0.1288776397705078 elapsed, loss: 2.0270186e-06\n",
      "step: 29480 train: 0.12911009788513184 elapsed, loss: 4.001382e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 29490 train: 0.1341238021850586 elapsed, loss: 1.7099059e-06\n",
      "step: 29500 train: 0.13503384590148926 elapsed, loss: 3.017006e-06\n",
      "step: 29510 train: 0.1274576187133789 elapsed, loss: 2.8568254e-06\n",
      "step: 29520 train: 0.13825702667236328 elapsed, loss: 1.729929e-06\n",
      "step: 29530 train: 0.12155270576477051 elapsed, loss: 5.156682e-06\n",
      "step: 29540 train: 0.14304208755493164 elapsed, loss: 1.6752536e-05\n",
      "step: 29550 train: 0.13435673713684082 elapsed, loss: 1.7232403e-05\n",
      "step: 29560 train: 0.1238095760345459 elapsed, loss: 1.3648132e-05\n",
      "step: 29570 train: 0.11889529228210449 elapsed, loss: 1.0408712e-05\n",
      "step: 29580 train: 0.13248038291931152 elapsed, loss: 7.0257356e-06\n",
      "step: 29590 train: 0.14187884330749512 elapsed, loss: 4.4386434e-06\n",
      "step: 29600 train: 0.13161993026733398 elapsed, loss: 5.833763e-06\n",
      "step: 29610 train: 0.12441587448120117 elapsed, loss: 3.9632287e-06\n",
      "step: 29620 train: 0.12598466873168945 elapsed, loss: 4.6449086e-06\n",
      "step: 29630 train: 0.12932133674621582 elapsed, loss: 2.7608958e-06\n",
      "step: 29640 train: 0.12122964859008789 elapsed, loss: 2.8544969e-06\n",
      "step: 29650 train: 0.12934470176696777 elapsed, loss: 2.6156072e-06\n",
      "step: 29660 train: 0.12053775787353516 elapsed, loss: 2.8503018e-06\n",
      "step: 29670 train: 0.13129734992980957 elapsed, loss: 2.2202676e-06\n",
      "step: 29680 train: 0.1208498477935791 elapsed, loss: 3.6079286e-06\n",
      "step: 29690 train: 0.13259625434875488 elapsed, loss: 6.8076324e-06\n",
      "step: 29700 train: 0.12024259567260742 elapsed, loss: 4.442848e-06\n",
      "step: 29710 train: 0.13034629821777344 elapsed, loss: 3.929693e-06\n",
      "step: 29720 train: 0.12852740287780762 elapsed, loss: 2.2547276e-06\n",
      "step: 29730 train: 0.12186908721923828 elapsed, loss: 3.2326066e-06\n",
      "step: 29740 train: 0.13227248191833496 elapsed, loss: 2.4456308e-06\n",
      "step: 29750 train: 0.13095712661743164 elapsed, loss: 1.8859238e-06\n",
      "step: 29760 train: 0.14026927947998047 elapsed, loss: 1.7066459e-06\n",
      "step: 29770 train: 0.13326454162597656 elapsed, loss: 2.5080449e-06\n",
      "step: 29780 train: 0.11580324172973633 elapsed, loss: 3.1203876e-06\n",
      "step: 29790 train: 0.1267223358154297 elapsed, loss: 2.5750842e-06\n",
      "step: 29800 train: 0.13550329208374023 elapsed, loss: 1.9590339e-06\n",
      "step: 29810 train: 0.1303720474243164 elapsed, loss: 4.4768394e-06\n",
      "step: 29820 train: 0.13362932205200195 elapsed, loss: 5.3775084e-06\n",
      "step: 29830 train: 0.13500452041625977 elapsed, loss: 4.841869e-06\n",
      "step: 29840 train: 0.13190484046936035 elapsed, loss: 3.686153e-06\n",
      "step: 29850 train: 0.1400759220123291 elapsed, loss: 2.4395936e-06\n",
      "step: 29860 train: 0.12903070449829102 elapsed, loss: 2.4032722e-06\n",
      "step: 29870 train: 0.12470245361328125 elapsed, loss: 2.9676507e-06\n",
      "step: 29880 train: 0.11997389793395996 elapsed, loss: 3.0086235e-06\n",
      "step: 29890 train: 0.12729191780090332 elapsed, loss: 2.2938389e-06\n",
      "step: 29900 train: 0.11909008026123047 elapsed, loss: 3.5729945e-06\n",
      "step: 29910 train: 0.12525296211242676 elapsed, loss: 3.43284e-06\n",
      "step: 29920 train: 0.13596224784851074 elapsed, loss: 2.034003e-06\n",
      "step: 29930 train: 0.1266956329345703 elapsed, loss: 3.8570124e-06\n",
      "step: 29940 train: 0.1405038833618164 elapsed, loss: 2.0819678e-06\n",
      "step: 29950 train: 0.1362910270690918 elapsed, loss: 6.0661064e-06\n",
      "step: 29960 train: 0.1374952793121338 elapsed, loss: 2.349256e-06\n",
      "step: 29970 train: 0.1307234764099121 elapsed, loss: 3.8588923e-06\n",
      "step: 29980 train: 0.12181377410888672 elapsed, loss: 4.148549e-06\n",
      "step: 29990 train: 0.13564038276672363 elapsed, loss: 2.128067e-06\n",
      "step: 30000 train: 0.12896966934204102 elapsed, loss: 3.2568153e-06\n",
      "step: 30010 train: 0.1274700164794922 elapsed, loss: 2.9620637e-06\n",
      "step: 30020 train: 0.12682700157165527 elapsed, loss: 3.68243e-06\n",
      "step: 30030 train: 0.12965989112854004 elapsed, loss: 2.9969751e-06\n",
      "step: 30040 train: 0.11837244033813477 elapsed, loss: 4.4191024e-06\n",
      "step: 30050 train: 0.13098454475402832 elapsed, loss: 2.5737002e-06\n",
      "step: 30060 train: 0.12932133674621582 elapsed, loss: 2.2472775e-06\n",
      "step: 30070 train: 0.13415098190307617 elapsed, loss: 2.4475075e-06\n",
      "step: 30080 train: 0.11635088920593262 elapsed, loss: 4.2384263e-06\n",
      "step: 30090 train: 0.13662505149841309 elapsed, loss: 2.8693971e-06\n",
      "step: 30100 train: 0.12997221946716309 elapsed, loss: 2.8177042e-06\n",
      "step: 30110 train: 0.1233057975769043 elapsed, loss: 3.7187554e-06\n",
      "step: 30120 train: 0.13277554512023926 elapsed, loss: 2.1606638e-06\n",
      "step: 30130 train: 0.13167786598205566 elapsed, loss: 2.623995e-06\n",
      "step: 30140 train: 0.13448405265808105 elapsed, loss: 2.5979168e-06\n",
      "step: 30150 train: 0.12711167335510254 elapsed, loss: 3.0808046e-06\n",
      "step: 30160 train: 0.12314391136169434 elapsed, loss: 3.3294655e-06\n",
      "step: 30170 train: 0.12437295913696289 elapsed, loss: 3.991631e-06\n",
      "step: 30180 train: 0.1255490779876709 elapsed, loss: 3.456126e-06\n",
      "step: 30190 train: 0.12567377090454102 elapsed, loss: 0.00012825882\n",
      "step: 30200 train: 0.12488842010498047 elapsed, loss: 3.305253e-06\n",
      "step: 30210 train: 0.1362450122833252 elapsed, loss: 3.0328365e-06\n",
      "step: 30220 train: 0.13157439231872559 elapsed, loss: 3.7290022e-06\n",
      "step: 30230 train: 0.11930727958679199 elapsed, loss: 3.6978017e-06\n",
      "step: 30240 train: 0.13167119026184082 elapsed, loss: 2.7632257e-06\n",
      "step: 30250 train: 0.1501929759979248 elapsed, loss: 2.023294e-06\n",
      "step: 30260 train: 0.1318800449371338 elapsed, loss: 3.5003566e-06\n",
      "step: 30270 train: 0.14740610122680664 elapsed, loss: 2.3022214e-06\n",
      "step: 30280 train: 0.1327359676361084 elapsed, loss: 2.3725352e-06\n",
      "step: 30290 train: 0.12899231910705566 elapsed, loss: 2.546229e-06\n",
      "step: 30300 train: 0.12701129913330078 elapsed, loss: 3.1888178e-06\n",
      "step: 30310 train: 0.12641692161560059 elapsed, loss: 2.8917461e-06\n",
      "step: 30320 train: 0.12952256202697754 elapsed, loss: 2.805135e-06\n",
      "step: 30330 train: 0.13362884521484375 elapsed, loss: 2.797193e-06\n",
      "step: 30340 train: 0.12773895263671875 elapsed, loss: 2.8232944e-06\n",
      "step: 30350 train: 0.1322634220123291 elapsed, loss: 2.1830156e-06\n",
      "step: 30360 train: 0.13134026527404785 elapsed, loss: 3.0407596e-06\n",
      "step: 30370 train: 0.13702607154846191 elapsed, loss: 2.1718395e-06\n",
      "step: 30380 train: 0.12255334854125977 elapsed, loss: 3.6964043e-06\n",
      "step: 30390 train: 0.11951184272766113 elapsed, loss: 3.720155e-06\n",
      "step: 30400 train: 0.12248873710632324 elapsed, loss: 4.573678e-06\n",
      "step: 30410 train: 0.15271425247192383 elapsed, loss: 2.1732355e-06\n",
      "step: 30420 train: 0.13049721717834473 elapsed, loss: 3.7237774e-06\n",
      "step: 30430 train: 0.13202571868896484 elapsed, loss: 2.7283e-06\n",
      "step: 30440 train: 0.1409773826599121 elapsed, loss: 2.210936e-06\n",
      "step: 30450 train: 0.12099504470825195 elapsed, loss: 3.485463e-06\n",
      "step: 30460 train: 0.1376323699951172 elapsed, loss: 3.0510014e-06\n",
      "step: 30470 train: 0.12136220932006836 elapsed, loss: 5.097973e-06\n",
      "step: 30480 train: 0.12190842628479004 elapsed, loss: 3.607926e-06\n",
      "step: 30490 train: 0.13699769973754883 elapsed, loss: 2.1383128e-06\n",
      "step: 30500 train: 0.13751435279846191 elapsed, loss: 2.5522816e-06\n",
      "step: 30510 train: 0.13658833503723145 elapsed, loss: 2.8586392e-06\n",
      "step: 30520 train: 0.12624478340148926 elapsed, loss: 4.1904605e-06\n",
      "step: 30530 train: 0.1289055347442627 elapsed, loss: 3.1525185e-06\n",
      "step: 30540 train: 0.1375267505645752 elapsed, loss: 3.2014127e-06\n",
      "step: 30550 train: 0.12726449966430664 elapsed, loss: 2.6323776e-06\n",
      "step: 30560 train: 0.1234283447265625 elapsed, loss: 2.7660203e-06\n",
      "step: 30570 train: 0.1239767074584961 elapsed, loss: 2.9751002e-06\n",
      "step: 30580 train: 0.12612032890319824 elapsed, loss: 2.9667199e-06\n",
      "step: 30590 train: 0.1351470947265625 elapsed, loss: 2.5289962e-06\n",
      "step: 30600 train: 0.12804388999938965 elapsed, loss: 3.901737e-06\n",
      "step: 30610 train: 0.12089800834655762 elapsed, loss: 3.304787e-06\n",
      "step: 30620 train: 0.13232111930847168 elapsed, loss: 2.2635752e-06\n",
      "step: 30630 train: 0.12221765518188477 elapsed, loss: 5.0570097e-06\n",
      "step: 30640 train: 0.13518524169921875 elapsed, loss: 3.525974e-06\n",
      "step: 30650 train: 0.12121009826660156 elapsed, loss: 5.7224543e-06\n",
      "step: 30660 train: 0.14037799835205078 elapsed, loss: 2.7050196e-06\n",
      "step: 30670 train: 0.12152934074401855 elapsed, loss: 5.021656e-06\n",
      "step: 30680 train: 0.11720752716064453 elapsed, loss: 3.9725405e-06\n",
      "step: 30690 train: 0.13251328468322754 elapsed, loss: 3.1823183e-06\n",
      "step: 30700 train: 0.13229918479919434 elapsed, loss: 2.8284203e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 30710 train: 0.1298666000366211 elapsed, loss: 4.2486745e-06\n",
      "step: 30720 train: 0.12681007385253906 elapsed, loss: 3.864965e-06\n",
      "step: 30730 train: 0.1336345672607422 elapsed, loss: 3.2442488e-06\n",
      "step: 30740 train: 0.12134385108947754 elapsed, loss: 3.3923263e-06\n",
      "step: 30750 train: 0.12657499313354492 elapsed, loss: 3.075218e-06\n",
      "step: 30760 train: 0.12621235847473145 elapsed, loss: 3.6326117e-06\n",
      "step: 30770 train: 0.12405180931091309 elapsed, loss: 4.308683e-06\n",
      "step: 30780 train: 0.12598752975463867 elapsed, loss: 2.582552e-06\n",
      "step: 30790 train: 0.12172937393188477 elapsed, loss: 3.5641592e-06\n",
      "step: 30800 train: 0.13659143447875977 elapsed, loss: 2.9359678e-06\n",
      "step: 30810 train: 0.11620807647705078 elapsed, loss: 4.3818413e-06\n",
      "step: 30820 train: 0.12311768531799316 elapsed, loss: 4.1667085e-06\n",
      "step: 30830 train: 0.14791035652160645 elapsed, loss: 1.3628781e-05\n",
      "step: 30840 train: 0.12947678565979004 elapsed, loss: 3.4915095e-06\n",
      "step: 30850 train: 0.14281964302062988 elapsed, loss: 3.1421762e-06\n",
      "step: 30860 train: 0.12481999397277832 elapsed, loss: 3.364011e-05\n",
      "step: 30870 train: 0.1303415298461914 elapsed, loss: 3.1222485e-06\n",
      "step: 30880 train: 0.12571954727172852 elapsed, loss: 5.679626e-06\n",
      "step: 30890 train: 0.14033246040344238 elapsed, loss: 2.432609e-06\n",
      "step: 30900 train: 0.1304173469543457 elapsed, loss: 4.2691518e-06\n",
      "step: 30910 train: 0.1258223056793213 elapsed, loss: 3.1529844e-06\n",
      "step: 30920 train: 0.1300945281982422 elapsed, loss: 2.6416903e-06\n",
      "step: 30930 train: 0.13538265228271484 elapsed, loss: 1.8700923e-06\n",
      "step: 30940 train: 0.13474082946777344 elapsed, loss: 2.733425e-06\n",
      "step: 30950 train: 0.12899112701416016 elapsed, loss: 3.2423884e-06\n",
      "step: 30960 train: 0.12111353874206543 elapsed, loss: 4.686851e-06\n",
      "step: 30970 train: 0.13471555709838867 elapsed, loss: 2.4363308e-06\n",
      "step: 30980 train: 0.12074565887451172 elapsed, loss: 4.7995436e-06\n",
      "step: 30990 train: 0.12803983688354492 elapsed, loss: 3.0482115e-06\n",
      "step: 31000 train: 0.14748239517211914 elapsed, loss: 2.406066e-06\n",
      "step: 31010 train: 0.12671136856079102 elapsed, loss: 3.258689e-06\n",
      "step: 31020 train: 0.12448954582214355 elapsed, loss: 3.4663708e-06\n",
      "step: 31030 train: 0.12837791442871094 elapsed, loss: 3.0505323e-06\n",
      "step: 31040 train: 0.1250629425048828 elapsed, loss: 4.2649726e-06\n",
      "step: 31050 train: 0.1315302848815918 elapsed, loss: 2.9345874e-06\n",
      "step: 31060 train: 0.1252460479736328 elapsed, loss: 4.8609054e-06\n",
      "step: 31070 train: 0.12796950340270996 elapsed, loss: 3.5562384e-06\n",
      "step: 31080 train: 0.11704397201538086 elapsed, loss: 9.667838e-06\n",
      "step: 31090 train: 0.12705659866333008 elapsed, loss: 3.5930293e-06\n",
      "step: 31100 train: 0.1328423023223877 elapsed, loss: 2.3525154e-06\n",
      "step: 31110 train: 0.11753296852111816 elapsed, loss: 4.3036234e-06\n",
      "step: 31120 train: 0.12842226028442383 elapsed, loss: 3.7955858e-06\n",
      "step: 31130 train: 0.12060832977294922 elapsed, loss: 5.4249113e-06\n",
      "step: 31140 train: 0.13397574424743652 elapsed, loss: 2.1988485e-06\n",
      "step: 31150 train: 0.12859201431274414 elapsed, loss: 6.3128055e-06\n",
      "step: 31160 train: 0.12116670608520508 elapsed, loss: 4.493605e-06\n",
      "step: 31170 train: 0.1277177333831787 elapsed, loss: 3.566487e-06\n",
      "step: 31180 train: 0.13502860069274902 elapsed, loss: 2.267766e-06\n",
      "step: 31190 train: 0.14438939094543457 elapsed, loss: 3.0840606e-06\n",
      "step: 31200 train: 0.1379082202911377 elapsed, loss: 3.5846426e-06\n",
      "step: 31210 train: 0.13246798515319824 elapsed, loss: 2.2943073e-06\n",
      "step: 31220 train: 0.1225271224975586 elapsed, loss: 4.2179345e-06\n",
      "step: 31230 train: 0.1262366771697998 elapsed, loss: 3.0277213e-06\n",
      "step: 31240 train: 0.12993884086608887 elapsed, loss: 2.3613622e-06\n",
      "step: 31250 train: 0.13033461570739746 elapsed, loss: 2.9210726e-06\n",
      "step: 31260 train: 0.14290642738342285 elapsed, loss: 2.5168915e-06\n",
      "step: 31270 train: 0.13074922561645508 elapsed, loss: 2.941109e-06\n",
      "step: 31280 train: 0.12429618835449219 elapsed, loss: 6.590686e-06\n",
      "step: 31290 train: 0.13057518005371094 elapsed, loss: 2.7390115e-06\n",
      "step: 31300 train: 0.12289857864379883 elapsed, loss: 4.3073364e-06\n",
      "step: 31310 train: 0.14481377601623535 elapsed, loss: 2.4246917e-06\n",
      "step: 31320 train: 0.1460742950439453 elapsed, loss: 2.2510017e-06\n",
      "step: 31330 train: 0.13662195205688477 elapsed, loss: 2.7115343e-06\n",
      "step: 31340 train: 0.12439918518066406 elapsed, loss: 4.645408e-06\n",
      "step: 31350 train: 0.13659071922302246 elapsed, loss: 2.4973356e-06\n",
      "step: 31360 train: 0.12810993194580078 elapsed, loss: 2.485229e-06\n",
      "step: 31370 train: 0.11678004264831543 elapsed, loss: 4.3352707e-06\n",
      "step: 31380 train: 0.12610721588134766 elapsed, loss: 3.0868614e-06\n",
      "step: 31390 train: 0.12643671035766602 elapsed, loss: 4.1336434e-06\n",
      "step: 31400 train: 0.13648414611816406 elapsed, loss: 2.8707911e-06\n",
      "step: 31410 train: 0.13228583335876465 elapsed, loss: 2.4926767e-06\n",
      "step: 31420 train: 0.12497615814208984 elapsed, loss: 3.4165432e-06\n",
      "step: 31430 train: 0.12784886360168457 elapsed, loss: 2.495007e-06\n",
      "step: 31440 train: 0.13079166412353516 elapsed, loss: 2.5611243e-06\n",
      "step: 31450 train: 0.12589097023010254 elapsed, loss: 0.0003682011\n",
      "step: 31460 train: 0.12063336372375488 elapsed, loss: 0.00010549814\n",
      "step: 31470 train: 0.12672019004821777 elapsed, loss: 2.119461e-05\n",
      "step: 31480 train: 0.12871026992797852 elapsed, loss: 1.9356488e-05\n",
      "step: 31490 train: 0.14036250114440918 elapsed, loss: 8.641991e-06\n",
      "step: 31500 train: 0.14045929908752441 elapsed, loss: 8.591708e-06\n",
      "step: 31510 train: 0.12066173553466797 elapsed, loss: 1.1114652e-05\n",
      "step: 31520 train: 0.12844109535217285 elapsed, loss: 6.4078886e-06\n",
      "step: 31530 train: 0.13184809684753418 elapsed, loss: 4.987202e-06\n",
      "step: 31540 train: 0.12546586990356445 elapsed, loss: 6.8823583e-06\n",
      "step: 31550 train: 0.1313016414642334 elapsed, loss: 4.460063e-06\n",
      "step: 31560 train: 0.12592411041259766 elapsed, loss: 3.615381e-06\n",
      "step: 31570 train: 0.12786221504211426 elapsed, loss: 3.9525175e-06\n",
      "step: 31580 train: 0.12839388847351074 elapsed, loss: 4.4139724e-06\n",
      "step: 31590 train: 0.13559675216674805 elapsed, loss: 3.3350473e-06\n",
      "step: 31600 train: 0.13493704795837402 elapsed, loss: 4.2668235e-06\n",
      "step: 31610 train: 0.12877368927001953 elapsed, loss: 2.7827837e-06\n",
      "step: 31620 train: 0.12785959243774414 elapsed, loss: 3.6731215e-06\n",
      "step: 31630 train: 0.12226343154907227 elapsed, loss: 3.4882323e-06\n",
      "step: 31640 train: 0.13251018524169922 elapsed, loss: 1.7071111e-06\n",
      "step: 31650 train: 0.13051891326904297 elapsed, loss: 2.291049e-06\n",
      "step: 31660 train: 0.12247371673583984 elapsed, loss: 6.9957987e-06\n",
      "step: 31670 train: 0.12272071838378906 elapsed, loss: 3.234007e-06\n",
      "step: 31680 train: 0.1269209384918213 elapsed, loss: 2.7701958e-06\n",
      "step: 31690 train: 0.1262807846069336 elapsed, loss: 2.0698603e-06\n",
      "step: 31700 train: 0.12925100326538086 elapsed, loss: 2.1420346e-06\n",
      "step: 31710 train: 0.1282966136932373 elapsed, loss: 2.4153787e-06\n",
      "step: 31720 train: 0.12618207931518555 elapsed, loss: 3.952001e-05\n",
      "step: 31730 train: 0.132965087890625 elapsed, loss: 5.380664e-06\n",
      "step: 31740 train: 0.1406114101409912 elapsed, loss: 4.637385e-06\n",
      "step: 31750 train: 0.13494205474853516 elapsed, loss: 3.6903382e-06\n",
      "step: 31760 train: 0.12905168533325195 elapsed, loss: 4.3841583e-06\n",
      "step: 31770 train: 0.1475505828857422 elapsed, loss: 2.135981e-06\n",
      "step: 31780 train: 0.13168740272521973 elapsed, loss: 4.0773043e-06\n",
      "step: 31790 train: 0.13899564743041992 elapsed, loss: 1.9343531e-06\n",
      "step: 31800 train: 0.1205291748046875 elapsed, loss: 3.5925607e-06\n",
      "step: 31810 train: 0.1279289722442627 elapsed, loss: 2.889421e-06\n",
      "step: 31820 train: 0.12989068031311035 elapsed, loss: 2.5988375e-06\n",
      "step: 31830 train: 0.1419062614440918 elapsed, loss: 3.0612418e-06\n",
      "step: 31840 train: 0.13247966766357422 elapsed, loss: 2.5555403e-06\n",
      "step: 31850 train: 0.1281452178955078 elapsed, loss: 2.2868571e-06\n",
      "step: 31860 train: 0.13154029846191406 elapsed, loss: 1.9893005e-06\n",
      "step: 31870 train: 0.12889862060546875 elapsed, loss: 2.3166567e-06\n",
      "step: 31880 train: 0.12861943244934082 elapsed, loss: 1.7164252e-06\n",
      "step: 31890 train: 0.13242244720458984 elapsed, loss: 3.1506465e-06\n",
      "step: 31900 train: 0.12125396728515625 elapsed, loss: 3.0393633e-06\n",
      "step: 31910 train: 0.1324751377105713 elapsed, loss: 2.5047857e-06\n",
      "step: 31920 train: 0.13077116012573242 elapsed, loss: 1.8007097e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31930 train: 0.13371634483337402 elapsed, loss: 2.0731206e-06\n",
      "step: 31940 train: 0.13210749626159668 elapsed, loss: 1.8761459e-06\n",
      "step: 31950 train: 0.1144866943359375 elapsed, loss: 8.1823324e-05\n",
      "step: 31960 train: 0.13393449783325195 elapsed, loss: 3.043083e-06\n",
      "step: 31970 train: 0.13421940803527832 elapsed, loss: 2.5303952e-06\n",
      "step: 31980 train: 0.12791895866394043 elapsed, loss: 2.881034e-06\n",
      "step: 31990 train: 0.12501287460327148 elapsed, loss: 3.9249976e-06\n",
      "step: 32000 train: 0.1320502758026123 elapsed, loss: 2.6025602e-06\n",
      "step: 32010 train: 0.12653326988220215 elapsed, loss: 3.1920931e-06\n",
      "step: 32020 train: 0.1358964443206787 elapsed, loss: 3.2223652e-06\n",
      "step: 32030 train: 0.12746405601501465 elapsed, loss: 4.4379835e-06\n",
      "step: 32040 train: 0.13977384567260742 elapsed, loss: 2.5024437e-06\n",
      "step: 32050 train: 0.14206385612487793 elapsed, loss: 1.8360997e-06\n",
      "step: 32060 train: 0.12516093254089355 elapsed, loss: 3.3876606e-06\n",
      "step: 32070 train: 0.1291804313659668 elapsed, loss: 2.2896513e-06\n",
      "step: 32080 train: 0.1310567855834961 elapsed, loss: 3.7080308e-06\n",
      "step: 32090 train: 0.12228059768676758 elapsed, loss: 4.212796e-06\n",
      "step: 32100 train: 0.1309652328491211 elapsed, loss: 3.4533264e-06\n",
      "step: 32110 train: 0.1272447109222412 elapsed, loss: 2.851236e-06\n",
      "step: 32120 train: 0.14632368087768555 elapsed, loss: 2.461941e-06\n",
      "step: 32130 train: 0.13609647750854492 elapsed, loss: 2.2407553e-06\n",
      "step: 32140 train: 0.11907553672790527 elapsed, loss: 1.27092935e-05\n",
      "step: 32150 train: 0.13588404655456543 elapsed, loss: 1.3968538e-05\n",
      "step: 32160 train: 0.1297595500946045 elapsed, loss: 4.7348094e-06\n",
      "step: 32170 train: 0.1268167495727539 elapsed, loss: 9.752981e-06\n",
      "step: 32180 train: 0.1457521915435791 elapsed, loss: 8.400288e-06\n",
      "step: 32190 train: 0.1212625503540039 elapsed, loss: 4.1918556e-06\n",
      "step: 32200 train: 0.12529611587524414 elapsed, loss: 3.5250362e-06\n",
      "step: 32210 train: 0.12584304809570312 elapsed, loss: 2.8596132e-06\n",
      "step: 32220 train: 0.12704873085021973 elapsed, loss: 2.5955828e-06\n",
      "step: 32230 train: 0.12376904487609863 elapsed, loss: 3.9147844e-06\n",
      "step: 32240 train: 0.12754368782043457 elapsed, loss: 2.6333064e-06\n",
      "step: 32250 train: 0.12183880805969238 elapsed, loss: 2.2090928e-06\n",
      "step: 32260 train: 0.13047361373901367 elapsed, loss: 1.7210789e-06\n",
      "step: 32270 train: 0.13226890563964844 elapsed, loss: 2.2570543e-06\n",
      "step: 32280 train: 0.12009167671203613 elapsed, loss: 3.1972218e-06\n",
      "step: 32290 train: 0.1159813404083252 elapsed, loss: 3.895234e-06\n",
      "step: 32300 train: 0.140899658203125 elapsed, loss: 1.5753299e-06\n",
      "step: 32310 train: 0.13431906700134277 elapsed, loss: 2.0731195e-06\n",
      "step: 32320 train: 0.13279938697814941 elapsed, loss: 1.9697418e-06\n",
      "step: 32330 train: 0.12650418281555176 elapsed, loss: 1.9120018e-06\n",
      "step: 32340 train: 0.12286257743835449 elapsed, loss: 2.3217817e-06\n",
      "step: 32350 train: 0.13730478286743164 elapsed, loss: 2.7469182e-06\n",
      "step: 32360 train: 0.12391471862792969 elapsed, loss: 2.7343567e-06\n",
      "step: 32370 train: 0.15188980102539062 elapsed, loss: 2.0223629e-06\n",
      "step: 32380 train: 0.14696717262268066 elapsed, loss: 2.2295749e-06\n",
      "step: 32390 train: 0.13601112365722656 elapsed, loss: 2.800466e-06\n",
      "step: 32400 train: 0.1271045207977295 elapsed, loss: 2.9355192e-06\n",
      "step: 32410 train: 0.13536429405212402 elapsed, loss: 2.0796394e-06\n",
      "step: 32420 train: 0.14210891723632812 elapsed, loss: 1.6610105e-06\n",
      "step: 32430 train: 0.13006377220153809 elapsed, loss: 2.23517e-06\n",
      "step: 32440 train: 0.12788939476013184 elapsed, loss: 2.575101e-06\n",
      "step: 32450 train: 0.13091588020324707 elapsed, loss: 2.4419223e-06\n",
      "step: 32460 train: 0.13620781898498535 elapsed, loss: 3.6092501e-06\n",
      "step: 32470 train: 0.1232147216796875 elapsed, loss: 3.3909328e-06\n",
      "step: 32480 train: 0.1403331756591797 elapsed, loss: 2.0130492e-06\n",
      "step: 32490 train: 0.12169313430786133 elapsed, loss: 3.5133971e-06\n",
      "step: 32500 train: 0.1391291618347168 elapsed, loss: 2.2998947e-06\n",
      "step: 32510 train: 0.12788629531860352 elapsed, loss: 4.5406273e-06\n",
      "step: 32520 train: 0.1208181381225586 elapsed, loss: 3.3764977e-06\n",
      "step: 32530 train: 0.13330864906311035 elapsed, loss: 2.9518142e-06\n",
      "step: 32540 train: 0.13263583183288574 elapsed, loss: 2.4111882e-06\n",
      "step: 32550 train: 0.13239598274230957 elapsed, loss: 2.0517002e-06\n",
      "step: 32560 train: 0.1292712688446045 elapsed, loss: 2.6766133e-06\n",
      "step: 32570 train: 0.11327505111694336 elapsed, loss: 4.155996e-06\n",
      "step: 32580 train: 0.12803292274475098 elapsed, loss: 3.993952e-06\n",
      "step: 32590 train: 0.13022494316101074 elapsed, loss: 0.00029063024\n",
      "step: 32600 train: 0.12810635566711426 elapsed, loss: 1.1508034e-05\n",
      "step: 32610 train: 0.14098215103149414 elapsed, loss: 8.3933355e-06\n",
      "step: 32620 train: 0.1303722858428955 elapsed, loss: 6.189979e-06\n",
      "step: 32630 train: 0.13705682754516602 elapsed, loss: 6.300715e-06\n",
      "step: 32640 train: 0.12730836868286133 elapsed, loss: 5.091977e-06\n",
      "step: 32650 train: 0.13375067710876465 elapsed, loss: 5.8932733e-06\n",
      "step: 32660 train: 0.12831449508666992 elapsed, loss: 3.871488e-06\n",
      "step: 32670 train: 0.14590716361999512 elapsed, loss: 3.6926672e-06\n",
      "step: 32680 train: 0.12844371795654297 elapsed, loss: 2.6314463e-06\n",
      "step: 32690 train: 0.12598538398742676 elapsed, loss: 3.1534491e-06\n",
      "step: 32700 train: 0.13193631172180176 elapsed, loss: 2.9028868e-06\n",
      "step: 32710 train: 0.1290419101715088 elapsed, loss: 2.562062e-06\n",
      "step: 32720 train: 0.1218564510345459 elapsed, loss: 3.845827e-06\n",
      "step: 32730 train: 0.13257241249084473 elapsed, loss: 2.3087418e-06\n",
      "step: 32740 train: 0.13716483116149902 elapsed, loss: 2.0349341e-06\n",
      "step: 32750 train: 0.12690234184265137 elapsed, loss: 2.6081602e-06\n",
      "step: 32760 train: 0.12926912307739258 elapsed, loss: 2.9741668e-06\n",
      "step: 32770 train: 0.1291956901550293 elapsed, loss: 2.3152627e-06\n",
      "step: 32780 train: 0.13416433334350586 elapsed, loss: 2.3930263e-06\n",
      "step: 32790 train: 0.12668251991271973 elapsed, loss: 3.4388945e-06\n",
      "step: 32800 train: 0.13941097259521484 elapsed, loss: 1.8910462e-06\n",
      "step: 32810 train: 0.1360328197479248 elapsed, loss: 2.6933772e-06\n",
      "step: 32820 train: 0.12867331504821777 elapsed, loss: 4.2605643e-06\n",
      "step: 32830 train: 0.12673234939575195 elapsed, loss: 1.7875984e-05\n",
      "step: 32840 train: 0.13388681411743164 elapsed, loss: 3.2861558e-06\n",
      "step: 32850 train: 0.12849712371826172 elapsed, loss: 4.4623757e-06\n",
      "step: 32860 train: 0.12844443321228027 elapsed, loss: 2.7585693e-06\n",
      "step: 32870 train: 0.13175010681152344 elapsed, loss: 2.1690457e-06\n",
      "step: 32880 train: 0.1276388168334961 elapsed, loss: 3.7583316e-06\n",
      "step: 32890 train: 0.11831402778625488 elapsed, loss: 3.3490226e-06\n",
      "step: 32900 train: 0.12937426567077637 elapsed, loss: 2.0069924e-06\n",
      "step: 32910 train: 0.12636375427246094 elapsed, loss: 2.3292314e-06\n",
      "step: 32920 train: 0.12027287483215332 elapsed, loss: 3.2312148e-06\n",
      "step: 32930 train: 0.13613605499267578 elapsed, loss: 2.2728855e-06\n",
      "step: 32940 train: 0.13282275199890137 elapsed, loss: 2.9790583e-05\n",
      "step: 32950 train: 0.13129711151123047 elapsed, loss: 5.243948e-05\n",
      "step: 32960 train: 0.1300187110900879 elapsed, loss: 1.6032429e-05\n",
      "step: 32970 train: 0.12822532653808594 elapsed, loss: 1.1079921e-05\n",
      "step: 32980 train: 0.11789226531982422 elapsed, loss: 1.2213644e-05\n",
      "step: 32990 train: 0.14795422554016113 elapsed, loss: 5.2004652e-06\n",
      "step: 33000 train: 0.13869905471801758 elapsed, loss: 6.161096e-06\n",
      "step: 33010 train: 0.14231348037719727 elapsed, loss: 5.067298e-06\n",
      "step: 33020 train: 0.13071680068969727 elapsed, loss: 4.1899993e-06\n",
      "step: 33030 train: 0.12760305404663086 elapsed, loss: 1.6967879e-05\n",
      "step: 33040 train: 0.12166690826416016 elapsed, loss: 5.3136173e-06\n",
      "step: 33050 train: 0.12710976600646973 elapsed, loss: 3.5203711e-06\n",
      "step: 33060 train: 0.11623692512512207 elapsed, loss: 5.854249e-06\n",
      "step: 33070 train: 0.1238565444946289 elapsed, loss: 3.6367985e-06\n",
      "step: 33080 train: 0.12737393379211426 elapsed, loss: 2.7678836e-06\n",
      "step: 33090 train: 0.12360620498657227 elapsed, loss: 3.444009e-06\n",
      "step: 33100 train: 0.12866735458374023 elapsed, loss: 2.8074653e-06\n",
      "step: 33110 train: 0.12817645072937012 elapsed, loss: 2.5201534e-06\n",
      "step: 33120 train: 0.13570404052734375 elapsed, loss: 2.3771888e-06\n",
      "step: 33130 train: 0.1328139305114746 elapsed, loss: 2.2756813e-06\n",
      "step: 33140 train: 0.12146282196044922 elapsed, loss: 2.4205024e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 33150 train: 0.12396836280822754 elapsed, loss: 2.4489077e-06\n",
      "step: 33160 train: 0.13357210159301758 elapsed, loss: 2.0870898e-06\n",
      "step: 33170 train: 0.13277673721313477 elapsed, loss: 2.4307442e-06\n",
      "step: 33180 train: 0.1261599063873291 elapsed, loss: 3.3233925e-06\n",
      "step: 33190 train: 0.12673044204711914 elapsed, loss: 3.0179303e-06\n",
      "step: 33200 train: 0.12630295753479004 elapsed, loss: 3.9212982e-06\n",
      "step: 33210 train: 0.1279449462890625 elapsed, loss: 1.8766111e-06\n",
      "step: 33220 train: 0.12638092041015625 elapsed, loss: 2.6882556e-06\n",
      "step: 33230 train: 0.1230630874633789 elapsed, loss: 2.6654263e-06\n",
      "step: 33240 train: 0.1376209259033203 elapsed, loss: 2.1890687e-06\n",
      "step: 33250 train: 0.12130451202392578 elapsed, loss: 3.0361036e-06\n",
      "step: 33260 train: 0.13692045211791992 elapsed, loss: 2.2644979e-06\n",
      "step: 33270 train: 0.12596344947814941 elapsed, loss: 3.611652e-06\n",
      "step: 33280 train: 0.12978410720825195 elapsed, loss: 2.3040877e-06\n",
      "step: 33290 train: 0.12767887115478516 elapsed, loss: 2.769747e-06\n",
      "step: 33300 train: 0.11909079551696777 elapsed, loss: 3.5450626e-06\n",
      "step: 33310 train: 0.12403583526611328 elapsed, loss: 2.7664855e-06\n",
      "step: 33320 train: 0.13068938255310059 elapsed, loss: 2.2025738e-06\n",
      "step: 33330 train: 0.12711691856384277 elapsed, loss: 1.47223955e-05\n",
      "step: 33340 train: 0.11738300323486328 elapsed, loss: 5.992547e-06\n",
      "step: 33350 train: 0.12777996063232422 elapsed, loss: 3.0854603e-06\n",
      "step: 33360 train: 0.13777804374694824 elapsed, loss: 2.4903493e-06\n",
      "step: 33370 train: 0.12510108947753906 elapsed, loss: 3.984638e-06\n",
      "step: 33380 train: 0.1400008201599121 elapsed, loss: 2.3110706e-06\n",
      "step: 33390 train: 0.12944912910461426 elapsed, loss: 3.7331795e-06\n",
      "step: 33400 train: 0.12165355682373047 elapsed, loss: 2.652865e-06\n",
      "step: 33410 train: 0.13231849670410156 elapsed, loss: 3.082184e-06\n",
      "step: 33420 train: 0.13285446166992188 elapsed, loss: 1.8700928e-06\n",
      "step: 33430 train: 0.12453985214233398 elapsed, loss: 2.212352e-06\n",
      "step: 33440 train: 0.1259007453918457 elapsed, loss: 3.7289956e-06\n",
      "step: 33450 train: 0.12632489204406738 elapsed, loss: 3.2656726e-06\n",
      "step: 33460 train: 0.1361067295074463 elapsed, loss: 2.2523923e-06\n",
      "step: 33470 train: 0.12346601486206055 elapsed, loss: 3.1767308e-06\n",
      "step: 33480 train: 0.12960052490234375 elapsed, loss: 2.0367909e-06\n",
      "step: 33490 train: 0.13471579551696777 elapsed, loss: 2.3869688e-06\n",
      "step: 33500 train: 0.1382613182067871 elapsed, loss: 3.2819235e-06\n",
      "step: 33510 train: 0.13309359550476074 elapsed, loss: 2.1806882e-06\n",
      "step: 33520 train: 0.12497305870056152 elapsed, loss: 3.945067e-06\n",
      "step: 33530 train: 0.12768340110778809 elapsed, loss: 2.355774e-06\n",
      "step: 33540 train: 0.1302199363708496 elapsed, loss: 2.5206189e-06\n",
      "step: 33550 train: 0.13153791427612305 elapsed, loss: 1.9934923e-06\n",
      "step: 33560 train: 0.1366879940032959 elapsed, loss: 2.4093165e-06\n",
      "step: 33570 train: 0.1335909366607666 elapsed, loss: 3.9059396e-06\n",
      "step: 33580 train: 0.13524532318115234 elapsed, loss: 3.901761e-06\n",
      "step: 33590 train: 0.13767504692077637 elapsed, loss: 3.5338883e-06\n",
      "step: 33600 train: 0.12917709350585938 elapsed, loss: 2.3231792e-06\n",
      "step: 33610 train: 0.12305760383605957 elapsed, loss: 3.3709116e-06\n",
      "step: 33620 train: 0.11786937713623047 elapsed, loss: 4.0540326e-06\n",
      "step: 33630 train: 0.13969087600708008 elapsed, loss: 2.244482e-06\n",
      "step: 33640 train: 0.12899446487426758 elapsed, loss: 2.5136324e-06\n",
      "step: 33650 train: 0.12620878219604492 elapsed, loss: 4.405013e-06\n",
      "step: 33660 train: 0.13417625427246094 elapsed, loss: 1.9599647e-06\n",
      "step: 33670 train: 0.12284231185913086 elapsed, loss: 3.6996648e-06\n",
      "step: 33680 train: 0.12260675430297852 elapsed, loss: 3.0868603e-06\n",
      "step: 33690 train: 0.1272599697113037 elapsed, loss: 2.4652047e-06\n",
      "step: 33700 train: 0.12964749336242676 elapsed, loss: 3.0244569e-06\n",
      "step: 33710 train: 0.13201212882995605 elapsed, loss: 3.037959e-06\n",
      "step: 33720 train: 0.14157986640930176 elapsed, loss: 3.0165381e-06\n",
      "step: 33730 train: 0.11738467216491699 elapsed, loss: 5.1986153e-06\n",
      "step: 33740 train: 0.12023520469665527 elapsed, loss: 4.3650653e-06\n",
      "step: 33750 train: 0.13419008255004883 elapsed, loss: 4.729689e-06\n",
      "step: 33760 train: 0.12605690956115723 elapsed, loss: 3.3103743e-06\n",
      "step: 33770 train: 0.12409377098083496 elapsed, loss: 3.5487606e-06\n",
      "step: 33780 train: 0.11872148513793945 elapsed, loss: 4.208162e-06\n",
      "step: 33790 train: 0.1398921012878418 elapsed, loss: 2.2500685e-06\n",
      "step: 33800 train: 0.1334855556488037 elapsed, loss: 3.2269372e-06\n",
      "step: 33810 train: 0.13437294960021973 elapsed, loss: 3.2144403e-06\n",
      "step: 33820 train: 0.14134693145751953 elapsed, loss: 7.3000247e-06\n",
      "step: 33830 train: 0.12964653968811035 elapsed, loss: 2.8568238e-06\n",
      "step: 33840 train: 0.12357234954833984 elapsed, loss: 3.1376144e-06\n",
      "step: 33850 train: 0.11615395545959473 elapsed, loss: 3.766248e-06\n",
      "step: 33860 train: 0.12218093872070312 elapsed, loss: 4.171841e-06\n",
      "step: 33870 train: 0.1287994384765625 elapsed, loss: 2.4903518e-06\n",
      "step: 33880 train: 0.13037800788879395 elapsed, loss: 3.5921005e-06\n",
      "step: 33890 train: 0.12455391883850098 elapsed, loss: 4.0079294e-06\n",
      "step: 33900 train: 0.12602829933166504 elapsed, loss: 3.770449e-06\n",
      "step: 33910 train: 0.13419532775878906 elapsed, loss: 4.6500004e-06\n",
      "step: 33920 train: 0.1259474754333496 elapsed, loss: 3.7778946e-06\n",
      "step: 33930 train: 0.13700604438781738 elapsed, loss: 9.970261e-06\n",
      "step: 33940 train: 0.1432325839996338 elapsed, loss: 3.2093271e-06\n",
      "step: 33950 train: 0.11798453330993652 elapsed, loss: 4.0242307e-06\n",
      "step: 33960 train: 0.12147355079650879 elapsed, loss: 3.567418e-06\n",
      "step: 33970 train: 0.12275338172912598 elapsed, loss: 3.3471365e-06\n",
      "step: 33980 train: 0.13318800926208496 elapsed, loss: 2.2808042e-06\n",
      "step: 33990 train: 0.12200760841369629 elapsed, loss: 3.1082818e-06\n",
      "step: 34000 train: 0.1275615692138672 elapsed, loss: 2.7990807e-06\n",
      "step: 34010 train: 0.128950834274292 elapsed, loss: 4.44844e-06\n",
      "step: 34020 train: 0.13183999061584473 elapsed, loss: 2.3096754e-06\n",
      "step: 34030 train: 0.11992049217224121 elapsed, loss: 3.5087442e-06\n",
      "step: 34040 train: 0.12866973876953125 elapsed, loss: 3.8407597e-06\n",
      "step: 34050 train: 0.1285417079925537 elapsed, loss: 6.479547e-06\n",
      "step: 34060 train: 0.1212315559387207 elapsed, loss: 3.999546e-06\n",
      "step: 34070 train: 0.12555384635925293 elapsed, loss: 4.000015e-06\n",
      "step: 34080 train: 0.12760210037231445 elapsed, loss: 3.2880184e-06\n",
      "step: 34090 train: 0.12808513641357422 elapsed, loss: 2.8083878e-06\n",
      "step: 34100 train: 0.13236665725708008 elapsed, loss: 2.2519325e-06\n",
      "step: 34110 train: 0.11535859107971191 elapsed, loss: 4.2603156e-06\n",
      "step: 34120 train: 0.1320812702178955 elapsed, loss: 2.8083878e-06\n",
      "step: 34130 train: 0.1324779987335205 elapsed, loss: 2.772065e-06\n",
      "step: 34140 train: 0.14496207237243652 elapsed, loss: 3.531537e-06\n",
      "step: 34150 train: 0.13022160530090332 elapsed, loss: 3.4467982e-06\n",
      "step: 34160 train: 0.14224672317504883 elapsed, loss: 3.067767e-06\n",
      "step: 34170 train: 0.1298971176147461 elapsed, loss: 3.630283e-06\n",
      "step: 34180 train: 0.12582135200500488 elapsed, loss: 5.198024e-06\n",
      "step: 34190 train: 0.12720966339111328 elapsed, loss: 2.5001286e-06\n",
      "step: 34200 train: 0.13361430168151855 elapsed, loss: 2.6659031e-06\n",
      "step: 34210 train: 0.12451410293579102 elapsed, loss: 3.673583e-06\n",
      "step: 34220 train: 0.12708210945129395 elapsed, loss: 2.8884883e-06\n",
      "step: 34230 train: 0.12456583976745605 elapsed, loss: 3.497106e-06\n",
      "step: 34240 train: 0.14029383659362793 elapsed, loss: 3.689422e-06\n",
      "step: 34250 train: 0.13492035865783691 elapsed, loss: 2.9634593e-06\n",
      "step: 34260 train: 0.12479543685913086 elapsed, loss: 3.2833668e-06\n",
      "step: 34270 train: 0.13435864448547363 elapsed, loss: 2.4847627e-06\n",
      "step: 34280 train: 0.12352228164672852 elapsed, loss: 5.6675035e-06\n",
      "step: 34290 train: 0.13237833976745605 elapsed, loss: 2.3366829e-06\n",
      "step: 34300 train: 0.1313033103942871 elapsed, loss: 2.8991997e-06\n",
      "step: 34310 train: 0.12898731231689453 elapsed, loss: 2.4866254e-06\n",
      "step: 34320 train: 0.12718915939331055 elapsed, loss: 4.372541e-06\n",
      "step: 34330 train: 0.1401386260986328 elapsed, loss: 5.5939245e-06\n",
      "step: 34340 train: 0.1409306526184082 elapsed, loss: 2.9215337e-06\n",
      "step: 34350 train: 0.1335909366607666 elapsed, loss: 5.086146e-06\n",
      "step: 34360 train: 0.13214540481567383 elapsed, loss: 3.3420336e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 34370 train: 0.12256598472595215 elapsed, loss: 3.9525094e-06\n",
      "step: 34380 train: 0.13677167892456055 elapsed, loss: 2.613749e-06\n",
      "step: 34390 train: 0.12088274955749512 elapsed, loss: 3.8025707e-06\n",
      "step: 34400 train: 0.12476158142089844 elapsed, loss: 1.0650476e-05\n",
      "step: 34410 train: 0.12906813621520996 elapsed, loss: 9.475141e-06\n",
      "step: 34420 train: 0.12727594375610352 elapsed, loss: 4.046114e-06\n",
      "step: 34430 train: 0.1318526268005371 elapsed, loss: 3.2116534e-06\n",
      "step: 34440 train: 0.12642765045166016 elapsed, loss: 3.907341e-06\n",
      "step: 34450 train: 0.13856792449951172 elapsed, loss: 0.0067700148\n",
      "step: 34460 train: 0.1365814208984375 elapsed, loss: 7.3997915e-05\n",
      "step: 34470 train: 0.12413239479064941 elapsed, loss: 2.3565222e-05\n",
      "step: 34480 train: 0.144578218460083 elapsed, loss: 1.1806522e-05\n",
      "step: 34490 train: 0.13616657257080078 elapsed, loss: 1.3282857e-05\n",
      "step: 34500 train: 0.12363600730895996 elapsed, loss: 8.152663e-06\n",
      "step: 34510 train: 0.12257695198059082 elapsed, loss: 2.251168e-05\n",
      "step: 34520 train: 0.12415814399719238 elapsed, loss: 7.658978e-06\n",
      "step: 34530 train: 0.1417834758758545 elapsed, loss: 3.6498104e-06\n",
      "step: 34540 train: 0.13079524040222168 elapsed, loss: 4.3580476e-06\n",
      "step: 34550 train: 0.13074445724487305 elapsed, loss: 3.5567057e-06\n",
      "step: 34560 train: 0.12932419776916504 elapsed, loss: 3.1595005e-06\n",
      "step: 34570 train: 0.1310884952545166 elapsed, loss: 2.529458e-06\n",
      "step: 34580 train: 0.12390875816345215 elapsed, loss: 4.1816165e-06\n",
      "step: 34590 train: 0.11948943138122559 elapsed, loss: 3.186508e-06\n",
      "step: 34600 train: 0.13736844062805176 elapsed, loss: 2.9327261e-06\n",
      "step: 34610 train: 0.12420463562011719 elapsed, loss: 4.334741e-06\n",
      "step: 34620 train: 0.12293648719787598 elapsed, loss: 3.2251573e-06\n",
      "step: 34630 train: 0.1280069351196289 elapsed, loss: 2.2896506e-06\n",
      "step: 34640 train: 0.14485406875610352 elapsed, loss: 1.9115232e-06\n",
      "step: 34650 train: 0.1305375099182129 elapsed, loss: 2.9098865e-06\n",
      "step: 34660 train: 0.13184642791748047 elapsed, loss: 2.3078098e-06\n",
      "step: 34670 train: 0.13167786598205566 elapsed, loss: 5.193824e-06\n",
      "step: 34680 train: 0.12945294380187988 elapsed, loss: 2.167649e-06\n",
      "step: 34690 train: 0.12675929069519043 elapsed, loss: 3.772298e-06\n",
      "step: 34700 train: 0.13884878158569336 elapsed, loss: 2.7026917e-06\n",
      "step: 34710 train: 0.13410019874572754 elapsed, loss: 2.6565872e-06\n",
      "step: 34720 train: 0.13208699226379395 elapsed, loss: 2.1513497e-06\n",
      "step: 34730 train: 0.12886261940002441 elapsed, loss: 2.104783e-06\n",
      "step: 34740 train: 0.1308126449584961 elapsed, loss: 2.5345876e-06\n",
      "step: 34750 train: 0.12777471542358398 elapsed, loss: 2.7301637e-06\n",
      "step: 34760 train: 0.12691903114318848 elapsed, loss: 2.5038541e-06\n",
      "step: 34770 train: 0.13439202308654785 elapsed, loss: 3.7643808e-06\n",
      "step: 34780 train: 0.12245798110961914 elapsed, loss: 2.8437805e-06\n",
      "step: 34790 train: 0.12904882431030273 elapsed, loss: 3.386743e-06\n",
      "step: 34800 train: 0.12140297889709473 elapsed, loss: 3.4151471e-06\n",
      "step: 34810 train: 0.12500524520874023 elapsed, loss: 7.8241304e-05\n",
      "step: 34820 train: 0.13115644454956055 elapsed, loss: 1.5335945e-05\n",
      "step: 34830 train: 0.13161110877990723 elapsed, loss: 1.1312015e-05\n",
      "step: 34840 train: 0.13318538665771484 elapsed, loss: 1.631739e-05\n",
      "step: 34850 train: 0.1326305866241455 elapsed, loss: 1.027181e-05\n",
      "step: 34860 train: 0.1459977626800537 elapsed, loss: 7.947954e-06\n",
      "step: 34870 train: 0.13614869117736816 elapsed, loss: 7.2977464e-06\n",
      "step: 34880 train: 0.14050817489624023 elapsed, loss: 4.1173607e-06\n",
      "step: 34890 train: 0.13338708877563477 elapsed, loss: 5.0700855e-06\n",
      "step: 34900 train: 0.1187596321105957 elapsed, loss: 6.449358e-06\n",
      "step: 34910 train: 0.13306903839111328 elapsed, loss: 3.5757719e-06\n",
      "step: 34920 train: 0.13182544708251953 elapsed, loss: 3.0579818e-06\n",
      "step: 34930 train: 0.13435149192810059 elapsed, loss: 2.918755e-06\n",
      "step: 34940 train: 0.11706089973449707 elapsed, loss: 5.2670475e-06\n",
      "step: 34950 train: 0.1302633285522461 elapsed, loss: 3.1860425e-06\n",
      "step: 34960 train: 0.1385953426361084 elapsed, loss: 2.0321413e-06\n",
      "step: 34970 train: 0.13255906105041504 elapsed, loss: 2.4447163e-06\n",
      "step: 34980 train: 0.12480044364929199 elapsed, loss: 2.4312126e-06\n",
      "step: 34990 train: 0.11997222900390625 elapsed, loss: 3.3955905e-06\n",
      "step: 35000 train: 0.13768577575683594 elapsed, loss: 2.1182855e-06\n",
      "step: 35010 train: 0.12595510482788086 elapsed, loss: 2.769742e-06\n",
      "step: 35020 train: 0.13791155815124512 elapsed, loss: 2.3534462e-06\n",
      "step: 35030 train: 0.13556361198425293 elapsed, loss: 2.7478598e-06\n",
      "step: 35040 train: 0.1262216567993164 elapsed, loss: 2.4172418e-06\n",
      "step: 35050 train: 0.12260985374450684 elapsed, loss: 2.8922152e-06\n",
      "step: 35060 train: 0.12834882736206055 elapsed, loss: 2.6989458e-06\n",
      "step: 35070 train: 0.12401676177978516 elapsed, loss: 2.4945425e-06\n",
      "step: 35080 train: 0.11844325065612793 elapsed, loss: 2.9895373e-06\n",
      "step: 35090 train: 0.1341564655303955 elapsed, loss: 2.0540265e-06\n",
      "step: 35100 train: 0.11727285385131836 elapsed, loss: 3.5967541e-06\n",
      "step: 35110 train: 0.12125015258789062 elapsed, loss: 3.1511186e-06\n",
      "step: 35120 train: 0.1262202262878418 elapsed, loss: 4.5979154e-06\n",
      "step: 35130 train: 0.12479233741760254 elapsed, loss: 3.7611335e-06\n",
      "step: 35140 train: 0.13653993606567383 elapsed, loss: 3.0333067e-06\n",
      "step: 35150 train: 0.13669514656066895 elapsed, loss: 2.9750995e-06\n",
      "step: 35160 train: 0.12350821495056152 elapsed, loss: 2.807466e-06\n",
      "step: 35170 train: 0.1437087059020996 elapsed, loss: 1.6926757e-06\n",
      "step: 35180 train: 0.12737417221069336 elapsed, loss: 3.5967541e-06\n",
      "step: 35190 train: 0.1350712776184082 elapsed, loss: 2.3241087e-06\n",
      "step: 35200 train: 0.133392333984375 elapsed, loss: 2.8544946e-06\n",
      "step: 35210 train: 0.1344132423400879 elapsed, loss: 2.8093239e-06\n",
      "step: 35220 train: 0.1312541961669922 elapsed, loss: 2.7646242e-06\n",
      "step: 35230 train: 0.13310933113098145 elapsed, loss: 2.9606651e-06\n",
      "step: 35240 train: 0.12769460678100586 elapsed, loss: 3.4086006e-06\n",
      "step: 35250 train: 0.12179255485534668 elapsed, loss: 0.0071619153\n",
      "step: 35260 train: 0.13286375999450684 elapsed, loss: 4.456298e-05\n",
      "step: 35270 train: 0.12447023391723633 elapsed, loss: 4.0348074e-05\n",
      "step: 35280 train: 0.12931561470031738 elapsed, loss: 1.3567573e-05\n",
      "step: 35290 train: 0.13387775421142578 elapsed, loss: 5.6160898e-05\n",
      "step: 35300 train: 0.12883496284484863 elapsed, loss: 1.1431597e-05\n",
      "step: 35310 train: 0.13004350662231445 elapsed, loss: 7.638387e-06\n",
      "step: 35320 train: 0.13529086112976074 elapsed, loss: 5.6852073e-06\n",
      "step: 35330 train: 0.13742661476135254 elapsed, loss: 5.9616714e-06\n",
      "step: 35340 train: 0.13314056396484375 elapsed, loss: 5.0244525e-06\n",
      "step: 35350 train: 0.12318944931030273 elapsed, loss: 6.3199004e-06\n",
      "step: 35360 train: 0.12893390655517578 elapsed, loss: 4.9582354e-06\n",
      "step: 35370 train: 0.1327822208404541 elapsed, loss: 3.3234128e-06\n",
      "step: 35380 train: 0.13261032104492188 elapsed, loss: 3.1678817e-06\n",
      "step: 35390 train: 0.12941884994506836 elapsed, loss: 3.4803227e-06\n",
      "step: 35400 train: 0.12010598182678223 elapsed, loss: 3.2582218e-06\n",
      "step: 35410 train: 0.12207341194152832 elapsed, loss: 4.3022187e-06\n",
      "step: 35420 train: 0.1259310245513916 elapsed, loss: 3.486835e-06\n",
      "step: 35430 train: 0.1358487606048584 elapsed, loss: 2.4642707e-06\n",
      "step: 35440 train: 0.12107396125793457 elapsed, loss: 2.612819e-06\n",
      "step: 35450 train: 0.13102364540100098 elapsed, loss: 2.8893803e-06\n",
      "step: 35460 train: 0.13247227668762207 elapsed, loss: 7.156628e-06\n",
      "step: 35470 train: 0.1373913288116455 elapsed, loss: 2.311072e-06\n",
      "step: 35480 train: 0.133286714553833 elapsed, loss: 1.8291147e-06\n",
      "step: 35490 train: 0.12100815773010254 elapsed, loss: 3.0910492e-06\n",
      "step: 35500 train: 0.1332414150238037 elapsed, loss: 2.1578612e-06\n",
      "step: 35510 train: 0.13280725479125977 elapsed, loss: 3.959035e-06\n",
      "step: 35520 train: 0.1285099983215332 elapsed, loss: 2.34227e-06\n",
      "step: 35530 train: 0.12778019905090332 elapsed, loss: 2.6640419e-06\n",
      "step: 35540 train: 0.1324310302734375 elapsed, loss: 3.3280617e-06\n",
      "step: 35550 train: 0.12277698516845703 elapsed, loss: 3.063572e-06\n",
      "step: 35560 train: 0.12428712844848633 elapsed, loss: 2.5187528e-06\n",
      "step: 35570 train: 0.1319446563720703 elapsed, loss: 2.2593767e-06\n",
      "step: 35580 train: 0.13094067573547363 elapsed, loss: 2.508044e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 35590 train: 0.1442873477935791 elapsed, loss: 1.911534e-06\n",
      "step: 35600 train: 0.13392043113708496 elapsed, loss: 3.3732354e-06\n",
      "step: 35610 train: 0.13997769355773926 elapsed, loss: 2.2742852e-06\n",
      "step: 35620 train: 0.1398756504058838 elapsed, loss: 2.2947738e-06\n",
      "step: 35630 train: 0.12237930297851562 elapsed, loss: 4.010254e-06\n",
      "step: 35640 train: 0.12375593185424805 elapsed, loss: 3.1697414e-06\n",
      "step: 35650 train: 0.1358942985534668 elapsed, loss: 3.049495e-06\n",
      "step: 35660 train: 0.11894440650939941 elapsed, loss: 2.8517031e-06\n",
      "step: 35670 train: 0.1265885829925537 elapsed, loss: 4.59786e-06\n",
      "step: 35680 train: 0.1383659839630127 elapsed, loss: 2.9904627e-06\n",
      "step: 35690 train: 0.12550830841064453 elapsed, loss: 4.0093128e-06\n",
      "step: 35700 train: 0.12790393829345703 elapsed, loss: 2.762294e-06\n",
      "step: 35710 train: 0.12048530578613281 elapsed, loss: 4.152701e-06\n",
      "step: 35720 train: 0.1292409896850586 elapsed, loss: 2.7441347e-06\n",
      "step: 35730 train: 0.13385438919067383 elapsed, loss: 3.2749808e-06\n",
      "step: 35740 train: 0.12738037109375 elapsed, loss: 2.7026913e-06\n",
      "step: 35750 train: 0.13061022758483887 elapsed, loss: 1.9385448e-06\n",
      "step: 35760 train: 0.13718366622924805 elapsed, loss: 6.2132503e-06\n",
      "step: 35770 train: 0.12500405311584473 elapsed, loss: 4.1276003e-06\n",
      "step: 35780 train: 0.1258084774017334 elapsed, loss: 3.4742875e-06\n",
      "step: 35790 train: 0.12941408157348633 elapsed, loss: 2.2659026e-06\n",
      "step: 35800 train: 0.13204407691955566 elapsed, loss: 3.631673e-06\n",
      "step: 35810 train: 0.1401689052581787 elapsed, loss: 2.936912e-06\n",
      "step: 35820 train: 0.13468408584594727 elapsed, loss: 2.2412212e-06\n",
      "step: 35830 train: 0.12859392166137695 elapsed, loss: 2.7501892e-06\n",
      "step: 35840 train: 0.13416790962219238 elapsed, loss: 1.6949252e-05\n",
      "step: 35850 train: 0.14169597625732422 elapsed, loss: 1.757403e-06\n",
      "step: 35860 train: 0.13386869430541992 elapsed, loss: 3.912471e-06\n",
      "step: 35870 train: 0.140045166015625 elapsed, loss: 2.4721903e-06\n",
      "step: 35880 train: 0.12839388847351074 elapsed, loss: 5.697779e-06\n",
      "step: 35890 train: 0.13030552864074707 elapsed, loss: 2.4437845e-06\n",
      "step: 35900 train: 0.13593530654907227 elapsed, loss: 2.4586727e-06\n",
      "step: 35910 train: 0.1308748722076416 elapsed, loss: 2.8884895e-06\n",
      "step: 35920 train: 0.12847447395324707 elapsed, loss: 2.447975e-06\n",
      "step: 35930 train: 0.12381696701049805 elapsed, loss: 3.8114201e-06\n",
      "step: 35940 train: 0.13234758377075195 elapsed, loss: 2.685928e-06\n",
      "step: 35950 train: 0.12956738471984863 elapsed, loss: 3.132028e-06\n",
      "step: 35960 train: 0.12871170043945312 elapsed, loss: 3.361938e-06\n",
      "step: 35970 train: 0.12547969818115234 elapsed, loss: 3.6405272e-06\n",
      "step: 35980 train: 0.12946295738220215 elapsed, loss: 2.9126916e-06\n",
      "step: 35990 train: 0.13554120063781738 elapsed, loss: 1.7939357e-05\n",
      "step: 36000 train: 0.13570475578308105 elapsed, loss: 3.043547e-06\n",
      "step: 36010 train: 0.13394880294799805 elapsed, loss: 2.7660185e-06\n",
      "step: 36020 train: 0.1281147003173828 elapsed, loss: 2.7213162e-06\n",
      "step: 36030 train: 0.13660645484924316 elapsed, loss: 4.106846e-05\n",
      "step: 36040 train: 0.13443565368652344 elapsed, loss: 1.2845338e-05\n",
      "step: 36050 train: 0.13361787796020508 elapsed, loss: 5.004436e-06\n",
      "step: 36060 train: 0.12989187240600586 elapsed, loss: 4.054962e-06\n",
      "step: 36070 train: 0.12196469306945801 elapsed, loss: 3.6405136e-06\n",
      "step: 36080 train: 0.13256573677062988 elapsed, loss: 2.1113046e-06\n",
      "step: 36090 train: 0.13595175743103027 elapsed, loss: 3.1613658e-06\n",
      "step: 36100 train: 0.12917757034301758 elapsed, loss: 2.4903482e-06\n",
      "step: 36110 train: 0.12078046798706055 elapsed, loss: 5.9892973e-06\n",
      "step: 36120 train: 0.11971282958984375 elapsed, loss: 3.7122375e-06\n",
      "step: 36130 train: 0.12763524055480957 elapsed, loss: 2.458221e-06\n",
      "step: 36140 train: 0.12872743606567383 elapsed, loss: 3.768086e-06\n",
      "step: 36150 train: 0.13308262825012207 elapsed, loss: 2.3576356e-06\n",
      "step: 36160 train: 0.12366056442260742 elapsed, loss: 3.1352897e-06\n",
      "step: 36170 train: 0.13288521766662598 elapsed, loss: 2.4996643e-06\n",
      "step: 36180 train: 0.1313915252685547 elapsed, loss: 3.4672958e-06\n",
      "step: 36190 train: 0.12769508361816406 elapsed, loss: 3.2773125e-06\n",
      "step: 36200 train: 0.1326296329498291 elapsed, loss: 2.2523986e-06\n",
      "step: 36210 train: 0.12745261192321777 elapsed, loss: 3.4207328e-06\n",
      "step: 36220 train: 0.12794160842895508 elapsed, loss: 2.9732205e-06\n",
      "step: 36230 train: 0.1421523094177246 elapsed, loss: 2.7865021e-06\n",
      "step: 36240 train: 0.1414027214050293 elapsed, loss: 3.6824295e-06\n",
      "step: 36250 train: 0.14380717277526855 elapsed, loss: 2.3362147e-06\n",
      "step: 36260 train: 0.1382918357849121 elapsed, loss: 2.1043193e-06\n",
      "step: 36270 train: 0.1378471851348877 elapsed, loss: 2.1643891e-06\n",
      "step: 36280 train: 0.13131999969482422 elapsed, loss: 2.3031498e-06\n",
      "step: 36290 train: 0.12100529670715332 elapsed, loss: 6.125691e-06\n",
      "step: 36300 train: 0.13290691375732422 elapsed, loss: 0.00017144089\n",
      "step: 36310 train: 0.12720274925231934 elapsed, loss: 1.751569e-05\n",
      "step: 36320 train: 0.12574434280395508 elapsed, loss: 1.1966142e-05\n",
      "step: 36330 train: 0.13303613662719727 elapsed, loss: 8.147526e-06\n",
      "step: 36340 train: 0.1405198574066162 elapsed, loss: 5.533879e-06\n",
      "step: 36350 train: 0.12532782554626465 elapsed, loss: 7.781447e-06\n",
      "step: 36360 train: 0.12037801742553711 elapsed, loss: 7.0034775e-06\n",
      "step: 36370 train: 0.12735247611999512 elapsed, loss: 4.589998e-06\n",
      "step: 36380 train: 0.12891530990600586 elapsed, loss: 4.1867374e-06\n",
      "step: 36390 train: 0.1222681999206543 elapsed, loss: 4.468465e-06\n",
      "step: 36400 train: 0.1350719928741455 elapsed, loss: 3.1841764e-06\n",
      "step: 36410 train: 0.13428974151611328 elapsed, loss: 3.886195e-05\n",
      "step: 36420 train: 0.12723302841186523 elapsed, loss: 1.3392934e-05\n",
      "step: 36430 train: 0.11899542808532715 elapsed, loss: 1.0554453e-05\n",
      "step: 36440 train: 0.1366736888885498 elapsed, loss: 4.195572e-06\n",
      "step: 36450 train: 0.13857626914978027 elapsed, loss: 2.879171e-06\n",
      "step: 36460 train: 0.12492895126342773 elapsed, loss: 4.9825467e-06\n",
      "step: 36470 train: 0.1517045497894287 elapsed, loss: 2.2761478e-06\n",
      "step: 36480 train: 0.14000821113586426 elapsed, loss: 3.6544366e-06\n",
      "step: 36490 train: 0.1278090476989746 elapsed, loss: 3.0845263e-06\n",
      "step: 36500 train: 0.12305736541748047 elapsed, loss: 3.9789134e-06\n",
      "step: 36510 train: 0.12165975570678711 elapsed, loss: 3.3313079e-06\n",
      "step: 36520 train: 0.12161827087402344 elapsed, loss: 3.2716923e-06\n",
      "step: 36530 train: 0.11940789222717285 elapsed, loss: 3.6433203e-06\n",
      "step: 36540 train: 0.1258866786956787 elapsed, loss: 3.1934946e-06\n",
      "step: 36550 train: 0.11719989776611328 elapsed, loss: 3.4510058e-06\n",
      "step: 36560 train: 0.12656235694885254 elapsed, loss: 2.9313305e-06\n",
      "step: 36570 train: 0.13698387145996094 elapsed, loss: 1.8849919e-06\n",
      "step: 36580 train: 0.14446473121643066 elapsed, loss: 5.402532e-06\n",
      "step: 36590 train: 0.13408946990966797 elapsed, loss: 3.6419233e-06\n",
      "step: 36600 train: 0.12857961654663086 elapsed, loss: 2.778593e-06\n",
      "step: 36610 train: 0.13542509078979492 elapsed, loss: 2.523412e-06\n",
      "step: 36620 train: 0.11641144752502441 elapsed, loss: 3.5622757e-06\n",
      "step: 36630 train: 0.12751078605651855 elapsed, loss: 2.357171e-06\n",
      "step: 36640 train: 0.13350987434387207 elapsed, loss: 2.0530947e-06\n",
      "step: 36650 train: 0.13664531707763672 elapsed, loss: 1.8379625e-06\n",
      "step: 36660 train: 0.1311321258544922 elapsed, loss: 3.6158415e-06\n",
      "step: 36670 train: 0.12889719009399414 elapsed, loss: 1.921313e-06\n",
      "step: 36680 train: 0.1258234977722168 elapsed, loss: 2.7823194e-06\n",
      "step: 36690 train: 0.1262826919555664 elapsed, loss: 2.3888363e-06\n",
      "step: 36700 train: 0.11763143539428711 elapsed, loss: 4.772054e-06\n",
      "step: 36710 train: 0.1350998878479004 elapsed, loss: 2.345993e-06\n",
      "step: 36720 train: 0.13104629516601562 elapsed, loss: 2.1853439e-06\n",
      "step: 36730 train: 0.13120269775390625 elapsed, loss: 1.9962868e-06\n",
      "step: 36740 train: 0.11798238754272461 elapsed, loss: 3.4905852e-06\n",
      "step: 36750 train: 0.13263225555419922 elapsed, loss: 1.8835967e-06\n",
      "step: 36760 train: 0.12778997421264648 elapsed, loss: 2.3273662e-06\n",
      "step: 36770 train: 0.13745999336242676 elapsed, loss: 1.6088575e-06\n",
      "step: 36780 train: 0.12483334541320801 elapsed, loss: 3.2549578e-06\n",
      "step: 36790 train: 0.14466547966003418 elapsed, loss: 4.0321274e-06\n",
      "step: 36800 train: 0.1405031681060791 elapsed, loss: 1.9594986e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 36810 train: 0.1270759105682373 elapsed, loss: 2.6980351e-06\n",
      "step: 36820 train: 0.1308457851409912 elapsed, loss: 2.080105e-06\n",
      "step: 36830 train: 0.12340068817138672 elapsed, loss: 2.7897695e-06\n",
      "step: 36840 train: 0.12847304344177246 elapsed, loss: 2.1308595e-06\n",
      "step: 36850 train: 0.13549375534057617 elapsed, loss: 2.565323e-06\n",
      "step: 36860 train: 0.1339716911315918 elapsed, loss: 2.3511188e-06\n",
      "step: 36870 train: 0.12742996215820312 elapsed, loss: 0.0002252515\n",
      "step: 36880 train: 0.12506818771362305 elapsed, loss: 2.2568496e-05\n",
      "step: 36890 train: 0.11778044700622559 elapsed, loss: 1.7930957e-05\n",
      "step: 36900 train: 0.12456321716308594 elapsed, loss: 1.0923137e-05\n",
      "step: 36910 train: 0.11791133880615234 elapsed, loss: 1.3915751e-05\n",
      "step: 36920 train: 0.1244802474975586 elapsed, loss: 7.097021e-06\n",
      "step: 36930 train: 0.1405177116394043 elapsed, loss: 1.20142395e-05\n",
      "step: 36940 train: 0.12744903564453125 elapsed, loss: 5.910547e-06\n",
      "step: 36950 train: 0.1329798698425293 elapsed, loss: 4.018174e-06\n",
      "step: 36960 train: 0.12004208564758301 elapsed, loss: 4.8391275e-06\n",
      "step: 36970 train: 0.12067055702209473 elapsed, loss: 4.7836993e-06\n",
      "step: 36980 train: 0.13456010818481445 elapsed, loss: 2.9448333e-06\n",
      "step: 36990 train: 0.12102580070495605 elapsed, loss: 4.6635314e-06\n",
      "step: 37000 train: 0.12582826614379883 elapsed, loss: 3.2461076e-06\n",
      "step: 37010 train: 0.12389898300170898 elapsed, loss: 3.7960526e-06\n",
      "step: 37020 train: 0.12496209144592285 elapsed, loss: 3.2209591e-06\n",
      "step: 37030 train: 0.13562536239624023 elapsed, loss: 2.5066488e-06\n",
      "step: 37040 train: 0.13752365112304688 elapsed, loss: 3.7648497e-06\n",
      "step: 37050 train: 0.1288318634033203 elapsed, loss: 3.036101e-06\n",
      "step: 37060 train: 0.11815118789672852 elapsed, loss: 3.5576384e-06\n",
      "step: 37070 train: 0.12346792221069336 elapsed, loss: 2.9108414e-06\n",
      "step: 37080 train: 0.12726187705993652 elapsed, loss: 3.455662e-06\n",
      "step: 37090 train: 0.13130497932434082 elapsed, loss: 2.2165407e-06\n",
      "step: 37100 train: 0.14127898216247559 elapsed, loss: 1.6656672e-06\n",
      "step: 37110 train: 0.12186169624328613 elapsed, loss: 5.2647483e-06\n",
      "step: 37120 train: 0.12300300598144531 elapsed, loss: 3.1282977e-06\n",
      "step: 37130 train: 0.13299226760864258 elapsed, loss: 1.8291148e-06\n",
      "step: 37140 train: 0.11594319343566895 elapsed, loss: 3.006297e-06\n",
      "step: 37150 train: 0.13410449028015137 elapsed, loss: 3.164757e-06\n",
      "step: 37160 train: 0.13143467903137207 elapsed, loss: 2.95496e-06\n",
      "step: 37170 train: 0.1278843879699707 elapsed, loss: 3.6950103e-06\n",
      "step: 37180 train: 0.13944530487060547 elapsed, loss: 2.1820847e-06\n",
      "step: 37190 train: 0.12860822677612305 elapsed, loss: 4.910353e-06\n",
      "step: 37200 train: 0.1311502456665039 elapsed, loss: 3.205604e-06\n",
      "step: 37210 train: 0.1263103485107422 elapsed, loss: 2.7255087e-06\n",
      "step: 37220 train: 0.1233816146850586 elapsed, loss: 4.1681037e-06\n",
      "step: 37230 train: 0.129408597946167 elapsed, loss: 2.4144465e-06\n",
      "step: 37240 train: 0.1253223419189453 elapsed, loss: 3.7564344e-06\n",
      "step: 37250 train: 0.12270402908325195 elapsed, loss: 3.5115395e-06\n",
      "step: 37260 train: 0.12209773063659668 elapsed, loss: 3.567411e-06\n",
      "step: 37270 train: 0.14577078819274902 elapsed, loss: 1.6163082e-06\n",
      "step: 37280 train: 0.1364305019378662 elapsed, loss: 2.3087441e-06\n",
      "step: 37290 train: 0.13442468643188477 elapsed, loss: 3.0044373e-06\n",
      "step: 37300 train: 0.1468524932861328 elapsed, loss: 2.6174723e-06\n",
      "step: 37310 train: 0.1412031650543213 elapsed, loss: 2.4661365e-06\n",
      "step: 37320 train: 0.13111519813537598 elapsed, loss: 2.4600786e-06\n",
      "step: 37330 train: 0.13314485549926758 elapsed, loss: 2.0470434e-06\n",
      "step: 37340 train: 0.12780237197875977 elapsed, loss: 4.227225e-06\n",
      "step: 37350 train: 0.13625264167785645 elapsed, loss: 3.999531e-06\n",
      "step: 37360 train: 0.14023971557617188 elapsed, loss: 1.9422687e-06\n",
      "step: 37370 train: 0.12449193000793457 elapsed, loss: 4.5162365e-06\n",
      "step: 37380 train: 0.12622714042663574 elapsed, loss: 8.840552e-06\n",
      "step: 37390 train: 0.12362933158874512 elapsed, loss: 4.041458e-06\n",
      "step: 37400 train: 0.12946867942810059 elapsed, loss: 3.0561262e-06\n",
      "step: 37410 train: 0.12856078147888184 elapsed, loss: 2.596987e-06\n",
      "step: 37420 train: 0.13039827346801758 elapsed, loss: 2.1830142e-06\n",
      "step: 37430 train: 0.12167906761169434 elapsed, loss: 2.8232976e-06\n",
      "step: 37440 train: 0.12446308135986328 elapsed, loss: 2.9620594e-06\n",
      "step: 37450 train: 0.12988948822021484 elapsed, loss: 3.0956996e-06\n",
      "step: 37460 train: 0.1272742748260498 elapsed, loss: 3.2926785e-06\n",
      "step: 37470 train: 0.1282215118408203 elapsed, loss: 4.486601e-06\n",
      "step: 37480 train: 0.1293942928314209 elapsed, loss: 2.92388e-06\n",
      "step: 37490 train: 0.12399554252624512 elapsed, loss: 3.4249294e-06\n",
      "step: 37500 train: 0.13009953498840332 elapsed, loss: 2.3650882e-06\n",
      "step: 37510 train: 0.13557910919189453 elapsed, loss: 2.9127045e-06\n",
      "step: 37520 train: 0.14508986473083496 elapsed, loss: 2.0875525e-06\n",
      "step: 37530 train: 0.13930320739746094 elapsed, loss: 2.4200338e-06\n",
      "step: 37540 train: 0.12259912490844727 elapsed, loss: 4.626297e-06\n",
      "step: 37550 train: 0.12161135673522949 elapsed, loss: 3.524114e-06\n",
      "step: 37560 train: 0.13101530075073242 elapsed, loss: 3.3997817e-06\n",
      "step: 37570 train: 0.12456130981445312 elapsed, loss: 2.578826e-06\n",
      "step: 37580 train: 0.1354537010192871 elapsed, loss: 3.3257306e-06\n",
      "step: 37590 train: 0.13565397262573242 elapsed, loss: 2.972762e-06\n",
      "step: 37600 train: 0.13103437423706055 elapsed, loss: 3.9301613e-06\n",
      "step: 37610 train: 0.13291454315185547 elapsed, loss: 2.7632211e-06\n",
      "step: 37620 train: 0.1402876377105713 elapsed, loss: 3.68942e-06\n",
      "step: 37630 train: 0.12758255004882812 elapsed, loss: 3.6856952e-06\n",
      "step: 37640 train: 0.13044261932373047 elapsed, loss: 3.0547262e-06\n",
      "step: 37650 train: 0.12892746925354004 elapsed, loss: 3.6596102e-06\n",
      "step: 37660 train: 0.13312172889709473 elapsed, loss: 6.345759e-05\n",
      "step: 37670 train: 0.12929415702819824 elapsed, loss: 4.3354277e-05\n",
      "step: 37680 train: 0.1396493911743164 elapsed, loss: 1.4646257e-05\n",
      "step: 37690 train: 0.13036084175109863 elapsed, loss: 1.720025e-05\n",
      "step: 37700 train: 0.12990617752075195 elapsed, loss: 1.27658805e-05\n",
      "step: 37710 train: 0.14278936386108398 elapsed, loss: 9.941168e-06\n",
      "step: 37720 train: 0.13518786430358887 elapsed, loss: 8.049807e-06\n",
      "step: 37730 train: 0.13587045669555664 elapsed, loss: 6.3981447e-06\n",
      "step: 37740 train: 0.13137340545654297 elapsed, loss: 4.7618128e-06\n",
      "step: 37750 train: 0.13817667961120605 elapsed, loss: 6.6695484e-06\n",
      "step: 37760 train: 0.12415695190429688 elapsed, loss: 6.1679893e-06\n",
      "step: 37770 train: 0.13206219673156738 elapsed, loss: 3.5092107e-06\n",
      "step: 37780 train: 0.11901235580444336 elapsed, loss: 5.5859437e-06\n",
      "step: 37790 train: 0.12257766723632812 elapsed, loss: 4.4074086e-06\n",
      "step: 37800 train: 0.13005352020263672 elapsed, loss: 2.7376152e-06\n",
      "step: 37810 train: 0.12292337417602539 elapsed, loss: 3.7136203e-06\n",
      "step: 37820 train: 0.13864660263061523 elapsed, loss: 2.5406412e-06\n",
      "step: 37830 train: 0.12692761421203613 elapsed, loss: 3.7201542e-06\n",
      "step: 37840 train: 0.1338651180267334 elapsed, loss: 2.0242255e-06\n",
      "step: 37850 train: 0.11926913261413574 elapsed, loss: 3.5553123e-06\n",
      "step: 37860 train: 0.1204366683959961 elapsed, loss: 3.4467846e-06\n",
      "step: 37870 train: 0.12308621406555176 elapsed, loss: 3.3252775e-06\n",
      "step: 37880 train: 0.1284172534942627 elapsed, loss: 3.2028058e-06\n",
      "step: 37890 train: 0.1312243938446045 elapsed, loss: 2.79303e-06\n",
      "step: 37900 train: 0.13693451881408691 elapsed, loss: 2.244482e-06\n",
      "step: 37910 train: 0.1352074146270752 elapsed, loss: 2.3925622e-06\n",
      "step: 37920 train: 0.11834049224853516 elapsed, loss: 3.7750788e-06\n",
      "step: 37930 train: 0.12922000885009766 elapsed, loss: 3.0896372e-06\n",
      "step: 37940 train: 0.12262964248657227 elapsed, loss: 3.6498413e-06\n",
      "step: 37950 train: 0.12668466567993164 elapsed, loss: 2.6170096e-06\n",
      "step: 37960 train: 0.12361598014831543 elapsed, loss: 4.4520893e-06\n",
      "step: 37970 train: 0.12627959251403809 elapsed, loss: 5.621847e-06\n",
      "step: 37980 train: 0.1261584758758545 elapsed, loss: 4.673347e-06\n",
      "step: 37990 train: 0.15125608444213867 elapsed, loss: 1.9217805e-06\n",
      "step: 38000 train: 0.13196372985839844 elapsed, loss: 5.9962467e-06\n",
      "step: 38010 train: 0.12015724182128906 elapsed, loss: 3.9017555e-06\n",
      "step: 38020 train: 0.12046003341674805 elapsed, loss: 3.6829028e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38030 train: 0.12686395645141602 elapsed, loss: 3.3690494e-06\n",
      "step: 38040 train: 0.1259768009185791 elapsed, loss: 2.6347047e-06\n",
      "step: 38050 train: 0.13509821891784668 elapsed, loss: 2.3841794e-06\n",
      "step: 38060 train: 0.12000131607055664 elapsed, loss: 4.345418e-06\n",
      "step: 38070 train: 0.13527560234069824 elapsed, loss: 2.2114127e-06\n",
      "step: 38080 train: 0.12327218055725098 elapsed, loss: 3.5241146e-06\n",
      "step: 38090 train: 0.13208818435668945 elapsed, loss: 2.7366732e-06\n",
      "step: 38100 train: 0.1286909580230713 elapsed, loss: 2.9238781e-06\n",
      "step: 38110 train: 0.12896418571472168 elapsed, loss: 1.9199188e-06\n",
      "step: 38120 train: 0.122161865234375 elapsed, loss: 5.212569e-06\n",
      "step: 38130 train: 0.13361763954162598 elapsed, loss: 2.748789e-06\n",
      "step: 38140 train: 0.12167763710021973 elapsed, loss: 3.0738183e-06\n",
      "step: 38150 train: 0.1280364990234375 elapsed, loss: 3.41934e-06\n",
      "step: 38160 train: 0.11858868598937988 elapsed, loss: 4.007921e-06\n",
      "step: 38170 train: 0.11249017715454102 elapsed, loss: 4.7096755e-06\n",
      "step: 38180 train: 0.13169121742248535 elapsed, loss: 3.3015262e-06\n",
      "step: 38190 train: 0.12125444412231445 elapsed, loss: 3.906417e-06\n",
      "step: 38200 train: 0.14617252349853516 elapsed, loss: 2.2053628e-06\n",
      "step: 38210 train: 0.1403822898864746 elapsed, loss: 3.0784759e-06\n",
      "step: 38220 train: 0.13750910758972168 elapsed, loss: 3.2344747e-06\n",
      "step: 38230 train: 0.1300973892211914 elapsed, loss: 2.619805e-06\n",
      "step: 38240 train: 0.15006184577941895 elapsed, loss: 2.2239944e-06\n",
      "step: 38250 train: 0.12299180030822754 elapsed, loss: 2.7879073e-06\n",
      "step: 38260 train: 0.13991785049438477 elapsed, loss: 2.0745179e-06\n",
      "step: 38270 train: 0.12057685852050781 elapsed, loss: 3.4225995e-06\n",
      "step: 38280 train: 0.12656879425048828 elapsed, loss: 5.2926725e-06\n",
      "step: 38290 train: 0.12052416801452637 elapsed, loss: 3.88079e-06\n",
      "step: 38300 train: 0.12418127059936523 elapsed, loss: 2.641225e-06\n",
      "step: 38310 train: 0.12413334846496582 elapsed, loss: 4.029352e-06\n",
      "step: 38320 train: 0.1447913646697998 elapsed, loss: 2.0689283e-06\n",
      "step: 38330 train: 0.12928318977355957 elapsed, loss: 2.6314456e-06\n",
      "step: 38340 train: 0.12132954597473145 elapsed, loss: 5.192085e-06\n",
      "step: 38350 train: 0.12904715538024902 elapsed, loss: 3.1897684e-06\n",
      "step: 38360 train: 0.13409423828125 elapsed, loss: 2.2267884e-06\n",
      "step: 38370 train: 0.13005709648132324 elapsed, loss: 2.5955862e-06\n",
      "step: 38380 train: 0.1254255771636963 elapsed, loss: 3.938987e-06\n",
      "step: 38390 train: 0.13928818702697754 elapsed, loss: 3.403039e-06\n",
      "step: 38400 train: 0.1282954216003418 elapsed, loss: 3.178594e-06\n",
      "step: 38410 train: 0.13318324089050293 elapsed, loss: 3.5813896e-06\n",
      "step: 38420 train: 0.12598514556884766 elapsed, loss: 3.292681e-06\n",
      "step: 38430 train: 0.1312549114227295 elapsed, loss: 3.1050174e-06\n",
      "step: 38440 train: 0.12099170684814453 elapsed, loss: 3.1236468e-06\n",
      "step: 38450 train: 0.13725519180297852 elapsed, loss: 2.8083928e-06\n",
      "step: 38460 train: 0.13287067413330078 elapsed, loss: 3.2023133e-06\n",
      "step: 38470 train: 0.12833738327026367 elapsed, loss: 2.732493e-06\n",
      "step: 38480 train: 0.12815070152282715 elapsed, loss: 4.265442e-06\n",
      "step: 38490 train: 0.13009333610534668 elapsed, loss: 2.2095578e-06\n",
      "step: 38500 train: 0.12996172904968262 elapsed, loss: 2.5988502e-06\n",
      "step: 38510 train: 0.11976122856140137 elapsed, loss: 5.1939605e-06\n",
      "step: 38520 train: 0.13272762298583984 elapsed, loss: 4.515239e-05\n",
      "step: 38530 train: 0.12932467460632324 elapsed, loss: 1.2613329e-05\n",
      "step: 38540 train: 0.12884736061096191 elapsed, loss: 6.5922945e-06\n",
      "step: 38550 train: 0.13329434394836426 elapsed, loss: 4.2663705e-06\n",
      "step: 38560 train: 0.13060307502746582 elapsed, loss: 2.8214326e-06\n",
      "step: 38570 train: 0.12375926971435547 elapsed, loss: 3.9567085e-06\n",
      "step: 38580 train: 0.11701679229736328 elapsed, loss: 3.768118e-06\n",
      "step: 38590 train: 0.13590216636657715 elapsed, loss: 2.34553e-06\n",
      "step: 38600 train: 0.12854552268981934 elapsed, loss: 2.9154826e-06\n",
      "step: 38610 train: 0.13210725784301758 elapsed, loss: 1.9916297e-06\n",
      "step: 38620 train: 0.12953901290893555 elapsed, loss: 2.450302e-06\n",
      "step: 38630 train: 0.12546277046203613 elapsed, loss: 2.2351699e-06\n",
      "step: 38640 train: 0.1262211799621582 elapsed, loss: 2.885694e-06\n",
      "step: 38650 train: 0.12843775749206543 elapsed, loss: 2.3357502e-06\n",
      "step: 38660 train: 0.12152981758117676 elapsed, loss: 3.5753374e-06\n",
      "step: 38670 train: 0.13324689865112305 elapsed, loss: 2.7385436e-06\n",
      "step: 38680 train: 0.13004589080810547 elapsed, loss: 3.2456433e-06\n",
      "step: 38690 train: 0.12145280838012695 elapsed, loss: 3.234007e-06\n",
      "step: 38700 train: 0.1331343650817871 elapsed, loss: 2.558803e-06\n",
      "step: 38710 train: 0.12079000473022461 elapsed, loss: 4.540626e-06\n",
      "step: 38720 train: 0.12682390213012695 elapsed, loss: 4.4502467e-06\n",
      "step: 38730 train: 0.1207571029663086 elapsed, loss: 2.745998e-06\n",
      "step: 38740 train: 0.1398789882659912 elapsed, loss: 3.0696237e-06\n",
      "step: 38750 train: 0.12971115112304688 elapsed, loss: 2.7003625e-06\n",
      "step: 38760 train: 0.1301581859588623 elapsed, loss: 8.4343665e-06\n",
      "step: 38770 train: 0.13274312019348145 elapsed, loss: 3.1231734e-06\n",
      "step: 38780 train: 0.1270308494567871 elapsed, loss: 4.4805606e-06\n",
      "step: 38790 train: 0.13974857330322266 elapsed, loss: 2.7725323e-06\n",
      "step: 38800 train: 0.13072419166564941 elapsed, loss: 3.900364e-06\n",
      "step: 38810 train: 0.12009406089782715 elapsed, loss: 4.187672e-06\n",
      "step: 38820 train: 0.12243843078613281 elapsed, loss: 3.2730793e-06\n",
      "step: 38830 train: 0.1355581283569336 elapsed, loss: 3.0598462e-06\n",
      "step: 38840 train: 0.12421655654907227 elapsed, loss: 4.0526347e-06\n",
      "step: 38850 train: 0.12688541412353516 elapsed, loss: 3.8724233e-06\n",
      "step: 38860 train: 0.1341991424560547 elapsed, loss: 2.3702087e-06\n",
      "step: 38870 train: 0.12070250511169434 elapsed, loss: 2.589536e-06\n",
      "step: 38880 train: 0.1336510181427002 elapsed, loss: 2.5313261e-06\n",
      "step: 38890 train: 0.13107633590698242 elapsed, loss: 2.0172406e-06\n",
      "step: 38900 train: 0.12819576263427734 elapsed, loss: 2.549956e-06\n",
      "step: 38910 train: 0.1358051300048828 elapsed, loss: 1.8300448e-06\n",
      "step: 38920 train: 0.1317586898803711 elapsed, loss: 1.0376463e-05\n",
      "step: 38930 train: 0.12620282173156738 elapsed, loss: 3.949257e-06\n",
      "step: 38940 train: 0.13382482528686523 elapsed, loss: 2.2938416e-06\n",
      "step: 38950 train: 0.12994623184204102 elapsed, loss: 0.0002336301\n",
      "step: 38960 train: 0.12147903442382812 elapsed, loss: 2.9077231e-05\n",
      "step: 38970 train: 0.12836074829101562 elapsed, loss: 1.6633418e-05\n",
      "step: 38980 train: 0.12889957427978516 elapsed, loss: 9.794568e-06\n",
      "step: 38990 train: 0.1274733543395996 elapsed, loss: 8.9707755e-06\n",
      "step: 39000 train: 0.12348294258117676 elapsed, loss: 8.996386e-06\n",
      "step: 39010 train: 0.12813258171081543 elapsed, loss: 5.5529686e-06\n",
      "step: 39020 train: 0.13312840461730957 elapsed, loss: 9.892041e-06\n",
      "step: 39030 train: 0.11671280860900879 elapsed, loss: 5.326191e-06\n",
      "step: 39040 train: 0.13210701942443848 elapsed, loss: 2.9564737e-06\n",
      "step: 39050 train: 0.11537981033325195 elapsed, loss: 4.522014e-06\n",
      "step: 39060 train: 0.11884260177612305 elapsed, loss: 3.78069e-06\n",
      "step: 39070 train: 0.13087964057922363 elapsed, loss: 2.267299e-06\n",
      "step: 39080 train: 0.12940573692321777 elapsed, loss: 1.8747492e-06\n",
      "step: 39090 train: 0.12570810317993164 elapsed, loss: 2.9019743e-06\n",
      "step: 39100 train: 0.14153194427490234 elapsed, loss: 2.0232892e-06\n",
      "step: 39110 train: 0.1397690773010254 elapsed, loss: 2.1098963e-06\n",
      "step: 39120 train: 0.1293809413909912 elapsed, loss: 2.6952398e-06\n",
      "step: 39130 train: 0.1339561939239502 elapsed, loss: 2.0144464e-06\n",
      "step: 39140 train: 0.14291596412658691 elapsed, loss: 1.820267e-06\n",
      "step: 39150 train: 0.12323856353759766 elapsed, loss: 3.4346172e-06\n",
      "step: 39160 train: 0.13570022583007812 elapsed, loss: 1.7671807e-06\n",
      "step: 39170 train: 0.11695647239685059 elapsed, loss: 5.273898e-06\n",
      "step: 39180 train: 0.13266801834106445 elapsed, loss: 2.3455298e-06\n",
      "step: 39190 train: 0.13001537322998047 elapsed, loss: 2.829337e-06\n",
      "step: 39200 train: 0.1312117576599121 elapsed, loss: 2.4926785e-06\n",
      "step: 39210 train: 0.12708187103271484 elapsed, loss: 3.4337163e-06\n",
      "step: 39220 train: 0.12538695335388184 elapsed, loss: 2.368814e-06\n",
      "step: 39230 train: 0.12681865692138672 elapsed, loss: 2.3571722e-06\n",
      "step: 39240 train: 0.12621235847473145 elapsed, loss: 2.033538e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 39250 train: 0.11679911613464355 elapsed, loss: 2.565319e-06\n",
      "step: 39260 train: 0.13028573989868164 elapsed, loss: 2.4852002e-06\n",
      "step: 39270 train: 0.12948870658874512 elapsed, loss: 1.9841796e-06\n",
      "step: 39280 train: 0.12992453575134277 elapsed, loss: 2.444247e-06\n",
      "step: 39290 train: 0.12824416160583496 elapsed, loss: 2.2947743e-06\n",
      "step: 39300 train: 0.13421869277954102 elapsed, loss: 2.0037369e-06\n",
      "step: 39310 train: 0.13397216796875 elapsed, loss: 2.5071138e-06\n",
      "step: 39320 train: 0.1338214874267578 elapsed, loss: 3.5468206e-06\n",
      "step: 39330 train: 0.12573719024658203 elapsed, loss: 2.6659054e-06\n",
      "step: 39340 train: 0.11709237098693848 elapsed, loss: 3.691727e-06\n",
      "step: 39350 train: 0.1377716064453125 elapsed, loss: 2.4689289e-06\n",
      "step: 39360 train: 0.13302826881408691 elapsed, loss: 2.6211956e-06\n",
      "step: 39370 train: 0.12859225273132324 elapsed, loss: 2.1960454e-06\n",
      "step: 39380 train: 0.12572383880615234 elapsed, loss: 3.7136344e-06\n",
      "step: 39390 train: 0.12552380561828613 elapsed, loss: 2.4465785e-06\n",
      "step: 39400 train: 0.129319429397583 elapsed, loss: 2.936451e-06\n",
      "step: 39410 train: 0.1267693042755127 elapsed, loss: 3.1292357e-06\n",
      "step: 39420 train: 0.12780284881591797 elapsed, loss: 2.5657876e-06\n",
      "step: 39430 train: 0.1383838653564453 elapsed, loss: 1.8835962e-06\n",
      "step: 39440 train: 0.1266462802886963 elapsed, loss: 3.7504224e-06\n",
      "step: 39450 train: 0.13556122779846191 elapsed, loss: 2.926658e-06\n",
      "step: 39460 train: 0.12556099891662598 elapsed, loss: 2.4111891e-06\n",
      "step: 39470 train: 0.12616896629333496 elapsed, loss: 3.7210839e-06\n",
      "step: 39480 train: 0.12926006317138672 elapsed, loss: 3.8760863e-06\n",
      "step: 39490 train: 0.11600995063781738 elapsed, loss: 4.0037407e-06\n",
      "step: 39500 train: 0.13302302360534668 elapsed, loss: 3.1194534e-06\n",
      "step: 39510 train: 0.15887165069580078 elapsed, loss: 1.7234089e-06\n",
      "step: 39520 train: 0.12435460090637207 elapsed, loss: 3.6191032e-06\n",
      "step: 39530 train: 0.13042664527893066 elapsed, loss: 5.434688e-06\n",
      "step: 39540 train: 0.1201181411743164 elapsed, loss: 2.9611313e-06\n",
      "step: 39550 train: 0.13741445541381836 elapsed, loss: 3.4796435e-06\n",
      "step: 39560 train: 0.12863469123840332 elapsed, loss: 2.7674178e-06\n",
      "step: 39570 train: 0.11680984497070312 elapsed, loss: 3.3979206e-06\n",
      "step: 39580 train: 0.13050198554992676 elapsed, loss: 2.3101393e-06\n",
      "step: 39590 train: 0.12122154235839844 elapsed, loss: 3.824929e-06\n",
      "step: 39600 train: 0.13234567642211914 elapsed, loss: 2.6742866e-06\n",
      "step: 39610 train: 0.12811946868896484 elapsed, loss: 3.7476254e-06\n",
      "step: 39620 train: 0.11982345581054688 elapsed, loss: 3.9166625e-06\n",
      "step: 39630 train: 0.12502217292785645 elapsed, loss: 2.7548444e-06\n",
      "step: 39640 train: 0.12445759773254395 elapsed, loss: 4.005571e-06\n",
      "step: 39650 train: 0.13124513626098633 elapsed, loss: 3.7517304e-06\n",
      "step: 39660 train: 0.1265571117401123 elapsed, loss: 3.3187566e-06\n",
      "step: 39670 train: 0.13086819648742676 elapsed, loss: 2.5471616e-06\n",
      "step: 39680 train: 0.11777305603027344 elapsed, loss: 3.8933804e-06\n",
      "step: 39690 train: 0.13405823707580566 elapsed, loss: 2.7683495e-06\n",
      "step: 39700 train: 0.1363985538482666 elapsed, loss: 3.9604324e-06\n",
      "step: 39710 train: 0.12645363807678223 elapsed, loss: 2.8242289e-06\n",
      "step: 39720 train: 0.12644124031066895 elapsed, loss: 2.9471619e-06\n",
      "step: 39730 train: 0.12794780731201172 elapsed, loss: 3.0863953e-06\n",
      "step: 39740 train: 0.12123322486877441 elapsed, loss: 3.9515826e-06\n",
      "step: 39750 train: 0.12885546684265137 elapsed, loss: 2.7315607e-06\n",
      "step: 39760 train: 0.13385224342346191 elapsed, loss: 2.0898833e-06\n",
      "step: 39770 train: 0.13620543479919434 elapsed, loss: 2.9913988e-06\n",
      "step: 39780 train: 0.13359761238098145 elapsed, loss: 2.341798e-06\n",
      "step: 39790 train: 0.12285566329956055 elapsed, loss: 3.2121234e-06\n",
      "step: 39800 train: 0.1270897388458252 elapsed, loss: 2.2351696e-06\n",
      "step: 39810 train: 0.12931013107299805 elapsed, loss: 2.2370327e-06\n",
      "step: 39820 train: 0.12428140640258789 elapsed, loss: 3.6265435e-06\n",
      "step: 39830 train: 0.12405633926391602 elapsed, loss: 4.5350416e-06\n",
      "step: 39840 train: 0.12482476234436035 elapsed, loss: 3.5264404e-06\n",
      "step: 39850 train: 0.12804603576660156 elapsed, loss: 4.439576e-06\n",
      "step: 39860 train: 0.12411880493164062 elapsed, loss: 3.360201e-06\n",
      "step: 39870 train: 0.13263630867004395 elapsed, loss: 3.6819683e-06\n",
      "step: 39880 train: 0.1235806941986084 elapsed, loss: 4.3869786e-06\n",
      "step: 39890 train: 0.12580275535583496 elapsed, loss: 3.7704408e-06\n",
      "step: 39900 train: 0.13293957710266113 elapsed, loss: 3.3993174e-06\n",
      "step: 39910 train: 0.12593698501586914 elapsed, loss: 2.9229489e-06\n",
      "step: 39920 train: 0.12988853454589844 elapsed, loss: 3.0561216e-06\n",
      "step: 39930 train: 0.12389898300170898 elapsed, loss: 4.3906703e-06\n",
      "step: 39940 train: 0.14021039009094238 elapsed, loss: 0.022044137\n",
      "step: 39950 train: 0.12800002098083496 elapsed, loss: 2.0739011e-05\n",
      "step: 39960 train: 0.1388840675354004 elapsed, loss: 1.19635115e-05\n",
      "step: 39970 train: 0.13830351829528809 elapsed, loss: 1.0429617e-05\n",
      "step: 39980 train: 0.11450672149658203 elapsed, loss: 1.4485433e-05\n",
      "step: 39990 train: 0.11886882781982422 elapsed, loss: 1.03068305e-05\n",
      "step: 40000 train: 0.12929058074951172 elapsed, loss: 9.146994e-06\n",
      "step: 40010 train: 0.1265580654144287 elapsed, loss: 6.332003e-06\n",
      "step: 40020 train: 0.1184227466583252 elapsed, loss: 6.626293e-06\n",
      "step: 40030 train: 0.12242603302001953 elapsed, loss: 5.4365546e-06\n",
      "step: 40040 train: 0.1395719051361084 elapsed, loss: 3.5082805e-06\n",
      "step: 40050 train: 0.11831831932067871 elapsed, loss: 5.3224812e-06\n",
      "step: 40060 train: 0.1241908073425293 elapsed, loss: 3.2507683e-06\n",
      "step: 40070 train: 0.13712191581726074 elapsed, loss: 2.620735e-06\n",
      "step: 40080 train: 0.12056207656860352 elapsed, loss: 3.384416e-06\n",
      "step: 40090 train: 0.13357996940612793 elapsed, loss: 3.0337635e-06\n",
      "step: 40100 train: 0.12887120246887207 elapsed, loss: 2.925743e-06\n",
      "step: 40110 train: 0.1282649040222168 elapsed, loss: 2.5839468e-06\n",
      "step: 40120 train: 0.1236410140991211 elapsed, loss: 2.917357e-06\n",
      "step: 40130 train: 0.11700773239135742 elapsed, loss: 3.428182e-06\n",
      "step: 40140 train: 0.1187448501586914 elapsed, loss: 3.2083867e-06\n",
      "step: 40150 train: 0.1202387809753418 elapsed, loss: 2.8419156e-06\n",
      "step: 40160 train: 0.1238565444946289 elapsed, loss: 2.4517024e-06\n",
      "step: 40170 train: 0.12677979469299316 elapsed, loss: 2.474511e-06\n",
      "step: 40180 train: 0.13102173805236816 elapsed, loss: 2.9252767e-06\n",
      "step: 40190 train: 0.12651634216308594 elapsed, loss: 4.0693844e-06\n",
      "step: 40200 train: 0.11942458152770996 elapsed, loss: 2.7459987e-06\n",
      "step: 40210 train: 0.11794161796569824 elapsed, loss: 2.951354e-06\n",
      "step: 40220 train: 0.13125157356262207 elapsed, loss: 2.2482066e-06\n",
      "step: 40230 train: 0.12465143203735352 elapsed, loss: 3.5934718e-06\n",
      "step: 40240 train: 0.13232803344726562 elapsed, loss: 3.923643e-06\n",
      "step: 40250 train: 0.12971043586730957 elapsed, loss: 2.4647366e-06\n",
      "step: 40260 train: 0.12467837333679199 elapsed, loss: 3.1734717e-06\n",
      "step: 40270 train: 0.14029407501220703 elapsed, loss: 2.586718e-06\n",
      "step: 40280 train: 0.12928557395935059 elapsed, loss: 2.9201467e-06\n",
      "step: 40290 train: 0.12794899940490723 elapsed, loss: 2.1196865e-06\n",
      "step: 40300 train: 0.12161087989807129 elapsed, loss: 2.8558923e-06\n",
      "step: 40310 train: 0.12910199165344238 elapsed, loss: 2.8642762e-06\n",
      "step: 40320 train: 0.12859249114990234 elapsed, loss: 3.6731258e-06\n",
      "step: 40330 train: 0.14339542388916016 elapsed, loss: 3.7736845e-06\n",
      "step: 40340 train: 0.13852858543395996 elapsed, loss: 2.3543773e-06\n",
      "step: 40350 train: 0.13136887550354004 elapsed, loss: 2.9220175e-06\n",
      "step: 40360 train: 0.12468314170837402 elapsed, loss: 4.11689e-06\n",
      "step: 40370 train: 0.1274116039276123 elapsed, loss: 3.7634595e-06\n",
      "step: 40380 train: 0.13042187690734863 elapsed, loss: 2.2072309e-06\n",
      "step: 40390 train: 0.1278541088104248 elapsed, loss: 2.0693958e-06\n",
      "step: 40400 train: 0.12868452072143555 elapsed, loss: 2.3702012e-06\n",
      "step: 40410 train: 0.12544631958007812 elapsed, loss: 2.544368e-06\n",
      "step: 40420 train: 0.12976789474487305 elapsed, loss: 2.0866237e-06\n",
      "step: 40430 train: 0.12461161613464355 elapsed, loss: 3.159967e-06\n",
      "step: 40440 train: 0.13051605224609375 elapsed, loss: 2.8377297e-06\n",
      "step: 40450 train: 0.13808822631835938 elapsed, loss: 3.0854635e-06\n",
      "step: 40460 train: 0.12970662117004395 elapsed, loss: 2.5802237e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 40470 train: 0.12894272804260254 elapsed, loss: 2.2072304e-06\n",
      "step: 40480 train: 0.12531757354736328 elapsed, loss: 2.8572842e-06\n",
      "step: 40490 train: 0.12283658981323242 elapsed, loss: 3.2237601e-06\n",
      "step: 40500 train: 0.127793550491333 elapsed, loss: 5.8063906e-06\n",
      "step: 40510 train: 0.13880372047424316 elapsed, loss: 2.3608932e-06\n",
      "step: 40520 train: 0.12632513046264648 elapsed, loss: 3.9310794e-06\n",
      "step: 40530 train: 0.12538433074951172 elapsed, loss: 3.7704397e-06\n",
      "step: 40540 train: 0.11774873733520508 elapsed, loss: 4.0642763e-06\n",
      "step: 40550 train: 0.12326478958129883 elapsed, loss: 4.768809e-06\n",
      "step: 40560 train: 0.11845779418945312 elapsed, loss: 3.304789e-06\n",
      "step: 40570 train: 0.13637328147888184 elapsed, loss: 2.36602e-06\n",
      "step: 40580 train: 0.13183283805847168 elapsed, loss: 2.4246904e-06\n",
      "step: 40590 train: 0.13127589225769043 elapsed, loss: 3.8638937e-06\n",
      "step: 40600 train: 0.13248157501220703 elapsed, loss: 3.1776522e-06\n",
      "step: 40610 train: 0.12814021110534668 elapsed, loss: 2.3264358e-06\n",
      "step: 40620 train: 0.12128758430480957 elapsed, loss: 4.874035e-06\n",
      "step: 40630 train: 0.1375126838684082 elapsed, loss: 2.4433186e-06\n",
      "step: 40640 train: 0.12661004066467285 elapsed, loss: 2.6673001e-06\n",
      "step: 40650 train: 0.13082313537597656 elapsed, loss: 2.4484393e-06\n",
      "step: 40660 train: 0.11832189559936523 elapsed, loss: 4.73899e-06\n",
      "step: 40670 train: 0.1164391040802002 elapsed, loss: 3.714098e-06\n",
      "step: 40680 train: 0.12331128120422363 elapsed, loss: 4.3771925e-06\n",
      "step: 40690 train: 0.13799428939819336 elapsed, loss: 2.4419228e-06\n",
      "step: 40700 train: 0.12759923934936523 elapsed, loss: 2.367417e-06\n",
      "step: 40710 train: 0.13629961013793945 elapsed, loss: 2.6696298e-06\n",
      "step: 40720 train: 0.12250208854675293 elapsed, loss: 4.735264e-06\n",
      "step: 40730 train: 0.13854193687438965 elapsed, loss: 2.5783593e-06\n",
      "step: 40740 train: 0.12956476211547852 elapsed, loss: 3.086395e-06\n",
      "step: 40750 train: 0.12178969383239746 elapsed, loss: 3.0659069e-06\n",
      "step: 40760 train: 0.13154840469360352 elapsed, loss: 2.5783606e-06\n",
      "step: 40770 train: 0.12496733665466309 elapsed, loss: 2.9140983e-06\n",
      "step: 40780 train: 0.12716960906982422 elapsed, loss: 0.00027938228\n",
      "step: 40790 train: 0.12072587013244629 elapsed, loss: 0.0001132567\n",
      "step: 40800 train: 0.12421846389770508 elapsed, loss: 2.4965251e-05\n",
      "step: 40810 train: 0.12575531005859375 elapsed, loss: 2.7611733e-05\n",
      "step: 40820 train: 0.11788630485534668 elapsed, loss: 2.545343e-05\n",
      "step: 40830 train: 0.1259770393371582 elapsed, loss: 1.0092131e-05\n",
      "step: 40840 train: 0.12295103073120117 elapsed, loss: 1.1328393e-05\n",
      "step: 40850 train: 0.13465237617492676 elapsed, loss: 5.5385176e-06\n",
      "step: 40860 train: 0.12573695182800293 elapsed, loss: 7.392252e-06\n",
      "step: 40870 train: 0.1240687370300293 elapsed, loss: 6.6802404e-06\n",
      "step: 40880 train: 0.13234400749206543 elapsed, loss: 5.666606e-06\n",
      "step: 40890 train: 0.13707542419433594 elapsed, loss: 4.595125e-06\n",
      "step: 40900 train: 0.14618277549743652 elapsed, loss: 3.5189828e-06\n",
      "step: 40910 train: 0.12734413146972656 elapsed, loss: 3.6861607e-06\n",
      "step: 40920 train: 0.14593744277954102 elapsed, loss: 2.4372557e-06\n",
      "step: 40930 train: 0.11669087409973145 elapsed, loss: 4.0870814e-06\n",
      "step: 40940 train: 0.12624883651733398 elapsed, loss: 3.5390142e-06\n",
      "step: 40950 train: 0.12761473655700684 elapsed, loss: 2.8544944e-06\n",
      "step: 40960 train: 0.1229400634765625 elapsed, loss: 2.627255e-06\n",
      "step: 40970 train: 0.1246347427368164 elapsed, loss: 2.7064175e-06\n",
      "step: 40980 train: 0.13028359413146973 elapsed, loss: 3.4137404e-06\n",
      "step: 40990 train: 0.1240227222442627 elapsed, loss: 2.2598492e-06\n",
      "step: 41000 train: 0.1154019832611084 elapsed, loss: 3.4756808e-06\n",
      "step: 41010 train: 0.1154780387878418 elapsed, loss: 3.8663648e-06\n",
      "step: 41020 train: 0.1345510482788086 elapsed, loss: 1.9222466e-06\n",
      "step: 41030 train: 0.12832260131835938 elapsed, loss: 2.7539122e-06\n",
      "step: 41040 train: 0.12647223472595215 elapsed, loss: 2.9392409e-06\n",
      "step: 41050 train: 0.1418600082397461 elapsed, loss: 2.2682311e-06\n",
      "step: 41060 train: 0.13809752464294434 elapsed, loss: 2.9541386e-06\n",
      "step: 41070 train: 0.13663816452026367 elapsed, loss: 2.5466957e-06\n",
      "step: 41080 train: 0.13032770156860352 elapsed, loss: 2.5331913e-06\n",
      "step: 41090 train: 0.12868356704711914 elapsed, loss: 2.8316615e-06\n",
      "step: 41100 train: 0.12884259223937988 elapsed, loss: 2.5695135e-06\n",
      "step: 41110 train: 0.14588618278503418 elapsed, loss: 2.1341216e-06\n",
      "step: 41120 train: 0.12515473365783691 elapsed, loss: 2.93226e-06\n",
      "step: 41130 train: 0.12140512466430664 elapsed, loss: 3.1916331e-06\n",
      "step: 41140 train: 0.13310456275939941 elapsed, loss: 2.5294605e-06\n",
      "step: 41150 train: 0.13740777969360352 elapsed, loss: 2.2155914e-06\n",
      "step: 41160 train: 0.14626502990722656 elapsed, loss: 1.776961e-06\n",
      "step: 41170 train: 0.12242770195007324 elapsed, loss: 3.040759e-06\n",
      "step: 41180 train: 0.13253331184387207 elapsed, loss: 2.4447158e-06\n",
      "step: 41190 train: 0.13925719261169434 elapsed, loss: 2.3273656e-06\n",
      "step: 41200 train: 0.1355578899383545 elapsed, loss: 2.1401756e-06\n",
      "step: 41210 train: 0.12659072875976562 elapsed, loss: 2.3627613e-06\n",
      "step: 41220 train: 0.13902997970581055 elapsed, loss: 2.8349327e-06\n",
      "step: 41230 train: 0.12689495086669922 elapsed, loss: 2.34134e-06\n",
      "step: 41240 train: 0.1266012191772461 elapsed, loss: 2.4526325e-06\n",
      "step: 41250 train: 0.12575554847717285 elapsed, loss: 4.13409e-05\n",
      "step: 41260 train: 0.1351642608642578 elapsed, loss: 1.0499429e-05\n",
      "step: 41270 train: 0.133131742477417 elapsed, loss: 9.068075e-06\n",
      "step: 41280 train: 0.13946032524108887 elapsed, loss: 0.00025076998\n",
      "step: 41290 train: 0.12900733947753906 elapsed, loss: 4.5056564e-05\n",
      "step: 41300 train: 0.13650107383728027 elapsed, loss: 2.9040963e-05\n",
      "step: 41310 train: 0.1270160675048828 elapsed, loss: 9.397392e-06\n",
      "step: 41320 train: 0.1198737621307373 elapsed, loss: 3.410321e-05\n",
      "step: 41330 train: 0.12957429885864258 elapsed, loss: 3.2525364e-05\n",
      "step: 41340 train: 0.1294083595275879 elapsed, loss: 8.312834e-06\n",
      "step: 41350 train: 0.12844252586364746 elapsed, loss: 8.155898e-06\n",
      "step: 41360 train: 0.12898612022399902 elapsed, loss: 4.4083536e-06\n",
      "step: 41370 train: 0.12866425514221191 elapsed, loss: 3.9157126e-06\n",
      "step: 41380 train: 0.12588119506835938 elapsed, loss: 3.561827e-06\n",
      "step: 41390 train: 0.12284183502197266 elapsed, loss: 3.415611e-06\n",
      "step: 41400 train: 0.12569761276245117 elapsed, loss: 2.8503043e-06\n",
      "step: 41410 train: 0.12735366821289062 elapsed, loss: 2.5569343e-06\n",
      "step: 41420 train: 0.12405920028686523 elapsed, loss: 2.6076948e-06\n",
      "step: 41430 train: 0.1270008087158203 elapsed, loss: 2.0721882e-06\n",
      "step: 41440 train: 0.13502788543701172 elapsed, loss: 2.0791733e-06\n",
      "step: 41450 train: 0.13781523704528809 elapsed, loss: 2.843317e-06\n",
      "step: 41460 train: 0.12820005416870117 elapsed, loss: 2.4302808e-06\n",
      "step: 41470 train: 0.12401628494262695 elapsed, loss: 2.4521669e-06\n",
      "step: 41480 train: 0.1254560947418213 elapsed, loss: 2.5224822e-06\n",
      "step: 41490 train: 0.12560153007507324 elapsed, loss: 2.2533245e-06\n",
      "step: 41500 train: 0.140608549118042 elapsed, loss: 1.623293e-06\n",
      "step: 41510 train: 0.12253975868225098 elapsed, loss: 2.3813855e-06\n",
      "step: 41520 train: 0.1335303783416748 elapsed, loss: 1.3299272e-06\n",
      "step: 41530 train: 0.12772297859191895 elapsed, loss: 1.9771942e-06\n",
      "step: 41540 train: 0.13281774520874023 elapsed, loss: 1.8384279e-06\n",
      "step: 41550 train: 0.13201141357421875 elapsed, loss: 1.8905812e-06\n",
      "step: 41560 train: 0.1226799488067627 elapsed, loss: 2.825153e-06\n",
      "step: 41570 train: 0.13135147094726562 elapsed, loss: 2.6910434e-06\n",
      "step: 41580 train: 0.13134384155273438 elapsed, loss: 2.341338e-06\n",
      "step: 41590 train: 0.13961482048034668 elapsed, loss: 1.9590334e-06\n",
      "step: 41600 train: 0.14369797706604004 elapsed, loss: 2.5457443e-06\n",
      "step: 41610 train: 0.13108587265014648 elapsed, loss: 1.8021058e-06\n",
      "step: 41620 train: 0.1230764389038086 elapsed, loss: 2.9345892e-06\n",
      "step: 41630 train: 0.12104392051696777 elapsed, loss: 2.747394e-06\n",
      "step: 41640 train: 0.13921070098876953 elapsed, loss: 1.7345851e-06\n",
      "step: 41650 train: 0.12655997276306152 elapsed, loss: 1.9674158e-06\n",
      "step: 41660 train: 0.1284923553466797 elapsed, loss: 2.027484e-06\n",
      "step: 41670 train: 0.1317272186279297 elapsed, loss: 3.2637286e-06\n",
      "step: 41680 train: 0.12907123565673828 elapsed, loss: 1.7946562e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 41690 train: 0.1454298496246338 elapsed, loss: 1.9185213e-06\n",
      "step: 41700 train: 0.1389482021331787 elapsed, loss: 2.4801075e-06\n",
      "step: 41710 train: 0.13250350952148438 elapsed, loss: 2.307812e-06\n",
      "step: 41720 train: 0.13193202018737793 elapsed, loss: 2.5206182e-06\n",
      "step: 41730 train: 0.12852692604064941 elapsed, loss: 2.4777778e-06\n",
      "step: 41740 train: 0.13407635688781738 elapsed, loss: 2.562525e-06\n",
      "step: 41750 train: 0.1313621997833252 elapsed, loss: 2.603041e-06\n",
      "step: 41760 train: 0.13064956665039062 elapsed, loss: 2.0014086e-06\n",
      "step: 41770 train: 0.1333599090576172 elapsed, loss: 4.127045e-06\n",
      "step: 41780 train: 0.12111902236938477 elapsed, loss: 3.2875541e-06\n",
      "step: 41790 train: 0.12899041175842285 elapsed, loss: 2.7306305e-06\n",
      "step: 41800 train: 0.1403052806854248 elapsed, loss: 2.188604e-06\n",
      "step: 41810 train: 0.13290739059448242 elapsed, loss: 2.1229462e-06\n",
      "step: 41820 train: 0.1299753189086914 elapsed, loss: 2.396285e-06\n",
      "step: 41830 train: 0.13357782363891602 elapsed, loss: 3.4579757e-06\n",
      "step: 41840 train: 0.1313154697418213 elapsed, loss: 2.0200355e-06\n",
      "step: 41850 train: 0.12297964096069336 elapsed, loss: 2.694776e-06\n",
      "step: 41860 train: 0.12573504447937012 elapsed, loss: 3.0528506e-06\n",
      "step: 41870 train: 0.1293189525604248 elapsed, loss: 3.2409584e-06\n",
      "step: 41880 train: 0.13017010688781738 elapsed, loss: 2.2971021e-06\n",
      "step: 41890 train: 0.1390089988708496 elapsed, loss: 2.6370294e-06\n",
      "step: 41900 train: 0.13640451431274414 elapsed, loss: 1.8826656e-06\n",
      "step: 41910 train: 0.1335771083831787 elapsed, loss: 3.613517e-06\n",
      "step: 41920 train: 0.11774182319641113 elapsed, loss: 3.97206e-06\n",
      "step: 41930 train: 0.13350868225097656 elapsed, loss: 3.1347602e-06\n",
      "step: 41940 train: 0.1291332244873047 elapsed, loss: 3.6982412e-06\n",
      "step: 41950 train: 0.12731218338012695 elapsed, loss: 3.0915173e-06\n",
      "step: 41960 train: 0.13064193725585938 elapsed, loss: 3.5990831e-06\n",
      "step: 41970 train: 0.12184429168701172 elapsed, loss: 3.8505386e-06\n",
      "step: 41980 train: 0.13828039169311523 elapsed, loss: 1.8775429e-06\n",
      "step: 41990 train: 0.1246798038482666 elapsed, loss: 3.8514495e-06\n",
      "step: 42000 train: 0.11603832244873047 elapsed, loss: 3.9730094e-06\n",
      "step: 42010 train: 0.12062883377075195 elapsed, loss: 3.5236487e-06\n",
      "step: 42020 train: 0.12555694580078125 elapsed, loss: 3.17068e-06\n",
      "step: 42030 train: 0.12671566009521484 elapsed, loss: 2.7413412e-06\n",
      "step: 42040 train: 0.12978649139404297 elapsed, loss: 3.0258584e-06\n",
      "step: 42050 train: 0.13471412658691406 elapsed, loss: 2.347856e-06\n",
      "step: 42060 train: 0.12004232406616211 elapsed, loss: 4.4791736e-06\n",
      "step: 42070 train: 0.14092445373535156 elapsed, loss: 2.8065324e-06\n",
      "step: 42080 train: 0.1284637451171875 elapsed, loss: 3.0230658e-06\n",
      "step: 42090 train: 0.13705849647521973 elapsed, loss: 2.6044388e-06\n",
      "step: 42100 train: 0.13141775131225586 elapsed, loss: 2.8223624e-06\n",
      "step: 42110 train: 0.1267380714416504 elapsed, loss: 2.7939618e-06\n",
      "step: 42120 train: 0.12440109252929688 elapsed, loss: 9.571963e-06\n",
      "step: 42130 train: 0.12276148796081543 elapsed, loss: 5.1059287e-06\n",
      "step: 42140 train: 0.12134242057800293 elapsed, loss: 5.453326e-06\n",
      "step: 42150 train: 0.13881230354309082 elapsed, loss: 3.7103741e-06\n",
      "step: 42160 train: 0.11694717407226562 elapsed, loss: 4.8562943e-06\n",
      "step: 42170 train: 0.12489962577819824 elapsed, loss: 2.6873247e-06\n",
      "step: 42180 train: 0.13846945762634277 elapsed, loss: 2.2388942e-06\n",
      "step: 42190 train: 0.12454605102539062 elapsed, loss: 4.658639e-06\n",
      "step: 42200 train: 0.1319410800933838 elapsed, loss: 7.701467e-06\n",
      "step: 42210 train: 0.12939763069152832 elapsed, loss: 2.590466e-06\n",
      "step: 42220 train: 0.13210725784301758 elapsed, loss: 1.8500691e-06\n",
      "step: 42230 train: 0.12466716766357422 elapsed, loss: 2.9541486e-06\n",
      "step: 42240 train: 0.11999893188476562 elapsed, loss: 3.2847665e-06\n",
      "step: 42250 train: 0.12808752059936523 elapsed, loss: 2.6482003e-06\n",
      "step: 42260 train: 0.1454463005065918 elapsed, loss: 1.8873218e-06\n",
      "step: 42270 train: 0.12420010566711426 elapsed, loss: 2.3739349e-06\n",
      "step: 42280 train: 0.12795376777648926 elapsed, loss: 2.2854608e-06\n",
      "step: 42290 train: 0.1306166648864746 elapsed, loss: 2.0102566e-06\n",
      "step: 42300 train: 0.12996983528137207 elapsed, loss: 2.77161e-06\n",
      "step: 42310 train: 0.13421177864074707 elapsed, loss: 2.8992008e-06\n",
      "step: 42320 train: 0.13248872756958008 elapsed, loss: 2.237964e-06\n",
      "step: 42330 train: 0.11764168739318848 elapsed, loss: 3.3257434e-06\n",
      "step: 42340 train: 0.1198265552520752 elapsed, loss: 4.0614827e-06\n",
      "step: 42350 train: 0.12406492233276367 elapsed, loss: 7.388979e-06\n",
      "step: 42360 train: 0.12771296501159668 elapsed, loss: 3.4700956e-06\n",
      "step: 42370 train: 0.12029385566711426 elapsed, loss: 3.248405e-06\n",
      "step: 42380 train: 0.12522029876708984 elapsed, loss: 3.0835965e-06\n",
      "step: 42390 train: 0.14426183700561523 elapsed, loss: 1.7206162e-06\n",
      "step: 42400 train: 0.13168859481811523 elapsed, loss: 2.5690479e-06\n",
      "step: 42410 train: 0.1338639259338379 elapsed, loss: 2.5834795e-06\n",
      "step: 42420 train: 0.13332796096801758 elapsed, loss: 2.1657866e-06\n",
      "step: 42430 train: 0.12140774726867676 elapsed, loss: 4.189533e-06\n",
      "step: 42440 train: 0.12531232833862305 elapsed, loss: 3.1744007e-06\n",
      "step: 42450 train: 0.13413262367248535 elapsed, loss: 2.2752156e-06\n",
      "step: 42460 train: 0.1240999698638916 elapsed, loss: 3.3159574e-06\n",
      "step: 42470 train: 0.13209795951843262 elapsed, loss: 2.2370307e-06\n",
      "step: 42480 train: 0.1220097541809082 elapsed, loss: 3.2563566e-06\n",
      "step: 42490 train: 0.12922430038452148 elapsed, loss: 3.3178253e-06\n",
      "step: 42500 train: 0.12597203254699707 elapsed, loss: 4.0740524e-06\n",
      "step: 42510 train: 0.1273202896118164 elapsed, loss: 0.00012961337\n",
      "step: 42520 train: 0.13272881507873535 elapsed, loss: 5.708073e-05\n",
      "step: 42530 train: 0.12817955017089844 elapsed, loss: 2.325524e-05\n",
      "step: 42540 train: 0.12643909454345703 elapsed, loss: 1.4931857e-05\n",
      "step: 42550 train: 0.12897944450378418 elapsed, loss: 1.15768935e-05\n",
      "step: 42560 train: 0.12357592582702637 elapsed, loss: 1.1484834e-05\n",
      "step: 42570 train: 0.12306904792785645 elapsed, loss: 9.505411e-06\n",
      "step: 42580 train: 0.13340067863464355 elapsed, loss: 6.6919506e-06\n",
      "step: 42590 train: 0.12121272087097168 elapsed, loss: 7.3150027e-06\n",
      "step: 42600 train: 0.13329243659973145 elapsed, loss: 6.5783343e-06\n",
      "step: 42610 train: 0.13237833976745605 elapsed, loss: 4.7082794e-06\n",
      "step: 42620 train: 0.13787031173706055 elapsed, loss: 4.5173547e-06\n",
      "step: 42630 train: 0.1205439567565918 elapsed, loss: 3.0272558e-06\n",
      "step: 42640 train: 0.12456798553466797 elapsed, loss: 3.4123386e-06\n",
      "step: 42650 train: 0.12781929969787598 elapsed, loss: 2.5527493e-06\n",
      "step: 42660 train: 0.1257178783416748 elapsed, loss: 1.8533285e-06\n",
      "step: 42670 train: 0.12462306022644043 elapsed, loss: 2.4144488e-06\n",
      "step: 42680 train: 0.11906862258911133 elapsed, loss: 2.8312134e-06\n",
      "step: 42690 train: 0.12666940689086914 elapsed, loss: 2.4475107e-06\n",
      "step: 42700 train: 0.1300182342529297 elapsed, loss: 2.2295744e-06\n",
      "step: 42710 train: 0.12864089012145996 elapsed, loss: 3.1762415e-06\n",
      "step: 42720 train: 0.1385498046875 elapsed, loss: 1.9571692e-06\n",
      "step: 42730 train: 0.1274728775024414 elapsed, loss: 2.6565913e-06\n",
      "step: 42740 train: 0.13089442253112793 elapsed, loss: 3.6925899e-06\n",
      "step: 42750 train: 0.1315932273864746 elapsed, loss: 2.100594e-06\n",
      "step: 42760 train: 0.12489914894104004 elapsed, loss: 2.7473855e-06\n",
      "step: 42770 train: 0.12357831001281738 elapsed, loss: 3.9050174e-06\n",
      "step: 42780 train: 0.1350393295288086 elapsed, loss: 2.7557776e-06\n",
      "step: 42790 train: 0.1243753433227539 elapsed, loss: 2.541573e-06\n",
      "step: 42800 train: 0.12347269058227539 elapsed, loss: 2.4386636e-06\n",
      "step: 42810 train: 0.12108159065246582 elapsed, loss: 3.981857e-06\n",
      "step: 42820 train: 0.12945103645324707 elapsed, loss: 2.8535587e-06\n",
      "step: 42830 train: 0.1224219799041748 elapsed, loss: 3.8812686e-06\n",
      "step: 42840 train: 0.12389850616455078 elapsed, loss: 3.450074e-06\n",
      "step: 42850 train: 0.1403663158416748 elapsed, loss: 2.3730054e-06\n",
      "step: 42860 train: 0.12965059280395508 elapsed, loss: 2.11922e-06\n",
      "step: 42870 train: 0.132293701171875 elapsed, loss: 1.9473914e-06\n",
      "step: 42880 train: 0.1244661808013916 elapsed, loss: 3.3206202e-06\n",
      "step: 42890 train: 0.11554384231567383 elapsed, loss: 3.8132873e-06\n",
      "step: 42900 train: 0.12497091293334961 elapsed, loss: 3.005835e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 42910 train: 0.12357139587402344 elapsed, loss: 3.4482114e-06\n",
      "step: 42920 train: 0.12245821952819824 elapsed, loss: 3.2633443e-06\n",
      "step: 42930 train: 0.12331414222717285 elapsed, loss: 4.081503e-06\n",
      "step: 42940 train: 0.12136983871459961 elapsed, loss: 3.0049055e-06\n",
      "step: 42950 train: 0.1367053985595703 elapsed, loss: 4.1383105e-06\n",
      "step: 42960 train: 0.1383211612701416 elapsed, loss: 2.9201524e-06\n",
      "step: 42970 train: 0.13994669914245605 elapsed, loss: 3.1874424e-06\n",
      "step: 42980 train: 0.14162373542785645 elapsed, loss: 2.3702114e-06\n",
      "step: 42990 train: 0.12464213371276855 elapsed, loss: 3.131558e-06\n",
      "step: 43000 train: 0.12713623046875 elapsed, loss: 2.389304e-06\n",
      "step: 43010 train: 0.1263124942779541 elapsed, loss: 2.5969825e-06\n",
      "step: 43020 train: 0.1314404010772705 elapsed, loss: 2.012585e-06\n",
      "step: 43030 train: 0.13212990760803223 elapsed, loss: 2.4642736e-06\n",
      "step: 43040 train: 0.12461066246032715 elapsed, loss: 2.5536806e-06\n",
      "step: 43050 train: 0.12263941764831543 elapsed, loss: 3.6787078e-06\n",
      "step: 43060 train: 0.1194605827331543 elapsed, loss: 4.1201547e-06\n",
      "step: 43070 train: 0.12184929847717285 elapsed, loss: 3.1487907e-06\n",
      "step: 43080 train: 0.1355576515197754 elapsed, loss: 2.5122376e-06\n",
      "step: 43090 train: 0.12894797325134277 elapsed, loss: 3.964623e-06\n",
      "step: 43100 train: 0.11991596221923828 elapsed, loss: 3.9427377e-06\n",
      "step: 43110 train: 0.12345004081726074 elapsed, loss: 3.8319017e-06\n",
      "step: 43120 train: 0.12952923774719238 elapsed, loss: 2.983947e-06\n",
      "step: 43130 train: 0.13195323944091797 elapsed, loss: 2.1182877e-06\n",
      "step: 43140 train: 0.12026476860046387 elapsed, loss: 5.577661e-06\n",
      "step: 43150 train: 0.12111139297485352 elapsed, loss: 3.478479e-06\n",
      "step: 43160 train: 0.12409758567810059 elapsed, loss: 2.7683504e-06\n",
      "step: 43170 train: 0.1267702579498291 elapsed, loss: 3.0170122e-06\n",
      "step: 43180 train: 0.13549137115478516 elapsed, loss: 2.4298129e-06\n",
      "step: 43190 train: 0.13750672340393066 elapsed, loss: 2.5033858e-06\n",
      "step: 43200 train: 0.11963653564453125 elapsed, loss: 3.5422722e-06\n",
      "step: 43210 train: 0.1167759895324707 elapsed, loss: 7.0770393e-06\n",
      "step: 43220 train: 0.11577630043029785 elapsed, loss: 4.4931417e-06\n",
      "step: 43230 train: 0.13431239128112793 elapsed, loss: 2.6356367e-06\n",
      "step: 43240 train: 0.13253569602966309 elapsed, loss: 2.98441e-06\n",
      "step: 43250 train: 0.1287519931793213 elapsed, loss: 3.906397e-06\n",
      "step: 43260 train: 0.13437366485595703 elapsed, loss: 2.4903507e-06\n",
      "step: 43270 train: 0.12694621086120605 elapsed, loss: 2.687791e-06\n",
      "step: 43280 train: 0.12300944328308105 elapsed, loss: 5.1841803e-06\n",
      "step: 43290 train: 0.13346409797668457 elapsed, loss: 2.4456476e-06\n",
      "step: 43300 train: 0.13473200798034668 elapsed, loss: 2.56346e-06\n",
      "step: 43310 train: 0.13962268829345703 elapsed, loss: 2.7259734e-06\n",
      "step: 43320 train: 0.13300800323486328 elapsed, loss: 2.703157e-06\n",
      "step: 43330 train: 0.14554524421691895 elapsed, loss: 2.60676e-06\n",
      "step: 43340 train: 0.13903594017028809 elapsed, loss: 2.7590374e-06\n",
      "step: 43350 train: 0.13271808624267578 elapsed, loss: 2.5755662e-06\n",
      "step: 43360 train: 0.13037538528442383 elapsed, loss: 3.7467003e-06\n",
      "step: 43370 train: 0.14114832878112793 elapsed, loss: 2.8358627e-06\n",
      "step: 43380 train: 0.12980890274047852 elapsed, loss: 3.944589e-06\n",
      "step: 43390 train: 0.13600802421569824 elapsed, loss: 2.212353e-06\n",
      "step: 43400 train: 0.13173317909240723 elapsed, loss: 3.706181e-06\n",
      "step: 43410 train: 0.13088583946228027 elapsed, loss: 3.2265561e-06\n",
      "step: 43420 train: 0.13135266304016113 elapsed, loss: 7.778473e-06\n",
      "step: 43430 train: 0.11928009986877441 elapsed, loss: 1.21192425e-05\n",
      "step: 43440 train: 0.1342153549194336 elapsed, loss: 2.8963923e-06\n",
      "step: 43450 train: 0.13119888305664062 elapsed, loss: 4.3171012e-06\n",
      "step: 43460 train: 0.14101862907409668 elapsed, loss: 3.7248096e-06\n",
      "step: 43470 train: 0.12950730323791504 elapsed, loss: 2.432144e-06\n",
      "step: 43480 train: 0.1270906925201416 elapsed, loss: 3.333194e-06\n",
      "step: 43490 train: 0.13269901275634766 elapsed, loss: 2.58814e-06\n",
      "step: 43500 train: 0.13358640670776367 elapsed, loss: 2.2835986e-06\n",
      "step: 43510 train: 0.12855005264282227 elapsed, loss: 2.834474e-06\n",
      "step: 43520 train: 0.13889312744140625 elapsed, loss: 2.4959331e-06\n",
      "step: 43530 train: 0.13204431533813477 elapsed, loss: 2.7092115e-06\n",
      "step: 43540 train: 0.1277456283569336 elapsed, loss: 2.9322507e-06\n",
      "step: 43550 train: 0.13657927513122559 elapsed, loss: 3.3536833e-06\n",
      "step: 43560 train: 0.13088297843933105 elapsed, loss: 3.6349388e-06\n",
      "step: 43570 train: 0.126784086227417 elapsed, loss: 3.872424e-06\n",
      "step: 43580 train: 0.1271836757659912 elapsed, loss: 3.0421534e-06\n",
      "step: 43590 train: 0.13033103942871094 elapsed, loss: 3.183718e-06\n",
      "step: 43600 train: 0.13074493408203125 elapsed, loss: 2.3739358e-06\n",
      "step: 43610 train: 0.1211862564086914 elapsed, loss: 3.3788292e-06\n",
      "step: 43620 train: 0.13075494766235352 elapsed, loss: 2.7525175e-06\n",
      "step: 43630 train: 0.12070727348327637 elapsed, loss: 3.7578627e-06\n",
      "step: 43640 train: 0.12426567077636719 elapsed, loss: 3.949721e-06\n",
      "step: 43650 train: 0.13194823265075684 elapsed, loss: 3.1022244e-06\n",
      "step: 43660 train: 0.12499308586120605 elapsed, loss: 5.3503627e-06\n",
      "step: 43670 train: 0.13059329986572266 elapsed, loss: 3.343891e-06\n",
      "step: 43680 train: 0.12352442741394043 elapsed, loss: 4.2533256e-06\n",
      "step: 43690 train: 0.14531707763671875 elapsed, loss: 2.184878e-06\n",
      "step: 43700 train: 0.13352274894714355 elapsed, loss: 5.3983827e-06\n",
      "step: 43710 train: 0.13779473304748535 elapsed, loss: 2.3310959e-06\n",
      "step: 43720 train: 0.1411571502685547 elapsed, loss: 2.6174716e-06\n",
      "step: 43730 train: 0.14148855209350586 elapsed, loss: 2.8014065e-06\n",
      "step: 43740 train: 0.12626290321350098 elapsed, loss: 4.286382e-06\n",
      "step: 43750 train: 0.12258458137512207 elapsed, loss: 3.9678534e-06\n",
      "step: 43760 train: 0.1382608413696289 elapsed, loss: 2.2384297e-06\n",
      "step: 43770 train: 0.12367010116577148 elapsed, loss: 4.122949e-06\n",
      "step: 43780 train: 0.12628626823425293 elapsed, loss: 2.6845316e-06\n",
      "step: 43790 train: 0.1311173439025879 elapsed, loss: 2.5783597e-06\n",
      "step: 43800 train: 0.13782620429992676 elapsed, loss: 3.9523898e-06\n",
      "step: 43810 train: 0.14985918998718262 elapsed, loss: 2.4330752e-06\n",
      "step: 43820 train: 0.11833930015563965 elapsed, loss: 3.939941e-06\n",
      "step: 43830 train: 0.12274575233459473 elapsed, loss: 4.116895e-06\n",
      "step: 43840 train: 0.13370370864868164 elapsed, loss: 2.1485575e-06\n",
      "step: 43850 train: 0.12037134170532227 elapsed, loss: 4.1844155e-06\n",
      "step: 43860 train: 0.12801551818847656 elapsed, loss: 3.1255054e-06\n",
      "step: 43870 train: 0.1364274024963379 elapsed, loss: 2.3972189e-06\n",
      "step: 43880 train: 0.12838244438171387 elapsed, loss: 3.041224e-06\n",
      "step: 43890 train: 0.12016010284423828 elapsed, loss: 4.3627488e-06\n",
      "step: 43900 train: 0.13354873657226562 elapsed, loss: 2.380922e-06\n",
      "step: 43910 train: 0.13557887077331543 elapsed, loss: 2.4703268e-06\n",
      "step: 43920 train: 0.1232302188873291 elapsed, loss: 3.168815e-06\n",
      "step: 43930 train: 0.12594175338745117 elapsed, loss: 3.4877871e-06\n",
      "step: 43940 train: 0.1396331787109375 elapsed, loss: 2.8293402e-06\n",
      "step: 43950 train: 0.13361406326293945 elapsed, loss: 2.0614784e-06\n",
      "step: 43960 train: 0.12723851203918457 elapsed, loss: 4.1588023e-06\n",
      "step: 43970 train: 0.13041067123413086 elapsed, loss: 5.775957e-06\n",
      "step: 43980 train: 0.13601970672607422 elapsed, loss: 2.947628e-06\n",
      "step: 43990 train: 0.12542486190795898 elapsed, loss: 2.6449497e-06\n",
      "step: 44000 train: 0.13477206230163574 elapsed, loss: 2.8498373e-06\n",
      "step: 44010 train: 0.1414039134979248 elapsed, loss: 2.1662504e-06\n",
      "step: 44020 train: 0.12478971481323242 elapsed, loss: 4.420468e-06\n",
      "step: 44030 train: 0.12320876121520996 elapsed, loss: 3.3308652e-06\n",
      "step: 44040 train: 0.13232183456420898 elapsed, loss: 3.188362e-06\n",
      "step: 44050 train: 0.1301419734954834 elapsed, loss: 4.096869e-06\n",
      "step: 44060 train: 0.13025164604187012 elapsed, loss: 6.6239804e-06\n",
      "step: 44070 train: 0.14082121849060059 elapsed, loss: 2.4130525e-06\n",
      "step: 44080 train: 0.13565683364868164 elapsed, loss: 2.2556583e-06\n",
      "step: 44090 train: 0.12642216682434082 elapsed, loss: 3.7746227e-06\n",
      "step: 44100 train: 0.12923645973205566 elapsed, loss: 4.7450626e-06\n",
      "step: 44110 train: 0.1314079761505127 elapsed, loss: 2.169978e-06\n",
      "step: 44120 train: 0.12106800079345703 elapsed, loss: 5.03703e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 44130 train: 0.1363048553466797 elapsed, loss: 4.971351e-06\n",
      "step: 44140 train: 0.13342857360839844 elapsed, loss: 2.8554168e-06\n",
      "step: 44150 train: 0.12434911727905273 elapsed, loss: 4.5536476e-06\n",
      "step: 44160 train: 0.13939142227172852 elapsed, loss: 2.7422277e-06\n",
      "step: 44170 train: 0.13043212890625 elapsed, loss: 8.247629e-06\n",
      "step: 44180 train: 0.13358616828918457 elapsed, loss: 3.6940787e-06\n",
      "step: 44190 train: 0.13278484344482422 elapsed, loss: 3.9734664e-06\n",
      "step: 44200 train: 0.1384437084197998 elapsed, loss: 3.3774315e-06\n",
      "step: 44210 train: 0.12856698036193848 elapsed, loss: 2.6761486e-06\n",
      "step: 44220 train: 0.1271820068359375 elapsed, loss: 2.7138676e-06\n",
      "step: 44230 train: 0.128615140914917 elapsed, loss: 3.1152633e-06\n",
      "step: 44240 train: 0.12301826477050781 elapsed, loss: 3.3327256e-06\n",
      "step: 44250 train: 0.13483095169067383 elapsed, loss: 2.6412242e-06\n",
      "step: 44260 train: 0.1405167579650879 elapsed, loss: 2.237964e-06\n",
      "step: 44270 train: 0.14999175071716309 elapsed, loss: 2.625854e-06\n",
      "step: 44280 train: 0.12781310081481934 elapsed, loss: 3.1911663e-06\n",
      "step: 44290 train: 0.14345121383666992 elapsed, loss: 2.239359e-06\n",
      "step: 44300 train: 0.12685465812683105 elapsed, loss: 3.4570567e-06\n",
      "step: 44310 train: 0.12346386909484863 elapsed, loss: 3.8155836e-06\n",
      "step: 44320 train: 0.13059782981872559 elapsed, loss: 2.978361e-06\n",
      "step: 44330 train: 0.13197064399719238 elapsed, loss: 3.9971646e-06\n",
      "step: 44340 train: 0.13337397575378418 elapsed, loss: 2.101059e-06\n",
      "step: 44350 train: 0.11811017990112305 elapsed, loss: 4.1448334e-06\n",
      "step: 44360 train: 0.13059163093566895 elapsed, loss: 2.3436685e-06\n",
      "step: 44370 train: 0.13087701797485352 elapsed, loss: 4.5317915e-06\n",
      "step: 44380 train: 0.12275099754333496 elapsed, loss: 2.802342e-06\n",
      "step: 44390 train: 0.1214289665222168 elapsed, loss: 0.00024164194\n",
      "step: 44400 train: 0.12815618515014648 elapsed, loss: 6.068383e-05\n",
      "step: 44410 train: 0.13226819038391113 elapsed, loss: 2.2173426e-05\n",
      "step: 44420 train: 0.12529397010803223 elapsed, loss: 1.7299306e-05\n",
      "step: 44430 train: 0.13267135620117188 elapsed, loss: 1.0338887e-05\n",
      "step: 44440 train: 0.12122011184692383 elapsed, loss: 1.2733715e-05\n",
      "step: 44450 train: 0.13033318519592285 elapsed, loss: 6.613722e-06\n",
      "step: 44460 train: 0.1334538459777832 elapsed, loss: 5.8463247e-06\n",
      "step: 44470 train: 0.12938928604125977 elapsed, loss: 5.0072276e-06\n",
      "step: 44480 train: 0.1186375617980957 elapsed, loss: 5.9855684e-06\n",
      "step: 44490 train: 0.13031005859375 elapsed, loss: 6.6458097e-06\n",
      "step: 44500 train: 0.12773466110229492 elapsed, loss: 4.8852276e-06\n",
      "step: 44510 train: 0.1259310245513916 elapsed, loss: 3.427256e-06\n",
      "step: 44520 train: 0.12950468063354492 elapsed, loss: 3.1552954e-06\n",
      "step: 44530 train: 0.12336897850036621 elapsed, loss: 4.17417e-06\n",
      "step: 44540 train: 0.12386250495910645 elapsed, loss: 2.861481e-06\n",
      "step: 44550 train: 0.14067387580871582 elapsed, loss: 2.0782425e-06\n",
      "step: 44560 train: 0.13316607475280762 elapsed, loss: 2.357172e-06\n",
      "step: 44570 train: 0.11701011657714844 elapsed, loss: 3.036104e-06\n",
      "step: 44580 train: 0.1454470157623291 elapsed, loss: 1.6684621e-06\n",
      "step: 44590 train: 0.13501715660095215 elapsed, loss: 2.1085104e-06\n",
      "step: 44600 train: 0.12991690635681152 elapsed, loss: 3.1571735e-06\n",
      "step: 44610 train: 0.12743210792541504 elapsed, loss: 2.5289983e-06\n",
      "step: 44620 train: 0.13960766792297363 elapsed, loss: 2.3483215e-06\n",
      "step: 44630 train: 0.1253519058227539 elapsed, loss: 2.9699722e-06\n",
      "step: 44640 train: 0.13438725471496582 elapsed, loss: 2.4568237e-06\n",
      "step: 44650 train: 0.1384751796722412 elapsed, loss: 1.7941887e-06\n",
      "step: 44660 train: 0.1316075325012207 elapsed, loss: 2.130395e-06\n",
      "step: 44670 train: 0.13529682159423828 elapsed, loss: 1.5636882e-06\n",
      "step: 44680 train: 0.1395435333251953 elapsed, loss: 1.939467e-06\n",
      "step: 44690 train: 0.13481426239013672 elapsed, loss: 2.7208407e-06\n",
      "step: 44700 train: 0.1273207664489746 elapsed, loss: 3.0663723e-06\n",
      "step: 44710 train: 0.14542245864868164 elapsed, loss: 8.622443e-06\n",
      "step: 44720 train: 0.13579869270324707 elapsed, loss: 4.6132814e-06\n",
      "step: 44730 train: 0.14084172248840332 elapsed, loss: 3.6530917e-06\n",
      "step: 44740 train: 0.13637304306030273 elapsed, loss: 3.257757e-06\n",
      "step: 44750 train: 0.13725852966308594 elapsed, loss: 2.1816195e-06\n",
      "step: 44760 train: 0.13605308532714844 elapsed, loss: 2.3627529e-06\n",
      "step: 44770 train: 0.1308140754699707 elapsed, loss: 2.3096732e-06\n",
      "step: 44780 train: 0.12678194046020508 elapsed, loss: 2.0908155e-06\n",
      "step: 44790 train: 0.11802268028259277 elapsed, loss: 3.147381e-06\n",
      "step: 44800 train: 0.12697172164916992 elapsed, loss: 2.3436683e-06\n",
      "step: 44810 train: 0.12575697898864746 elapsed, loss: 3.0514645e-06\n",
      "step: 44820 train: 0.12624859809875488 elapsed, loss: 2.2947743e-06\n",
      "step: 44830 train: 0.14121127128601074 elapsed, loss: 1.6526292e-06\n",
      "step: 44840 train: 0.13123512268066406 elapsed, loss: 2.1164258e-06\n",
      "step: 44850 train: 0.1258397102355957 elapsed, loss: 2.3860428e-06\n",
      "step: 44860 train: 0.12978053092956543 elapsed, loss: 2.5364448e-06\n",
      "step: 44870 train: 0.13058781623840332 elapsed, loss: 2.5681136e-06\n",
      "step: 44880 train: 0.11520075798034668 elapsed, loss: 3.2144508e-06\n",
      "step: 44890 train: 0.13137340545654297 elapsed, loss: 2.067532e-06\n",
      "step: 44900 train: 0.12552189826965332 elapsed, loss: 3.8794096e-06\n",
      "step: 44910 train: 0.11640119552612305 elapsed, loss: 5.367182e-06\n",
      "step: 44920 train: 0.12403655052185059 elapsed, loss: 3.6763765e-06\n",
      "step: 44930 train: 0.12859487533569336 elapsed, loss: 2.4666028e-06\n",
      "step: 44940 train: 0.1260523796081543 elapsed, loss: 2.8214254e-06\n",
      "step: 44950 train: 0.12438750267028809 elapsed, loss: 3.1380828e-06\n",
      "step: 44960 train: 0.13773632049560547 elapsed, loss: 1.9729973e-06\n",
      "step: 44970 train: 0.14423656463623047 elapsed, loss: 2.142969e-06\n",
      "step: 44980 train: 0.1480703353881836 elapsed, loss: 2.2980341e-06\n",
      "step: 44990 train: 0.13220596313476562 elapsed, loss: 3.1413379e-06\n",
      "step: 45000 train: 0.13035297393798828 elapsed, loss: 3.6651984e-06\n",
      "step: 45010 train: 0.11687469482421875 elapsed, loss: 3.436569e-06\n",
      "step: 45020 train: 0.12143516540527344 elapsed, loss: 3.1995485e-06\n",
      "step: 45030 train: 0.1218564510345459 elapsed, loss: 3.7513423e-06\n",
      "step: 45040 train: 0.13616228103637695 elapsed, loss: 2.8335385e-06\n",
      "step: 45050 train: 0.13647842407226562 elapsed, loss: 2.7985943e-06\n",
      "step: 45060 train: 0.12697935104370117 elapsed, loss: 2.5494905e-06\n",
      "step: 45070 train: 0.12315225601196289 elapsed, loss: 4.416031e-06\n",
      "step: 45080 train: 0.1347341537475586 elapsed, loss: 2.6682285e-06\n",
      "step: 45090 train: 0.14647221565246582 elapsed, loss: 2.3939579e-06\n",
      "step: 45100 train: 0.11597299575805664 elapsed, loss: 0.0002560609\n",
      "step: 45110 train: 0.12964582443237305 elapsed, loss: 5.883249e-05\n",
      "step: 45120 train: 0.1306767463684082 elapsed, loss: 1.7779937e-05\n",
      "step: 45130 train: 0.1265416145324707 elapsed, loss: 1.0934351e-05\n",
      "step: 45140 train: 0.12169981002807617 elapsed, loss: 1.6341537e-05\n",
      "step: 45150 train: 0.12706851959228516 elapsed, loss: 1.6699556e-05\n",
      "step: 45160 train: 0.13623809814453125 elapsed, loss: 4.4419203e-06\n",
      "step: 45170 train: 0.13799619674682617 elapsed, loss: 6.2197673e-06\n",
      "step: 45180 train: 0.12890195846557617 elapsed, loss: 8.014993e-06\n",
      "step: 45190 train: 0.13067889213562012 elapsed, loss: 4.982523e-06\n",
      "step: 45200 train: 0.12589311599731445 elapsed, loss: 4.8777697e-06\n",
      "step: 45210 train: 0.14066219329833984 elapsed, loss: 2.7706733e-06\n",
      "step: 45220 train: 0.13576245307922363 elapsed, loss: 5.4342592e-05\n",
      "step: 45230 train: 0.12424397468566895 elapsed, loss: 8.699064e-06\n",
      "step: 45240 train: 0.13625884056091309 elapsed, loss: 3.614909e-06\n",
      "step: 45250 train: 0.13115239143371582 elapsed, loss: 2.7255082e-06\n",
      "step: 45260 train: 0.11994147300720215 elapsed, loss: 3.6349304e-06\n",
      "step: 45270 train: 0.13817954063415527 elapsed, loss: 2.1676492e-06\n",
      "step: 45280 train: 0.11764287948608398 elapsed, loss: 3.1157301e-06\n",
      "step: 45290 train: 0.12846922874450684 elapsed, loss: 2.4433189e-06\n",
      "step: 45300 train: 0.12835073471069336 elapsed, loss: 2.1215487e-06\n",
      "step: 45310 train: 0.11850595474243164 elapsed, loss: 2.8270229e-06\n",
      "step: 45320 train: 0.13717937469482422 elapsed, loss: 1.9175868e-06\n",
      "step: 45330 train: 0.12266969680786133 elapsed, loss: 2.4889532e-06\n",
      "step: 45340 train: 0.12917637825012207 elapsed, loss: 2.781851e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 45350 train: 0.1321089267730713 elapsed, loss: 2.146693e-06\n",
      "step: 45360 train: 0.11977076530456543 elapsed, loss: 3.1627526e-06\n",
      "step: 45370 train: 0.1364591121673584 elapsed, loss: 2.9168648e-06\n",
      "step: 45380 train: 0.13690686225891113 elapsed, loss: 2.663574e-06\n",
      "step: 45390 train: 0.13091826438903809 elapsed, loss: 2.6747525e-06\n",
      "step: 45400 train: 0.12438344955444336 elapsed, loss: 3.7563696e-06\n",
      "step: 45410 train: 0.1286323070526123 elapsed, loss: 2.3436642e-06\n",
      "step: 45420 train: 0.13291287422180176 elapsed, loss: 2.3445991e-06\n",
      "step: 45430 train: 0.13239455223083496 elapsed, loss: 2.3646235e-06\n",
      "step: 45440 train: 0.1458137035369873 elapsed, loss: 4.6472232e-06\n",
      "step: 45450 train: 0.13535261154174805 elapsed, loss: 0.00022873863\n",
      "step: 45460 train: 0.11668586730957031 elapsed, loss: 4.5342604e-05\n",
      "step: 45470 train: 0.12868356704711914 elapsed, loss: 2.9916553e-05\n",
      "step: 45480 train: 0.12624812126159668 elapsed, loss: 1.92181e-05\n",
      "step: 45490 train: 0.13004136085510254 elapsed, loss: 1.5885147e-05\n",
      "step: 45500 train: 0.12801575660705566 elapsed, loss: 3.6276055e-05\n",
      "step: 45510 train: 0.12473535537719727 elapsed, loss: 2.1687112e-05\n",
      "step: 45520 train: 0.12116599082946777 elapsed, loss: 1.1318998e-05\n",
      "step: 45530 train: 0.12483668327331543 elapsed, loss: 9.404452e-06\n",
      "step: 45540 train: 0.12304282188415527 elapsed, loss: 4.953656e-06\n",
      "step: 45550 train: 0.12725353240966797 elapsed, loss: 2.3906e-05\n",
      "step: 45560 train: 0.13109230995178223 elapsed, loss: 4.0754467e-06\n",
      "step: 45570 train: 0.13698983192443848 elapsed, loss: 7.421435e-06\n",
      "step: 45580 train: 0.12414431571960449 elapsed, loss: 3.6754468e-06\n",
      "step: 45590 train: 0.12660908699035645 elapsed, loss: 2.9653218e-06\n",
      "step: 45600 train: 0.13356566429138184 elapsed, loss: 2.5611068e-06\n",
      "step: 45610 train: 0.1276545524597168 elapsed, loss: 2.353913e-06\n",
      "step: 45620 train: 0.1263127326965332 elapsed, loss: 2.3599662e-06\n",
      "step: 45630 train: 0.12154889106750488 elapsed, loss: 2.3208468e-06\n",
      "step: 45640 train: 0.13410115242004395 elapsed, loss: 1.6470402e-06\n",
      "step: 45650 train: 0.13112306594848633 elapsed, loss: 1.4118824e-06\n",
      "step: 45660 train: 0.12241816520690918 elapsed, loss: 2.7418077e-06\n",
      "step: 45670 train: 0.1261904239654541 elapsed, loss: 2.0321413e-06\n",
      "step: 45680 train: 0.13700628280639648 elapsed, loss: 1.7243415e-06\n",
      "step: 45690 train: 0.13235974311828613 elapsed, loss: 2.3576304e-06\n",
      "step: 45700 train: 0.13346004486083984 elapsed, loss: 2.4316778e-06\n",
      "step: 45710 train: 0.13540029525756836 elapsed, loss: 2.1294654e-06\n",
      "step: 45720 train: 0.13876605033874512 elapsed, loss: 2.6081461e-06\n",
      "step: 45730 train: 0.13096117973327637 elapsed, loss: 1.677775e-06\n",
      "step: 45740 train: 0.12451481819152832 elapsed, loss: 1.9725383e-06\n",
      "step: 45750 train: 0.12602472305297852 elapsed, loss: 4.712573e-06\n",
      "step: 45760 train: 0.12465071678161621 elapsed, loss: 2.1876722e-06\n",
      "step: 45770 train: 0.13556981086730957 elapsed, loss: 1.4100208e-06\n",
      "step: 45780 train: 0.13186883926391602 elapsed, loss: 2.9415678e-06\n",
      "step: 45790 train: 0.12776994705200195 elapsed, loss: 2.0833645e-06\n",
      "step: 45800 train: 0.1296520233154297 elapsed, loss: 1.8952367e-06\n",
      "step: 45810 train: 0.12303614616394043 elapsed, loss: 2.4829008e-06\n",
      "step: 45820 train: 0.12946772575378418 elapsed, loss: 1.7718316e-06\n",
      "step: 45830 train: 0.12684297561645508 elapsed, loss: 2.331561e-06\n",
      "step: 45840 train: 0.1388552188873291 elapsed, loss: 1.9813842e-06\n",
      "step: 45850 train: 0.12987136840820312 elapsed, loss: 2.7376168e-06\n",
      "step: 45860 train: 0.12300252914428711 elapsed, loss: 2.518757e-06\n",
      "step: 45870 train: 0.1311655044555664 elapsed, loss: 1.7997779e-06\n",
      "step: 45880 train: 0.12729763984680176 elapsed, loss: 2.304088e-06\n",
      "step: 45890 train: 0.1285233497619629 elapsed, loss: 3.0454098e-06\n",
      "step: 45900 train: 0.12080883979797363 elapsed, loss: 2.7175936e-06\n",
      "step: 45910 train: 0.12839388847351074 elapsed, loss: 2.2361012e-06\n",
      "step: 45920 train: 0.13010621070861816 elapsed, loss: 2.1792898e-06\n",
      "step: 45930 train: 0.132232666015625 elapsed, loss: 2.4950073e-06\n",
      "step: 45940 train: 0.12339115142822266 elapsed, loss: 2.201643e-06\n",
      "step: 45950 train: 0.12864398956298828 elapsed, loss: 2.5154918e-06\n",
      "step: 45960 train: 0.13575983047485352 elapsed, loss: 3.4034529e-06\n",
      "step: 45970 train: 0.1213228702545166 elapsed, loss: 3.1571756e-06\n",
      "step: 45980 train: 0.13388466835021973 elapsed, loss: 2.155541e-06\n",
      "step: 45990 train: 0.1238090991973877 elapsed, loss: 3.2456398e-06\n",
      "step: 46000 train: 0.12372183799743652 elapsed, loss: 3.0007109e-06\n",
      "step: 46010 train: 0.12940692901611328 elapsed, loss: 1.9706754e-06\n",
      "step: 46020 train: 0.13610219955444336 elapsed, loss: 2.3236453e-06\n",
      "step: 46030 train: 0.13781952857971191 elapsed, loss: 1.8114197e-06\n",
      "step: 46040 train: 0.13328218460083008 elapsed, loss: 2.7636913e-06\n",
      "step: 46050 train: 0.12895560264587402 elapsed, loss: 3.6870836e-06\n",
      "step: 46060 train: 0.1354379653930664 elapsed, loss: 2.0237594e-06\n",
      "step: 46070 train: 0.11556792259216309 elapsed, loss: 3.199549e-06\n",
      "step: 46080 train: 0.12505841255187988 elapsed, loss: 3.7047862e-06\n",
      "step: 46090 train: 0.12771320343017578 elapsed, loss: 1.8384277e-06\n",
      "step: 46100 train: 0.1323554515838623 elapsed, loss: 2.6779985e-06\n",
      "step: 46110 train: 0.13066935539245605 elapsed, loss: 2.2151457e-06\n",
      "step: 46120 train: 0.11375665664672852 elapsed, loss: 0.00074942136\n",
      "step: 46130 train: 0.12014198303222656 elapsed, loss: 0.00011753143\n",
      "step: 46140 train: 0.1313321590423584 elapsed, loss: 2.1476037e-05\n",
      "step: 46150 train: 0.11802411079406738 elapsed, loss: 2.3367716e-05\n",
      "step: 46160 train: 0.12791180610656738 elapsed, loss: 1.7942777e-05\n",
      "step: 46170 train: 0.13846564292907715 elapsed, loss: 1.3768657e-05\n",
      "step: 46180 train: 0.13458466529846191 elapsed, loss: 1.0081373e-05\n",
      "step: 46190 train: 0.13249659538269043 elapsed, loss: 1.0718645e-05\n",
      "step: 46200 train: 0.13452982902526855 elapsed, loss: 5.778722e-06\n",
      "step: 46210 train: 0.13541078567504883 elapsed, loss: 5.0388944e-06\n",
      "step: 46220 train: 0.13846087455749512 elapsed, loss: 4.454492e-06\n",
      "step: 46230 train: 0.13659048080444336 elapsed, loss: 3.9408746e-06\n",
      "step: 46240 train: 0.12546610832214355 elapsed, loss: 4.5602005e-06\n",
      "step: 46250 train: 0.1339092254638672 elapsed, loss: 2.560665e-06\n",
      "step: 46260 train: 0.12172269821166992 elapsed, loss: 2.8526342e-06\n",
      "step: 46270 train: 0.13431358337402344 elapsed, loss: 2.1764956e-06\n",
      "step: 46280 train: 0.12605690956115723 elapsed, loss: 2.7134015e-06\n",
      "step: 46290 train: 0.13411355018615723 elapsed, loss: 3.5948758e-06\n",
      "step: 46300 train: 0.12840580940246582 elapsed, loss: 2.5485583e-06\n",
      "step: 46310 train: 0.12668609619140625 elapsed, loss: 2.3092086e-06\n",
      "step: 46320 train: 0.13030529022216797 elapsed, loss: 2.2728877e-06\n",
      "step: 46330 train: 0.13289761543273926 elapsed, loss: 0.00033261566\n",
      "step: 46340 train: 0.13599443435668945 elapsed, loss: 0.00080269104\n",
      "step: 46350 train: 0.12688994407653809 elapsed, loss: 0.000118192926\n",
      "step: 46360 train: 0.13375115394592285 elapsed, loss: 1.4485377e-05\n",
      "step: 46370 train: 0.13919377326965332 elapsed, loss: 1.0964142e-05\n",
      "step: 46380 train: 0.13698983192443848 elapsed, loss: 1.1400414e-05\n",
      "step: 46390 train: 0.1328740119934082 elapsed, loss: 8.250323e-06\n",
      "step: 46400 train: 0.1295311450958252 elapsed, loss: 7.414641e-06\n",
      "step: 46410 train: 0.11966180801391602 elapsed, loss: 7.2120797e-06\n",
      "step: 46420 train: 0.13736510276794434 elapsed, loss: 4.3231757e-06\n",
      "step: 46430 train: 0.13443779945373535 elapsed, loss: 3.3350514e-06\n",
      "step: 46440 train: 0.13152575492858887 elapsed, loss: 2.805601e-06\n",
      "step: 46450 train: 0.12386536598205566 elapsed, loss: 4.0298028e-06\n",
      "step: 46460 train: 0.12194228172302246 elapsed, loss: 2.9597222e-06\n",
      "step: 46470 train: 0.13193058967590332 elapsed, loss: 2.558334e-06\n",
      "step: 46480 train: 0.1255481243133545 elapsed, loss: 2.1858068e-06\n",
      "step: 46490 train: 0.12466788291931152 elapsed, loss: 2.4563578e-06\n",
      "step: 46500 train: 0.12230753898620605 elapsed, loss: 2.6076955e-06\n",
      "step: 46510 train: 0.13428235054016113 elapsed, loss: 1.8579852e-06\n",
      "step: 46520 train: 0.11751794815063477 elapsed, loss: 4.8045567e-06\n",
      "step: 46530 train: 0.1312401294708252 elapsed, loss: 1.591628e-06\n",
      "step: 46540 train: 0.12466716766357422 elapsed, loss: 2.2030329e-06\n",
      "step: 46550 train: 0.13058090209960938 elapsed, loss: 2.1271355e-06\n",
      "step: 46560 train: 0.1287400722503662 elapsed, loss: 2.0493721e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 46570 train: 0.133711576461792 elapsed, loss: 1.7192186e-06\n",
      "step: 46580 train: 0.1303088665008545 elapsed, loss: 1.6954692e-06\n",
      "step: 46590 train: 0.12365889549255371 elapsed, loss: 2.4838287e-06\n",
      "step: 46600 train: 0.12871003150939941 elapsed, loss: 2.6928228e-06\n",
      "step: 46610 train: 0.126145601272583 elapsed, loss: 1.9185213e-06\n",
      "step: 46620 train: 0.13105368614196777 elapsed, loss: 4.019916e-06\n",
      "step: 46630 train: 0.1242823600769043 elapsed, loss: 2.397684e-06\n",
      "step: 46640 train: 0.13840436935424805 elapsed, loss: 1.8752136e-06\n",
      "step: 46650 train: 0.14268898963928223 elapsed, loss: 1.993957e-06\n",
      "step: 46660 train: 0.13578438758850098 elapsed, loss: 1.9236431e-06\n",
      "step: 46670 train: 0.14185118675231934 elapsed, loss: 1.311766e-06\n",
      "step: 46680 train: 0.13475751876831055 elapsed, loss: 1.7741661e-06\n",
      "step: 46690 train: 0.11506271362304688 elapsed, loss: 3.3522697e-06\n",
      "step: 46700 train: 0.12027740478515625 elapsed, loss: 2.9378452e-06\n",
      "step: 46710 train: 0.13442158699035645 elapsed, loss: 1.9050105e-06\n",
      "step: 46720 train: 0.12447381019592285 elapsed, loss: 3.0859276e-06\n",
      "step: 46730 train: 0.136016845703125 elapsed, loss: 1.8365656e-06\n",
      "step: 46740 train: 0.1325995922088623 elapsed, loss: 2.176031e-06\n",
      "step: 46750 train: 0.13784575462341309 elapsed, loss: 1.474282e-06\n",
      "step: 46760 train: 0.13745689392089844 elapsed, loss: 2.2849908e-06\n",
      "step: 46770 train: 0.12572145462036133 elapsed, loss: 3.064039e-06\n",
      "step: 46780 train: 0.12760448455810547 elapsed, loss: 1.9855754e-06\n",
      "step: 46790 train: 0.14219355583190918 elapsed, loss: 2.0786576e-06\n",
      "step: 46800 train: 0.12638592720031738 elapsed, loss: 1.9217812e-06\n",
      "step: 46810 train: 0.14051008224487305 elapsed, loss: 1.6568204e-06\n",
      "step: 46820 train: 0.13313961029052734 elapsed, loss: 2.956941e-06\n",
      "step: 46830 train: 0.13953065872192383 elapsed, loss: 2.044711e-06\n",
      "step: 46840 train: 0.12279939651489258 elapsed, loss: 2.4870924e-06\n",
      "step: 46850 train: 0.12642693519592285 elapsed, loss: 3.1157313e-06\n",
      "step: 46860 train: 0.13384270668029785 elapsed, loss: 1.9068796e-06\n",
      "step: 46870 train: 0.1324748992919922 elapsed, loss: 1.7667165e-06\n",
      "step: 46880 train: 0.12640118598937988 elapsed, loss: 2.6062987e-06\n",
      "step: 46890 train: 0.13298964500427246 elapsed, loss: 2.698965e-06\n",
      "step: 46900 train: 0.11916661262512207 elapsed, loss: 3.349026e-06\n",
      "step: 46910 train: 0.12685871124267578 elapsed, loss: 2.190467e-06\n",
      "step: 46920 train: 0.13320398330688477 elapsed, loss: 2.558338e-06\n",
      "step: 46930 train: 0.1301441192626953 elapsed, loss: 3.500828e-06\n",
      "step: 46940 train: 0.12362837791442871 elapsed, loss: 3.517594e-06\n",
      "step: 46950 train: 0.11139249801635742 elapsed, loss: 3.5841854e-06\n",
      "step: 46960 train: 0.12870335578918457 elapsed, loss: 2.7487906e-06\n",
      "step: 46970 train: 0.118316650390625 elapsed, loss: 3.4277216e-06\n",
      "step: 46980 train: 0.12373495101928711 elapsed, loss: 4.5955526e-06\n",
      "step: 46990 train: 0.13206100463867188 elapsed, loss: 2.143435e-06\n",
      "step: 47000 train: 0.14044475555419922 elapsed, loss: 3.1599145e-06\n",
      "step: 47010 train: 0.13552570343017578 elapsed, loss: 2.061478e-06\n",
      "step: 47020 train: 0.1298816204071045 elapsed, loss: 2.6696293e-06\n",
      "step: 47030 train: 0.1308901309967041 elapsed, loss: 2.3483244e-06\n",
      "step: 47040 train: 0.13106560707092285 elapsed, loss: 3.468467e-06\n",
      "step: 47050 train: 0.12206220626831055 elapsed, loss: 3.907816e-06\n",
      "step: 47060 train: 0.12182807922363281 elapsed, loss: 2.8815036e-06\n",
      "step: 47070 train: 0.13182377815246582 elapsed, loss: 2.3590346e-06\n",
      "step: 47080 train: 0.13389921188354492 elapsed, loss: 2.138313e-06\n",
      "step: 47090 train: 0.12953758239746094 elapsed, loss: 2.432142e-06\n",
      "step: 47100 train: 0.13999223709106445 elapsed, loss: 2.031211e-06\n",
      "step: 47110 train: 0.1235358715057373 elapsed, loss: 3.4328443e-06\n",
      "step: 47120 train: 0.12773728370666504 elapsed, loss: 2.3655543e-06\n",
      "step: 47130 train: 0.1241302490234375 elapsed, loss: 4.8377306e-06\n",
      "step: 47140 train: 0.1311800479888916 elapsed, loss: 2.3744014e-06\n",
      "step: 47150 train: 0.1291046142578125 elapsed, loss: 9.353568e-06\n",
      "step: 47160 train: 0.1336672306060791 elapsed, loss: 5.261687e-06\n",
      "step: 47170 train: 0.12718677520751953 elapsed, loss: 3.3904707e-06\n",
      "step: 47180 train: 0.1275923252105713 elapsed, loss: 3.5804546e-06\n",
      "step: 47190 train: 0.1337275505065918 elapsed, loss: 2.9732344e-06\n",
      "step: 47200 train: 0.11925983428955078 elapsed, loss: 3.3043123e-06\n",
      "step: 47210 train: 0.1334991455078125 elapsed, loss: 3.2838348e-06\n",
      "step: 47220 train: 0.1274549961090088 elapsed, loss: 4.071263e-06\n",
      "step: 47230 train: 0.14568614959716797 elapsed, loss: 2.1122353e-06\n",
      "step: 47240 train: 0.12659502029418945 elapsed, loss: 3.589298e-06\n",
      "step: 47250 train: 0.13400030136108398 elapsed, loss: 2.0619439e-06\n",
      "step: 47260 train: 0.1332697868347168 elapsed, loss: 4.5844163e-06\n",
      "step: 47270 train: 0.14713215827941895 elapsed, loss: 2.0083937e-06\n",
      "step: 47280 train: 0.11903023719787598 elapsed, loss: 5.344284e-06\n",
      "step: 47290 train: 0.1246337890625 elapsed, loss: 3.3965227e-06\n",
      "step: 47300 train: 0.13476204872131348 elapsed, loss: 3.0295841e-06\n",
      "step: 47310 train: 0.12313127517700195 elapsed, loss: 3.9068827e-06\n",
      "step: 47320 train: 0.1192018985748291 elapsed, loss: 3.414211e-06\n",
      "step: 47330 train: 0.1352851390838623 elapsed, loss: 2.713401e-06\n",
      "step: 47340 train: 0.12619638442993164 elapsed, loss: 3.2335338e-06\n",
      "step: 47350 train: 0.12758445739746094 elapsed, loss: 3.045883e-06\n",
      "step: 47360 train: 0.13526701927185059 elapsed, loss: 3.1678856e-06\n",
      "step: 47370 train: 0.15152883529663086 elapsed, loss: 1.9986144e-06\n",
      "step: 47380 train: 0.13178205490112305 elapsed, loss: 3.3229496e-06\n",
      "step: 47390 train: 0.12795686721801758 elapsed, loss: 2.6272483e-06\n",
      "step: 47400 train: 0.12509584426879883 elapsed, loss: 4.754311e-06\n",
      "step: 47410 train: 0.12492680549621582 elapsed, loss: 2.8889551e-06\n",
      "step: 47420 train: 0.13049602508544922 elapsed, loss: 2.6151488e-06\n",
      "step: 47430 train: 0.1272904872894287 elapsed, loss: 3.4360992e-06\n",
      "step: 47440 train: 0.13263964653015137 elapsed, loss: 2.1760304e-06\n",
      "step: 47450 train: 0.1256239414215088 elapsed, loss: 3.8789412e-06\n",
      "step: 47460 train: 0.12127065658569336 elapsed, loss: 3.9264396e-06\n",
      "step: 47470 train: 0.14069652557373047 elapsed, loss: 2.2686982e-06\n",
      "step: 47480 train: 0.13387298583984375 elapsed, loss: 2.4684655e-06\n",
      "step: 47490 train: 0.1171419620513916 elapsed, loss: 5.0072313e-06\n",
      "step: 47500 train: 0.1475224494934082 elapsed, loss: 2.1262058e-06\n",
      "step: 47510 train: 0.13927483558654785 elapsed, loss: 2.3469288e-06\n",
      "step: 47520 train: 0.13304352760314941 elapsed, loss: 2.9164248e-06\n",
      "step: 47530 train: 0.1317765712738037 elapsed, loss: 2.332955e-06\n",
      "step: 47540 train: 0.12986159324645996 elapsed, loss: 3.236802e-06\n",
      "step: 47550 train: 0.12227320671081543 elapsed, loss: 4.7780954e-06\n",
      "step: 47560 train: 0.12608003616333008 elapsed, loss: 3.0212025e-06\n",
      "step: 47570 train: 0.13004589080810547 elapsed, loss: 2.2863928e-06\n",
      "step: 47580 train: 0.13362693786621094 elapsed, loss: 2.7133995e-06\n",
      "step: 47590 train: 0.12064456939697266 elapsed, loss: 3.3955926e-06\n",
      "step: 47600 train: 0.12181568145751953 elapsed, loss: 3.265673e-06\n",
      "step: 47610 train: 0.13130831718444824 elapsed, loss: 2.4563506e-06\n",
      "step: 47620 train: 0.12168002128601074 elapsed, loss: 3.9134034e-06\n",
      "step: 47630 train: 0.13298988342285156 elapsed, loss: 2.339935e-06\n",
      "step: 47640 train: 0.1374351978302002 elapsed, loss: 2.9941427e-06\n",
      "step: 47650 train: 0.1335582733154297 elapsed, loss: 2.5690472e-06\n",
      "step: 47660 train: 0.11757802963256836 elapsed, loss: 4.1830217e-06\n",
      "step: 47670 train: 0.128678560256958 elapsed, loss: 2.685462e-06\n",
      "step: 47680 train: 0.14320063591003418 elapsed, loss: 2.2007116e-06\n",
      "step: 47690 train: 0.1405940055847168 elapsed, loss: 1.3807181e-05\n",
      "step: 47700 train: 0.12780976295471191 elapsed, loss: 4.6039722e-06\n",
      "step: 47710 train: 0.1392683982849121 elapsed, loss: 2.0302803e-06\n",
      "step: 47720 train: 0.12012553215026855 elapsed, loss: 4.9583314e-06\n",
      "step: 47730 train: 0.12849020957946777 elapsed, loss: 3.2340092e-06\n",
      "step: 47740 train: 0.13230276107788086 elapsed, loss: 2.8652066e-06\n",
      "step: 47750 train: 0.12896156311035156 elapsed, loss: 3.0873266e-06\n",
      "step: 47760 train: 0.12055468559265137 elapsed, loss: 3.6922124e-06\n",
      "step: 47770 train: 0.12801599502563477 elapsed, loss: 3.1748687e-06\n",
      "step: 47780 train: 0.12564802169799805 elapsed, loss: 2.8605484e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 47790 train: 0.14245367050170898 elapsed, loss: 2.5760323e-06\n",
      "step: 47800 train: 0.1391599178314209 elapsed, loss: 1.873016e-05\n",
      "step: 47810 train: 0.12755799293518066 elapsed, loss: 5.6647114e-06\n",
      "step: 47820 train: 0.12403297424316406 elapsed, loss: 0.00014283933\n",
      "step: 47830 train: 0.14300251007080078 elapsed, loss: 0.0031523888\n",
      "step: 47840 train: 0.12424659729003906 elapsed, loss: 2.9786755e-05\n",
      "step: 47850 train: 0.13568520545959473 elapsed, loss: 2.3425522e-05\n",
      "step: 47860 train: 0.12426877021789551 elapsed, loss: 1.4270323e-05\n",
      "step: 47870 train: 0.13436126708984375 elapsed, loss: 8.012519e-06\n",
      "step: 47880 train: 0.13073444366455078 elapsed, loss: 8.649078e-06\n",
      "step: 47890 train: 0.1426682472229004 elapsed, loss: 8.694267e-06\n",
      "step: 47900 train: 0.1323258876800537 elapsed, loss: 5.6107037e-06\n",
      "step: 47910 train: 0.1363990306854248 elapsed, loss: 5.337302e-06\n",
      "step: 47920 train: 0.1365373134613037 elapsed, loss: 4.9087985e-06\n",
      "step: 47930 train: 0.12108397483825684 elapsed, loss: 3.6340084e-06\n",
      "step: 47940 train: 0.1280663013458252 elapsed, loss: 2.8344593e-06\n",
      "step: 47950 train: 0.1266176700592041 elapsed, loss: 3.5562334e-06\n",
      "step: 47960 train: 0.127030611038208 elapsed, loss: 3.6544957e-06\n",
      "step: 47970 train: 0.12738776206970215 elapsed, loss: 2.6235302e-06\n",
      "step: 47980 train: 0.12447953224182129 elapsed, loss: 2.8949953e-06\n",
      "step: 47990 train: 0.12990736961364746 elapsed, loss: 1.6274837e-06\n",
      "step: 48000 train: 0.1296689510345459 elapsed, loss: 2.7254825e-06\n",
      "step: 48010 train: 0.13154911994934082 elapsed, loss: 1.4142119e-06\n",
      "step: 48020 train: 0.12342619895935059 elapsed, loss: 1.940873e-06\n",
      "step: 48030 train: 0.14091897010803223 elapsed, loss: 9.9021945e-06\n",
      "step: 48040 train: 0.13801980018615723 elapsed, loss: 1.7541433e-06\n",
      "step: 48050 train: 0.13187646865844727 elapsed, loss: 1.4025698e-06\n",
      "step: 48060 train: 0.12408447265625 elapsed, loss: 1.9743998e-06\n",
      "step: 48070 train: 0.12338590621948242 elapsed, loss: 2.5108404e-06\n",
      "step: 48080 train: 0.1286482810974121 elapsed, loss: 1.4682278e-06\n",
      "step: 48090 train: 0.13718080520629883 elapsed, loss: 1.9180557e-06\n",
      "step: 48100 train: 0.1414351463317871 elapsed, loss: 1.7443641e-06\n",
      "step: 48110 train: 0.13249659538269043 elapsed, loss: 1.8770779e-06\n",
      "step: 48120 train: 0.12963104248046875 elapsed, loss: 1.6847599e-06\n",
      "step: 48130 train: 0.12610721588134766 elapsed, loss: 2.4191022e-06\n",
      "step: 48140 train: 0.12315249443054199 elapsed, loss: 1.7816175e-06\n",
      "step: 48150 train: 0.1457216739654541 elapsed, loss: 1.6861572e-06\n",
      "step: 48160 train: 0.12163090705871582 elapsed, loss: 2.236567e-06\n",
      "step: 48170 train: 0.13344502449035645 elapsed, loss: 2.6421417e-06\n",
      "step: 48180 train: 0.13709330558776855 elapsed, loss: 2.6989617e-06\n",
      "step: 48190 train: 0.13008427619934082 elapsed, loss: 2.1131664e-06\n",
      "step: 48200 train: 0.12641072273254395 elapsed, loss: 2.268694e-06\n",
      "step: 48210 train: 0.11748933792114258 elapsed, loss: 2.891749e-06\n",
      "step: 48220 train: 0.12824392318725586 elapsed, loss: 2.4102578e-06\n",
      "step: 48230 train: 0.14042997360229492 elapsed, loss: 2.0246912e-06\n",
      "step: 48240 train: 0.13783979415893555 elapsed, loss: 1.912465e-06\n",
      "step: 48250 train: 0.1223142147064209 elapsed, loss: 3.9264264e-06\n",
      "step: 48260 train: 0.11942243576049805 elapsed, loss: 2.8978036e-06\n",
      "step: 48270 train: 0.13139653205871582 elapsed, loss: 1.7280665e-06\n",
      "step: 48280 train: 0.1227579116821289 elapsed, loss: 2.7860456e-06\n",
      "step: 48290 train: 0.1366419792175293 elapsed, loss: 2.4149106e-06\n",
      "step: 48300 train: 0.13207316398620605 elapsed, loss: 2.5280688e-06\n",
      "step: 48310 train: 0.14223289489746094 elapsed, loss: 1.6344688e-06\n",
      "step: 48320 train: 0.12389087677001953 elapsed, loss: 2.2379613e-06\n",
      "step: 48330 train: 0.12213921546936035 elapsed, loss: 2.8530999e-06\n",
      "step: 48340 train: 0.1266481876373291 elapsed, loss: 3.0919805e-06\n",
      "step: 48350 train: 0.11665821075439453 elapsed, loss: 4.836508e-06\n",
      "step: 48360 train: 0.12531256675720215 elapsed, loss: 2.2225975e-06\n",
      "step: 48370 train: 0.1272869110107422 elapsed, loss: 2.1415708e-06\n",
      "step: 48380 train: 0.12400579452514648 elapsed, loss: 2.9574076e-06\n",
      "step: 48390 train: 0.13180756568908691 elapsed, loss: 2.6640405e-06\n",
      "step: 48400 train: 0.12747836112976074 elapsed, loss: 2.34134e-06\n",
      "step: 48410 train: 0.12092947959899902 elapsed, loss: 3.4333116e-06\n",
      "step: 48420 train: 0.13117074966430664 elapsed, loss: 5.4225684e-06\n",
      "step: 48430 train: 0.13277554512023926 elapsed, loss: 2.7650904e-06\n",
      "step: 48440 train: 0.1293175220489502 elapsed, loss: 2.5844151e-06\n",
      "step: 48450 train: 0.12756991386413574 elapsed, loss: 2.919689e-06\n",
      "step: 48460 train: 0.12677597999572754 elapsed, loss: 2.322714e-06\n",
      "step: 48470 train: 0.1278986930847168 elapsed, loss: 2.84332e-06\n",
      "step: 48480 train: 0.1381206512451172 elapsed, loss: 3.0929127e-06\n",
      "step: 48490 train: 0.13269257545471191 elapsed, loss: 2.7832502e-06\n",
      "step: 48500 train: 0.12229037284851074 elapsed, loss: 3.7378506e-06\n",
      "step: 48510 train: 0.1240541934967041 elapsed, loss: 3.5697494e-06\n",
      "step: 48520 train: 0.12825894355773926 elapsed, loss: 3.3676533e-06\n",
      "step: 48530 train: 0.13728022575378418 elapsed, loss: 2.5629943e-06\n",
      "step: 48540 train: 0.12297725677490234 elapsed, loss: 0.00016073702\n",
      "step: 48550 train: 0.12442183494567871 elapsed, loss: 3.0353462e-05\n",
      "step: 48560 train: 0.13515830039978027 elapsed, loss: 2.0854051e-05\n",
      "step: 48570 train: 0.13634800910949707 elapsed, loss: 1.789058e-05\n",
      "step: 48580 train: 0.14604544639587402 elapsed, loss: 1.1332113e-05\n",
      "step: 48590 train: 0.12646842002868652 elapsed, loss: 1.1484338e-05\n",
      "step: 48600 train: 0.12126684188842773 elapsed, loss: 1.0157684e-05\n",
      "step: 48610 train: 0.126847505569458 elapsed, loss: 8.016721e-06\n",
      "step: 48620 train: 0.1339266300201416 elapsed, loss: 4.373468e-06\n",
      "step: 48630 train: 0.12679338455200195 elapsed, loss: 4.152752e-06\n",
      "step: 48640 train: 0.12822532653808594 elapsed, loss: 3.3224837e-06\n",
      "step: 48650 train: 0.12236237525939941 elapsed, loss: 4.87218e-06\n",
      "step: 48660 train: 0.1301281452178955 elapsed, loss: 2.4135174e-06\n",
      "step: 48670 train: 0.13178253173828125 elapsed, loss: 2.871717e-06\n",
      "step: 48680 train: 0.12689709663391113 elapsed, loss: 2.851235e-06\n",
      "step: 48690 train: 0.12389183044433594 elapsed, loss: 3.2735884e-06\n",
      "step: 48700 train: 0.12468552589416504 elapsed, loss: 2.2831334e-06\n",
      "step: 48710 train: 0.13409709930419922 elapsed, loss: 1.7690443e-06\n",
      "step: 48720 train: 0.13860559463500977 elapsed, loss: 2.8502984e-06\n",
      "step: 48730 train: 0.11586308479309082 elapsed, loss: 3.7425057e-06\n",
      "step: 48740 train: 0.13837337493896484 elapsed, loss: 2.0661244e-06\n",
      "step: 48750 train: 0.14233040809631348 elapsed, loss: 2.3930247e-06\n",
      "step: 48760 train: 0.13151001930236816 elapsed, loss: 2.623995e-06\n",
      "step: 48770 train: 0.13694000244140625 elapsed, loss: 2.5550712e-06\n",
      "step: 48780 train: 0.13776350021362305 elapsed, loss: 2.1853439e-06\n",
      "step: 48790 train: 0.12775373458862305 elapsed, loss: 2.4624046e-06\n",
      "step: 48800 train: 0.15135860443115234 elapsed, loss: 1.645179e-06\n",
      "step: 48810 train: 0.11980962753295898 elapsed, loss: 3.0347067e-06\n",
      "step: 48820 train: 0.13825607299804688 elapsed, loss: 2.0018747e-06\n",
      "step: 48830 train: 0.13980960845947266 elapsed, loss: 2.0088567e-06\n",
      "step: 48840 train: 0.1313481330871582 elapsed, loss: 2.7702035e-06\n",
      "step: 48850 train: 0.1189570426940918 elapsed, loss: 2.8642758e-06\n",
      "step: 48860 train: 0.11957526206970215 elapsed, loss: 3.154843e-06\n",
      "step: 48870 train: 0.12151360511779785 elapsed, loss: 3.1022282e-06\n",
      "step: 48880 train: 0.1230630874633789 elapsed, loss: 3.4086288e-06\n",
      "step: 48890 train: 0.11554098129272461 elapsed, loss: 3.5306343e-06\n",
      "step: 48900 train: 0.12932205200195312 elapsed, loss: 2.1639248e-06\n",
      "step: 48910 train: 0.12769746780395508 elapsed, loss: 2.7529773e-06\n",
      "step: 48920 train: 0.13509368896484375 elapsed, loss: 3.4887216e-06\n",
      "step: 48930 train: 0.1336677074432373 elapsed, loss: 2.43261e-06\n",
      "step: 48940 train: 0.1262962818145752 elapsed, loss: 3.4132863e-06\n",
      "step: 48950 train: 0.13291335105895996 elapsed, loss: 2.4693936e-06\n",
      "step: 48960 train: 0.12254977226257324 elapsed, loss: 4.090318e-06\n",
      "step: 48970 train: 0.1332988739013672 elapsed, loss: 2.1527467e-06\n",
      "step: 48980 train: 0.12393522262573242 elapsed, loss: 5.3802014e-06\n",
      "step: 48990 train: 0.12828516960144043 elapsed, loss: 2.1043202e-06\n",
      "step: 49000 train: 0.13162636756896973 elapsed, loss: 2.0926777e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 49010 train: 0.11704611778259277 elapsed, loss: 3.4915136e-06\n",
      "step: 49020 train: 0.1255042552947998 elapsed, loss: 2.3930284e-06\n",
      "step: 49030 train: 0.13324618339538574 elapsed, loss: 2.1345872e-06\n",
      "step: 49040 train: 0.12907195091247559 elapsed, loss: 3.0943006e-06\n",
      "step: 49050 train: 0.1341876983642578 elapsed, loss: 2.4554265e-06\n",
      "step: 49060 train: 0.14495229721069336 elapsed, loss: 2.4093251e-06\n",
      "step: 49070 train: 0.12720704078674316 elapsed, loss: 4.0409914e-06\n",
      "step: 49080 train: 0.1307203769683838 elapsed, loss: 1.8868562e-06\n",
      "step: 49090 train: 0.13191604614257812 elapsed, loss: 2.3343541e-06\n",
      "step: 49100 train: 0.12521147727966309 elapsed, loss: 2.6933767e-06\n",
      "step: 49110 train: 0.13291501998901367 elapsed, loss: 2.336218e-06\n",
      "step: 49120 train: 0.13008618354797363 elapsed, loss: 3.3839447e-06\n",
      "step: 49130 train: 0.12068581581115723 elapsed, loss: 5.197133e-06\n",
      "step: 49140 train: 0.13322663307189941 elapsed, loss: 2.9066402e-06\n",
      "step: 49150 train: 0.14236855506896973 elapsed, loss: 2.1108383e-06\n",
      "step: 49160 train: 0.12288212776184082 elapsed, loss: 4.045175e-06\n",
      "step: 49170 train: 0.1199193000793457 elapsed, loss: 3.0952428e-06\n",
      "step: 49180 train: 0.12244200706481934 elapsed, loss: 0.00082124234\n",
      "step: 49190 train: 0.12824726104736328 elapsed, loss: 7.376394e-05\n",
      "step: 49200 train: 0.14449644088745117 elapsed, loss: 5.7478057e-05\n",
      "step: 49210 train: 0.12758159637451172 elapsed, loss: 4.7664413e-05\n",
      "step: 49220 train: 0.1338484287261963 elapsed, loss: 2.2295671e-05\n",
      "step: 49230 train: 0.13080835342407227 elapsed, loss: 1.5924314e-05\n",
      "step: 49240 train: 0.12363457679748535 elapsed, loss: 1.3519633e-05\n",
      "step: 49250 train: 0.1374821662902832 elapsed, loss: 8.974063e-06\n",
      "step: 49260 train: 0.1379375457763672 elapsed, loss: 9.150741e-06\n",
      "step: 49270 train: 0.14063715934753418 elapsed, loss: 3.1658892e-05\n",
      "step: 49280 train: 0.12680459022521973 elapsed, loss: 1.3417639e-05\n",
      "step: 49290 train: 0.1295757293701172 elapsed, loss: 5.804607e-06\n",
      "step: 49300 train: 0.12108278274536133 elapsed, loss: 5.880276e-06\n",
      "step: 49310 train: 0.12444186210632324 elapsed, loss: 4.27938e-06\n",
      "step: 49320 train: 0.11881470680236816 elapsed, loss: 4.14763e-06\n",
      "step: 49330 train: 0.11543774604797363 elapsed, loss: 3.1408765e-06\n",
      "step: 49340 train: 0.12917041778564453 elapsed, loss: 2.318523e-06\n",
      "step: 49350 train: 0.12169218063354492 elapsed, loss: 2.1429694e-06\n",
      "step: 49360 train: 0.1270768642425537 elapsed, loss: 2.028416e-06\n",
      "step: 49370 train: 0.12880611419677734 elapsed, loss: 2.2728814e-06\n",
      "step: 49380 train: 0.13206028938293457 elapsed, loss: 1.832839e-06\n",
      "step: 49390 train: 0.12324142456054688 elapsed, loss: 2.9252737e-06\n",
      "step: 49400 train: 0.12168502807617188 elapsed, loss: 2.0498373e-06\n",
      "step: 49410 train: 0.12653803825378418 elapsed, loss: 1.6591489e-06\n",
      "step: 49420 train: 0.12176799774169922 elapsed, loss: 2.2798738e-06\n",
      "step: 49430 train: 0.13956069946289062 elapsed, loss: 1.431441e-06\n",
      "step: 49440 train: 0.13583755493164062 elapsed, loss: 1.9608951e-06\n",
      "step: 49450 train: 0.13132143020629883 elapsed, loss: 1.6381941e-06\n",
      "step: 49460 train: 0.12318611145019531 elapsed, loss: 2.5443637e-06\n",
      "step: 49470 train: 0.13327717781066895 elapsed, loss: 2.3194536e-06\n",
      "step: 49480 train: 0.13335919380187988 elapsed, loss: 2.6440166e-06\n",
      "step: 49490 train: 0.12404966354370117 elapsed, loss: 2.542505e-06\n",
      "step: 49500 train: 0.1358046531677246 elapsed, loss: 1.602804e-06\n",
      "step: 49510 train: 0.12542939186096191 elapsed, loss: 0.00012836758\n",
      "step: 49520 train: 0.13572025299072266 elapsed, loss: 6.5312242e-06\n",
      "step: 49530 train: 0.13510513305664062 elapsed, loss: 4.2406464e-06\n",
      "step: 49540 train: 0.13759946823120117 elapsed, loss: 2.5499546e-06\n",
      "step: 49550 train: 0.1271963119506836 elapsed, loss: 4.0167247e-06\n",
      "step: 49560 train: 0.1306757926940918 elapsed, loss: 2.3152638e-06\n",
      "step: 49570 train: 0.12586617469787598 elapsed, loss: 4.200245e-06\n",
      "step: 49580 train: 0.12598109245300293 elapsed, loss: 2.6505375e-06\n",
      "step: 49590 train: 0.12288093566894531 elapsed, loss: 2.4484425e-06\n",
      "step: 49600 train: 0.12306642532348633 elapsed, loss: 2.8079303e-06\n",
      "step: 49610 train: 0.13401412963867188 elapsed, loss: 1.7038491e-06\n",
      "step: 49620 train: 0.13535428047180176 elapsed, loss: 1.5143288e-06\n",
      "step: 49630 train: 0.12656641006469727 elapsed, loss: 2.6752173e-06\n",
      "step: 49640 train: 0.13857722282409668 elapsed, loss: 1.8472755e-06\n",
      "step: 49650 train: 0.13916683197021484 elapsed, loss: 1.7746318e-06\n",
      "step: 49660 train: 0.13955354690551758 elapsed, loss: 2.106639e-06\n",
      "step: 49670 train: 0.1348564624786377 elapsed, loss: 1.7303946e-06\n",
      "step: 49680 train: 0.12998533248901367 elapsed, loss: 2.024225e-06\n",
      "step: 49690 train: 0.13226985931396484 elapsed, loss: 1.5855746e-06\n",
      "step: 49700 train: 0.11521530151367188 elapsed, loss: 2.7026924e-06\n",
      "step: 49710 train: 0.12269353866577148 elapsed, loss: 3.0291071e-06\n",
      "step: 49720 train: 0.12177491188049316 elapsed, loss: 2.58814e-06\n",
      "step: 49730 train: 0.13694214820861816 elapsed, loss: 2.1294654e-06\n",
      "step: 49740 train: 0.11604428291320801 elapsed, loss: 2.7376168e-06\n",
      "step: 49750 train: 0.12901616096496582 elapsed, loss: 2.3026907e-06\n",
      "step: 49760 train: 0.13140296936035156 elapsed, loss: 1.9972172e-06\n",
      "step: 49770 train: 0.12565898895263672 elapsed, loss: 2.6547286e-06\n",
      "step: 49780 train: 0.13906025886535645 elapsed, loss: 1.7448295e-06\n",
      "step: 49790 train: 0.12220144271850586 elapsed, loss: 2.6163347e-05\n",
      "step: 49800 train: 0.1236414909362793 elapsed, loss: 4.8097954e-06\n",
      "step: 49810 train: 0.1373133659362793 elapsed, loss: 2.7283018e-06\n",
      "step: 49820 train: 0.13758349418640137 elapsed, loss: 1.8724212e-06\n",
      "step: 49830 train: 0.13039588928222656 elapsed, loss: 3.8826624e-06\n",
      "step: 49840 train: 0.13871383666992188 elapsed, loss: 2.6780094e-06\n",
      "step: 49850 train: 0.14592504501342773 elapsed, loss: 2.5685783e-06\n",
      "step: 49860 train: 0.13260293006896973 elapsed, loss: 1.7913962e-06\n",
      "step: 49870 train: 0.1151576042175293 elapsed, loss: 3.4165453e-06\n",
      "step: 49880 train: 0.1205587387084961 elapsed, loss: 3.3634587e-06\n",
      "step: 49890 train: 0.1358027458190918 elapsed, loss: 1.9818503e-06\n",
      "step: 49900 train: 0.1347675323486328 elapsed, loss: 1.997218e-06\n",
      "step: 49910 train: 0.14117836952209473 elapsed, loss: 1.7583343e-06\n",
      "step: 49920 train: 0.1226954460144043 elapsed, loss: 2.4666028e-06\n",
      "step: 49930 train: 0.13570427894592285 elapsed, loss: 2.9583384e-06\n",
      "step: 49940 train: 0.13997817039489746 elapsed, loss: 2.119221e-06\n",
      "step: 49950 train: 0.13523173332214355 elapsed, loss: 2.1662495e-06\n",
      "step: 49960 train: 0.1360030174255371 elapsed, loss: 2.5024547e-06\n",
      "step: 49970 train: 0.12956476211547852 elapsed, loss: 2.0335387e-06\n",
      "step: 49980 train: 0.12321639060974121 elapsed, loss: 3.1636923e-06\n",
      "step: 49990 train: 0.1285262107849121 elapsed, loss: 3.2116548e-06\n",
      "step: 50000 train: 0.12856483459472656 elapsed, loss: 2.2305135e-06\n",
      "step: 50010 train: 0.13027071952819824 elapsed, loss: 2.206295e-06\n",
      "step: 50020 train: 0.12540078163146973 elapsed, loss: 9.396617e-06\n",
      "step: 50030 train: 0.11668157577514648 elapsed, loss: 1.512775e-05\n",
      "step: 50040 train: 0.13101911544799805 elapsed, loss: 4.6020987e-06\n",
      "step: 50050 train: 0.1269536018371582 elapsed, loss: 3.6861557e-06\n",
      "step: 50060 train: 0.13105463981628418 elapsed, loss: 3.433307e-06\n",
      "step: 50070 train: 0.12368011474609375 elapsed, loss: 3.0975712e-06\n",
      "step: 50080 train: 0.11858963966369629 elapsed, loss: 3.0416916e-06\n",
      "step: 50090 train: 0.11826086044311523 elapsed, loss: 3.234474e-06\n",
      "step: 50100 train: 0.14618492126464844 elapsed, loss: 1.9473905e-06\n",
      "step: 50110 train: 0.1286160945892334 elapsed, loss: 2.8526329e-06\n",
      "step: 50120 train: 0.13519930839538574 elapsed, loss: 2.0582036e-06\n",
      "step: 50130 train: 0.12432360649108887 elapsed, loss: 3.3094448e-06\n",
      "step: 50140 train: 0.13396382331848145 elapsed, loss: 2.8079317e-06\n",
      "step: 50150 train: 0.13987398147583008 elapsed, loss: 2.1401752e-06\n",
      "step: 50160 train: 0.13920331001281738 elapsed, loss: 2.0461111e-06\n",
      "step: 50170 train: 0.13511991500854492 elapsed, loss: 3.0891874e-06\n",
      "step: 50180 train: 0.1381990909576416 elapsed, loss: 1.8794057e-06\n",
      "step: 50190 train: 0.12417244911193848 elapsed, loss: 2.4340063e-06\n",
      "step: 50200 train: 0.12309145927429199 elapsed, loss: 2.598384e-06\n",
      "step: 50210 train: 0.1260991096496582 elapsed, loss: 2.1890696e-06\n",
      "step: 50220 train: 0.1334211826324463 elapsed, loss: 3.4183577e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50230 train: 0.14287352561950684 elapsed, loss: 1.6372628e-06\n",
      "step: 50240 train: 0.1323375701904297 elapsed, loss: 2.7720723e-06\n",
      "step: 50250 train: 0.14154291152954102 elapsed, loss: 1.8593823e-06\n",
      "step: 50260 train: 0.14714431762695312 elapsed, loss: 1.7452962e-06\n",
      "step: 50270 train: 0.13601040840148926 elapsed, loss: 1.3613893e-05\n",
      "step: 50280 train: 0.13158178329467773 elapsed, loss: 3.940404e-06\n",
      "step: 50290 train: 0.12582135200500488 elapsed, loss: 5.7252105e-06\n",
      "step: 50300 train: 0.13348817825317383 elapsed, loss: 2.4796395e-06\n",
      "step: 50310 train: 0.1218411922454834 elapsed, loss: 3.765785e-06\n",
      "step: 50320 train: 0.1297464370727539 elapsed, loss: 2.5522843e-06\n",
      "step: 50330 train: 0.1275644302368164 elapsed, loss: 2.9629934e-06\n",
      "step: 50340 train: 0.1262376308441162 elapsed, loss: 2.9648572e-06\n",
      "step: 50350 train: 0.12560057640075684 elapsed, loss: 2.3446003e-06\n",
      "step: 50360 train: 0.13544178009033203 elapsed, loss: 1.915727e-06\n",
      "step: 50370 train: 0.12795686721801758 elapsed, loss: 2.5299291e-06\n",
      "step: 50380 train: 0.1204073429107666 elapsed, loss: 4.259351e-06\n",
      "step: 50390 train: 0.12983918190002441 elapsed, loss: 9.720672e-06\n",
      "step: 50400 train: 0.12386035919189453 elapsed, loss: 3.1343516e-06\n",
      "step: 50410 train: 0.12698006629943848 elapsed, loss: 1.9706756e-06\n",
      "step: 50420 train: 0.12014627456665039 elapsed, loss: 3.6121228e-06\n",
      "step: 50430 train: 0.13358640670776367 elapsed, loss: 2.1015253e-06\n",
      "step: 50440 train: 0.12648987770080566 elapsed, loss: 3.4598493e-06\n",
      "step: 50450 train: 0.13633346557617188 elapsed, loss: 1.973934e-06\n",
      "step: 50460 train: 0.1351151466369629 elapsed, loss: 2.9066505e-06\n",
      "step: 50470 train: 0.14276933670043945 elapsed, loss: 2.5639258e-06\n",
      "step: 50480 train: 0.13484907150268555 elapsed, loss: 2.3329585e-06\n",
      "step: 50490 train: 0.13231706619262695 elapsed, loss: 1.7723044e-06\n",
      "step: 50500 train: 0.12690114974975586 elapsed, loss: 3.6144484e-06\n",
      "step: 50510 train: 0.12048506736755371 elapsed, loss: 4.1811545e-06\n",
      "step: 50520 train: 0.13288450241088867 elapsed, loss: 3.1096752e-06\n",
      "step: 50530 train: 0.12514138221740723 elapsed, loss: 3.0626331e-06\n",
      "step: 50540 train: 0.12500953674316406 elapsed, loss: 2.844718e-06\n",
      "step: 50550 train: 0.13147330284118652 elapsed, loss: 2.1797562e-06\n",
      "step: 50560 train: 0.13241124153137207 elapsed, loss: 2.0121192e-06\n",
      "step: 50570 train: 0.14140081405639648 elapsed, loss: 2.2267868e-06\n",
      "step: 50580 train: 0.12988018989562988 elapsed, loss: 3.2479788e-06\n",
      "step: 50590 train: 0.13041448593139648 elapsed, loss: 3.0519348e-06\n",
      "step: 50600 train: 0.13081860542297363 elapsed, loss: 3.1296995e-06\n",
      "step: 50610 train: 0.13038158416748047 elapsed, loss: 2.0051339e-06\n",
      "step: 50620 train: 0.13668298721313477 elapsed, loss: 2.0810305e-06\n",
      "step: 50630 train: 0.13526487350463867 elapsed, loss: 3.3653246e-06\n",
      "step: 50640 train: 0.13271808624267578 elapsed, loss: 4.242558e-06\n",
      "step: 50650 train: 0.1354084014892578 elapsed, loss: 4.326891e-06\n",
      "step: 50660 train: 0.13285613059997559 elapsed, loss: 2.4381973e-06\n",
      "step: 50670 train: 0.14318466186523438 elapsed, loss: 1.0501224e-05\n",
      "step: 50680 train: 0.13269567489624023 elapsed, loss: 3.304786e-06\n",
      "step: 50690 train: 0.12381863594055176 elapsed, loss: 3.3522851e-06\n",
      "step: 50700 train: 0.1267094612121582 elapsed, loss: 2.1606647e-06\n",
      "step: 50710 train: 0.13018155097961426 elapsed, loss: 2.113633e-06\n",
      "step: 50720 train: 0.12021207809448242 elapsed, loss: 3.4104942e-06\n",
      "step: 50730 train: 0.12762045860290527 elapsed, loss: 7.3918213e-06\n",
      "step: 50740 train: 0.13830161094665527 elapsed, loss: 6.3757843e-06\n",
      "step: 50750 train: 0.1287989616394043 elapsed, loss: 4.3315613e-06\n",
      "step: 50760 train: 0.13176441192626953 elapsed, loss: 3.1869708e-06\n",
      "step: 50770 train: 0.13299775123596191 elapsed, loss: 3.7727762e-06\n",
      "step: 50780 train: 0.12408566474914551 elapsed, loss: 3.300132e-06\n",
      "step: 50790 train: 0.1247398853302002 elapsed, loss: 3.5832531e-06\n",
      "step: 50800 train: 0.12151002883911133 elapsed, loss: 3.8807934e-06\n",
      "step: 50810 train: 0.12819743156433105 elapsed, loss: 3.2749836e-06\n",
      "step: 50820 train: 0.12952876091003418 elapsed, loss: 2.956011e-06\n",
      "step: 50830 train: 0.13109540939331055 elapsed, loss: 2.6542627e-06\n",
      "step: 50840 train: 0.12135863304138184 elapsed, loss: 2.8549628e-06\n",
      "step: 50850 train: 0.1410844326019287 elapsed, loss: 2.5643856e-06\n",
      "step: 50860 train: 0.1205606460571289 elapsed, loss: 3.7313293e-06\n",
      "step: 50870 train: 0.13478374481201172 elapsed, loss: 1.939476e-06\n",
      "step: 50880 train: 0.13011479377746582 elapsed, loss: 2.6612488e-06\n",
      "step: 50890 train: 0.1289525032043457 elapsed, loss: 1.9613615e-06\n",
      "step: 50900 train: 0.12011957168579102 elapsed, loss: 3.0109572e-06\n",
      "step: 50910 train: 0.12795400619506836 elapsed, loss: 2.7255014e-06\n",
      "step: 50920 train: 0.12980031967163086 elapsed, loss: 2.3492566e-06\n",
      "step: 50930 train: 0.12741303443908691 elapsed, loss: 2.4372662e-06\n",
      "step: 50940 train: 0.13400959968566895 elapsed, loss: 2.8391296e-06\n",
      "step: 50950 train: 0.1384894847869873 elapsed, loss: 2.8544946e-06\n",
      "step: 50960 train: 0.1286618709564209 elapsed, loss: 3.4528675e-06\n",
      "step: 50970 train: 0.1268174648284912 elapsed, loss: 3.099434e-06\n",
      "step: 50980 train: 0.13324522972106934 elapsed, loss: 2.079639e-06\n",
      "step: 50990 train: 0.11935257911682129 elapsed, loss: 6.937802e-06\n",
      "step: 51000 train: 0.13556814193725586 elapsed, loss: 2.573704e-06\n",
      "step: 51010 train: 0.14317893981933594 elapsed, loss: 2.1243422e-06\n",
      "step: 51020 train: 0.1364274024963379 elapsed, loss: 3.1380832e-06\n",
      "step: 51030 train: 0.13520026206970215 elapsed, loss: 2.3464609e-06\n",
      "step: 51040 train: 0.12283778190612793 elapsed, loss: 3.979513e-06\n",
      "step: 51050 train: 0.1349937915802002 elapsed, loss: 2.8088616e-06\n",
      "step: 51060 train: 0.1328446865081787 elapsed, loss: 2.9383086e-06\n",
      "step: 51070 train: 0.12303805351257324 elapsed, loss: 3.8551934e-06\n",
      "step: 51080 train: 0.13132596015930176 elapsed, loss: 2.23843e-06\n",
      "step: 51090 train: 0.1365041732788086 elapsed, loss: 2.2025727e-06\n",
      "step: 51100 train: 0.13148283958435059 elapsed, loss: 2.2817358e-06\n",
      "step: 51110 train: 0.12552523612976074 elapsed, loss: 3.789039e-06\n",
      "step: 51120 train: 0.1277165412902832 elapsed, loss: 3.4407574e-06\n",
      "step: 51130 train: 0.13473057746887207 elapsed, loss: 6.059582e-06\n",
      "step: 51140 train: 0.13947677612304688 elapsed, loss: 2.932261e-06\n",
      "step: 51150 train: 0.1373276710510254 elapsed, loss: 3.135755e-06\n",
      "step: 51160 train: 0.13637137413024902 elapsed, loss: 3.0305068e-06\n",
      "step: 51170 train: 0.14091110229492188 elapsed, loss: 2.4326084e-06\n",
      "step: 51180 train: 0.13166236877441406 elapsed, loss: 4.3868185e-06\n",
      "step: 51190 train: 0.13988304138183594 elapsed, loss: 2.635637e-06\n",
      "step: 51200 train: 0.12601304054260254 elapsed, loss: 2.8400602e-06\n",
      "step: 51210 train: 0.1262950897216797 elapsed, loss: 3.4649743e-06\n",
      "step: 51220 train: 0.11883687973022461 elapsed, loss: 3.5511184e-06\n",
      "step: 51230 train: 0.1357440948486328 elapsed, loss: 2.414447e-06\n",
      "step: 51240 train: 0.12947964668273926 elapsed, loss: 2.495939e-06\n",
      "step: 51250 train: 0.13372445106506348 elapsed, loss: 4.1252697e-06\n",
      "step: 51260 train: 0.13394474983215332 elapsed, loss: 2.1974513e-06\n",
      "step: 51270 train: 0.12340617179870605 elapsed, loss: 3.5781297e-06\n",
      "step: 51280 train: 0.13004326820373535 elapsed, loss: 8.145271e-06\n",
      "step: 51290 train: 0.11844515800476074 elapsed, loss: 4.6486703e-06\n",
      "step: 51300 train: 0.13030004501342773 elapsed, loss: 2.6547227e-06\n",
      "step: 51310 train: 0.13783478736877441 elapsed, loss: 2.207693e-06\n",
      "step: 51320 train: 0.12781429290771484 elapsed, loss: 3.1632285e-06\n",
      "step: 51330 train: 0.1464095115661621 elapsed, loss: 2.6333066e-06\n",
      "step: 51340 train: 0.13442444801330566 elapsed, loss: 3.3755632e-06\n",
      "step: 51350 train: 0.13183045387268066 elapsed, loss: 2.7157296e-06\n",
      "step: 51360 train: 0.14328265190124512 elapsed, loss: 2.2072306e-06\n",
      "step: 51370 train: 0.13182950019836426 elapsed, loss: 2.8712611e-06\n",
      "step: 51380 train: 0.13708281517028809 elapsed, loss: 2.8773e-06\n",
      "step: 51390 train: 0.1309816837310791 elapsed, loss: 2.5196869e-06\n",
      "step: 51400 train: 0.11965012550354004 elapsed, loss: 0.00016208051\n",
      "step: 51410 train: 0.1309342384338379 elapsed, loss: 8.070758e-06\n",
      "step: 51420 train: 0.12600469589233398 elapsed, loss: 9.321824e-06\n",
      "step: 51430 train: 0.12397241592407227 elapsed, loss: 5.124105e-06\n",
      "step: 51440 train: 0.12006092071533203 elapsed, loss: 7.665764e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 51450 train: 0.12662100791931152 elapsed, loss: 5.05657e-06\n",
      "step: 51460 train: 0.1320335865020752 elapsed, loss: 3.2596024e-06\n",
      "step: 51470 train: 0.12659978866577148 elapsed, loss: 3.7229477e-06\n",
      "step: 51480 train: 0.13138556480407715 elapsed, loss: 2.4144488e-06\n",
      "step: 51490 train: 0.1323089599609375 elapsed, loss: 3.7168666e-06\n",
      "step: 51500 train: 0.1359398365020752 elapsed, loss: 3.1408367e-06\n",
      "step: 51510 train: 0.12715625762939453 elapsed, loss: 2.9057114e-06\n",
      "step: 51520 train: 0.12400507926940918 elapsed, loss: 2.299431e-06\n",
      "step: 51530 train: 0.1258554458618164 elapsed, loss: 2.5727657e-06\n",
      "step: 51540 train: 0.11884498596191406 elapsed, loss: 3.0165468e-06\n",
      "step: 51550 train: 0.13642239570617676 elapsed, loss: 2.420501e-06\n",
      "step: 51560 train: 0.12329339981079102 elapsed, loss: 2.8428544e-06\n",
      "step: 51570 train: 0.1291651725769043 elapsed, loss: 2.2076958e-06\n",
      "step: 51580 train: 0.12943363189697266 elapsed, loss: 2.031677e-06\n",
      "step: 51590 train: 0.12264370918273926 elapsed, loss: 2.971843e-06\n",
      "step: 51600 train: 0.12646174430847168 elapsed, loss: 2.4205026e-06\n",
      "step: 51610 train: 0.12447309494018555 elapsed, loss: 2.6081636e-06\n",
      "step: 51620 train: 0.12935328483581543 elapsed, loss: 5.092435e-06\n",
      "step: 51630 train: 0.12842130661010742 elapsed, loss: 4.7184913e-06\n",
      "step: 51640 train: 0.1214442253112793 elapsed, loss: 3.3364533e-06\n",
      "step: 51650 train: 0.12880587577819824 elapsed, loss: 2.7334247e-06\n",
      "step: 51660 train: 0.13582587242126465 elapsed, loss: 1.8430832e-06\n",
      "step: 51670 train: 0.12812280654907227 elapsed, loss: 2.3068778e-06\n",
      "step: 51680 train: 0.1291491985321045 elapsed, loss: 2.0633415e-06\n",
      "step: 51690 train: 0.1313314437866211 elapsed, loss: 2.5182894e-06\n",
      "step: 51700 train: 0.12338733673095703 elapsed, loss: 3.166009e-06\n",
      "step: 51710 train: 0.12163782119750977 elapsed, loss: 2.832146e-06\n",
      "step: 51720 train: 0.1300215721130371 elapsed, loss: 2.9173532e-06\n",
      "step: 51730 train: 0.12400007247924805 elapsed, loss: 2.2761483e-06\n",
      "step: 51740 train: 0.12853288650512695 elapsed, loss: 1.9753322e-06\n",
      "step: 51750 train: 0.13413214683532715 elapsed, loss: 1.9664847e-06\n",
      "step: 51760 train: 0.12473392486572266 elapsed, loss: 3.4817385e-06\n",
      "step: 51770 train: 0.12807869911193848 elapsed, loss: 6.1676437e-06\n",
      "step: 51780 train: 0.12716436386108398 elapsed, loss: 3.9366655e-06\n",
      "step: 51790 train: 0.13462495803833008 elapsed, loss: 2.6919802e-06\n",
      "step: 51800 train: 0.1186525821685791 elapsed, loss: 4.4804046e-06\n",
      "step: 51810 train: 0.12547612190246582 elapsed, loss: 2.4838323e-06\n",
      "step: 51820 train: 0.12339043617248535 elapsed, loss: 4.2648944e-06\n",
      "step: 51830 train: 0.12488627433776855 elapsed, loss: 3.173936e-06\n",
      "step: 51840 train: 0.12490129470825195 elapsed, loss: 3.6083957e-06\n",
      "step: 51850 train: 0.12116193771362305 elapsed, loss: 2.7897693e-06\n",
      "step: 51860 train: 0.12008857727050781 elapsed, loss: 3.730369e-06\n",
      "step: 51870 train: 0.13264799118041992 elapsed, loss: 2.5965192e-06\n",
      "step: 51880 train: 0.1280660629272461 elapsed, loss: 0.0003010477\n",
      "step: 51890 train: 0.12821435928344727 elapsed, loss: 0.012116056\n",
      "step: 51900 train: 0.13106679916381836 elapsed, loss: 2.6762858e-05\n",
      "step: 51910 train: 0.13898801803588867 elapsed, loss: 8.797674e-05\n",
      "step: 51920 train: 0.12893199920654297 elapsed, loss: 1.5362108e-05\n",
      "step: 51930 train: 0.12662792205810547 elapsed, loss: 1.1315777e-05\n",
      "step: 51940 train: 0.13184738159179688 elapsed, loss: 9.29864e-06\n",
      "step: 51950 train: 0.13555073738098145 elapsed, loss: 7.2309713e-06\n",
      "step: 51960 train: 0.12631940841674805 elapsed, loss: 5.013284e-06\n",
      "step: 51970 train: 0.12165594100952148 elapsed, loss: 5.6432264e-06\n",
      "step: 51980 train: 0.14408278465270996 elapsed, loss: 4.15925e-06\n",
      "step: 51990 train: 0.13190841674804688 elapsed, loss: 3.54134e-06\n",
      "step: 52000 train: 0.13172698020935059 elapsed, loss: 2.4936103e-06\n",
      "step: 52010 train: 0.13147640228271484 elapsed, loss: 2.1788226e-06\n",
      "step: 52020 train: 0.1338043212890625 elapsed, loss: 1.8463442e-06\n",
      "step: 52030 train: 0.1228184700012207 elapsed, loss: 2.20723e-06\n",
      "step: 52040 train: 0.12365603446960449 elapsed, loss: 2.6160797e-06\n",
      "step: 52050 train: 0.12694859504699707 elapsed, loss: 3.3513454e-06\n",
      "step: 52060 train: 0.12529325485229492 elapsed, loss: 1.7001272e-06\n",
      "step: 52070 train: 0.13225531578063965 elapsed, loss: 1.8849912e-06\n",
      "step: 52080 train: 0.12644243240356445 elapsed, loss: 2.3650891e-06\n",
      "step: 52090 train: 0.13782048225402832 elapsed, loss: 1.7685793e-06\n",
      "step: 52100 train: 0.1331191062927246 elapsed, loss: 2.705948e-06\n",
      "step: 52110 train: 0.1316518783569336 elapsed, loss: 2.5476265e-06\n",
      "step: 52120 train: 0.11754631996154785 elapsed, loss: 2.5341237e-06\n",
      "step: 52130 train: 0.12032222747802734 elapsed, loss: 2.5397107e-06\n",
      "step: 52140 train: 0.12772679328918457 elapsed, loss: 4.95879e-06\n",
      "step: 52150 train: 0.12383222579956055 elapsed, loss: 3.0444855e-06\n",
      "step: 52160 train: 0.12552642822265625 elapsed, loss: 2.416311e-06\n",
      "step: 52170 train: 0.13565802574157715 elapsed, loss: 1.5497188e-06\n",
      "step: 52180 train: 0.12800002098083496 elapsed, loss: 2.034471e-06\n",
      "step: 52190 train: 0.1318056583404541 elapsed, loss: 1.9483234e-06\n",
      "step: 52200 train: 0.13019657135009766 elapsed, loss: 1.9962827e-06\n",
      "step: 52210 train: 0.1286015510559082 elapsed, loss: 2.8451832e-06\n",
      "step: 52220 train: 0.12842440605163574 elapsed, loss: 3.7606233e-06\n",
      "step: 52230 train: 0.14263153076171875 elapsed, loss: 2.5881338e-06\n",
      "step: 52240 train: 0.12438750267028809 elapsed, loss: 2.648676e-06\n",
      "step: 52250 train: 0.12572836875915527 elapsed, loss: 2.6402922e-06\n",
      "step: 52260 train: 0.12849688529968262 elapsed, loss: 2.064273e-06\n",
      "step: 52270 train: 0.1380162239074707 elapsed, loss: 1.3587982e-06\n",
      "step: 52280 train: 0.1341385841369629 elapsed, loss: 2.8507693e-06\n",
      "step: 52290 train: 0.12219500541687012 elapsed, loss: 3.174869e-06\n",
      "step: 52300 train: 0.1352376937866211 elapsed, loss: 2.7017595e-06\n",
      "step: 52310 train: 0.13499832153320312 elapsed, loss: 2.4670685e-06\n",
      "step: 52320 train: 0.138655424118042 elapsed, loss: 1.8561226e-06\n",
      "step: 52330 train: 0.13074445724487305 elapsed, loss: 2.108974e-06\n",
      "step: 52340 train: 0.13151955604553223 elapsed, loss: 1.7168908e-06\n",
      "step: 52350 train: 0.1286933422088623 elapsed, loss: 3.3574092e-06\n",
      "step: 52360 train: 0.13208532333374023 elapsed, loss: 2.8181748e-06\n",
      "step: 52370 train: 0.13579607009887695 elapsed, loss: 1.9967515e-06\n",
      "step: 52380 train: 0.13268685340881348 elapsed, loss: 2.9010562e-06\n",
      "step: 52390 train: 0.1349353790283203 elapsed, loss: 1.7988465e-06\n",
      "step: 52400 train: 0.13691997528076172 elapsed, loss: 1.89291e-06\n",
      "step: 52410 train: 0.11717605590820312 elapsed, loss: 3.4598493e-06\n",
      "step: 52420 train: 0.13024616241455078 elapsed, loss: 2.115495e-06\n",
      "step: 52430 train: 0.13309121131896973 elapsed, loss: 2.1904657e-06\n",
      "step: 52440 train: 0.13852190971374512 elapsed, loss: 1.9045494e-06\n",
      "step: 52450 train: 0.12907671928405762 elapsed, loss: 3.088258e-06\n",
      "step: 52460 train: 0.13544273376464844 elapsed, loss: 2.6095609e-06\n",
      "step: 52470 train: 0.13167905807495117 elapsed, loss: 2.1732349e-06\n",
      "step: 52480 train: 0.12784194946289062 elapsed, loss: 2.9294652e-06\n",
      "step: 52490 train: 0.13217759132385254 elapsed, loss: 1.8579858e-06\n",
      "step: 52500 train: 0.1343991756439209 elapsed, loss: 1.9231784e-06\n",
      "step: 52510 train: 0.13436079025268555 elapsed, loss: 2.2184058e-06\n",
      "step: 52520 train: 0.12751054763793945 elapsed, loss: 2.87079e-06\n",
      "step: 52530 train: 0.1254715919494629 elapsed, loss: 2.7147958e-06\n",
      "step: 52540 train: 0.1281294822692871 elapsed, loss: 3.7606649e-06\n",
      "step: 52550 train: 0.12940549850463867 elapsed, loss: 3.3727308e-06\n",
      "step: 52560 train: 0.13224005699157715 elapsed, loss: 1.9920963e-06\n",
      "step: 52570 train: 0.13445496559143066 elapsed, loss: 2.74367e-06\n",
      "step: 52580 train: 0.13698267936706543 elapsed, loss: 2.284064e-06\n",
      "step: 52590 train: 0.13128280639648438 elapsed, loss: 2.667768e-06\n",
      "step: 52600 train: 0.12772631645202637 elapsed, loss: 3.1818531e-06\n",
      "step: 52610 train: 0.12741327285766602 elapsed, loss: 3.4640432e-06\n",
      "step: 52620 train: 0.12840700149536133 elapsed, loss: 0.011944897\n",
      "step: 52630 train: 0.12853455543518066 elapsed, loss: 7.159542e-05\n",
      "step: 52640 train: 0.12694096565246582 elapsed, loss: 2.8867235e-05\n",
      "step: 52650 train: 0.11984062194824219 elapsed, loss: 2.5897822e-05\n",
      "step: 52660 train: 0.12928557395935059 elapsed, loss: 2.182989e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 52670 train: 0.12479305267333984 elapsed, loss: 3.9236234e-05\n",
      "step: 52680 train: 0.1287992000579834 elapsed, loss: 1.485949e-05\n",
      "step: 52690 train: 0.1259458065032959 elapsed, loss: 1.45499325e-05\n",
      "step: 52700 train: 0.12859201431274414 elapsed, loss: 1.0127653e-05\n",
      "step: 52710 train: 0.13999581336975098 elapsed, loss: 5.447233e-06\n",
      "step: 52720 train: 0.13851428031921387 elapsed, loss: 6.275646e-06\n",
      "step: 52730 train: 0.13596439361572266 elapsed, loss: 5.3587746e-06\n",
      "step: 52740 train: 0.12711405754089355 elapsed, loss: 6.5032254e-06\n",
      "step: 52750 train: 0.12947583198547363 elapsed, loss: 5.095235e-06\n",
      "step: 52760 train: 0.14265871047973633 elapsed, loss: 2.57417e-06\n",
      "step: 52770 train: 0.12281298637390137 elapsed, loss: 4.7310486e-06\n",
      "step: 52780 train: 0.12378597259521484 elapsed, loss: 2.8232819e-06\n",
      "step: 52790 train: 0.12978243827819824 elapsed, loss: 2.3506532e-06\n",
      "step: 52800 train: 0.12932968139648438 elapsed, loss: 3.02027e-06\n",
      "step: 52810 train: 0.14228105545043945 elapsed, loss: 4.018144e-05\n",
      "step: 52820 train: 0.13051104545593262 elapsed, loss: 2.1019741e-05\n",
      "step: 52830 train: 0.12256288528442383 elapsed, loss: 1.0643744e-05\n",
      "step: 52840 train: 0.12113070487976074 elapsed, loss: 7.2586613e-06\n",
      "step: 52850 train: 0.13502001762390137 elapsed, loss: 4.6211912e-06\n",
      "step: 52860 train: 0.13648056983947754 elapsed, loss: 3.034698e-06\n",
      "step: 52870 train: 0.136505126953125 elapsed, loss: 2.7920887e-06\n",
      "step: 52880 train: 0.13599681854248047 elapsed, loss: 3.807206e-06\n",
      "step: 52890 train: 0.14171504974365234 elapsed, loss: 2.6365683e-06\n",
      "step: 52900 train: 0.13057637214660645 elapsed, loss: 2.7222468e-06\n",
      "step: 52910 train: 0.14502596855163574 elapsed, loss: 1.5492527e-06\n",
      "step: 52920 train: 0.12325024604797363 elapsed, loss: 1.9962863e-06\n",
      "step: 52930 train: 0.12102365493774414 elapsed, loss: 2.275217e-06\n",
      "step: 52940 train: 0.1366901397705078 elapsed, loss: 1.6912794e-06\n",
      "step: 52950 train: 0.12647676467895508 elapsed, loss: 2.2738177e-06\n",
      "step: 52960 train: 0.1349477767944336 elapsed, loss: 1.7834802e-06\n",
      "step: 52970 train: 0.12324738502502441 elapsed, loss: 5.5222204e-06\n",
      "step: 52980 train: 0.1313002109527588 elapsed, loss: 2.25193e-06\n",
      "step: 52990 train: 0.14024066925048828 elapsed, loss: 1.4658997e-06\n",
      "step: 53000 train: 0.13206148147583008 elapsed, loss: 1.6959352e-06\n",
      "step: 53010 train: 0.13779687881469727 elapsed, loss: 1.5264357e-06\n",
      "step: 53020 train: 0.1422572135925293 elapsed, loss: 1.5133973e-06\n",
      "step: 53030 train: 0.13834691047668457 elapsed, loss: 1.4416853e-06\n",
      "step: 53040 train: 0.1298370361328125 elapsed, loss: 1.679172e-06\n",
      "step: 53050 train: 0.13173389434814453 elapsed, loss: 1.6912794e-06\n",
      "step: 53060 train: 0.13668227195739746 elapsed, loss: 1.7452926e-06\n",
      "step: 53070 train: 0.13195013999938965 elapsed, loss: 1.8170078e-06\n",
      "step: 53080 train: 0.1306629180908203 elapsed, loss: 1.722013e-06\n",
      "step: 53090 train: 0.12843966484069824 elapsed, loss: 2.3227126e-06\n",
      "step: 53100 train: 0.12521696090698242 elapsed, loss: 2.5192219e-06\n",
      "step: 53110 train: 0.14058303833007812 elapsed, loss: 1.5362145e-06\n",
      "step: 53120 train: 0.1293325424194336 elapsed, loss: 1.7010575e-06\n",
      "step: 53130 train: 0.12769246101379395 elapsed, loss: 2.0656698e-06\n",
      "step: 53140 train: 0.12188053131103516 elapsed, loss: 3.2596163e-06\n",
      "step: 53150 train: 0.13154149055480957 elapsed, loss: 1.5636886e-06\n",
      "step: 53160 train: 0.12906122207641602 elapsed, loss: 2.250536e-06\n",
      "step: 53170 train: 0.12747550010681152 elapsed, loss: 2.243552e-06\n",
      "step: 53180 train: 0.1319289207458496 elapsed, loss: 3.3320564e-06\n",
      "step: 53190 train: 0.13393306732177734 elapsed, loss: 2.5653203e-06\n",
      "step: 53200 train: 0.12208318710327148 elapsed, loss: 8.520632e-06\n",
      "step: 53210 train: 0.12444281578063965 elapsed, loss: 3.3965148e-06\n",
      "step: 53220 train: 0.1178286075592041 elapsed, loss: 3.873357e-06\n",
      "step: 53230 train: 0.12260127067565918 elapsed, loss: 2.6803405e-06\n",
      "step: 53240 train: 0.12357640266418457 elapsed, loss: 2.2798736e-06\n",
      "step: 53250 train: 0.13899683952331543 elapsed, loss: 1.5501844e-06\n",
      "step: 53260 train: 0.1221156120300293 elapsed, loss: 3.1394798e-06\n",
      "step: 53270 train: 0.15041303634643555 elapsed, loss: 1.953446e-06\n",
      "step: 53280 train: 0.1405200958251953 elapsed, loss: 2.0763803e-06\n",
      "step: 53290 train: 0.12647438049316406 elapsed, loss: 3.2237524e-06\n",
      "step: 53300 train: 0.14197587966918945 elapsed, loss: 2.3664857e-06\n",
      "step: 53310 train: 0.12537884712219238 elapsed, loss: 3.3774295e-06\n",
      "step: 53320 train: 0.1290111541748047 elapsed, loss: 1.9883705e-06\n",
      "step: 53330 train: 0.13138222694396973 elapsed, loss: 1.7052487e-06\n",
      "step: 53340 train: 0.13536381721496582 elapsed, loss: 2.0647365e-06\n",
      "step: 53350 train: 0.13820862770080566 elapsed, loss: 1.7131651e-06\n",
      "step: 53360 train: 0.13573932647705078 elapsed, loss: 2.100594e-06\n",
      "step: 53370 train: 0.12711167335510254 elapsed, loss: 2.7636906e-06\n",
      "step: 53380 train: 0.12809276580810547 elapsed, loss: 0.029812668\n",
      "step: 53390 train: 0.13191461563110352 elapsed, loss: 3.361706e-05\n",
      "step: 53400 train: 0.12849211692810059 elapsed, loss: 2.4759007e-05\n",
      "step: 53410 train: 0.12701010704040527 elapsed, loss: 2.4635523e-05\n",
      "step: 53420 train: 0.12387657165527344 elapsed, loss: 1.9717801e-05\n",
      "step: 53430 train: 0.12578296661376953 elapsed, loss: 1.2848021e-05\n",
      "step: 53440 train: 0.12158608436584473 elapsed, loss: 1.0316122e-05\n",
      "step: 53450 train: 0.1313009262084961 elapsed, loss: 6.59674e-06\n",
      "step: 53460 train: 0.14866948127746582 elapsed, loss: 4.9848604e-06\n",
      "step: 53470 train: 0.12105536460876465 elapsed, loss: 7.475655e-06\n",
      "step: 53480 train: 0.1400620937347412 elapsed, loss: 3.7010545e-06\n",
      "step: 53490 train: 0.11662769317626953 elapsed, loss: 7.1742415e-06\n",
      "step: 53500 train: 0.13330793380737305 elapsed, loss: 1.1995269e-05\n",
      "step: 53510 train: 0.1300954818725586 elapsed, loss: 2.7581052e-06\n",
      "step: 53520 train: 0.12629961967468262 elapsed, loss: 2.7283024e-06\n",
      "step: 53530 train: 0.1161189079284668 elapsed, loss: 3.1390146e-06\n",
      "step: 53540 train: 0.12280583381652832 elapsed, loss: 2.5615977e-06\n",
      "step: 53550 train: 0.13435006141662598 elapsed, loss: 1.627018e-06\n",
      "step: 53560 train: 0.1253345012664795 elapsed, loss: 2.7338892e-06\n",
      "step: 53570 train: 0.12562298774719238 elapsed, loss: 2.1741687e-06\n",
      "step: 53580 train: 0.11934685707092285 elapsed, loss: 2.4908168e-06\n",
      "step: 53590 train: 0.126129150390625 elapsed, loss: 2.7194553e-06\n",
      "step: 53600 train: 0.12804055213928223 elapsed, loss: 3.5515884e-06\n",
      "step: 53610 train: 0.13160943984985352 elapsed, loss: 2.8926697e-06\n",
      "step: 53620 train: 0.12555646896362305 elapsed, loss: 2.946694e-06\n",
      "step: 53630 train: 0.12085223197937012 elapsed, loss: 2.6835978e-06\n",
      "step: 53640 train: 0.13138222694396973 elapsed, loss: 3.415613e-06\n",
      "step: 53650 train: 0.1284170150756836 elapsed, loss: 2.6854627e-06\n",
      "step: 53660 train: 0.1255345344543457 elapsed, loss: 3.167414e-06\n",
      "step: 53670 train: 0.13866710662841797 elapsed, loss: 1.9483218e-06\n",
      "step: 53680 train: 0.1318047046661377 elapsed, loss: 1.9930246e-06\n",
      "step: 53690 train: 0.1186058521270752 elapsed, loss: 2.7208532e-06\n",
      "step: 53700 train: 0.1285555362701416 elapsed, loss: 2.8354052e-06\n",
      "step: 53710 train: 0.1422717571258545 elapsed, loss: 0.00012678283\n",
      "step: 53720 train: 0.13862133026123047 elapsed, loss: 2.440564e-05\n",
      "step: 53730 train: 0.14641880989074707 elapsed, loss: 1.1132831e-05\n",
      "step: 53740 train: 0.13045954704284668 elapsed, loss: 1.4771289e-05\n",
      "step: 53750 train: 0.12848162651062012 elapsed, loss: 1.0836426e-05\n",
      "step: 53760 train: 0.1272902488708496 elapsed, loss: 6.5075646e-06\n",
      "step: 53770 train: 0.12495899200439453 elapsed, loss: 6.8242143e-06\n",
      "step: 53780 train: 0.12589335441589355 elapsed, loss: 4.047978e-06\n",
      "step: 53790 train: 0.12566661834716797 elapsed, loss: 4.0465757e-06\n",
      "step: 53800 train: 0.12732934951782227 elapsed, loss: 4.173699e-06\n",
      "step: 53810 train: 0.11732196807861328 elapsed, loss: 4.249605e-06\n",
      "step: 53820 train: 0.12785840034484863 elapsed, loss: 3.1390096e-06\n",
      "step: 53830 train: 0.12552642822265625 elapsed, loss: 3.4947766e-06\n",
      "step: 53840 train: 0.13602662086486816 elapsed, loss: 1.9604286e-06\n",
      "step: 53850 train: 0.12818431854248047 elapsed, loss: 2.1844132e-06\n",
      "step: 53860 train: 0.12571048736572266 elapsed, loss: 2.1569394e-06\n",
      "step: 53870 train: 0.1273179054260254 elapsed, loss: 2.00234e-06\n",
      "step: 53880 train: 0.12541651725769043 elapsed, loss: 2.4167757e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 53890 train: 0.12287092208862305 elapsed, loss: 2.1215492e-06\n",
      "step: 53900 train: 0.12806463241577148 elapsed, loss: 2.0209638e-06\n",
      "step: 53910 train: 0.13150525093078613 elapsed, loss: 2.0614784e-06\n",
      "step: 53920 train: 0.1448681354522705 elapsed, loss: 1.9483082e-06\n",
      "step: 53930 train: 0.13903450965881348 elapsed, loss: 1.527833e-06\n",
      "step: 53940 train: 0.13073110580444336 elapsed, loss: 2.4004735e-06\n",
      "step: 53950 train: 0.1287519931793213 elapsed, loss: 2.5625118e-06\n",
      "step: 53960 train: 0.14248132705688477 elapsed, loss: 2.8554248e-06\n",
      "step: 53970 train: 0.1340632438659668 elapsed, loss: 2.4158462e-06\n",
      "step: 53980 train: 0.13817071914672852 elapsed, loss: 1.6102543e-06\n",
      "step: 53990 train: 0.11938762664794922 elapsed, loss: 3.2060648e-06\n",
      "step: 54000 train: 0.1389598846435547 elapsed, loss: 1.6540249e-06\n",
      "step: 54010 train: 0.12298393249511719 elapsed, loss: 3.094311e-06\n",
      "step: 54020 train: 0.14238429069519043 elapsed, loss: 2.0302787e-06\n",
      "step: 54030 train: 0.1332089900970459 elapsed, loss: 3.632538e-06\n",
      "step: 54040 train: 0.127394437789917 elapsed, loss: 2.4000142e-06\n",
      "step: 54050 train: 0.1272728443145752 elapsed, loss: 2.1057172e-06\n",
      "step: 54060 train: 0.1226644515991211 elapsed, loss: 2.8996653e-06\n",
      "step: 54070 train: 0.1308307647705078 elapsed, loss: 2.414913e-06\n",
      "step: 54080 train: 0.13913249969482422 elapsed, loss: 1.9310942e-06\n",
      "step: 54090 train: 0.12551379203796387 elapsed, loss: 2.7483268e-06\n",
      "step: 54100 train: 0.12942743301391602 elapsed, loss: 2.3199195e-06\n",
      "step: 54110 train: 0.12430906295776367 elapsed, loss: 3.1674194e-06\n",
      "step: 54120 train: 0.13237929344177246 elapsed, loss: 2.759035e-06\n",
      "step: 54130 train: 0.12371444702148438 elapsed, loss: 3.0952265e-06\n",
      "step: 54140 train: 0.12386965751647949 elapsed, loss: 2.5676509e-06\n",
      "step: 54150 train: 0.12386465072631836 elapsed, loss: 2.6766156e-06\n",
      "step: 54160 train: 0.12506914138793945 elapsed, loss: 2.6216662e-06\n",
      "step: 54170 train: 0.11980628967285156 elapsed, loss: 3.5008286e-06\n",
      "step: 54180 train: 0.13649702072143555 elapsed, loss: 1.9837141e-06\n",
      "step: 54190 train: 0.1200258731842041 elapsed, loss: 3.286163e-06\n",
      "step: 54200 train: 0.13128161430358887 elapsed, loss: 2.8176223e-06\n",
      "step: 54210 train: 0.12816143035888672 elapsed, loss: 2.693843e-06\n",
      "step: 54220 train: 0.13513875007629395 elapsed, loss: 2.547627e-06\n",
      "step: 54230 train: 0.11787009239196777 elapsed, loss: 3.1371524e-06\n",
      "step: 54240 train: 0.12973260879516602 elapsed, loss: 2.5173595e-06\n",
      "step: 54250 train: 0.1270911693572998 elapsed, loss: 2.442389e-06\n",
      "step: 54260 train: 0.1218712329864502 elapsed, loss: 2.875918e-06\n",
      "step: 54270 train: 0.12375259399414062 elapsed, loss: 3.6586885e-06\n",
      "step: 54280 train: 0.13602113723754883 elapsed, loss: 2.4218969e-06\n",
      "step: 54290 train: 0.13297581672668457 elapsed, loss: 2.2090933e-06\n",
      "step: 54300 train: 0.12591814994812012 elapsed, loss: 2.2146814e-06\n",
      "step: 54310 train: 0.1266014575958252 elapsed, loss: 2.2184047e-06\n",
      "step: 54320 train: 0.13161849975585938 elapsed, loss: 2.3920932e-06\n",
      "step: 54330 train: 0.1278376579284668 elapsed, loss: 1.2388774e-05\n",
      "step: 54340 train: 0.14187860488891602 elapsed, loss: 3.4561244e-06\n",
      "step: 54350 train: 0.13388705253601074 elapsed, loss: 2.8954712e-06\n",
      "step: 54360 train: 0.12668108940124512 elapsed, loss: 2.723647e-06\n",
      "step: 54370 train: 0.1278531551361084 elapsed, loss: 2.3031566e-06\n",
      "step: 54380 train: 0.13373923301696777 elapsed, loss: 3.0509773e-06\n",
      "step: 54390 train: 0.12471652030944824 elapsed, loss: 7.92597e-06\n",
      "step: 54400 train: 0.14145684242248535 elapsed, loss: 2.3087432e-06\n",
      "step: 54410 train: 0.13103461265563965 elapsed, loss: 2.255658e-06\n",
      "step: 54420 train: 0.13176536560058594 elapsed, loss: 2.2076965e-06\n",
      "step: 54430 train: 0.12961626052856445 elapsed, loss: 2.2719578e-06\n",
      "step: 54440 train: 0.1289379596710205 elapsed, loss: 3.896623e-06\n",
      "step: 54450 train: 0.12154459953308105 elapsed, loss: 3.4305176e-06\n",
      "step: 54460 train: 0.13753271102905273 elapsed, loss: 2.5196823e-06\n",
      "step: 54470 train: 0.12376117706298828 elapsed, loss: 2.7934962e-06\n",
      "step: 54480 train: 0.1312272548675537 elapsed, loss: 2.5462289e-06\n",
      "step: 54490 train: 0.13347482681274414 elapsed, loss: 1.9813856e-06\n",
      "step: 54500 train: 0.1188971996307373 elapsed, loss: 3.4435525e-06\n",
      "step: 54510 train: 0.12276458740234375 elapsed, loss: 2.9578737e-06\n",
      "step: 54520 train: 0.13276100158691406 elapsed, loss: 3.6624135e-06\n",
      "step: 54530 train: 0.13354730606079102 elapsed, loss: 2.1741673e-06\n",
      "step: 54540 train: 0.1262345314025879 elapsed, loss: 2.7953563e-06\n",
      "step: 54550 train: 0.12664246559143066 elapsed, loss: 4.3380473e-06\n",
      "step: 54560 train: 0.12979841232299805 elapsed, loss: 2.3203852e-06\n",
      "step: 54570 train: 0.12734508514404297 elapsed, loss: 2.994649e-06\n",
      "step: 54580 train: 0.1217949390411377 elapsed, loss: 3.70991e-06\n",
      "step: 54590 train: 0.12411618232727051 elapsed, loss: 3.1376157e-06\n",
      "step: 54600 train: 0.12359166145324707 elapsed, loss: 3.2223675e-06\n",
      "step: 54610 train: 0.1332411766052246 elapsed, loss: 2.4875578e-06\n",
      "step: 54620 train: 0.12361717224121094 elapsed, loss: 3.7699615e-06\n",
      "step: 54630 train: 0.11953115463256836 elapsed, loss: 4.615434e-06\n",
      "step: 54640 train: 0.14800524711608887 elapsed, loss: 2.162993e-06\n",
      "step: 54650 train: 0.13345098495483398 elapsed, loss: 2.6696284e-06\n",
      "step: 54660 train: 0.13659977912902832 elapsed, loss: 2.771608e-06\n",
      "step: 54670 train: 0.13060688972473145 elapsed, loss: 3.2959406e-06\n",
      "step: 54680 train: 0.13666105270385742 elapsed, loss: 2.6072314e-06\n",
      "step: 54690 train: 0.12656164169311523 elapsed, loss: 3.8756843e-06\n",
      "step: 54700 train: 0.13179588317871094 elapsed, loss: 2.5741706e-06\n",
      "step: 54710 train: 0.12391018867492676 elapsed, loss: 3.4603195e-06\n",
      "step: 54720 train: 0.1412045955657959 elapsed, loss: 2.2211966e-06\n",
      "step: 54730 train: 0.13141918182373047 elapsed, loss: 2.6607825e-06\n",
      "step: 54740 train: 0.12615466117858887 elapsed, loss: 3.3345923e-06\n",
      "step: 54750 train: 0.13584494590759277 elapsed, loss: 2.206299e-06\n",
      "step: 54760 train: 0.12765979766845703 elapsed, loss: 2.257988e-06\n",
      "step: 54770 train: 0.11675333976745605 elapsed, loss: 4.8866195e-06\n",
      "step: 54780 train: 0.13670969009399414 elapsed, loss: 3.1781199e-06\n",
      "step: 54790 train: 0.12224721908569336 elapsed, loss: 3.05799e-06\n",
      "step: 54800 train: 0.13588881492614746 elapsed, loss: 1.966485e-06\n",
      "step: 54810 train: 0.1313309669494629 elapsed, loss: 3.450536e-06\n",
      "step: 54820 train: 0.1293630599975586 elapsed, loss: 4.0493665e-06\n",
      "step: 54830 train: 0.11605167388916016 elapsed, loss: 3.8239823e-06\n",
      "step: 54840 train: 0.1230473518371582 elapsed, loss: 7.5332105e-06\n",
      "step: 54850 train: 0.13305354118347168 elapsed, loss: 2.566719e-06\n",
      "step: 54860 train: 0.12786507606506348 elapsed, loss: 3.411421e-06\n",
      "step: 54870 train: 0.12404561042785645 elapsed, loss: 2.9904656e-06\n",
      "step: 54880 train: 0.13104724884033203 elapsed, loss: 2.8340014e-06\n",
      "step: 54890 train: 0.12118744850158691 elapsed, loss: 3.4300424e-06\n",
      "step: 54900 train: 0.13153600692749023 elapsed, loss: 9.472101e-06\n",
      "step: 54910 train: 0.14516162872314453 elapsed, loss: 2.5848801e-06\n",
      "step: 54920 train: 0.1411299705505371 elapsed, loss: 3.228886e-06\n",
      "step: 54930 train: 0.12090611457824707 elapsed, loss: 6.6141747e-06\n",
      "step: 54940 train: 0.11980414390563965 elapsed, loss: 3.9213187e-06\n",
      "step: 54950 train: 0.13884377479553223 elapsed, loss: 4.308263e-06\n",
      "step: 54960 train: 0.12712526321411133 elapsed, loss: 3.5855805e-06\n",
      "step: 54970 train: 0.12825226783752441 elapsed, loss: 2.4605474e-06\n",
      "step: 54980 train: 0.12780165672302246 elapsed, loss: 3.1082793e-06\n",
      "step: 54990 train: 0.15115594863891602 elapsed, loss: 2.106181e-06\n",
      "step: 55000 train: 0.1219778060913086 elapsed, loss: 3.0025776e-06\n",
      "step: 55010 train: 0.12699317932128906 elapsed, loss: 2.2384302e-06\n",
      "step: 55020 train: 0.12076473236083984 elapsed, loss: 3.2624139e-06\n",
      "step: 55030 train: 0.12870478630065918 elapsed, loss: 2.328768e-06\n",
      "step: 55040 train: 0.12632989883422852 elapsed, loss: 3.760658e-06\n",
      "step: 55050 train: 0.13236641883850098 elapsed, loss: 2.1913957e-06\n",
      "step: 55060 train: 0.12733840942382812 elapsed, loss: 3.97394e-06\n",
      "step: 55070 train: 0.13801908493041992 elapsed, loss: 2.5546078e-06\n",
      "step: 55080 train: 0.1297924518585205 elapsed, loss: 1.8510008e-06\n",
      "step: 55090 train: 0.1327824592590332 elapsed, loss: 1.9767292e-06\n",
      "step: 55100 train: 0.12697076797485352 elapsed, loss: 2.5988506e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 55110 train: 0.11782312393188477 elapsed, loss: 3.963696e-06\n",
      "step: 55120 train: 0.12298798561096191 elapsed, loss: 3.6251467e-06\n",
      "step: 55130 train: 0.12406444549560547 elapsed, loss: 2.8978038e-06\n",
      "step: 55140 train: 0.1355454921722412 elapsed, loss: 2.0484406e-06\n",
      "step: 55150 train: 0.14862966537475586 elapsed, loss: 1.7667162e-06\n",
      "step: 55160 train: 0.13352680206298828 elapsed, loss: 1.3487528e-05\n",
      "step: 55170 train: 0.1183633804321289 elapsed, loss: 6.911291e-06\n",
      "step: 55180 train: 0.13112735748291016 elapsed, loss: 2.9443677e-06\n",
      "step: 55190 train: 0.12663912773132324 elapsed, loss: 3.3485508e-06\n",
      "step: 55200 train: 0.1205284595489502 elapsed, loss: 3.5613643e-06\n",
      "step: 55210 train: 0.12929749488830566 elapsed, loss: 2.4135172e-06\n",
      "step: 55220 train: 0.13760995864868164 elapsed, loss: 1.876146e-06\n",
      "step: 55230 train: 0.13050150871276855 elapsed, loss: 2.056822e-06\n",
      "step: 55240 train: 0.12374329566955566 elapsed, loss: 3.6917459e-06\n",
      "step: 55250 train: 0.13487911224365234 elapsed, loss: 8.206677e-06\n",
      "step: 55260 train: 0.1220855712890625 elapsed, loss: 3.7448185e-06\n",
      "step: 55270 train: 0.12337470054626465 elapsed, loss: 4.7059534e-06\n",
      "step: 55280 train: 0.13151335716247559 elapsed, loss: 2.748792e-06\n",
      "step: 55290 train: 0.13331198692321777 elapsed, loss: 3.0049048e-06\n",
      "step: 55300 train: 0.13940930366516113 elapsed, loss: 2.5900001e-06\n",
      "step: 55310 train: 0.14206933975219727 elapsed, loss: 2.495939e-06\n",
      "step: 55320 train: 0.12592077255249023 elapsed, loss: 2.5792924e-06\n",
      "step: 55330 train: 0.12116456031799316 elapsed, loss: 3.9147862e-06\n",
      "step: 55340 train: 0.13097739219665527 elapsed, loss: 3.6149165e-06\n",
      "step: 55350 train: 0.1392345428466797 elapsed, loss: 2.6212024e-06\n",
      "step: 55360 train: 0.12037968635559082 elapsed, loss: 3.8510043e-06\n",
      "step: 55370 train: 0.13261938095092773 elapsed, loss: 2.6523999e-06\n",
      "step: 55380 train: 0.1283574104309082 elapsed, loss: 2.1439e-06\n",
      "step: 55390 train: 0.12856078147888184 elapsed, loss: 2.4856927e-06\n",
      "step: 55400 train: 0.12711429595947266 elapsed, loss: 3.2326102e-06\n",
      "step: 55410 train: 0.14646220207214355 elapsed, loss: 2.95973e-06\n",
      "step: 55420 train: 0.12215352058410645 elapsed, loss: 4.939231e-06\n",
      "step: 55430 train: 0.1227877140045166 elapsed, loss: 3.1269083e-06\n",
      "step: 55440 train: 0.1282792091369629 elapsed, loss: 2.7939618e-06\n",
      "step: 55450 train: 0.11904788017272949 elapsed, loss: 4.1257435e-06\n",
      "step: 55460 train: 0.12874221801757812 elapsed, loss: 2.8130537e-06\n",
      "step: 55470 train: 0.14104962348937988 elapsed, loss: 2.3115367e-06\n",
      "step: 55480 train: 0.12311768531799316 elapsed, loss: 3.2717278e-06\n",
      "step: 55490 train: 0.1251692771911621 elapsed, loss: 3.2889575e-06\n",
      "step: 55500 train: 0.1374502182006836 elapsed, loss: 3.7527413e-06\n",
      "step: 55510 train: 0.13086342811584473 elapsed, loss: 2.740876e-06\n",
      "step: 55520 train: 0.12364077568054199 elapsed, loss: 3.1380832e-06\n",
      "step: 55530 train: 0.12954139709472656 elapsed, loss: 2.4414576e-06\n",
      "step: 55540 train: 0.12488222122192383 elapsed, loss: 3.1981328e-06\n",
      "step: 55550 train: 0.12418389320373535 elapsed, loss: 4.4316785e-06\n",
      "step: 55560 train: 0.13287615776062012 elapsed, loss: 2.6128205e-06\n",
      "step: 55570 train: 0.12330055236816406 elapsed, loss: 3.2940743e-06\n",
      "step: 55580 train: 0.12444949150085449 elapsed, loss: 5.5520313e-06\n",
      "step: 55590 train: 0.12912702560424805 elapsed, loss: 2.821431e-06\n",
      "step: 55600 train: 0.12533903121948242 elapsed, loss: 3.5594926e-06\n",
      "step: 55610 train: 0.12584280967712402 elapsed, loss: 3.0049043e-06\n",
      "step: 55620 train: 0.12437033653259277 elapsed, loss: 3.3778974e-06\n",
      "step: 55630 train: 0.12927556037902832 elapsed, loss: 3.822135e-06\n",
      "step: 55640 train: 0.12460494041442871 elapsed, loss: 4.321271e-06\n",
      "step: 55650 train: 0.12802982330322266 elapsed, loss: 2.587674e-06\n",
      "step: 55660 train: 0.13808822631835938 elapsed, loss: 2.2784764e-06\n",
      "step: 55670 train: 0.1337578296661377 elapsed, loss: 2.1094422e-06\n",
      "step: 55680 train: 0.1261608600616455 elapsed, loss: 4.9116843e-06\n",
      "step: 55690 train: 0.1287844181060791 elapsed, loss: 3.4542652e-06\n",
      "step: 55700 train: 0.12343382835388184 elapsed, loss: 4.1131625e-06\n",
      "step: 55710 train: 0.12352538108825684 elapsed, loss: 9.795969e-06\n",
      "step: 55720 train: 0.13208985328674316 elapsed, loss: 3.3126776e-06\n",
      "step: 55730 train: 0.12420916557312012 elapsed, loss: 5.6374565e-06\n",
      "step: 55740 train: 0.13206863403320312 elapsed, loss: 2.7427157e-06\n",
      "step: 55750 train: 0.12831521034240723 elapsed, loss: 0.027836736\n",
      "step: 55760 train: 0.12922263145446777 elapsed, loss: 0.00015197908\n",
      "step: 55770 train: 0.1288926601409912 elapsed, loss: 3.0973282e-05\n",
      "step: 55780 train: 0.1357419490814209 elapsed, loss: 1.5951524e-05\n",
      "step: 55790 train: 0.12417054176330566 elapsed, loss: 1.8899389e-05\n",
      "step: 55800 train: 0.13408231735229492 elapsed, loss: 9.918429e-06\n",
      "step: 55810 train: 0.12772679328918457 elapsed, loss: 1.1499669e-05\n",
      "step: 55820 train: 0.13103818893432617 elapsed, loss: 7.749454e-06\n",
      "step: 55830 train: 0.13867759704589844 elapsed, loss: 1.3685346e-05\n",
      "step: 55840 train: 0.1327989101409912 elapsed, loss: 7.996507e-06\n",
      "step: 55850 train: 0.14011359214782715 elapsed, loss: 4.755733e-06\n",
      "step: 55860 train: 0.1289224624633789 elapsed, loss: 6.157804e-06\n",
      "step: 55870 train: 0.13011980056762695 elapsed, loss: 3.410026e-06\n",
      "step: 55880 train: 0.1282823085784912 elapsed, loss: 3.4435525e-06\n",
      "step: 55890 train: 0.13235116004943848 elapsed, loss: 3.83284e-06\n",
      "step: 55900 train: 0.12629270553588867 elapsed, loss: 2.9373828e-06\n",
      "step: 55910 train: 0.13000845909118652 elapsed, loss: 1.9399413e-06\n",
      "step: 55920 train: 0.13002347946166992 elapsed, loss: 2.1876726e-06\n",
      "step: 55930 train: 0.1321392059326172 elapsed, loss: 1.5478563e-06\n",
      "step: 55940 train: 0.12771129608154297 elapsed, loss: 1.8826656e-06\n",
      "step: 55950 train: 0.1278374195098877 elapsed, loss: 2.1299306e-06\n",
      "step: 55960 train: 0.12376141548156738 elapsed, loss: 2.5988493e-06\n",
      "step: 55970 train: 0.1218569278717041 elapsed, loss: 2.1662524e-06\n",
      "step: 55980 train: 0.12009167671203613 elapsed, loss: 2.1741662e-06\n",
      "step: 55990 train: 0.13442230224609375 elapsed, loss: 1.8682251e-06\n",
      "step: 56000 train: 0.13251829147338867 elapsed, loss: 1.5986129e-06\n",
      "step: 56010 train: 0.1229095458984375 elapsed, loss: 2.4903516e-06\n",
      "step: 56020 train: 0.12049102783203125 elapsed, loss: 2.8088612e-06\n",
      "step: 56030 train: 0.14276814460754395 elapsed, loss: 1.6437818e-06\n",
      "step: 56040 train: 0.11667251586914062 elapsed, loss: 2.676149e-06\n",
      "step: 56050 train: 0.1344316005706787 elapsed, loss: 1.5711391e-06\n",
      "step: 56060 train: 0.12260007858276367 elapsed, loss: 2.9355162e-06\n",
      "step: 56070 train: 0.12264871597290039 elapsed, loss: 2.3213165e-06\n",
      "step: 56080 train: 0.1249687671661377 elapsed, loss: 2.3501868e-06\n",
      "step: 56090 train: 0.1298067569732666 elapsed, loss: 2.2053678e-06\n",
      "step: 56100 train: 0.12636280059814453 elapsed, loss: 2.6659027e-06\n",
      "step: 56110 train: 0.12386679649353027 elapsed, loss: 3.1231818e-06\n",
      "step: 56120 train: 0.14341998100280762 elapsed, loss: 2.6156138e-06\n",
      "step: 56130 train: 0.14146018028259277 elapsed, loss: 1.94879e-06\n",
      "step: 56140 train: 0.1300806999206543 elapsed, loss: 3.3867432e-06\n",
      "step: 56150 train: 0.13500261306762695 elapsed, loss: 2.355775e-06\n",
      "step: 56160 train: 0.12841153144836426 elapsed, loss: 2.2551937e-06\n",
      "step: 56170 train: 0.1287856101989746 elapsed, loss: 2.0931423e-06\n",
      "step: 56180 train: 0.12564492225646973 elapsed, loss: 2.4470442e-06\n",
      "step: 56190 train: 0.12990617752075195 elapsed, loss: 3.2237454e-06\n",
      "step: 56200 train: 0.12678837776184082 elapsed, loss: 2.6267896e-06\n",
      "step: 56210 train: 0.1266162395477295 elapsed, loss: 3.1813806e-06\n",
      "step: 56220 train: 0.13439178466796875 elapsed, loss: 1.9241065e-06\n",
      "step: 56230 train: 0.14367055892944336 elapsed, loss: 1.8007097e-06\n",
      "step: 56240 train: 0.12276530265808105 elapsed, loss: 3.2335438e-06\n",
      "step: 56250 train: 0.13171100616455078 elapsed, loss: 2.5583383e-06\n",
      "step: 56260 train: 0.13645219802856445 elapsed, loss: 2.4530973e-06\n",
      "step: 56270 train: 0.13215017318725586 elapsed, loss: 2.3664866e-06\n",
      "step: 56280 train: 0.13994121551513672 elapsed, loss: 2.024224e-06\n",
      "step: 56290 train: 0.12190485000610352 elapsed, loss: 3.0514689e-06\n",
      "step: 56300 train: 0.13430333137512207 elapsed, loss: 2.1452925e-06\n",
      "step: 56310 train: 0.1289517879486084 elapsed, loss: 2.6584548e-06\n",
      "step: 56320 train: 0.12429475784301758 elapsed, loss: 2.5201539e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 56330 train: 0.12418794631958008 elapsed, loss: 2.6114221e-06\n",
      "step: 56340 train: 0.1291663646697998 elapsed, loss: 3.257756e-06\n",
      "step: 56350 train: 0.13346219062805176 elapsed, loss: 2.0437838e-06\n",
      "step: 56360 train: 0.12778496742248535 elapsed, loss: 2.5951247e-06\n",
      "step: 56370 train: 0.12881183624267578 elapsed, loss: 2.322714e-06\n",
      "step: 56380 train: 0.11592721939086914 elapsed, loss: 3.2661387e-06\n",
      "step: 56390 train: 0.13084983825683594 elapsed, loss: 2.8745208e-06\n",
      "step: 56400 train: 0.14052844047546387 elapsed, loss: 1.7075779e-06\n",
      "step: 56410 train: 0.12361478805541992 elapsed, loss: 3.1455338e-06\n",
      "step: 56420 train: 0.13207674026489258 elapsed, loss: 2.3874327e-06\n",
      "step: 56430 train: 0.13232922554016113 elapsed, loss: 2.1234118e-06\n",
      "step: 56440 train: 0.12672042846679688 elapsed, loss: 2.8819713e-06\n",
      "step: 56450 train: 0.13375163078308105 elapsed, loss: 2.4153774e-06\n",
      "step: 56460 train: 0.12947559356689453 elapsed, loss: 2.5019904e-06\n",
      "step: 56470 train: 0.11647367477416992 elapsed, loss: 5.373693e-06\n",
      "step: 56480 train: 0.13751649856567383 elapsed, loss: 2.4544956e-06\n",
      "step: 56490 train: 0.12751126289367676 elapsed, loss: 3.6009455e-06\n",
      "step: 56500 train: 0.12181949615478516 elapsed, loss: 3.8938415e-06\n",
      "step: 56510 train: 0.11620712280273438 elapsed, loss: 6.751084e-06\n",
      "step: 56520 train: 0.11606884002685547 elapsed, loss: 5.235389e-06\n",
      "step: 56530 train: 0.12276864051818848 elapsed, loss: 3.0938463e-06\n",
      "step: 56540 train: 0.13105201721191406 elapsed, loss: 2.4903486e-06\n",
      "step: 56550 train: 0.13075494766235352 elapsed, loss: 2.4251565e-06\n",
      "step: 56560 train: 0.12160897254943848 elapsed, loss: 2.9876746e-06\n",
      "step: 56570 train: 0.13272500038146973 elapsed, loss: 2.4912788e-06\n",
      "step: 56580 train: 0.13385534286499023 elapsed, loss: 2.1317933e-06\n",
      "step: 56590 train: 0.12928438186645508 elapsed, loss: 3.22467e-06\n",
      "step: 56600 train: 0.11970353126525879 elapsed, loss: 3.3751041e-06\n",
      "step: 56610 train: 0.13273119926452637 elapsed, loss: 3.5771952e-06\n",
      "step: 56620 train: 0.13546133041381836 elapsed, loss: 2.1839473e-06\n",
      "step: 56630 train: 0.12938618659973145 elapsed, loss: 3.3234141e-06\n",
      "step: 56640 train: 0.1382758617401123 elapsed, loss: 1.7601965e-06\n",
      "step: 56650 train: 0.12290787696838379 elapsed, loss: 3.7117757e-06\n",
      "step: 56660 train: 0.12968826293945312 elapsed, loss: 2.0940756e-06\n",
      "step: 56670 train: 0.12358665466308594 elapsed, loss: 2.8158474e-06\n",
      "step: 56680 train: 0.12889552116394043 elapsed, loss: 2.5764987e-06\n",
      "step: 56690 train: 0.11723613739013672 elapsed, loss: 3.980459e-06\n",
      "step: 56700 train: 0.1262972354888916 elapsed, loss: 2.8102593e-06\n",
      "step: 56710 train: 0.13096117973327637 elapsed, loss: 2.6416915e-06\n",
      "step: 56720 train: 0.1185598373413086 elapsed, loss: 3.5585715e-06\n",
      "step: 56730 train: 0.1323537826538086 elapsed, loss: 2.7785784e-06\n",
      "step: 56740 train: 0.12857317924499512 elapsed, loss: 3.0943024e-06\n",
      "step: 56750 train: 0.12983274459838867 elapsed, loss: 3.3718436e-06\n",
      "step: 56760 train: 0.12536311149597168 elapsed, loss: 4.924792e-06\n",
      "step: 56770 train: 0.12177824974060059 elapsed, loss: 3.4728926e-06\n",
      "step: 56780 train: 0.1244511604309082 elapsed, loss: 3.4412249e-06\n",
      "step: 56790 train: 0.13138294219970703 elapsed, loss: 2.8423901e-06\n",
      "step: 56800 train: 0.13463425636291504 elapsed, loss: 2.7813862e-06\n",
      "step: 56810 train: 0.12539005279541016 elapsed, loss: 3.7364505e-06\n",
      "step: 56820 train: 0.1212465763092041 elapsed, loss: 3.822565e-06\n",
      "step: 56830 train: 0.11801934242248535 elapsed, loss: 4.134114e-06\n",
      "step: 56840 train: 0.13176870346069336 elapsed, loss: 2.5611248e-06\n",
      "step: 56850 train: 0.1263408660888672 elapsed, loss: 2.9290018e-06\n",
      "step: 56860 train: 0.1328294277191162 elapsed, loss: 3.2354055e-06\n",
      "step: 56870 train: 0.12862229347229004 elapsed, loss: 3.854645e-06\n",
      "step: 56880 train: 0.12057352066040039 elapsed, loss: 3.6582198e-06\n",
      "step: 56890 train: 0.13193559646606445 elapsed, loss: 2.3762655e-06\n",
      "step: 56900 train: 0.13844513893127441 elapsed, loss: 2.5294662e-06\n",
      "step: 56910 train: 0.13256001472473145 elapsed, loss: 2.4661372e-06\n",
      "step: 56920 train: 0.1254720687866211 elapsed, loss: 2.9406456e-06\n",
      "step: 56930 train: 0.1335906982421875 elapsed, loss: 2.15368e-06\n",
      "step: 56940 train: 0.1287851333618164 elapsed, loss: 4.233311e-06\n",
      "step: 56950 train: 0.1492612361907959 elapsed, loss: 4.0093037e-06\n",
      "step: 56960 train: 0.1387805938720703 elapsed, loss: 2.8638078e-06\n",
      "step: 56970 train: 0.139190673828125 elapsed, loss: 3.1664879e-06\n",
      "step: 56980 train: 0.14362525939941406 elapsed, loss: 2.3646217e-06\n",
      "step: 56990 train: 0.12670516967773438 elapsed, loss: 4.972865e-06\n",
      "step: 57000 train: 0.12875723838806152 elapsed, loss: 2.5643913e-06\n",
      "step: 57010 train: 0.12359476089477539 elapsed, loss: 3.5529868e-06\n",
      "step: 57020 train: 0.12318849563598633 elapsed, loss: 2.7031563e-06\n",
      "step: 57030 train: 0.11814522743225098 elapsed, loss: 3.5823225e-06\n",
      "step: 57040 train: 0.13628673553466797 elapsed, loss: 3.0621213e-06\n",
      "step: 57050 train: 0.13650012016296387 elapsed, loss: 2.992796e-06\n",
      "step: 57060 train: 0.12949609756469727 elapsed, loss: 2.89594e-06\n",
      "step: 57070 train: 0.1256852149963379 elapsed, loss: 3.9259658e-06\n",
      "step: 57080 train: 0.1371605396270752 elapsed, loss: 2.901059e-06\n",
      "step: 57090 train: 0.1174325942993164 elapsed, loss: 3.2051394e-06\n",
      "step: 57100 train: 0.11873912811279297 elapsed, loss: 3.2782464e-06\n",
      "step: 57110 train: 0.12447810173034668 elapsed, loss: 3.482205e-06\n",
      "step: 57120 train: 0.1282958984375 elapsed, loss: 3.0314254e-06\n",
      "step: 57130 train: 0.12752413749694824 elapsed, loss: 3.3597357e-06\n",
      "step: 57140 train: 0.12416791915893555 elapsed, loss: 3.495708e-06\n",
      "step: 57150 train: 0.12822580337524414 elapsed, loss: 3.057059e-06\n",
      "step: 57160 train: 0.13081693649291992 elapsed, loss: 0.03847813\n",
      "step: 57170 train: 0.12499356269836426 elapsed, loss: 7.0384776e-05\n",
      "step: 57180 train: 0.12302732467651367 elapsed, loss: 0.0045572673\n",
      "step: 57190 train: 0.128159761428833 elapsed, loss: 0.00033089548\n",
      "step: 57200 train: 0.12848496437072754 elapsed, loss: 6.0942904e-05\n",
      "step: 57210 train: 0.12440872192382812 elapsed, loss: 4.1816413e-05\n",
      "step: 57220 train: 0.13208603858947754 elapsed, loss: 3.2770462e-05\n",
      "step: 57230 train: 0.1265733242034912 elapsed, loss: 3.708317e-05\n",
      "step: 57240 train: 0.13203191757202148 elapsed, loss: 1.5312346e-05\n",
      "step: 57250 train: 0.12062191963195801 elapsed, loss: 2.9864676e-05\n",
      "step: 57260 train: 0.13337302207946777 elapsed, loss: 7.4807913e-06\n",
      "step: 57270 train: 0.13611626625061035 elapsed, loss: 9.163174e-06\n",
      "step: 57280 train: 0.13519072532653809 elapsed, loss: 4.553202e-06\n",
      "step: 57290 train: 0.1234138011932373 elapsed, loss: 6.757478e-06\n",
      "step: 57300 train: 0.1346590518951416 elapsed, loss: 5.6483814e-06\n",
      "step: 57310 train: 0.12529468536376953 elapsed, loss: 4.6090777e-06\n",
      "step: 57320 train: 0.1358504295349121 elapsed, loss: 2.1257376e-06\n",
      "step: 57330 train: 0.12790632247924805 elapsed, loss: 3.3192216e-06\n",
      "step: 57340 train: 0.13294076919555664 elapsed, loss: 3.254954e-06\n",
      "step: 57350 train: 0.1258082389831543 elapsed, loss: 2.0661346e-06\n",
      "step: 57360 train: 0.13225507736206055 elapsed, loss: 2.2044196e-06\n",
      "step: 57370 train: 0.12372088432312012 elapsed, loss: 2.1569394e-06\n",
      "step: 57380 train: 0.1307535171508789 elapsed, loss: 1.5571693e-06\n",
      "step: 57390 train: 0.12360024452209473 elapsed, loss: 2.2314427e-06\n",
      "step: 57400 train: 0.11797666549682617 elapsed, loss: 2.2891863e-06\n",
      "step: 57410 train: 0.13202285766601562 elapsed, loss: 1.5352836e-06\n",
      "step: 57420 train: 0.13167166709899902 elapsed, loss: 2.2770776e-06\n",
      "step: 57430 train: 0.12994098663330078 elapsed, loss: 4.480993e-06\n",
      "step: 57440 train: 0.1251382827758789 elapsed, loss: 2.1397097e-06\n",
      "step: 57450 train: 0.12870454788208008 elapsed, loss: 2.5867375e-06\n",
      "step: 57460 train: 0.1307523250579834 elapsed, loss: 1.9483234e-06\n",
      "step: 57470 train: 0.12937068939208984 elapsed, loss: 2.1629903e-06\n",
      "step: 57480 train: 0.1345534324645996 elapsed, loss: 2.4386609e-06\n",
      "step: 57490 train: 0.14882588386535645 elapsed, loss: 1.880337e-06\n",
      "step: 57500 train: 0.133042573928833 elapsed, loss: 1.8733523e-06\n",
      "step: 57510 train: 0.13495373725891113 elapsed, loss: 1.6908134e-06\n",
      "step: 57520 train: 0.12180185317993164 elapsed, loss: 2.567649e-06\n",
      "step: 57530 train: 0.1170966625213623 elapsed, loss: 2.8693948e-06\n",
      "step: 57540 train: 0.12042450904846191 elapsed, loss: 2.5983845e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 57550 train: 0.13470125198364258 elapsed, loss: 2.177425e-06\n",
      "step: 57560 train: 0.12920856475830078 elapsed, loss: 1.950186e-06\n",
      "step: 57570 train: 0.13165807723999023 elapsed, loss: 1.7094346e-06\n",
      "step: 57580 train: 0.12007522583007812 elapsed, loss: 3.3466947e-06\n",
      "step: 57590 train: 0.1192159652709961 elapsed, loss: 3.3611004e-06\n",
      "step: 57600 train: 0.12601900100708008 elapsed, loss: 5.5361825e-06\n",
      "step: 57610 train: 0.12907147407531738 elapsed, loss: 3.126432e-06\n",
      "step: 57620 train: 0.1288471221923828 elapsed, loss: 2.0870905e-06\n",
      "step: 57630 train: 0.11418747901916504 elapsed, loss: 3.1818547e-06\n",
      "step: 57640 train: 0.13652539253234863 elapsed, loss: 2.399544e-06\n",
      "step: 57650 train: 0.1313629150390625 elapsed, loss: 2.027486e-06\n",
      "step: 57660 train: 0.130357027053833 elapsed, loss: 2.2589174e-06\n",
      "step: 57670 train: 0.13525080680847168 elapsed, loss: 1.789998e-06\n",
      "step: 57680 train: 0.12534523010253906 elapsed, loss: 3.0463489e-06\n",
      "step: 57690 train: 0.13740921020507812 elapsed, loss: 1.8202672e-06\n",
      "step: 57700 train: 0.12923932075500488 elapsed, loss: 3.4356299e-06\n",
      "step: 57710 train: 0.13396286964416504 elapsed, loss: 1.845412e-06\n",
      "step: 57720 train: 0.1291828155517578 elapsed, loss: 2.43074e-06\n",
      "step: 57730 train: 0.11665177345275879 elapsed, loss: 3.207001e-06\n",
      "step: 57740 train: 0.12825608253479004 elapsed, loss: 2.576964e-06\n",
      "step: 57750 train: 0.13054132461547852 elapsed, loss: 2.119219e-06\n",
      "step: 57760 train: 0.13510441780090332 elapsed, loss: 2.7106014e-06\n",
      "step: 57770 train: 0.12699222564697266 elapsed, loss: 3.87747e-06\n",
      "step: 57780 train: 0.11815428733825684 elapsed, loss: 3.0821989e-06\n",
      "step: 57790 train: 0.12782549858093262 elapsed, loss: 6.9233747e-06\n",
      "step: 57800 train: 0.12895989418029785 elapsed, loss: 3.9967563e-06\n",
      "step: 57810 train: 0.12558960914611816 elapsed, loss: 4.1168933e-06\n",
      "step: 57820 train: 0.1427147388458252 elapsed, loss: 1.9804543e-06\n",
      "step: 57830 train: 0.13426780700683594 elapsed, loss: 2.4666024e-06\n",
      "step: 57840 train: 0.1365654468536377 elapsed, loss: 2.1969859e-06\n",
      "step: 57850 train: 0.12842941284179688 elapsed, loss: 2.4409906e-06\n",
      "step: 57860 train: 0.12685561180114746 elapsed, loss: 2.061011e-06\n",
      "step: 57870 train: 0.12149357795715332 elapsed, loss: 3.3667168e-06\n",
      "step: 57880 train: 0.1351935863494873 elapsed, loss: 1.9292302e-06\n",
      "step: 57890 train: 0.12116599082946777 elapsed, loss: 3.3383153e-06\n",
      "step: 57900 train: 0.13118267059326172 elapsed, loss: 2.9279825e-06\n",
      "step: 57910 train: 0.13042902946472168 elapsed, loss: 2.7012784e-06\n",
      "step: 57920 train: 0.13721513748168945 elapsed, loss: 4.2919814e-06\n",
      "step: 57930 train: 0.12682628631591797 elapsed, loss: 3.030515e-06\n",
      "step: 57940 train: 0.12697386741638184 elapsed, loss: 2.4670676e-06\n",
      "step: 57950 train: 0.12370443344116211 elapsed, loss: 2.7036235e-06\n",
      "step: 57960 train: 0.1370849609375 elapsed, loss: 2.7213187e-06\n",
      "step: 57970 train: 0.12980437278747559 elapsed, loss: 3.2065345e-06\n",
      "step: 57980 train: 0.14521408081054688 elapsed, loss: 1.5168913e-05\n",
      "step: 57990 train: 0.13123703002929688 elapsed, loss: 3.6493714e-06\n",
      "step: 58000 train: 0.12036776542663574 elapsed, loss: 3.2228336e-06\n",
      "step: 58010 train: 0.13878130912780762 elapsed, loss: 1.741105e-06\n",
      "step: 58020 train: 0.13630270957946777 elapsed, loss: 2.1983815e-06\n",
      "step: 58030 train: 0.13208699226379395 elapsed, loss: 5.488082e-06\n",
      "step: 58040 train: 0.13144183158874512 elapsed, loss: 6.6714256e-06\n",
      "step: 58050 train: 0.13079500198364258 elapsed, loss: 3.4049056e-06\n",
      "step: 58060 train: 0.13535761833190918 elapsed, loss: 3.694996e-06\n",
      "step: 58070 train: 0.13343095779418945 elapsed, loss: 3.2749663e-06\n",
      "step: 58080 train: 0.13232779502868652 elapsed, loss: 1.9450642e-06\n",
      "step: 58090 train: 0.13234162330627441 elapsed, loss: 2.1192213e-06\n",
      "step: 58100 train: 0.11986422538757324 elapsed, loss: 3.3210847e-06\n",
      "step: 58110 train: 0.13247227668762207 elapsed, loss: 2.5853465e-06\n",
      "step: 58120 train: 0.12471151351928711 elapsed, loss: 2.6756775e-06\n",
      "step: 58130 train: 0.12405562400817871 elapsed, loss: 2.1173587e-06\n",
      "step: 58140 train: 0.1276109218597412 elapsed, loss: 9.285643e-06\n",
      "step: 58150 train: 0.12640070915222168 elapsed, loss: 4.712e-06\n",
      "step: 58160 train: 0.12172317504882812 elapsed, loss: 4.2263277e-06\n",
      "step: 58170 train: 0.127638578414917 elapsed, loss: 2.703157e-06\n",
      "step: 58180 train: 0.12923932075500488 elapsed, loss: 2.7628415e-05\n",
      "step: 58190 train: 0.12893271446228027 elapsed, loss: 0.00011777821\n",
      "step: 58200 train: 0.131669282913208 elapsed, loss: 3.7682905e-05\n",
      "step: 58210 train: 0.12844109535217285 elapsed, loss: 3.110942e-05\n",
      "step: 58220 train: 0.12283873558044434 elapsed, loss: 1.4334565e-05\n",
      "step: 58230 train: 0.1362295150756836 elapsed, loss: 8.523795e-06\n",
      "step: 58240 train: 0.12805676460266113 elapsed, loss: 9.6120475e-06\n",
      "step: 58250 train: 0.13257360458374023 elapsed, loss: 1.9835814e-05\n",
      "step: 58260 train: 0.12396883964538574 elapsed, loss: 1.1217038e-05\n",
      "step: 58270 train: 0.13542437553405762 elapsed, loss: 6.074484e-06\n",
      "step: 58280 train: 0.12810635566711426 elapsed, loss: 5.357851e-06\n",
      "step: 58290 train: 0.1365954875946045 elapsed, loss: 3.2926805e-06\n",
      "step: 58300 train: 0.1278998851776123 elapsed, loss: 3.8845264e-06\n",
      "step: 58310 train: 0.13352084159851074 elapsed, loss: 4.4647295e-06\n",
      "step: 58320 train: 0.14608097076416016 elapsed, loss: 1.8496032e-06\n",
      "step: 58330 train: 0.12977194786071777 elapsed, loss: 2.373005e-06\n",
      "step: 58340 train: 0.13049745559692383 elapsed, loss: 1.8258553e-06\n",
      "step: 58350 train: 0.13521862030029297 elapsed, loss: 1.5632228e-06\n",
      "step: 58360 train: 0.14026546478271484 elapsed, loss: 2.4484366e-06\n",
      "step: 58370 train: 0.14052534103393555 elapsed, loss: 1.6111852e-06\n",
      "step: 58380 train: 0.1384134292602539 elapsed, loss: 1.9045511e-06\n",
      "step: 58390 train: 0.13604497909545898 elapsed, loss: 2.4470305e-06\n",
      "step: 58400 train: 0.1295933723449707 elapsed, loss: 1.6726528e-06\n",
      "step: 58410 train: 0.11957263946533203 elapsed, loss: 3.8319117e-06\n",
      "step: 58420 train: 0.1275620460510254 elapsed, loss: 2.0042025e-06\n",
      "step: 58430 train: 0.1212925910949707 elapsed, loss: 2.4521664e-06\n",
      "step: 58440 train: 0.1237485408782959 elapsed, loss: 2.0265547e-06\n",
      "step: 58450 train: 0.13161563873291016 elapsed, loss: 1.5879027e-06\n",
      "step: 58460 train: 0.12134218215942383 elapsed, loss: 2.0815005e-06\n",
      "step: 58470 train: 0.11598849296569824 elapsed, loss: 2.9285357e-06\n",
      "step: 58480 train: 0.12710332870483398 elapsed, loss: 2.1331894e-06\n",
      "step: 58490 train: 0.12896108627319336 elapsed, loss: 2.1862757e-06\n",
      "step: 58500 train: 0.13231539726257324 elapsed, loss: 2.0582195e-06\n",
      "step: 58510 train: 0.12894606590270996 elapsed, loss: 2.5723043e-06\n",
      "step: 58520 train: 0.11677765846252441 elapsed, loss: 3.2111855e-06\n",
      "step: 58530 train: 0.1470775604248047 elapsed, loss: 1.5934907e-06\n",
      "step: 58540 train: 0.1370837688446045 elapsed, loss: 3.811409e-06\n",
      "step: 58550 train: 0.12781023979187012 elapsed, loss: 2.9927937e-06\n",
      "step: 58560 train: 0.15207242965698242 elapsed, loss: 1.7816162e-06\n",
      "step: 58570 train: 0.12758708000183105 elapsed, loss: 2.5187562e-06\n",
      "step: 58580 train: 0.13211703300476074 elapsed, loss: 2.100593e-06\n",
      "step: 58590 train: 0.12935733795166016 elapsed, loss: 2.395355e-06\n",
      "step: 58600 train: 0.12314105033874512 elapsed, loss: 2.3827847e-06\n",
      "step: 58610 train: 0.12479186058044434 elapsed, loss: 2.527136e-06\n",
      "step: 58620 train: 0.13650083541870117 elapsed, loss: 2.384181e-06\n",
      "step: 58630 train: 0.12844491004943848 elapsed, loss: 2.3194525e-06\n",
      "step: 58640 train: 0.12857770919799805 elapsed, loss: 1.8528642e-06\n",
      "step: 58650 train: 0.13933086395263672 elapsed, loss: 2.128068e-06\n",
      "step: 58660 train: 0.1238698959350586 elapsed, loss: 3.3695144e-06\n",
      "step: 58670 train: 0.13165569305419922 elapsed, loss: 2.6919772e-06\n",
      "step: 58680 train: 0.12555956840515137 elapsed, loss: 3.7255588e-06\n",
      "step: 58690 train: 0.13379645347595215 elapsed, loss: 5.551512e-06\n",
      "step: 58700 train: 0.13356828689575195 elapsed, loss: 2.3152616e-06\n",
      "step: 58710 train: 0.1312863826751709 elapsed, loss: 2.320386e-06\n",
      "step: 58720 train: 0.13081574440002441 elapsed, loss: 1.943667e-06\n",
      "step: 58730 train: 0.12312173843383789 elapsed, loss: 2.3595017e-06\n",
      "step: 58740 train: 0.12367606163024902 elapsed, loss: 3.2186401e-06\n",
      "step: 58750 train: 0.11588048934936523 elapsed, loss: 4.1662515e-06\n",
      "step: 58760 train: 0.13891863822937012 elapsed, loss: 2.230046e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 58770 train: 0.15062594413757324 elapsed, loss: 5.875178e-06\n",
      "step: 58780 train: 0.13715314865112305 elapsed, loss: 3.3932624e-06\n",
      "step: 58790 train: 0.13141584396362305 elapsed, loss: 2.758104e-06\n",
      "step: 58800 train: 0.13708162307739258 elapsed, loss: 1.9711408e-06\n",
      "step: 58810 train: 0.12128853797912598 elapsed, loss: 5.803933e-06\n",
      "step: 58820 train: 0.12547826766967773 elapsed, loss: 3.0160702e-06\n",
      "step: 58830 train: 0.1311328411102295 elapsed, loss: 2.5937043e-06\n",
      "step: 58840 train: 0.12624096870422363 elapsed, loss: 2.0805692e-06\n",
      "step: 58850 train: 0.12589406967163086 elapsed, loss: 2.814447e-06\n",
      "step: 58860 train: 0.13329410552978516 elapsed, loss: 1.8589171e-06\n",
      "step: 58870 train: 0.12414407730102539 elapsed, loss: 2.716662e-06\n",
      "step: 58880 train: 0.11971569061279297 elapsed, loss: 3.0668373e-06\n",
      "step: 58890 train: 0.14322686195373535 elapsed, loss: 2.273819e-06\n",
      "step: 58900 train: 0.13146471977233887 elapsed, loss: 2.6798753e-06\n",
      "step: 58910 train: 0.12984848022460938 elapsed, loss: 2.7767287e-06\n",
      "step: 58920 train: 0.13923382759094238 elapsed, loss: 1.9003601e-06\n",
      "step: 58930 train: 0.12615537643432617 elapsed, loss: 3.3690276e-06\n",
      "step: 58940 train: 0.1377265453338623 elapsed, loss: 2.0014093e-06\n",
      "step: 58950 train: 0.1360766887664795 elapsed, loss: 2.190467e-06\n",
      "step: 58960 train: 0.12546610832214355 elapsed, loss: 2.540643e-06\n",
      "step: 58970 train: 0.12097644805908203 elapsed, loss: 3.0733559e-06\n",
      "step: 58980 train: 0.13605856895446777 elapsed, loss: 1.9473919e-06\n",
      "step: 58990 train: 0.13294768333435059 elapsed, loss: 2.0442492e-06\n",
      "step: 59000 train: 0.1212306022644043 elapsed, loss: 5.14929e-06\n",
      "step: 59010 train: 0.1215517520904541 elapsed, loss: 3.1781299e-06\n",
      "step: 59020 train: 0.13525652885437012 elapsed, loss: 2.9056455e-06\n",
      "step: 59030 train: 0.12385964393615723 elapsed, loss: 3.258213e-06\n",
      "step: 59040 train: 0.12340998649597168 elapsed, loss: 3.6135204e-06\n",
      "step: 59050 train: 0.13527846336364746 elapsed, loss: 2.2798731e-06\n",
      "step: 59060 train: 0.12218332290649414 elapsed, loss: 3.450529e-06\n",
      "step: 59070 train: 0.13350462913513184 elapsed, loss: 2.850761e-06\n",
      "step: 59080 train: 0.12473416328430176 elapsed, loss: 2.9252762e-06\n",
      "step: 59090 train: 0.12516546249389648 elapsed, loss: 2.4181736e-06\n",
      "step: 59100 train: 0.12546491622924805 elapsed, loss: 3.7713771e-06\n",
      "step: 59110 train: 0.13257956504821777 elapsed, loss: 2.406531e-06\n",
      "step: 59120 train: 0.13660907745361328 elapsed, loss: 3.615315e-06\n",
      "step: 59130 train: 0.13075900077819824 elapsed, loss: 2.1639244e-06\n",
      "step: 59140 train: 0.12477779388427734 elapsed, loss: 4.1243347e-06\n",
      "step: 59150 train: 0.13289928436279297 elapsed, loss: 2.5774298e-06\n",
      "step: 59160 train: 0.13290834426879883 elapsed, loss: 2.4214337e-06\n",
      "step: 59170 train: 0.139801025390625 elapsed, loss: 2.2919808e-06\n",
      "step: 59180 train: 0.12964749336242676 elapsed, loss: 5.1441284e-06\n",
      "step: 59190 train: 0.131974458694458 elapsed, loss: 3.8798535e-06\n",
      "step: 59200 train: 0.12439656257629395 elapsed, loss: 2.6207363e-06\n",
      "step: 59210 train: 0.12908077239990234 elapsed, loss: 2.0815023e-06\n",
      "step: 59220 train: 0.1270918846130371 elapsed, loss: 2.6686985e-06\n",
      "step: 59230 train: 0.12941813468933105 elapsed, loss: 2.0354014e-06\n",
      "step: 59240 train: 0.12404060363769531 elapsed, loss: 2.9969883e-06\n",
      "step: 59250 train: 0.12321305274963379 elapsed, loss: 0.04642749\n",
      "step: 59260 train: 0.12315726280212402 elapsed, loss: 7.689826e-05\n",
      "step: 59270 train: 0.12120389938354492 elapsed, loss: 4.9942966e-05\n",
      "step: 59280 train: 0.12405014038085938 elapsed, loss: 2.8222605e-05\n",
      "step: 59290 train: 0.132279634475708 elapsed, loss: 1.4319718e-05\n",
      "step: 59300 train: 0.1266641616821289 elapsed, loss: 1.3427952e-05\n",
      "step: 59310 train: 0.1353304386138916 elapsed, loss: 9.7898e-06\n",
      "step: 59320 train: 0.12232112884521484 elapsed, loss: 1.0550719e-05\n",
      "step: 59330 train: 0.13321352005004883 elapsed, loss: 7.3508454e-06\n",
      "step: 59340 train: 0.11717700958251953 elapsed, loss: 7.1566906e-06\n",
      "step: 59350 train: 0.12470698356628418 elapsed, loss: 5.090533e-06\n",
      "step: 59360 train: 0.14807415008544922 elapsed, loss: 2.9457653e-06\n",
      "step: 59370 train: 0.12773346900939941 elapsed, loss: 4.2635797e-06\n",
      "step: 59380 train: 0.14865660667419434 elapsed, loss: 2.939684e-06\n",
      "step: 59390 train: 0.12923979759216309 elapsed, loss: 2.746458e-06\n",
      "step: 59400 train: 0.1298842430114746 elapsed, loss: 2.201176e-06\n",
      "step: 59410 train: 0.12551188468933105 elapsed, loss: 2.5234128e-06\n",
      "step: 59420 train: 0.11971116065979004 elapsed, loss: 0.0002942787\n",
      "step: 59430 train: 0.1232602596282959 elapsed, loss: 2.2990835e-05\n",
      "step: 59440 train: 0.1288444995880127 elapsed, loss: 1.3064089e-05\n",
      "step: 59450 train: 0.1405506134033203 elapsed, loss: 7.432689e-06\n",
      "step: 59460 train: 0.13828110694885254 elapsed, loss: 6.8738937e-06\n",
      "step: 59470 train: 0.13040637969970703 elapsed, loss: 6.223442e-06\n",
      "step: 59480 train: 0.13226008415222168 elapsed, loss: 4.935029e-06\n",
      "step: 59490 train: 0.13606715202331543 elapsed, loss: 4.978816e-06\n",
      "step: 59500 train: 0.14191126823425293 elapsed, loss: 3.3108395e-06\n",
      "step: 59510 train: 0.1268312931060791 elapsed, loss: 4.1336543e-06\n",
      "step: 59520 train: 0.1308281421661377 elapsed, loss: 3.048675e-06\n",
      "step: 59530 train: 0.1258549690246582 elapsed, loss: 2.9993093e-06\n",
      "step: 59540 train: 0.1184854507446289 elapsed, loss: 3.3219962e-06\n",
      "step: 59550 train: 0.13376522064208984 elapsed, loss: 1.8863905e-06\n",
      "step: 59560 train: 0.1161649227142334 elapsed, loss: 2.769742e-06\n",
      "step: 59570 train: 0.13817763328552246 elapsed, loss: 1.3583325e-06\n",
      "step: 59580 train: 0.12450051307678223 elapsed, loss: 1.6731187e-06\n",
      "step: 59590 train: 0.12682485580444336 elapsed, loss: 1.9120014e-06\n",
      "step: 59600 train: 0.1242513656616211 elapsed, loss: 1.7872057e-06\n",
      "step: 59610 train: 0.1411898136138916 elapsed, loss: 1.2624062e-06\n",
      "step: 59620 train: 0.12538433074951172 elapsed, loss: 2.7082745e-06\n",
      "step: 59630 train: 0.13149428367614746 elapsed, loss: 1.8323742e-06\n",
      "step: 59640 train: 0.12740182876586914 elapsed, loss: 1.9292195e-06\n",
      "step: 59650 train: 0.1312119960784912 elapsed, loss: 1.4482048e-06\n",
      "step: 59660 train: 0.12238693237304688 elapsed, loss: 1.9860422e-06\n",
      "step: 59670 train: 0.12766170501708984 elapsed, loss: 1.6274839e-06\n",
      "step: 59680 train: 0.12581205368041992 elapsed, loss: 3.3330602e-06\n",
      "step: 59690 train: 0.12747740745544434 elapsed, loss: 1.7569365e-06\n",
      "step: 59700 train: 0.13370656967163086 elapsed, loss: 2.3222483e-06\n",
      "step: 59710 train: 0.12335705757141113 elapsed, loss: 3.3769506e-06\n",
      "step: 59720 train: 0.12384653091430664 elapsed, loss: 2.1075791e-06\n",
      "step: 59730 train: 0.12863373756408691 elapsed, loss: 2.7306269e-06\n",
      "step: 59740 train: 0.12152862548828125 elapsed, loss: 2.3776597e-06\n",
      "step: 59750 train: 0.12609505653381348 elapsed, loss: 2.8605507e-06\n",
      "step: 59760 train: 0.1224515438079834 elapsed, loss: 3.1473958e-06\n",
      "step: 59770 train: 0.12496733665466309 elapsed, loss: 2.0295822e-05\n",
      "step: 59780 train: 0.13210725784301758 elapsed, loss: 1.0249994e-05\n",
      "step: 59790 train: 0.1341264247894287 elapsed, loss: 3.7411041e-06\n",
      "step: 59800 train: 0.12902092933654785 elapsed, loss: 4.0963787e-06\n",
      "step: 59810 train: 0.13013458251953125 elapsed, loss: 3.0128142e-06\n",
      "step: 59820 train: 0.12813830375671387 elapsed, loss: 2.8256184e-06\n",
      "step: 59830 train: 0.11591935157775879 elapsed, loss: 2.7338913e-06\n",
      "step: 59840 train: 0.12786316871643066 elapsed, loss: 1.6670649e-06\n",
      "step: 59850 train: 0.13427400588989258 elapsed, loss: 2.3930272e-06\n",
      "step: 59860 train: 0.13881206512451172 elapsed, loss: 1.7182866e-06\n",
      "step: 59870 train: 0.13430047035217285 elapsed, loss: 1.8547254e-06\n",
      "step: 59880 train: 0.13451027870178223 elapsed, loss: 1.7322574e-06\n",
      "step: 59890 train: 0.126542329788208 elapsed, loss: 2.2947656e-06\n",
      "step: 59900 train: 0.12085342407226562 elapsed, loss: 2.1411e-06\n",
      "step: 59910 train: 0.12653017044067383 elapsed, loss: 1.8398247e-06\n",
      "step: 59920 train: 0.12614750862121582 elapsed, loss: 1.8840623e-06\n",
      "step: 59930 train: 0.12357378005981445 elapsed, loss: 1.6880201e-06\n",
      "step: 59940 train: 0.1307673454284668 elapsed, loss: 1.7932578e-06\n",
      "step: 59950 train: 0.1259622573852539 elapsed, loss: 1.6381944e-06\n",
      "step: 59960 train: 0.12560343742370605 elapsed, loss: 4.0483965e-06\n",
      "step: 59970 train: 0.1280679702758789 elapsed, loss: 2.7920883e-06\n",
      "step: 59980 train: 0.12878084182739258 elapsed, loss: 2.142504e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 59990 train: 0.12396574020385742 elapsed, loss: 2.135518e-06\n",
      "step: 60000 train: 0.13390374183654785 elapsed, loss: 1.5879031e-06\n",
      "step: 60010 train: 0.1312546730041504 elapsed, loss: 1.5413365e-06\n",
      "step: 60020 train: 0.12815260887145996 elapsed, loss: 3.3276006e-06\n",
      "step: 60030 train: 0.13086318969726562 elapsed, loss: 1.9976837e-06\n",
      "step: 60040 train: 0.11661577224731445 elapsed, loss: 2.8549593e-06\n",
      "step: 60050 train: 0.12521648406982422 elapsed, loss: 2.6309617e-06\n",
      "step: 60060 train: 0.1267704963684082 elapsed, loss: 2.0200346e-06\n",
      "step: 60070 train: 0.12328267097473145 elapsed, loss: 2.5862762e-06\n",
      "step: 60080 train: 0.1283402442932129 elapsed, loss: 1.8021067e-06\n",
      "step: 60090 train: 0.12909793853759766 elapsed, loss: 2.6919815e-06\n",
      "step: 60100 train: 0.13193392753601074 elapsed, loss: 2.6882558e-06\n",
      "step: 60110 train: 0.13846373558044434 elapsed, loss: 1.9129307e-06\n",
      "step: 60120 train: 0.12723660469055176 elapsed, loss: 2.8097925e-06\n",
      "step: 60130 train: 0.12801027297973633 elapsed, loss: 2.2747504e-06\n",
      "step: 60140 train: 0.12831974029541016 elapsed, loss: 2.1276014e-06\n",
      "step: 60150 train: 0.130570650100708 elapsed, loss: 1.8998942e-06\n",
      "step: 60160 train: 0.12840628623962402 elapsed, loss: 3.0752199e-06\n",
      "step: 60170 train: 0.12734222412109375 elapsed, loss: 2.922014e-06\n",
      "step: 60180 train: 0.11907744407653809 elapsed, loss: 3.0207377e-06\n",
      "step: 60190 train: 0.11736917495727539 elapsed, loss: 3.4337704e-06\n",
      "step: 60200 train: 0.1256260871887207 elapsed, loss: 3.2335395e-06\n",
      "step: 60210 train: 0.1327073574066162 elapsed, loss: 2.2198033e-06\n",
      "step: 60220 train: 0.13419485092163086 elapsed, loss: 2.2025738e-06\n",
      "step: 60230 train: 0.12067437171936035 elapsed, loss: 7.4141917e-06\n",
      "step: 60240 train: 0.12774872779846191 elapsed, loss: 2.594194e-06\n",
      "step: 60250 train: 0.1329817771911621 elapsed, loss: 2.3567059e-06\n",
      "step: 60260 train: 0.1410973072052002 elapsed, loss: 1.7965181e-06\n",
      "step: 60270 train: 0.13052105903625488 elapsed, loss: 1.9958206e-06\n",
      "step: 60280 train: 0.12829899787902832 elapsed, loss: 2.1676497e-06\n",
      "step: 60290 train: 0.1374051570892334 elapsed, loss: 2.5010618e-06\n",
      "step: 60300 train: 0.13392233848571777 elapsed, loss: 3.4901207e-06\n",
      "step: 60310 train: 0.1346113681793213 elapsed, loss: 3.0979916e-06\n",
      "step: 60320 train: 0.1245267391204834 elapsed, loss: 2.5392449e-06\n",
      "step: 60330 train: 0.1259012222290039 elapsed, loss: 2.8176974e-06\n",
      "step: 60340 train: 0.12389969825744629 elapsed, loss: 2.9750988e-06\n",
      "step: 60350 train: 0.12191152572631836 elapsed, loss: 4.6207347e-06\n",
      "step: 60360 train: 0.12032651901245117 elapsed, loss: 2.9951239e-06\n",
      "step: 60370 train: 0.13144278526306152 elapsed, loss: 2.2486718e-06\n",
      "step: 60380 train: 0.12021493911743164 elapsed, loss: 5.805701e-06\n",
      "step: 60390 train: 0.13561582565307617 elapsed, loss: 2.8726577e-06\n",
      "step: 60400 train: 0.12256264686584473 elapsed, loss: 7.4411632e-06\n",
      "step: 60410 train: 0.13782954216003418 elapsed, loss: 4.3390096e-06\n",
      "step: 60420 train: 0.14633846282958984 elapsed, loss: 2.3776583e-06\n",
      "step: 60430 train: 0.1371016502380371 elapsed, loss: 3.1292361e-06\n",
      "step: 60440 train: 0.1287379264831543 elapsed, loss: 3.229819e-06\n",
      "step: 60450 train: 0.12677764892578125 elapsed, loss: 2.2933762e-06\n",
      "step: 60460 train: 0.12464237213134766 elapsed, loss: 3.056589e-06\n",
      "step: 60470 train: 0.13172531127929688 elapsed, loss: 2.0666012e-06\n",
      "step: 60480 train: 0.12814736366271973 elapsed, loss: 2.8028094e-06\n",
      "step: 60490 train: 0.13300418853759766 elapsed, loss: 3.7806726e-06\n",
      "step: 60500 train: 0.12758874893188477 elapsed, loss: 3.147392e-06\n",
      "step: 60510 train: 0.14065289497375488 elapsed, loss: 2.1234116e-06\n",
      "step: 60520 train: 0.12956666946411133 elapsed, loss: 3.2004791e-06\n",
      "step: 60530 train: 0.12729907035827637 elapsed, loss: 2.2617128e-06\n",
      "step: 60540 train: 0.13196706771850586 elapsed, loss: 1.8984972e-06\n",
      "step: 60550 train: 0.12843561172485352 elapsed, loss: 2.6803389e-06\n",
      "step: 60560 train: 0.13071417808532715 elapsed, loss: 2.569979e-06\n",
      "step: 60570 train: 0.13957810401916504 elapsed, loss: 2.1723063e-06\n",
      "step: 60580 train: 0.13710832595825195 elapsed, loss: 2.0940756e-06\n",
      "step: 60590 train: 0.14180946350097656 elapsed, loss: 3.4607556e-06\n",
      "step: 60600 train: 0.12917590141296387 elapsed, loss: 2.3748678e-06\n",
      "step: 60610 train: 0.1356065273284912 elapsed, loss: 2.196516e-06\n",
      "step: 60620 train: 0.12628626823425293 elapsed, loss: 3.1245786e-06\n",
      "step: 60630 train: 0.12649011611938477 elapsed, loss: 2.5913996e-06\n",
      "step: 60640 train: 0.13834095001220703 elapsed, loss: 2.0256232e-06\n",
      "step: 60650 train: 0.12473869323730469 elapsed, loss: 2.9657895e-06\n",
      "step: 60660 train: 0.1288437843322754 elapsed, loss: 3.4281793e-06\n",
      "step: 60670 train: 0.12504124641418457 elapsed, loss: 4.300823e-06\n",
      "step: 60680 train: 0.13780903816223145 elapsed, loss: 1.915262e-06\n",
      "step: 60690 train: 0.13992786407470703 elapsed, loss: 0.00094421254\n",
      "step: 60700 train: 0.1261425018310547 elapsed, loss: 4.5430366e-05\n",
      "step: 60710 train: 0.12474226951599121 elapsed, loss: 2.3296629e-05\n",
      "step: 60720 train: 0.12932610511779785 elapsed, loss: 1.4158166e-05\n",
      "step: 60730 train: 0.14022469520568848 elapsed, loss: 1.609643e-05\n",
      "step: 60740 train: 0.12796497344970703 elapsed, loss: 1.0598276e-05\n",
      "step: 60750 train: 0.12775516510009766 elapsed, loss: 8.168536e-06\n",
      "step: 60760 train: 0.12878704071044922 elapsed, loss: 1.0331489e-05\n",
      "step: 60770 train: 0.13595843315124512 elapsed, loss: 5.9844815e-06\n",
      "step: 60780 train: 0.13089799880981445 elapsed, loss: 5.9529575e-06\n",
      "step: 60790 train: 0.12630748748779297 elapsed, loss: 5.1599645e-06\n",
      "step: 60800 train: 0.11631035804748535 elapsed, loss: 4.3576383e-06\n",
      "step: 60810 train: 0.13628649711608887 elapsed, loss: 2.1373817e-06\n",
      "step: 60820 train: 0.13099980354309082 elapsed, loss: 2.6700936e-06\n",
      "step: 60830 train: 0.12386274337768555 elapsed, loss: 3.3569404e-06\n",
      "step: 60840 train: 0.12826967239379883 elapsed, loss: 2.806064e-06\n",
      "step: 60850 train: 0.14023637771606445 elapsed, loss: 1.8351682e-06\n",
      "step: 60860 train: 0.12378573417663574 elapsed, loss: 2.3385453e-06\n",
      "step: 60870 train: 0.13107681274414062 elapsed, loss: 1.4780068e-06\n",
      "step: 60880 train: 0.1358025074005127 elapsed, loss: 1.8169901e-06\n",
      "step: 60890 train: 0.12143731117248535 elapsed, loss: 2.5508855e-06\n",
      "step: 60900 train: 0.12878966331481934 elapsed, loss: 2.316195e-06\n",
      "step: 60910 train: 0.14337968826293945 elapsed, loss: 8.008267e-06\n",
      "step: 60920 train: 0.11999630928039551 elapsed, loss: 3.4985014e-06\n",
      "step: 60930 train: 0.1290912628173828 elapsed, loss: 1.8747494e-06\n",
      "step: 60940 train: 0.1273179054260254 elapsed, loss: 2.8316786e-06\n",
      "step: 60950 train: 0.13306736946105957 elapsed, loss: 2.722709e-06\n",
      "step: 60960 train: 0.1393110752105713 elapsed, loss: 2.025156e-06\n",
      "step: 60970 train: 0.12028217315673828 elapsed, loss: 3.295473e-06\n",
      "step: 60980 train: 0.11663150787353516 elapsed, loss: 2.3902353e-06\n",
      "step: 60990 train: 0.12976431846618652 elapsed, loss: 1.8635733e-06\n",
      "step: 61000 train: 0.12231945991516113 elapsed, loss: 2.5839468e-06\n",
      "step: 61010 train: 0.12781977653503418 elapsed, loss: 2.6318658e-06\n",
      "step: 61020 train: 0.13164830207824707 elapsed, loss: 2.643088e-06\n",
      "step: 61030 train: 0.12990331649780273 elapsed, loss: 2.2188722e-06\n",
      "step: 61040 train: 0.13341951370239258 elapsed, loss: 1.5916282e-06\n",
      "step: 61050 train: 0.12188005447387695 elapsed, loss: 2.569514e-06\n",
      "step: 61060 train: 0.12086653709411621 elapsed, loss: 2.662177e-06\n",
      "step: 61070 train: 0.122833251953125 elapsed, loss: 2.8754503e-06\n",
      "step: 61080 train: 0.12059712409973145 elapsed, loss: 5.4612383e-06\n",
      "step: 61090 train: 0.13252568244934082 elapsed, loss: 2.142504e-06\n",
      "step: 61100 train: 0.12855935096740723 elapsed, loss: 2.9392468e-06\n",
      "step: 61110 train: 0.13118386268615723 elapsed, loss: 1.0955138e-05\n",
      "step: 61120 train: 0.13246965408325195 elapsed, loss: 3.10083e-06\n",
      "step: 61130 train: 0.13353586196899414 elapsed, loss: 2.34227e-06\n",
      "step: 61140 train: 0.12982869148254395 elapsed, loss: 2.1881365e-06\n",
      "step: 61150 train: 0.1327369213104248 elapsed, loss: 2.1439002e-06\n",
      "step: 61160 train: 0.1350710391998291 elapsed, loss: 2.4912822e-06\n",
      "step: 61170 train: 0.1228632926940918 elapsed, loss: 3.525977e-06\n",
      "step: 61180 train: 0.13615727424621582 elapsed, loss: 2.4656704e-06\n",
      "step: 61190 train: 0.14156031608581543 elapsed, loss: 3.0090935e-06\n",
      "step: 61200 train: 0.1364428997039795 elapsed, loss: 1.8696257e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 61210 train: 0.13179731369018555 elapsed, loss: 2.1988471e-06\n",
      "step: 61220 train: 0.12486481666564941 elapsed, loss: 3.368584e-06\n",
      "step: 61230 train: 0.12428689002990723 elapsed, loss: 2.4796414e-06\n",
      "step: 61240 train: 0.13231778144836426 elapsed, loss: 2.9704415e-06\n",
      "step: 61250 train: 0.12585210800170898 elapsed, loss: 1.5562535e-05\n",
      "step: 61260 train: 0.11918354034423828 elapsed, loss: 5.1688025e-06\n",
      "step: 61270 train: 0.13036561012268066 elapsed, loss: 5.930758e-06\n",
      "step: 61280 train: 0.13622045516967773 elapsed, loss: 2.8209674e-06\n",
      "step: 61290 train: 0.11548256874084473 elapsed, loss: 3.5371504e-06\n",
      "step: 61300 train: 0.1240987777709961 elapsed, loss: 2.4242281e-06\n",
      "step: 61310 train: 0.13942527770996094 elapsed, loss: 2.37718e-06\n",
      "step: 61320 train: 0.13019990921020508 elapsed, loss: 3.330401e-06\n",
      "step: 61330 train: 0.12523388862609863 elapsed, loss: 2.5532145e-06\n",
      "step: 61340 train: 0.1322021484375 elapsed, loss: 3.2302175e-06\n",
      "step: 61350 train: 0.14004898071289062 elapsed, loss: 1.7657849e-06\n",
      "step: 61360 train: 0.12932085990905762 elapsed, loss: 2.284065e-06\n",
      "step: 61370 train: 0.12913084030151367 elapsed, loss: 2.2966374e-06\n",
      "step: 61380 train: 0.14510869979858398 elapsed, loss: 1.5748642e-06\n",
      "step: 61390 train: 0.12708568572998047 elapsed, loss: 2.4675346e-06\n",
      "step: 61400 train: 0.1255784034729004 elapsed, loss: 2.9923294e-06\n",
      "step: 61410 train: 0.13231587409973145 elapsed, loss: 2.2705601e-06\n",
      "step: 61420 train: 0.1342475414276123 elapsed, loss: 2.2188717e-06\n",
      "step: 61430 train: 0.12473368644714355 elapsed, loss: 2.6002472e-06\n",
      "step: 61440 train: 0.12879204750061035 elapsed, loss: 2.7417823e-06\n",
      "step: 61450 train: 0.1266040802001953 elapsed, loss: 7.1822933e-06\n",
      "step: 61460 train: 0.1260976791381836 elapsed, loss: 3.7466857e-06\n",
      "step: 61470 train: 0.12934184074401855 elapsed, loss: 3.2898838e-06\n",
      "step: 61480 train: 0.12061047554016113 elapsed, loss: 3.3694973e-06\n",
      "step: 61490 train: 0.13126301765441895 elapsed, loss: 2.3720727e-06\n",
      "step: 61500 train: 0.1224358081817627 elapsed, loss: 3.607928e-06\n",
      "step: 61510 train: 0.12932944297790527 elapsed, loss: 2.6579887e-06\n",
      "step: 61520 train: 0.132951021194458 elapsed, loss: 2.681736e-06\n",
      "step: 61530 train: 0.13243746757507324 elapsed, loss: 2.8554277e-06\n",
      "step: 61540 train: 0.12661314010620117 elapsed, loss: 3.0872868e-06\n",
      "step: 61550 train: 0.12218713760375977 elapsed, loss: 1.8026512e-05\n",
      "step: 61560 train: 0.1411600112915039 elapsed, loss: 3.0705496e-06\n",
      "step: 61570 train: 0.12785959243774414 elapsed, loss: 2.2239942e-06\n",
      "step: 61580 train: 0.13431596755981445 elapsed, loss: 2.7785932e-06\n",
      "step: 61590 train: 0.14007186889648438 elapsed, loss: 2.2035053e-06\n",
      "step: 61600 train: 0.13020849227905273 elapsed, loss: 3.0971046e-06\n",
      "step: 61610 train: 0.12904763221740723 elapsed, loss: 5.355075e-06\n",
      "step: 61620 train: 0.12515521049499512 elapsed, loss: 3.3848814e-06\n",
      "step: 61630 train: 0.12186384201049805 elapsed, loss: 3.4221337e-06\n",
      "step: 61640 train: 0.133195161819458 elapsed, loss: 2.3022253e-06\n",
      "step: 61650 train: 0.11864113807678223 elapsed, loss: 4.379526e-06\n",
      "step: 61660 train: 0.1279296875 elapsed, loss: 3.2391304e-06\n",
      "step: 61670 train: 0.13085412979125977 elapsed, loss: 3.1799925e-06\n",
      "step: 61680 train: 0.14044737815856934 elapsed, loss: 3.0225974e-06\n",
      "step: 61690 train: 0.13575339317321777 elapsed, loss: 1.7262039e-06\n",
      "step: 61700 train: 0.13036775588989258 elapsed, loss: 2.2919805e-06\n",
      "step: 61710 train: 0.13179874420166016 elapsed, loss: 2.8521683e-06\n",
      "step: 61720 train: 0.13024258613586426 elapsed, loss: 3.1990837e-06\n",
      "step: 61730 train: 0.13424444198608398 elapsed, loss: 1.9245724e-06\n",
      "step: 61740 train: 0.13203763961791992 elapsed, loss: 2.7608996e-06\n",
      "step: 61750 train: 0.12662863731384277 elapsed, loss: 2.8572745e-06\n",
      "step: 61760 train: 0.13116097450256348 elapsed, loss: 2.1806877e-06\n",
      "step: 61770 train: 0.1363966464996338 elapsed, loss: 2.3702082e-06\n",
      "step: 61780 train: 0.12362027168273926 elapsed, loss: 4.060553e-06\n",
      "step: 61790 train: 0.13280415534973145 elapsed, loss: 2.1010555e-06\n",
      "step: 61800 train: 0.13335895538330078 elapsed, loss: 2.7483243e-06\n",
      "step: 61810 train: 0.12491393089294434 elapsed, loss: 9.774495e-06\n",
      "step: 61820 train: 0.13169503211975098 elapsed, loss: 3.2447185e-06\n",
      "step: 61830 train: 0.12830305099487305 elapsed, loss: 3.6582237e-06\n",
      "step: 61840 train: 0.1309645175933838 elapsed, loss: 1.8258548e-06\n",
      "step: 61850 train: 0.12719011306762695 elapsed, loss: 2.199779e-06\n",
      "step: 61860 train: 0.13333368301391602 elapsed, loss: 2.8065344e-06\n",
      "step: 61870 train: 0.1316087245941162 elapsed, loss: 2.3222485e-06\n",
      "step: 61880 train: 0.13337039947509766 elapsed, loss: 2.129931e-06\n",
      "step: 61890 train: 0.1172032356262207 elapsed, loss: 3.8961757e-06\n",
      "step: 61900 train: 0.12681818008422852 elapsed, loss: 2.59885e-06\n",
      "step: 61910 train: 0.1328291893005371 elapsed, loss: 2.6025746e-06\n",
      "step: 61920 train: 0.12068867683410645 elapsed, loss: 3.2558946e-06\n",
      "step: 61930 train: 0.13433194160461426 elapsed, loss: 2.343668e-06\n",
      "step: 61940 train: 0.12598037719726562 elapsed, loss: 3.7657674e-06\n",
      "step: 61950 train: 0.12027668952941895 elapsed, loss: 3.3173574e-06\n",
      "step: 61960 train: 0.1345500946044922 elapsed, loss: 3.4877935e-06\n",
      "step: 61970 train: 0.13036489486694336 elapsed, loss: 5.961706e-06\n",
      "step: 61980 train: 0.12700772285461426 elapsed, loss: 3.0714928e-06\n",
      "step: 61990 train: 0.1235959529876709 elapsed, loss: 4.31014e-06\n",
      "step: 62000 train: 0.13016080856323242 elapsed, loss: 2.7050146e-06\n",
      "step: 62010 train: 0.12150168418884277 elapsed, loss: 3.4039738e-06\n",
      "step: 62020 train: 0.12748241424560547 elapsed, loss: 1.9287663e-06\n",
      "step: 62030 train: 0.13556575775146484 elapsed, loss: 2.5718352e-06\n",
      "step: 62040 train: 0.145277738571167 elapsed, loss: 2.0168012e-05\n",
      "step: 62050 train: 0.12337398529052734 elapsed, loss: 0.0014564681\n",
      "step: 62060 train: 0.13535428047180176 elapsed, loss: 6.116353e-05\n",
      "step: 62070 train: 0.13635659217834473 elapsed, loss: 2.1086438e-05\n",
      "step: 62080 train: 0.12031865119934082 elapsed, loss: 2.9869516e-05\n",
      "step: 62090 train: 0.13642430305480957 elapsed, loss: 1.4602097e-05\n",
      "step: 62100 train: 0.12537002563476562 elapsed, loss: 1.7090402e-05\n",
      "step: 62110 train: 0.1278982162475586 elapsed, loss: 1.2706219e-05\n",
      "step: 62120 train: 0.13242769241333008 elapsed, loss: 8.092176e-06\n",
      "step: 62130 train: 0.12951040267944336 elapsed, loss: 5.9175627e-06\n",
      "step: 62140 train: 0.13582396507263184 elapsed, loss: 5.4984803e-06\n",
      "step: 62150 train: 0.13338351249694824 elapsed, loss: 3.8314342e-06\n",
      "step: 62160 train: 0.13405108451843262 elapsed, loss: 3.5128944e-06\n",
      "step: 62170 train: 0.12729310989379883 elapsed, loss: 3.730401e-06\n",
      "step: 62180 train: 0.12394094467163086 elapsed, loss: 8.527943e-06\n",
      "step: 62190 train: 0.1319112777709961 elapsed, loss: 5.3936974e-06\n",
      "step: 62200 train: 0.13580703735351562 elapsed, loss: 2.0298135e-06\n",
      "step: 62210 train: 0.1220557689666748 elapsed, loss: 3.0049023e-06\n",
      "step: 62220 train: 0.12582969665527344 elapsed, loss: 2.9462108e-06\n",
      "step: 62230 train: 0.13081669807434082 elapsed, loss: 1.6419194e-06\n",
      "step: 62240 train: 0.1217350959777832 elapsed, loss: 2.5373818e-06\n",
      "step: 62250 train: 0.1231381893157959 elapsed, loss: 2.9443677e-06\n",
      "step: 62260 train: 0.13092660903930664 elapsed, loss: 1.7899957e-06\n",
      "step: 62270 train: 0.13673090934753418 elapsed, loss: 2.0260886e-06\n",
      "step: 62280 train: 0.12802958488464355 elapsed, loss: 2.012585e-06\n",
      "step: 62290 train: 0.13869547843933105 elapsed, loss: 1.964621e-06\n",
      "step: 62300 train: 0.12317991256713867 elapsed, loss: 2.6868597e-06\n",
      "step: 62310 train: 0.1339724063873291 elapsed, loss: 1.9902327e-06\n",
      "step: 62320 train: 0.13416051864624023 elapsed, loss: 1.51526e-06\n",
      "step: 62330 train: 0.12948274612426758 elapsed, loss: 2.5550771e-06\n",
      "step: 62340 train: 0.1309194564819336 elapsed, loss: 1.6461099e-06\n",
      "step: 62350 train: 0.12121772766113281 elapsed, loss: 1.9050166e-06\n",
      "step: 62360 train: 0.1378936767578125 elapsed, loss: 1.4961681e-06\n",
      "step: 62370 train: 0.1314380168914795 elapsed, loss: 5.4199514e-05\n",
      "step: 62380 train: 0.13646817207336426 elapsed, loss: 6.4279125e-06\n",
      "step: 62390 train: 0.13912725448608398 elapsed, loss: 6.4842934e-06\n",
      "step: 62400 train: 0.13450050354003906 elapsed, loss: 2.7664864e-06\n",
      "step: 62410 train: 0.1465623378753662 elapsed, loss: 8.771469e-06\n",
      "step: 62420 train: 0.12784767150878906 elapsed, loss: 5.4366865e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 62430 train: 0.13172221183776855 elapsed, loss: 2.0894176e-06\n",
      "step: 62440 train: 0.13363075256347656 elapsed, loss: 2.1434337e-06\n",
      "step: 62450 train: 0.13836932182312012 elapsed, loss: 1.4579831e-06\n",
      "step: 62460 train: 0.13073110580444336 elapsed, loss: 1.3527447e-06\n",
      "step: 62470 train: 0.11802816390991211 elapsed, loss: 2.2579873e-06\n",
      "step: 62480 train: 0.12879681587219238 elapsed, loss: 1.5972162e-06\n",
      "step: 62490 train: 0.1258530616760254 elapsed, loss: 2.3608973e-06\n",
      "step: 62500 train: 0.13245463371276855 elapsed, loss: 1.5157252e-06\n",
      "step: 62510 train: 0.12511396408081055 elapsed, loss: 2.0824336e-06\n",
      "step: 62520 train: 0.13033795356750488 elapsed, loss: 2.3888367e-06\n",
      "step: 62530 train: 0.13213586807250977 elapsed, loss: 1.390463e-06\n",
      "step: 62540 train: 0.12258505821228027 elapsed, loss: 2.8805553e-06\n",
      "step: 62550 train: 0.1412801742553711 elapsed, loss: 1.401173e-06\n",
      "step: 62560 train: 0.13591337203979492 elapsed, loss: 1.3350491e-06\n",
      "step: 62570 train: 0.13541960716247559 elapsed, loss: 1.5469241e-06\n",
      "step: 62580 train: 0.13988208770751953 elapsed, loss: 1.7266691e-06\n",
      "step: 62590 train: 0.1361994743347168 elapsed, loss: 2.1569367e-06\n",
      "step: 62600 train: 0.13188672065734863 elapsed, loss: 1.7043152e-06\n",
      "step: 62610 train: 0.12509870529174805 elapsed, loss: 2.3599666e-06\n",
      "step: 62620 train: 0.12928175926208496 elapsed, loss: 1.6107205e-06\n",
      "step: 62630 train: 0.12977218627929688 elapsed, loss: 4.180669e-06\n",
      "step: 62640 train: 0.12536358833312988 elapsed, loss: 2.4614783e-06\n",
      "step: 62650 train: 0.11981797218322754 elapsed, loss: 3.58232e-06\n",
      "step: 62660 train: 0.13193583488464355 elapsed, loss: 1.5846435e-06\n",
      "step: 62670 train: 0.13953614234924316 elapsed, loss: 1.760663e-06\n",
      "step: 62680 train: 0.12937164306640625 elapsed, loss: 2.5397119e-06\n",
      "step: 62690 train: 0.14323997497558594 elapsed, loss: 1.4924422e-06\n",
      "step: 62700 train: 0.13002419471740723 elapsed, loss: 1.8044349e-06\n",
      "step: 62710 train: 0.1282792091369629 elapsed, loss: 2.1131643e-06\n",
      "step: 62720 train: 0.12086868286132812 elapsed, loss: 2.8759123e-06\n",
      "step: 62730 train: 0.12044048309326172 elapsed, loss: 2.7376122e-06\n",
      "step: 62740 train: 0.12654399871826172 elapsed, loss: 3.646113e-06\n",
      "step: 62750 train: 0.11874961853027344 elapsed, loss: 3.105019e-06\n",
      "step: 62760 train: 0.13815641403198242 elapsed, loss: 2.4200363e-06\n",
      "step: 62770 train: 0.13314414024353027 elapsed, loss: 1.615842e-06\n",
      "step: 62780 train: 0.12006330490112305 elapsed, loss: 3.2097944e-06\n",
      "step: 62790 train: 0.14041662216186523 elapsed, loss: 2.9466974e-06\n",
      "step: 62800 train: 0.13236331939697266 elapsed, loss: 2.1210835e-06\n",
      "step: 62810 train: 0.1333611011505127 elapsed, loss: 1.8286488e-06\n",
      "step: 62820 train: 0.12444138526916504 elapsed, loss: 3.118524e-06\n",
      "step: 62830 train: 0.12595653533935547 elapsed, loss: 2.347394e-06\n",
      "step: 62840 train: 0.12476229667663574 elapsed, loss: 3.6567885e-06\n",
      "step: 62850 train: 0.12070965766906738 elapsed, loss: 2.7026929e-06\n",
      "step: 62860 train: 0.13694453239440918 elapsed, loss: 1.769976e-06\n",
      "step: 62870 train: 0.12463593482971191 elapsed, loss: 3.7527075e-06\n",
      "step: 62880 train: 0.13418316841125488 elapsed, loss: 2.5294657e-06\n",
      "step: 62890 train: 0.12220120429992676 elapsed, loss: 3.298269e-06\n",
      "step: 62900 train: 0.1270003318786621 elapsed, loss: 2.5732393e-06\n",
      "step: 62910 train: 0.12896108627319336 elapsed, loss: 0.003398751\n",
      "step: 62920 train: 0.14305472373962402 elapsed, loss: 3.528041e-05\n",
      "step: 62930 train: 0.13788247108459473 elapsed, loss: 4.0318824e-05\n",
      "step: 62940 train: 0.13350963592529297 elapsed, loss: 2.5369856e-05\n",
      "step: 62950 train: 0.12708353996276855 elapsed, loss: 1.649708e-05\n",
      "step: 62960 train: 0.12006950378417969 elapsed, loss: 1.5420817e-05\n",
      "step: 62970 train: 0.1327662467956543 elapsed, loss: 8.571798e-06\n",
      "step: 62980 train: 0.12654805183410645 elapsed, loss: 6.6393213e-06\n",
      "step: 62990 train: 0.13016462326049805 elapsed, loss: 9.19834e-06\n",
      "step: 63000 train: 0.12972736358642578 elapsed, loss: 4.641217e-06\n",
      "step: 63010 train: 0.12350845336914062 elapsed, loss: 5.087326e-06\n",
      "step: 63020 train: 0.13732409477233887 elapsed, loss: 2.6863922e-06\n",
      "step: 63030 train: 0.12572121620178223 elapsed, loss: 3.0668334e-06\n",
      "step: 63040 train: 0.14014840126037598 elapsed, loss: 2.2728877e-06\n",
      "step: 63050 train: 0.12575173377990723 elapsed, loss: 3.0705628e-06\n",
      "step: 63060 train: 0.12569618225097656 elapsed, loss: 2.7376166e-06\n",
      "step: 63070 train: 0.11912918090820312 elapsed, loss: 2.7553115e-06\n",
      "step: 63080 train: 0.12233185768127441 elapsed, loss: 2.7739284e-06\n",
      "step: 63090 train: 0.12669134140014648 elapsed, loss: 3.0998963e-06\n",
      "step: 63100 train: 0.12305808067321777 elapsed, loss: 3.0775352e-06\n",
      "step: 63110 train: 0.12816190719604492 elapsed, loss: 3.1660213e-06\n",
      "step: 63120 train: 0.1331782341003418 elapsed, loss: 1.9576364e-06\n",
      "step: 63130 train: 0.1251225471496582 elapsed, loss: 3.0393608e-06\n",
      "step: 63140 train: 0.1253213882446289 elapsed, loss: 1.893375e-06\n",
      "step: 63150 train: 0.1353142261505127 elapsed, loss: 4.8263173e-06\n",
      "step: 63160 train: 0.131317138671875 elapsed, loss: 2.7818478e-06\n",
      "step: 63170 train: 0.12683343887329102 elapsed, loss: 2.1154963e-06\n",
      "step: 63180 train: 0.1271648406982422 elapsed, loss: 2.7092115e-06\n",
      "step: 63190 train: 0.13754510879516602 elapsed, loss: 1.410021e-06\n",
      "step: 63200 train: 0.1218869686126709 elapsed, loss: 2.8829027e-06\n",
      "step: 63210 train: 0.1339101791381836 elapsed, loss: 1.6805693e-06\n",
      "step: 63220 train: 0.13123059272766113 elapsed, loss: 1.8458773e-06\n",
      "step: 63230 train: 0.12999892234802246 elapsed, loss: 2.1709075e-06\n",
      "step: 63240 train: 0.1301429271697998 elapsed, loss: 3.0579884e-06\n",
      "step: 63250 train: 0.11424803733825684 elapsed, loss: 3.2894222e-06\n",
      "step: 63260 train: 0.12576532363891602 elapsed, loss: 3.800229e-06\n",
      "step: 63270 train: 0.12799692153930664 elapsed, loss: 1.907811e-06\n",
      "step: 63280 train: 0.12009215354919434 elapsed, loss: 2.5764982e-06\n",
      "step: 63290 train: 0.1270284652709961 elapsed, loss: 3.6707966e-06\n",
      "step: 63300 train: 0.13067960739135742 elapsed, loss: 2.2095592e-06\n",
      "step: 63310 train: 0.1214609146118164 elapsed, loss: 2.6072323e-06\n",
      "step: 63320 train: 0.13365459442138672 elapsed, loss: 2.2365662e-06\n",
      "step: 63330 train: 0.1293492317199707 elapsed, loss: 3.4165437e-06\n",
      "step: 63340 train: 0.13456201553344727 elapsed, loss: 3.13389e-06\n",
      "step: 63350 train: 0.12653613090515137 elapsed, loss: 3.3080082e-06\n",
      "step: 63360 train: 0.13137555122375488 elapsed, loss: 2.4088613e-06\n",
      "step: 63370 train: 0.136000394821167 elapsed, loss: 3.7040363e-06\n",
      "step: 63380 train: 0.1280820369720459 elapsed, loss: 3.1664827e-06\n",
      "step: 63390 train: 0.12346601486206055 elapsed, loss: 4.1811572e-06\n",
      "step: 63400 train: 0.13291263580322266 elapsed, loss: 2.4246924e-06\n",
      "step: 63410 train: 0.1303083896636963 elapsed, loss: 2.8582215e-06\n",
      "step: 63420 train: 0.13316082954406738 elapsed, loss: 1.8062974e-06\n",
      "step: 63430 train: 0.11583352088928223 elapsed, loss: 3.2246944e-06\n",
      "step: 63440 train: 0.12357807159423828 elapsed, loss: 2.3767302e-06\n",
      "step: 63450 train: 0.12735676765441895 elapsed, loss: 2.7669487e-06\n",
      "step: 63460 train: 0.12606334686279297 elapsed, loss: 2.5941936e-06\n",
      "step: 63470 train: 0.12897396087646484 elapsed, loss: 4.91886e-06\n",
      "step: 63480 train: 0.13744282722473145 elapsed, loss: 1.7937248e-06\n",
      "step: 63490 train: 0.13306045532226562 elapsed, loss: 2.2551935e-06\n",
      "step: 63500 train: 0.12520408630371094 elapsed, loss: 2.8423904e-06\n",
      "step: 63510 train: 0.1202538013458252 elapsed, loss: 3.0775461e-06\n",
      "step: 63520 train: 0.12558960914611816 elapsed, loss: 3.1175907e-06\n",
      "step: 63530 train: 0.14614510536193848 elapsed, loss: 2.0167736e-06\n",
      "step: 63540 train: 0.12652158737182617 elapsed, loss: 2.3883708e-06\n",
      "step: 63550 train: 0.12832856178283691 elapsed, loss: 1.8961699e-06\n",
      "step: 63560 train: 0.13739323616027832 elapsed, loss: 1.8998949e-06\n",
      "step: 63570 train: 0.14350652694702148 elapsed, loss: 0.00031857286\n",
      "step: 63580 train: 0.12437319755554199 elapsed, loss: 7.514136e-05\n",
      "step: 63590 train: 0.12860941886901855 elapsed, loss: 5.8918155e-05\n",
      "step: 63600 train: 0.12987971305847168 elapsed, loss: 1.6661723e-05\n",
      "step: 63610 train: 0.12230062484741211 elapsed, loss: 1.8985915e-05\n",
      "step: 63620 train: 0.12632322311401367 elapsed, loss: 1.4323011e-05\n",
      "step: 63630 train: 0.14106106758117676 elapsed, loss: 0.00012760569\n",
      "step: 63640 train: 0.1411139965057373 elapsed, loss: 4.2016254e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 63650 train: 0.13302302360534668 elapsed, loss: 8.5928084e-05\n",
      "step: 63660 train: 0.13012003898620605 elapsed, loss: 2.765618e-05\n",
      "step: 63670 train: 0.128021240234375 elapsed, loss: 1.17376685e-05\n",
      "step: 63680 train: 0.12046360969543457 elapsed, loss: 1.8836496e-05\n",
      "step: 63690 train: 0.12668538093566895 elapsed, loss: 1.5321153e-05\n",
      "step: 63700 train: 0.14278078079223633 elapsed, loss: 6.205347e-06\n",
      "step: 63710 train: 0.12492823600769043 elapsed, loss: 5.1562406e-06\n",
      "step: 63720 train: 0.12291693687438965 elapsed, loss: 5.902684e-06\n",
      "step: 63730 train: 0.13130426406860352 elapsed, loss: 3.6866193e-06\n",
      "step: 63740 train: 0.12980890274047852 elapsed, loss: 3.3564643e-06\n",
      "step: 63750 train: 0.13105487823486328 elapsed, loss: 3.4617135e-06\n",
      "step: 63760 train: 0.14055991172790527 elapsed, loss: 2.229581e-06\n",
      "step: 63770 train: 0.13665175437927246 elapsed, loss: 1.6782408e-06\n",
      "step: 63780 train: 0.12509655952453613 elapsed, loss: 2.0922112e-06\n",
      "step: 63790 train: 0.12562775611877441 elapsed, loss: 2.1499475e-06\n",
      "step: 63800 train: 0.1196742057800293 elapsed, loss: 2.3236448e-06\n",
      "step: 63810 train: 0.12175154685974121 elapsed, loss: 2.1867409e-06\n",
      "step: 63820 train: 0.13055753707885742 elapsed, loss: 1.5990787e-06\n",
      "step: 63830 train: 0.12381601333618164 elapsed, loss: 2.090816e-06\n",
      "step: 63840 train: 0.1316840648651123 elapsed, loss: 1.983247e-06\n",
      "step: 63850 train: 0.13603687286376953 elapsed, loss: 1.4761437e-06\n",
      "step: 63860 train: 0.13398027420043945 elapsed, loss: 1.9380789e-06\n",
      "step: 63870 train: 0.1304020881652832 elapsed, loss: 2.5117704e-06\n",
      "step: 63880 train: 0.1341872215270996 elapsed, loss: 2.0060656e-06\n",
      "step: 63890 train: 0.12723875045776367 elapsed, loss: 1.9320241e-06\n",
      "step: 63900 train: 0.13176536560058594 elapsed, loss: 1.925504e-06\n",
      "step: 63910 train: 0.12388253211975098 elapsed, loss: 1.5613606e-06\n",
      "step: 63920 train: 0.1213541030883789 elapsed, loss: 1.9106055e-06\n",
      "step: 63930 train: 0.12780141830444336 elapsed, loss: 1.5492529e-06\n",
      "step: 63940 train: 0.12322449684143066 elapsed, loss: 1.6349345e-06\n",
      "step: 63950 train: 0.12218260765075684 elapsed, loss: 2.2221282e-06\n",
      "step: 63960 train: 0.12920784950256348 elapsed, loss: 1.934353e-06\n",
      "step: 63970 train: 0.1314542293548584 elapsed, loss: 1.8184046e-06\n",
      "step: 63980 train: 0.13491106033325195 elapsed, loss: 2.1960548e-06\n",
      "step: 63990 train: 0.13209056854248047 elapsed, loss: 3.0290919e-06\n",
      "step: 64000 train: 0.126906156539917 elapsed, loss: 2.6803405e-06\n",
      "step: 64010 train: 0.1260814666748047 elapsed, loss: 2.1890696e-06\n",
      "step: 64020 train: 0.1386570930480957 elapsed, loss: 1.5096716e-06\n",
      "step: 64030 train: 0.1274564266204834 elapsed, loss: 2.1546095e-06\n",
      "step: 64040 train: 0.12286877632141113 elapsed, loss: 2.75624e-06\n",
      "step: 64050 train: 0.132843017578125 elapsed, loss: 1.934353e-06\n",
      "step: 64060 train: 0.12322187423706055 elapsed, loss: 2.7064175e-06\n",
      "step: 64070 train: 0.13281774520874023 elapsed, loss: 3.293142e-06\n",
      "step: 64080 train: 0.13109564781188965 elapsed, loss: 1.9310942e-06\n",
      "step: 64090 train: 0.12911510467529297 elapsed, loss: 1.8472742e-06\n",
      "step: 64100 train: 0.12772274017333984 elapsed, loss: 2.3897162e-06\n",
      "step: 64110 train: 0.12917304039001465 elapsed, loss: 1.8952386e-06\n",
      "step: 64120 train: 0.12467360496520996 elapsed, loss: 2.5289983e-06\n",
      "step: 64130 train: 0.1336524486541748 elapsed, loss: 1.7620598e-06\n",
      "step: 64140 train: 0.12698578834533691 elapsed, loss: 3.0412245e-06\n",
      "step: 64150 train: 0.11689639091491699 elapsed, loss: 2.7208537e-06\n",
      "step: 64160 train: 0.13444924354553223 elapsed, loss: 2.2784752e-06\n",
      "step: 64170 train: 0.1323833465576172 elapsed, loss: 2.982927e-06\n",
      "step: 64180 train: 0.14029836654663086 elapsed, loss: 1.7777806e-05\n",
      "step: 64190 train: 0.12465977668762207 elapsed, loss: 7.593443e-06\n",
      "step: 64200 train: 0.12459516525268555 elapsed, loss: 5.3532103e-06\n",
      "step: 64210 train: 0.12455964088439941 elapsed, loss: 4.9932505e-06\n",
      "step: 64220 train: 0.12167954444885254 elapsed, loss: 4.568117e-06\n",
      "step: 64230 train: 0.13984203338623047 elapsed, loss: 3.2577477e-06\n",
      "step: 64240 train: 0.128462553024292 elapsed, loss: 2.5378472e-06\n",
      "step: 64250 train: 0.1226191520690918 elapsed, loss: 2.4381968e-06\n",
      "step: 64260 train: 0.13411211967468262 elapsed, loss: 2.7655558e-06\n",
      "step: 64270 train: 0.12626051902770996 elapsed, loss: 2.6049033e-06\n",
      "step: 64280 train: 0.13083362579345703 elapsed, loss: 1.8593826e-06\n",
      "step: 64290 train: 0.1288306713104248 elapsed, loss: 2.2309782e-06\n",
      "step: 64300 train: 0.1380455493927002 elapsed, loss: 2.264041e-06\n",
      "step: 64310 train: 0.12597203254699707 elapsed, loss: 2.3287662e-06\n",
      "step: 64320 train: 0.12312436103820801 elapsed, loss: 2.4246929e-06\n",
      "step: 64330 train: 0.1277601718902588 elapsed, loss: 2.9941932e-06\n",
      "step: 64340 train: 0.1339561939239502 elapsed, loss: 1.9739346e-06\n",
      "step: 64350 train: 0.13020801544189453 elapsed, loss: 5.2497297e-05\n",
      "step: 64360 train: 0.12662172317504883 elapsed, loss: 8.7604145e-05\n",
      "step: 64370 train: 0.12833118438720703 elapsed, loss: 2.4131921e-05\n",
      "step: 64380 train: 0.13101935386657715 elapsed, loss: 1.39356835e-05\n",
      "step: 64390 train: 0.13344240188598633 elapsed, loss: 1.3564714e-05\n",
      "step: 64400 train: 0.1251223087310791 elapsed, loss: 7.6023193e-06\n",
      "step: 64410 train: 0.12463521957397461 elapsed, loss: 7.224153e-06\n",
      "step: 64420 train: 0.1350269317626953 elapsed, loss: 5.803963e-06\n",
      "step: 64430 train: 0.1329813003540039 elapsed, loss: 4.676146e-06\n",
      "step: 64440 train: 0.13556814193725586 elapsed, loss: 4.718988e-06\n",
      "step: 64450 train: 0.13467669486999512 elapsed, loss: 6.0602624e-06\n",
      "step: 64460 train: 0.13103985786437988 elapsed, loss: 3.0547285e-06\n",
      "step: 64470 train: 0.1302030086517334 elapsed, loss: 3.994785e-06\n",
      "step: 64480 train: 0.12738466262817383 elapsed, loss: 2.9448333e-06\n",
      "step: 64490 train: 0.13855457305908203 elapsed, loss: 1.7886016e-06\n",
      "step: 64500 train: 0.13759851455688477 elapsed, loss: 2.8377108e-06\n",
      "step: 64510 train: 0.1260678768157959 elapsed, loss: 2.506183e-06\n",
      "step: 64520 train: 0.11757397651672363 elapsed, loss: 2.4512344e-06\n",
      "step: 64530 train: 0.1342449188232422 elapsed, loss: 1.341103e-06\n",
      "step: 64540 train: 0.14753198623657227 elapsed, loss: 1.1874349e-06\n",
      "step: 64550 train: 0.13402438163757324 elapsed, loss: 1.8891845e-06\n",
      "step: 64560 train: 0.13587236404418945 elapsed, loss: 2.0628736e-06\n",
      "step: 64570 train: 0.12161493301391602 elapsed, loss: 2.1904655e-06\n",
      "step: 64580 train: 0.12117123603820801 elapsed, loss: 1.9660183e-06\n",
      "step: 64590 train: 0.12888216972351074 elapsed, loss: 1.5594966e-06\n",
      "step: 64600 train: 0.13450217247009277 elapsed, loss: 1.3736993e-06\n",
      "step: 64610 train: 0.1265711784362793 elapsed, loss: 2.6281843e-06\n",
      "step: 64620 train: 0.12810301780700684 elapsed, loss: 1.6880199e-06\n",
      "step: 64630 train: 0.12752866744995117 elapsed, loss: 1.8733526e-06\n",
      "step: 64640 train: 0.13059329986572266 elapsed, loss: 2.0665982e-06\n",
      "step: 64650 train: 0.13518714904785156 elapsed, loss: 3.064431e-06\n",
      "step: 64660 train: 0.1201632022857666 elapsed, loss: 2.7869758e-06\n",
      "step: 64670 train: 0.12447643280029297 elapsed, loss: 2.2696286e-06\n",
      "step: 64680 train: 0.12010812759399414 elapsed, loss: 2.4135174e-06\n",
      "step: 64690 train: 0.1366727352142334 elapsed, loss: 1.9967501e-06\n",
      "step: 64700 train: 0.12509822845458984 elapsed, loss: 1.9930271e-06\n",
      "step: 64710 train: 0.1274886131286621 elapsed, loss: 1.6926765e-06\n",
      "step: 64720 train: 0.11711621284484863 elapsed, loss: 2.5876745e-06\n",
      "step: 64730 train: 0.11671853065490723 elapsed, loss: 2.7012939e-06\n",
      "step: 64740 train: 0.12262511253356934 elapsed, loss: 2.8056004e-06\n",
      "step: 64750 train: 0.12282776832580566 elapsed, loss: 2.3366833e-06\n",
      "step: 64760 train: 0.13541626930236816 elapsed, loss: 1.7546089e-06\n",
      "step: 64770 train: 0.12234306335449219 elapsed, loss: 3.103611e-06\n",
      "step: 64780 train: 0.13846445083618164 elapsed, loss: 2.1774274e-06\n",
      "step: 64790 train: 0.12797927856445312 elapsed, loss: 2.4805736e-06\n",
      "step: 64800 train: 0.13475823402404785 elapsed, loss: 2.1811536e-06\n",
      "step: 64810 train: 0.14435672760009766 elapsed, loss: 1.7257382e-06\n",
      "step: 64820 train: 0.1289079189300537 elapsed, loss: 2.630515e-06\n",
      "step: 64830 train: 0.12905073165893555 elapsed, loss: 1.9832482e-06\n",
      "step: 64840 train: 0.13211941719055176 elapsed, loss: 2.1769583e-06\n",
      "step: 64850 train: 0.1289825439453125 elapsed, loss: 2.7525148e-06\n",
      "step: 64860 train: 0.13228416442871094 elapsed, loss: 2.8535642e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 64870 train: 0.13511133193969727 elapsed, loss: 2.260782e-06\n",
      "step: 64880 train: 0.13061738014221191 elapsed, loss: 2.233307e-06\n",
      "step: 64890 train: 0.12893176078796387 elapsed, loss: 2.6063012e-06\n",
      "step: 64900 train: 0.12430453300476074 elapsed, loss: 2.3818534e-06\n",
      "step: 64910 train: 0.13103032112121582 elapsed, loss: 2.2491395e-06\n",
      "step: 64920 train: 0.12136030197143555 elapsed, loss: 4.1154963e-06\n",
      "step: 64930 train: 0.12669944763183594 elapsed, loss: 2.5331315e-06\n",
      "step: 64940 train: 0.12671351432800293 elapsed, loss: 4.988594e-06\n",
      "step: 64950 train: 0.13657832145690918 elapsed, loss: 2.2686977e-06\n",
      "step: 64960 train: 0.12486433982849121 elapsed, loss: 2.509908e-06\n",
      "step: 64970 train: 0.1235816478729248 elapsed, loss: 3.179414e-05\n",
      "step: 64980 train: 0.12522268295288086 elapsed, loss: 6.264085e-06\n",
      "step: 64990 train: 0.12683987617492676 elapsed, loss: 4.237033e-06\n",
      "step: 65000 train: 0.12353134155273438 elapsed, loss: 3.2158443e-06\n",
      "step: 65010 train: 0.13510346412658691 elapsed, loss: 1.9837144e-06\n",
      "step: 65020 train: 0.12763667106628418 elapsed, loss: 3.1343575e-06\n",
      "step: 65030 train: 0.12778162956237793 elapsed, loss: 4.2309243e-06\n",
      "step: 65040 train: 0.13922333717346191 elapsed, loss: 2.948091e-06\n",
      "step: 65050 train: 0.13004398345947266 elapsed, loss: 2.4749838e-06\n",
      "step: 65060 train: 0.1347181797027588 elapsed, loss: 2.0423872e-06\n",
      "step: 65070 train: 0.12771248817443848 elapsed, loss: 4.453552e-06\n",
      "step: 65080 train: 0.1384732723236084 elapsed, loss: 2.1131482e-06\n",
      "step: 65090 train: 0.13072800636291504 elapsed, loss: 1.8612441e-06\n",
      "step: 65100 train: 0.13308405876159668 elapsed, loss: 1.7797549e-06\n",
      "step: 65110 train: 0.13117241859436035 elapsed, loss: 2.0866223e-06\n",
      "step: 65120 train: 0.1357872486114502 elapsed, loss: 2.729235e-06\n",
      "step: 65130 train: 0.12701964378356934 elapsed, loss: 4.2029606e-06\n",
      "step: 65140 train: 0.12978744506835938 elapsed, loss: 2.200711e-06\n",
      "step: 65150 train: 0.12468504905700684 elapsed, loss: 3.414218e-06\n",
      "step: 65160 train: 0.12666082382202148 elapsed, loss: 2.6090938e-06\n",
      "step: 65170 train: 0.1286609172821045 elapsed, loss: 3.1995492e-06\n",
      "step: 65180 train: 0.12750983238220215 elapsed, loss: 1.8742842e-06\n",
      "step: 65190 train: 0.13409996032714844 elapsed, loss: 1.8668333e-06\n",
      "step: 65200 train: 0.12833762168884277 elapsed, loss: 2.406527e-06\n",
      "step: 65210 train: 0.14302873611450195 elapsed, loss: 1.7317916e-06\n",
      "step: 65220 train: 0.12066173553466797 elapsed, loss: 3.1995498e-06\n",
      "step: 65230 train: 0.1444385051727295 elapsed, loss: 1.0735139e-05\n",
      "step: 65240 train: 0.13893795013427734 elapsed, loss: 6.469313e-05\n",
      "step: 65250 train: 0.12871003150939941 elapsed, loss: 0.0004167961\n",
      "step: 65260 train: 0.1371917724609375 elapsed, loss: 8.212902e-05\n",
      "step: 65270 train: 0.13460874557495117 elapsed, loss: 0.00014661855\n",
      "step: 65280 train: 0.12580609321594238 elapsed, loss: 4.4319495e-05\n",
      "step: 65290 train: 0.12976932525634766 elapsed, loss: 2.3208566e-05\n",
      "step: 65300 train: 0.12195563316345215 elapsed, loss: 1.8840019e-05\n",
      "step: 65310 train: 0.1292438507080078 elapsed, loss: 2.1109161e-05\n",
      "step: 65320 train: 0.12423300743103027 elapsed, loss: 1.3185769e-05\n",
      "step: 65330 train: 0.1401503086090088 elapsed, loss: 6.880091e-06\n",
      "step: 65340 train: 0.12865471839904785 elapsed, loss: 7.385321e-06\n",
      "step: 65350 train: 0.14436769485473633 elapsed, loss: 5.403962e-06\n",
      "step: 65360 train: 0.128068208694458 elapsed, loss: 4.381385e-06\n",
      "step: 65370 train: 0.1336987018585205 elapsed, loss: 4.479585e-06\n",
      "step: 65380 train: 0.14464688301086426 elapsed, loss: 2.6817363e-06\n",
      "step: 65390 train: 0.13574504852294922 elapsed, loss: 2.983949e-06\n",
      "step: 65400 train: 0.12004828453063965 elapsed, loss: 2.7385481e-06\n",
      "step: 65410 train: 0.12552142143249512 elapsed, loss: 1.927835e-06\n",
      "step: 65420 train: 0.1352994441986084 elapsed, loss: 7.779216e-06\n",
      "step: 65430 train: 0.13651227951049805 elapsed, loss: 2.7110736e-06\n",
      "step: 65440 train: 0.13272452354431152 elapsed, loss: 1.7234083e-06\n",
      "step: 65450 train: 0.12514662742614746 elapsed, loss: 2.6337748e-06\n",
      "step: 65460 train: 0.1265571117401123 elapsed, loss: 2.1690466e-06\n",
      "step: 65470 train: 0.13136839866638184 elapsed, loss: 1.6824322e-06\n",
      "step: 65480 train: 0.12227201461791992 elapsed, loss: 2.2882525e-06\n",
      "step: 65490 train: 0.13499927520751953 elapsed, loss: 1.426784e-06\n",
      "step: 65500 train: 0.1262521743774414 elapsed, loss: 2.5061806e-06\n",
      "step: 65510 train: 0.12923383712768555 elapsed, loss: 2.1043193e-06\n",
      "step: 65520 train: 0.12452530860900879 elapsed, loss: 1.9022239e-06\n",
      "step: 65530 train: 0.13155126571655273 elapsed, loss: 1.73412e-06\n",
      "step: 65540 train: 0.13304638862609863 elapsed, loss: 1.5599633e-06\n",
      "step: 65550 train: 0.1283421516418457 elapsed, loss: 2.3841808e-06\n",
      "step: 65560 train: 0.13712143898010254 elapsed, loss: 1.932026e-06\n",
      "step: 65570 train: 0.13147926330566406 elapsed, loss: 2.3916075e-06\n",
      "step: 65580 train: 0.1329491138458252 elapsed, loss: 1.8570545e-06\n",
      "step: 65590 train: 0.1395418643951416 elapsed, loss: 1.634002e-06\n",
      "step: 65600 train: 0.12424230575561523 elapsed, loss: 3.1472541e-06\n",
      "step: 65610 train: 0.12077546119689941 elapsed, loss: 2.5699799e-06\n",
      "step: 65620 train: 0.12750744819641113 elapsed, loss: 2.0652035e-06\n",
      "step: 65630 train: 0.1313188076019287 elapsed, loss: 2.65472e-06\n",
      "step: 65640 train: 0.14565110206604004 elapsed, loss: 1.4649685e-06\n",
      "step: 65650 train: 0.12698054313659668 elapsed, loss: 2.1629914e-06\n",
      "step: 65660 train: 0.13525032997131348 elapsed, loss: 3.7024615e-06\n",
      "step: 65670 train: 0.13943934440612793 elapsed, loss: 2.381371e-06\n",
      "step: 65680 train: 0.13916254043579102 elapsed, loss: 1.834237e-06\n",
      "step: 65690 train: 0.12385725975036621 elapsed, loss: 2.4288822e-06\n",
      "step: 65700 train: 0.12781262397766113 elapsed, loss: 2.6249268e-06\n",
      "step: 65710 train: 0.13213849067687988 elapsed, loss: 2.0139796e-06\n",
      "step: 65720 train: 0.1206827163696289 elapsed, loss: 4.1466974e-06\n",
      "step: 65730 train: 0.12979912757873535 elapsed, loss: 2.6700968e-06\n",
      "step: 65740 train: 0.13047313690185547 elapsed, loss: 1.9771949e-06\n",
      "step: 65750 train: 0.12733888626098633 elapsed, loss: 2.571376e-06\n",
      "step: 65760 train: 0.13432693481445312 elapsed, loss: 2.5405948e-06\n",
      "step: 65770 train: 0.13559627532958984 elapsed, loss: 2.0624102e-06\n",
      "step: 65780 train: 0.1313004493713379 elapsed, loss: 2.1210835e-06\n",
      "step: 65790 train: 0.1325225830078125 elapsed, loss: 2.2193358e-06\n",
      "step: 65800 train: 0.12913298606872559 elapsed, loss: 3.3192036e-06\n",
      "step: 65810 train: 0.11781668663024902 elapsed, loss: 4.9490154e-06\n",
      "step: 65820 train: 0.12533855438232422 elapsed, loss: 3.4659051e-06\n",
      "step: 65830 train: 0.1252908706665039 elapsed, loss: 2.8680006e-06\n",
      "step: 65840 train: 0.12751507759094238 elapsed, loss: 2.8735699e-06\n",
      "step: 65850 train: 0.12979483604431152 elapsed, loss: 4.4842945e-06\n",
      "step: 65860 train: 0.13562989234924316 elapsed, loss: 2.8675295e-06\n",
      "step: 65870 train: 0.13718819618225098 elapsed, loss: 2.4060676e-06\n",
      "step: 65880 train: 0.1334702968597412 elapsed, loss: 2.108045e-06\n",
      "step: 65890 train: 0.12408041954040527 elapsed, loss: 3.4691634e-06\n",
      "step: 65900 train: 0.1394510269165039 elapsed, loss: 2.2645067e-06\n",
      "step: 65910 train: 0.13546538352966309 elapsed, loss: 2.9434282e-06\n",
      "step: 65920 train: 0.12711286544799805 elapsed, loss: 4.0335435e-06\n",
      "step: 65930 train: 0.12864971160888672 elapsed, loss: 4.193701e-06\n",
      "step: 65940 train: 0.14467406272888184 elapsed, loss: 1.9776598e-06\n",
      "step: 65950 train: 0.13088417053222656 elapsed, loss: 2.1988485e-06\n",
      "step: 65960 train: 0.11669230461120605 elapsed, loss: 3.773238e-06\n",
      "step: 65970 train: 0.13123631477355957 elapsed, loss: 2.6528216e-05\n",
      "step: 65980 train: 0.14714598655700684 elapsed, loss: 1.0553627e-05\n",
      "step: 65990 train: 0.1324939727783203 elapsed, loss: 5.998615e-06\n",
      "step: 66000 train: 0.1291501522064209 elapsed, loss: 4.2630845e-06\n",
      "step: 66010 train: 0.13321447372436523 elapsed, loss: 2.6645064e-06\n",
      "step: 66020 train: 0.12933874130249023 elapsed, loss: 5.8330547e-06\n",
      "step: 66030 train: 0.12331938743591309 elapsed, loss: 4.114564e-06\n",
      "step: 66040 train: 0.1317737102508545 elapsed, loss: 1.979056e-06\n",
      "step: 66050 train: 0.1220705509185791 elapsed, loss: 3.3639283e-06\n",
      "step: 66060 train: 0.12695980072021484 elapsed, loss: 2.5574045e-06\n",
      "step: 66070 train: 0.13309240341186523 elapsed, loss: 2.326904e-06\n",
      "step: 66080 train: 0.13110041618347168 elapsed, loss: 2.021432e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 66090 train: 0.12781834602355957 elapsed, loss: 3.1729937e-06\n",
      "step: 66100 train: 0.11928105354309082 elapsed, loss: 2.3534471e-06\n",
      "step: 66110 train: 0.13561701774597168 elapsed, loss: 1.6367965e-06\n",
      "step: 66120 train: 0.13724756240844727 elapsed, loss: 2.760896e-06\n",
      "step: 66130 train: 0.13458871841430664 elapsed, loss: 2.9974517e-06\n",
      "step: 66140 train: 0.12434983253479004 elapsed, loss: 2.7799924e-06\n",
      "step: 66150 train: 0.13533854484558105 elapsed, loss: 1.7438988e-06\n",
      "step: 66160 train: 0.14223885536193848 elapsed, loss: 1.8645048e-06\n",
      "step: 66170 train: 0.12560296058654785 elapsed, loss: 4.270565e-06\n",
      "step: 66180 train: 0.14802312850952148 elapsed, loss: 2.0232953e-06\n",
      "step: 66190 train: 0.12483382225036621 elapsed, loss: 2.4824344e-06\n",
      "step: 66200 train: 0.13645625114440918 elapsed, loss: 1.9208496e-06\n",
      "step: 66210 train: 0.1360483169555664 elapsed, loss: 1.9892982e-06\n",
      "step: 66220 train: 0.1262669563293457 elapsed, loss: 3.2316807e-06\n",
      "step: 66230 train: 0.1237339973449707 elapsed, loss: 2.5764982e-06\n",
      "step: 66240 train: 0.1256866455078125 elapsed, loss: 3.0756742e-06\n",
      "step: 66250 train: 0.1284017562866211 elapsed, loss: 2.2132836e-06\n",
      "step: 66260 train: 0.13437271118164062 elapsed, loss: 2.2761474e-06\n",
      "step: 66270 train: 0.12822365760803223 elapsed, loss: 1.8319089e-06\n",
      "step: 66280 train: 0.12130928039550781 elapsed, loss: 2.9583393e-06\n",
      "step: 66290 train: 0.12638616561889648 elapsed, loss: 4.939708e-06\n",
      "step: 66300 train: 0.13060784339904785 elapsed, loss: 2.8293443e-06\n",
      "step: 66310 train: 0.12896966934204102 elapsed, loss: 2.5681056e-06\n",
      "step: 66320 train: 0.1373744010925293 elapsed, loss: 2.2505346e-06\n",
      "step: 66330 train: 0.1304912567138672 elapsed, loss: 2.7045553e-06\n",
      "step: 66340 train: 0.12897396087646484 elapsed, loss: 2.211887e-06\n",
      "step: 66350 train: 0.11792325973510742 elapsed, loss: 3.5301691e-06\n",
      "step: 66360 train: 0.13036680221557617 elapsed, loss: 1.221087e-05\n",
      "step: 66370 train: 0.12595772743225098 elapsed, loss: 3.9250453e-06\n",
      "step: 66380 train: 0.13149404525756836 elapsed, loss: 2.5047857e-06\n",
      "step: 66390 train: 0.11902952194213867 elapsed, loss: 4.0256173e-06\n",
      "step: 66400 train: 0.13648033142089844 elapsed, loss: 2.1588016e-06\n",
      "step: 66410 train: 0.13526487350463867 elapsed, loss: 2.2412237e-06\n",
      "step: 66420 train: 0.12080192565917969 elapsed, loss: 3.8244643e-06\n",
      "step: 66430 train: 0.13255882263183594 elapsed, loss: 2.5238783e-06\n",
      "step: 66440 train: 0.13309049606323242 elapsed, loss: 1.8700921e-06\n",
      "step: 66450 train: 0.12338447570800781 elapsed, loss: 4.173707e-06\n",
      "step: 66460 train: 0.12366628646850586 elapsed, loss: 2.8945428e-06\n",
      "step: 66470 train: 0.12044548988342285 elapsed, loss: 3.0295857e-06\n",
      "step: 66480 train: 0.12558341026306152 elapsed, loss: 0.003231118\n",
      "step: 66490 train: 0.13206052780151367 elapsed, loss: 0.00013792014\n",
      "step: 66500 train: 0.12726068496704102 elapsed, loss: 2.1010557e-05\n",
      "step: 66510 train: 0.14130425453186035 elapsed, loss: 2.4916284e-05\n",
      "step: 66520 train: 0.12251138687133789 elapsed, loss: 1.9109215e-05\n",
      "step: 66530 train: 0.12117218971252441 elapsed, loss: 1.6163147e-05\n",
      "step: 66540 train: 0.12723565101623535 elapsed, loss: 1.26804225e-05\n",
      "step: 66550 train: 0.12024593353271484 elapsed, loss: 9.817882e-06\n",
      "step: 66560 train: 0.12554144859313965 elapsed, loss: 6.7445776e-06\n",
      "step: 66570 train: 0.12822747230529785 elapsed, loss: 4.5858114e-06\n",
      "step: 66580 train: 0.11788010597229004 elapsed, loss: 6.4889437e-06\n",
      "step: 66590 train: 0.13587403297424316 elapsed, loss: 3.5343571e-06\n",
      "step: 66600 train: 0.13637924194335938 elapsed, loss: 4.182539e-06\n",
      "step: 66610 train: 0.14483237266540527 elapsed, loss: 2.6347043e-06\n",
      "step: 66620 train: 0.1344599723815918 elapsed, loss: 2.7087442e-06\n",
      "step: 66630 train: 0.12218284606933594 elapsed, loss: 3.2824364e-06\n",
      "step: 66640 train: 0.13463449478149414 elapsed, loss: 1.7285317e-06\n",
      "step: 66650 train: 0.12525153160095215 elapsed, loss: 2.6435432e-06\n",
      "step: 66660 train: 0.12591028213500977 elapsed, loss: 2.2454144e-06\n",
      "step: 66670 train: 0.12376117706298828 elapsed, loss: 1.0122844e-05\n",
      "step: 66680 train: 0.12913966178894043 elapsed, loss: 4.2127936e-06\n",
      "step: 66690 train: 0.1245429515838623 elapsed, loss: 3.2344715e-06\n",
      "step: 66700 train: 0.12332439422607422 elapsed, loss: 3.2968721e-06\n",
      "step: 66710 train: 0.13542938232421875 elapsed, loss: 2.3012935e-06\n",
      "step: 66720 train: 0.1295030117034912 elapsed, loss: 2.7930296e-06\n",
      "step: 66730 train: 0.1286325454711914 elapsed, loss: 2.558338e-06\n",
      "step: 66740 train: 0.14168071746826172 elapsed, loss: 2.291005e-06\n",
      "step: 66750 train: 0.13566064834594727 elapsed, loss: 1.5869705e-06\n",
      "step: 66760 train: 0.13559365272521973 elapsed, loss: 2.2081606e-06\n",
      "step: 66770 train: 0.11826038360595703 elapsed, loss: 2.0540432e-05\n",
      "step: 66780 train: 0.13376617431640625 elapsed, loss: 1.58745e-05\n",
      "step: 66790 train: 0.12715864181518555 elapsed, loss: 1.148116e-05\n",
      "step: 66800 train: 0.12898612022399902 elapsed, loss: 1.0802262e-05\n",
      "step: 66810 train: 0.13016486167907715 elapsed, loss: 1.0401712e-05\n",
      "step: 66820 train: 0.13254618644714355 elapsed, loss: 6.1932483e-06\n",
      "step: 66830 train: 0.1372840404510498 elapsed, loss: 9.109201e-06\n",
      "step: 66840 train: 0.1359577178955078 elapsed, loss: 3.588836e-06\n",
      "step: 66850 train: 0.12735891342163086 elapsed, loss: 4.130393e-06\n",
      "step: 66860 train: 0.1262507438659668 elapsed, loss: 3.603273e-06\n",
      "step: 66870 train: 0.13421273231506348 elapsed, loss: 2.6677553e-06\n",
      "step: 66880 train: 0.1250290870666504 elapsed, loss: 3.0291183e-06\n",
      "step: 66890 train: 0.12407946586608887 elapsed, loss: 3.1227135e-06\n",
      "step: 66900 train: 0.1310276985168457 elapsed, loss: 1.786737e-06\n",
      "step: 66910 train: 0.1373119354248047 elapsed, loss: 3.651704e-06\n",
      "step: 66920 train: 0.12744498252868652 elapsed, loss: 2.9820785e-06\n",
      "step: 66930 train: 0.12244248390197754 elapsed, loss: 2.6286489e-06\n",
      "step: 66940 train: 0.1308155059814453 elapsed, loss: 2.1839455e-06\n",
      "step: 66950 train: 0.12935233116149902 elapsed, loss: 1.905474e-06\n",
      "step: 66960 train: 0.1213841438293457 elapsed, loss: 3.0356327e-06\n",
      "step: 66970 train: 0.1306467056274414 elapsed, loss: 1.6675264e-06\n",
      "step: 66980 train: 0.13125348091125488 elapsed, loss: 1.8486721e-06\n",
      "step: 66990 train: 0.14185166358947754 elapsed, loss: 1.3066441e-06\n",
      "step: 67000 train: 0.1270594596862793 elapsed, loss: 1.8742827e-06\n",
      "step: 67010 train: 0.13418984413146973 elapsed, loss: 1.5096707e-06\n",
      "step: 67020 train: 0.12783241271972656 elapsed, loss: 1.6158427e-06\n",
      "step: 67030 train: 0.12468695640563965 elapsed, loss: 1.9217807e-06\n",
      "step: 67040 train: 0.12512564659118652 elapsed, loss: 1.5543756e-06\n",
      "step: 67050 train: 0.12784409523010254 elapsed, loss: 1.5897631e-06\n",
      "step: 67060 train: 0.12646722793579102 elapsed, loss: 2.2463455e-06\n",
      "step: 67070 train: 0.12867498397827148 elapsed, loss: 1.7187526e-06\n",
      "step: 67080 train: 0.1331925392150879 elapsed, loss: 1.8952384e-06\n",
      "step: 67090 train: 0.13006162643432617 elapsed, loss: 1.626087e-06\n",
      "step: 67100 train: 0.12943649291992188 elapsed, loss: 1.3969823e-06\n",
      "step: 67110 train: 0.1422595977783203 elapsed, loss: 1.5827808e-06\n",
      "step: 67120 train: 0.1440420150756836 elapsed, loss: 1.5636876e-06\n",
      "step: 67130 train: 0.13547396659851074 elapsed, loss: 1.8649694e-06\n",
      "step: 67140 train: 0.12911176681518555 elapsed, loss: 2.0102557e-06\n",
      "step: 67150 train: 0.14022469520568848 elapsed, loss: 1.6354004e-06\n",
      "step: 67160 train: 0.12003040313720703 elapsed, loss: 4.012566e-06\n",
      "step: 67170 train: 0.13207316398620605 elapsed, loss: 2.3012915e-06\n",
      "step: 67180 train: 0.12459397315979004 elapsed, loss: 2.0088598e-06\n",
      "step: 67190 train: 0.12789225578308105 elapsed, loss: 1.8561226e-06\n",
      "step: 67200 train: 0.12401819229125977 elapsed, loss: 2.3078137e-06\n",
      "step: 67210 train: 0.1294863224029541 elapsed, loss: 2.1452965e-06\n",
      "step: 67220 train: 0.1371476650238037 elapsed, loss: 1.6777751e-06\n",
      "step: 67230 train: 0.12788677215576172 elapsed, loss: 1.8239928e-06\n",
      "step: 67240 train: 0.12450695037841797 elapsed, loss: 2.4875574e-06\n",
      "step: 67250 train: 0.12047147750854492 elapsed, loss: 4.044692e-06\n",
      "step: 67260 train: 0.1334686279296875 elapsed, loss: 1.9008246e-06\n",
      "step: 67270 train: 0.12561535835266113 elapsed, loss: 4.6057467e-06\n",
      "step: 67280 train: 0.12305378913879395 elapsed, loss: 3.0454164e-06\n",
      "step: 67290 train: 0.1233522891998291 elapsed, loss: 2.834935e-06\n",
      "step: 67300 train: 0.1316084861755371 elapsed, loss: 3.9948936e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 67310 train: 0.14362335205078125 elapsed, loss: 1.5795206e-06\n",
      "step: 67320 train: 0.13645100593566895 elapsed, loss: 1.812351e-06\n",
      "step: 67330 train: 0.14539718627929688 elapsed, loss: 1.3373779e-06\n",
      "step: 67340 train: 0.12332797050476074 elapsed, loss: 2.539711e-06\n",
      "step: 67350 train: 0.13048458099365234 elapsed, loss: 2.7180388e-06\n",
      "step: 67360 train: 0.115753173828125 elapsed, loss: 3.81701e-06\n",
      "step: 67370 train: 0.13005542755126953 elapsed, loss: 2.2416891e-06\n",
      "step: 67380 train: 0.13411211967468262 elapsed, loss: 2.5820823e-06\n",
      "step: 67390 train: 0.13501954078674316 elapsed, loss: 3.539871e-06\n",
      "step: 67400 train: 0.14011240005493164 elapsed, loss: 2.1918618e-06\n",
      "step: 67410 train: 0.13576889038085938 elapsed, loss: 2.1527408e-06\n",
      "step: 67420 train: 0.12408089637756348 elapsed, loss: 2.4982685e-06\n",
      "step: 67430 train: 0.13324379920959473 elapsed, loss: 4.1000117e-06\n",
      "step: 67440 train: 0.13152265548706055 elapsed, loss: 3.1948862e-06\n",
      "step: 67450 train: 0.13584423065185547 elapsed, loss: 2.058685e-06\n",
      "step: 67460 train: 0.12363266944885254 elapsed, loss: 3.0691667e-06\n",
      "step: 67470 train: 0.1371781826019287 elapsed, loss: 2.786044e-06\n",
      "step: 67480 train: 0.13347339630126953 elapsed, loss: 3.1166612e-06\n",
      "step: 67490 train: 0.13139748573303223 elapsed, loss: 2.2719569e-06\n",
      "step: 67500 train: 0.12475347518920898 elapsed, loss: 4.5210695e-06\n",
      "step: 67510 train: 0.12464785575866699 elapsed, loss: 3.0803399e-06\n",
      "step: 67520 train: 0.12721514701843262 elapsed, loss: 2.8870936e-06\n",
      "step: 67530 train: 0.12969040870666504 elapsed, loss: 3.0784795e-06\n",
      "step: 67540 train: 0.14001131057739258 elapsed, loss: 2.1671842e-06\n",
      "step: 67550 train: 0.11784839630126953 elapsed, loss: 3.405365e-06\n",
      "step: 67560 train: 0.12800049781799316 elapsed, loss: 2.4316787e-06\n",
      "step: 67570 train: 0.12214398384094238 elapsed, loss: 4.4773137e-06\n",
      "step: 67580 train: 0.13297390937805176 elapsed, loss: 3.0747524e-06\n",
      "step: 67590 train: 0.12130308151245117 elapsed, loss: 3.5911587e-06\n",
      "step: 67600 train: 0.11992669105529785 elapsed, loss: 2.756709e-06\n",
      "step: 67610 train: 0.13412117958068848 elapsed, loss: 2.957873e-06\n",
      "step: 67620 train: 0.12882494926452637 elapsed, loss: 4.9187515e-06\n",
      "step: 67630 train: 0.13588380813598633 elapsed, loss: 3.9092088e-06\n",
      "step: 67640 train: 0.12087655067443848 elapsed, loss: 4.5541437e-06\n",
      "step: 67650 train: 0.12359738349914551 elapsed, loss: 3.4603192e-06\n",
      "step: 67660 train: 0.1434476375579834 elapsed, loss: 1.7322573e-06\n",
      "step: 67670 train: 0.1253981590270996 elapsed, loss: 2.9508747e-06\n",
      "step: 67680 train: 0.12720370292663574 elapsed, loss: 2.2384297e-06\n",
      "step: 67690 train: 0.1315751075744629 elapsed, loss: 4.491747e-06\n",
      "step: 67700 train: 0.12766265869140625 elapsed, loss: 2.8158481e-06\n",
      "step: 67710 train: 0.1250302791595459 elapsed, loss: 2.6551947e-06\n",
      "step: 67720 train: 0.12693071365356445 elapsed, loss: 4.092194e-06\n",
      "step: 67730 train: 0.1237022876739502 elapsed, loss: 2.8493753e-06\n",
      "step: 67740 train: 0.12999463081359863 elapsed, loss: 3.3117713e-06\n",
      "step: 67750 train: 0.13582301139831543 elapsed, loss: 3.5869666e-06\n",
      "step: 67760 train: 0.12891674041748047 elapsed, loss: 2.5811544e-06\n",
      "step: 67770 train: 0.1331641674041748 elapsed, loss: 3.2139176e-06\n",
      "step: 67780 train: 0.13811540603637695 elapsed, loss: 2.72318e-06\n",
      "step: 67790 train: 0.12563753128051758 elapsed, loss: 3.2511914e-06\n",
      "step: 67800 train: 0.12530112266540527 elapsed, loss: 2.5359864e-06\n",
      "step: 67810 train: 0.12307381629943848 elapsed, loss: 2.5494905e-06\n",
      "step: 67820 train: 0.13091778755187988 elapsed, loss: 2.7259755e-06\n",
      "step: 67830 train: 0.13646483421325684 elapsed, loss: 2.5150243e-06\n",
      "step: 67840 train: 0.14107275009155273 elapsed, loss: 2.0335392e-06\n",
      "step: 67850 train: 0.12736177444458008 elapsed, loss: 3.996758e-06\n",
      "step: 67860 train: 0.12326765060424805 elapsed, loss: 3.7466973e-06\n",
      "step: 67870 train: 0.12578654289245605 elapsed, loss: 2.6151445e-06\n",
      "step: 67880 train: 0.12097048759460449 elapsed, loss: 3.933893e-06\n",
      "step: 67890 train: 0.1261439323425293 elapsed, loss: 6.7631563e-06\n",
      "step: 67900 train: 0.12444734573364258 elapsed, loss: 2.991399e-06\n",
      "step: 67910 train: 0.1279609203338623 elapsed, loss: 2.9997827e-06\n",
      "step: 67920 train: 0.12809491157531738 elapsed, loss: 3.2954767e-06\n",
      "step: 67930 train: 0.12589097023010254 elapsed, loss: 2.698957e-06\n",
      "step: 67940 train: 0.11930394172668457 elapsed, loss: 0.00046974153\n",
      "step: 67950 train: 0.130859375 elapsed, loss: 8.115726e-05\n",
      "step: 67960 train: 0.13230013847351074 elapsed, loss: 8.562938e-05\n",
      "step: 67970 train: 0.13632869720458984 elapsed, loss: 2.7193078e-05\n",
      "step: 67980 train: 0.12256217002868652 elapsed, loss: 2.6559268e-05\n",
      "step: 67990 train: 0.12911510467529297 elapsed, loss: 2.1891074e-05\n",
      "step: 68000 train: 0.13079524040222168 elapsed, loss: 1.2830647e-05\n",
      "step: 68010 train: 0.13003873825073242 elapsed, loss: 1.0593631e-05\n",
      "step: 68020 train: 0.125718355178833 elapsed, loss: 0.053255893\n",
      "step: 68030 train: 0.13363051414489746 elapsed, loss: 9.55875e-05\n",
      "step: 68040 train: 0.12584233283996582 elapsed, loss: 2.5481173e-05\n",
      "step: 68050 train: 0.12443041801452637 elapsed, loss: 3.4746823e-05\n",
      "step: 68060 train: 0.12305426597595215 elapsed, loss: 1.9030875e-05\n",
      "step: 68070 train: 0.12916803359985352 elapsed, loss: 1.681377e-05\n",
      "step: 68080 train: 0.14180684089660645 elapsed, loss: 9.075575e-06\n",
      "step: 68090 train: 0.11536002159118652 elapsed, loss: 1.21209305e-05\n",
      "step: 68100 train: 0.13174819946289062 elapsed, loss: 6.0418724e-06\n",
      "step: 68110 train: 0.1310105323791504 elapsed, loss: 9.43259e-06\n",
      "step: 68120 train: 0.13975143432617188 elapsed, loss: 7.725304e-06\n",
      "step: 68130 train: 0.13302373886108398 elapsed, loss: 3.7820769e-06\n",
      "step: 68140 train: 0.1251966953277588 elapsed, loss: 3.8691614e-06\n",
      "step: 68150 train: 0.1415700912475586 elapsed, loss: 2.1620585e-06\n",
      "step: 68160 train: 0.14252305030822754 elapsed, loss: 2.4125848e-06\n",
      "step: 68170 train: 0.12851953506469727 elapsed, loss: 2.2449462e-06\n",
      "step: 68180 train: 0.1333615779876709 elapsed, loss: 1.8291139e-06\n",
      "step: 68190 train: 0.1272125244140625 elapsed, loss: 1.8179388e-06\n",
      "step: 68200 train: 0.11684226989746094 elapsed, loss: 2.655194e-06\n",
      "step: 68210 train: 0.13516497611999512 elapsed, loss: 1.7657844e-06\n",
      "step: 68220 train: 0.1226041316986084 elapsed, loss: 2.0568223e-06\n",
      "step: 68230 train: 0.12770628929138184 elapsed, loss: 2.4745175e-06\n",
      "step: 68240 train: 0.1305708885192871 elapsed, loss: 2.9061853e-06\n",
      "step: 68250 train: 0.13802051544189453 elapsed, loss: 1.262872e-06\n",
      "step: 68260 train: 0.1193857192993164 elapsed, loss: 2.2663683e-06\n",
      "step: 68270 train: 0.13115215301513672 elapsed, loss: 1.9813847e-06\n",
      "step: 68280 train: 0.12621617317199707 elapsed, loss: 1.82213e-06\n",
      "step: 68290 train: 0.14268994331359863 elapsed, loss: 1.6977934e-06\n",
      "step: 68300 train: 0.12434601783752441 elapsed, loss: 1.8756807e-06\n",
      "step: 68310 train: 0.12119507789611816 elapsed, loss: 2.3282744e-06\n",
      "step: 68320 train: 0.12111878395080566 elapsed, loss: 2.4577541e-06\n",
      "step: 68330 train: 0.12308406829833984 elapsed, loss: 1.7881351e-06\n",
      "step: 68340 train: 0.13293099403381348 elapsed, loss: 1.3578667e-06\n",
      "step: 68350 train: 0.12363839149475098 elapsed, loss: 2.2398267e-06\n",
      "step: 68360 train: 0.1193697452545166 elapsed, loss: 0.0004014708\n",
      "step: 68370 train: 0.12723827362060547 elapsed, loss: 1.572188e-05\n",
      "step: 68380 train: 0.13530802726745605 elapsed, loss: 1.170822e-05\n",
      "step: 68390 train: 0.1417708396911621 elapsed, loss: 1.5500947e-05\n",
      "step: 68400 train: 0.12304353713989258 elapsed, loss: 6.878692e-06\n",
      "step: 68410 train: 0.12055706977844238 elapsed, loss: 6.752923e-06\n",
      "step: 68420 train: 0.12369775772094727 elapsed, loss: 4.489885e-06\n",
      "step: 68430 train: 0.12786579132080078 elapsed, loss: 4.1954668e-06\n",
      "step: 68440 train: 0.1335773468017578 elapsed, loss: 3.0053634e-06\n",
      "step: 68450 train: 0.1229860782623291 elapsed, loss: 3.5692756e-06\n",
      "step: 68460 train: 0.13546252250671387 elapsed, loss: 1.9678794e-06\n",
      "step: 68470 train: 0.12366318702697754 elapsed, loss: 2.6989665e-06\n",
      "step: 68480 train: 0.1311948299407959 elapsed, loss: 2.1168921e-06\n",
      "step: 68490 train: 0.12458086013793945 elapsed, loss: 2.3334242e-06\n",
      "step: 68500 train: 0.13408517837524414 elapsed, loss: 1.9716056e-06\n",
      "step: 68510 train: 0.12723612785339355 elapsed, loss: 3.027711e-06\n",
      "step: 68520 train: 0.12807440757751465 elapsed, loss: 2.0521657e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 68530 train: 0.1369485855102539 elapsed, loss: 1.4882519e-06\n",
      "step: 68540 train: 0.14336323738098145 elapsed, loss: 1.6475007e-06\n",
      "step: 68550 train: 0.1532762050628662 elapsed, loss: 1.3895317e-06\n",
      "step: 68560 train: 0.1302356719970703 elapsed, loss: 2.0018747e-06\n",
      "step: 68570 train: 0.12581634521484375 elapsed, loss: 1.9762635e-06\n",
      "step: 68580 train: 0.15369629859924316 elapsed, loss: 1.3094382e-06\n",
      "step: 68590 train: 0.12943530082702637 elapsed, loss: 2.1522828e-06\n",
      "step: 68600 train: 0.12781929969787598 elapsed, loss: 2.0167759e-06\n",
      "step: 68610 train: 0.14902162551879883 elapsed, loss: 3.3852389e-06\n",
      "step: 68620 train: 0.12978076934814453 elapsed, loss: 2.5015265e-06\n",
      "step: 68630 train: 0.12127065658569336 elapsed, loss: 2.3604314e-06\n",
      "step: 68640 train: 0.14449667930603027 elapsed, loss: 1.4849911e-06\n",
      "step: 68650 train: 0.12941193580627441 elapsed, loss: 2.6044363e-06\n",
      "step: 68660 train: 0.14004778861999512 elapsed, loss: 2.665438e-06\n",
      "step: 68670 train: 0.1235659122467041 elapsed, loss: 3.1585691e-06\n",
      "step: 68680 train: 0.14255857467651367 elapsed, loss: 1.2982623e-06\n",
      "step: 68690 train: 0.12912726402282715 elapsed, loss: 2.4414576e-06\n",
      "step: 68700 train: 0.12453842163085938 elapsed, loss: 2.427479e-06\n",
      "step: 68710 train: 0.1264786720275879 elapsed, loss: 2.1639246e-06\n",
      "step: 68720 train: 0.1215968132019043 elapsed, loss: 2.6989542e-06\n",
      "step: 68730 train: 0.1243898868560791 elapsed, loss: 2.3106072e-06\n",
      "step: 68740 train: 0.13969707489013672 elapsed, loss: 1.8575204e-06\n",
      "step: 68750 train: 0.13301968574523926 elapsed, loss: 0.0010385218\n",
      "step: 68760 train: 0.12893080711364746 elapsed, loss: 3.4367262e-05\n",
      "step: 68770 train: 0.13476133346557617 elapsed, loss: 1.0024547e-05\n",
      "step: 68780 train: 0.11750602722167969 elapsed, loss: 1.0136751e-05\n",
      "step: 68790 train: 0.1338808536529541 elapsed, loss: 5.9269005e-06\n",
      "step: 68800 train: 0.12936925888061523 elapsed, loss: 4.56206e-06\n",
      "step: 68810 train: 0.12359809875488281 elapsed, loss: 5.2786963e-06\n",
      "step: 68820 train: 0.13323974609375 elapsed, loss: 3.924046e-06\n",
      "step: 68830 train: 0.12644171714782715 elapsed, loss: 3.302923e-06\n",
      "step: 68840 train: 0.12865352630615234 elapsed, loss: 2.2756817e-06\n",
      "step: 68850 train: 0.11857366561889648 elapsed, loss: 3.0705444e-06\n",
      "step: 68860 train: 0.13426876068115234 elapsed, loss: 2.4358603e-06\n",
      "step: 68870 train: 0.1300504207611084 elapsed, loss: 2.0451785e-06\n",
      "step: 68880 train: 0.1292867660522461 elapsed, loss: 1.4645032e-06\n",
      "step: 68890 train: 0.13681459426879883 elapsed, loss: 2.445644e-06\n",
      "step: 68900 train: 0.1342623233795166 elapsed, loss: 2.5475915e-06\n",
      "step: 68910 train: 0.12175464630126953 elapsed, loss: 2.3548396e-06\n",
      "step: 68920 train: 0.1296851634979248 elapsed, loss: 1.3811499e-06\n",
      "step: 68930 train: 0.12973546981811523 elapsed, loss: 1.91619e-06\n",
      "step: 68940 train: 0.12620925903320312 elapsed, loss: 1.7653196e-06\n",
      "step: 68950 train: 0.13241219520568848 elapsed, loss: 1.5208477e-06\n",
      "step: 68960 train: 0.13177800178527832 elapsed, loss: 2.136445e-06\n",
      "step: 68970 train: 0.13109445571899414 elapsed, loss: 2.0181515e-06\n",
      "step: 68980 train: 0.14084839820861816 elapsed, loss: 1.5413372e-06\n",
      "step: 68990 train: 0.148024320602417 elapsed, loss: 1.3979134e-06\n",
      "step: 69000 train: 0.1405942440032959 elapsed, loss: 2.463311e-06\n",
      "step: 69010 train: 0.13251018524169922 elapsed, loss: 2.8959403e-06\n",
      "step: 69020 train: 0.1318674087524414 elapsed, loss: 2.1881388e-06\n",
      "step: 69030 train: 0.12340211868286133 elapsed, loss: 2.4959375e-06\n",
      "step: 69040 train: 0.12974762916564941 elapsed, loss: 2.4591523e-06\n",
      "step: 69050 train: 0.14054322242736816 elapsed, loss: 1.8053656e-06\n",
      "step: 69060 train: 0.1300830841064453 elapsed, loss: 3.3308643e-06\n",
      "step: 69070 train: 0.12533974647521973 elapsed, loss: 2.4191036e-06\n",
      "step: 69080 train: 0.13168621063232422 elapsed, loss: 2.0973348e-06\n",
      "step: 69090 train: 0.13754773139953613 elapsed, loss: 1.7723037e-06\n",
      "step: 69100 train: 0.12409543991088867 elapsed, loss: 2.2784754e-06\n",
      "step: 69110 train: 0.14022397994995117 elapsed, loss: 1.5557712e-06\n",
      "step: 69120 train: 0.12064838409423828 elapsed, loss: 2.4107242e-06\n",
      "step: 69130 train: 0.1266186237335205 elapsed, loss: 2.3976818e-06\n",
      "step: 69140 train: 0.12018179893493652 elapsed, loss: 2.404203e-06\n",
      "step: 69150 train: 0.1319446563720703 elapsed, loss: 3.0845324e-06\n",
      "step: 69160 train: 0.12614917755126953 elapsed, loss: 2.9676512e-06\n",
      "step: 69170 train: 0.13158082962036133 elapsed, loss: 3.6582126e-06\n",
      "step: 69180 train: 0.13498473167419434 elapsed, loss: 2.0861562e-06\n",
      "step: 69190 train: 0.13996005058288574 elapsed, loss: 2.2137472e-06\n",
      "step: 69200 train: 0.13112425804138184 elapsed, loss: 2.5099093e-06\n",
      "step: 69210 train: 0.1254875659942627 elapsed, loss: 2.7338924e-06\n",
      "step: 69220 train: 0.14608383178710938 elapsed, loss: 1.9841777e-06\n",
      "step: 69230 train: 0.12946391105651855 elapsed, loss: 1.9073458e-06\n",
      "step: 69240 train: 0.13007521629333496 elapsed, loss: 1.9315594e-06\n",
      "step: 69250 train: 0.12587690353393555 elapsed, loss: 2.3855705e-06\n",
      "step: 69260 train: 0.13066387176513672 elapsed, loss: 2.7669525e-06\n",
      "step: 69270 train: 0.1291358470916748 elapsed, loss: 2.3562416e-06\n",
      "step: 69280 train: 0.12004876136779785 elapsed, loss: 3.255429e-06\n",
      "step: 69290 train: 0.13498330116271973 elapsed, loss: 1.9194517e-06\n",
      "step: 69300 train: 0.12947368621826172 elapsed, loss: 2.6197995e-06\n",
      "step: 69310 train: 0.13089609146118164 elapsed, loss: 3.3215115e-06\n",
      "step: 69320 train: 0.1211550235748291 elapsed, loss: 3.254498e-06\n",
      "step: 69330 train: 0.12872886657714844 elapsed, loss: 1.8314431e-06\n",
      "step: 69340 train: 0.12230587005615234 elapsed, loss: 3.1827867e-06\n",
      "step: 69350 train: 0.11830353736877441 elapsed, loss: 3.4584568e-06\n",
      "step: 69360 train: 0.12308502197265625 elapsed, loss: 3.037501e-06\n",
      "step: 69370 train: 0.13729023933410645 elapsed, loss: 2.4302806e-06\n",
      "step: 69380 train: 0.14065122604370117 elapsed, loss: 2.7180577e-06\n",
      "step: 69390 train: 0.13948392868041992 elapsed, loss: 2.5602003e-06\n",
      "step: 69400 train: 0.12590861320495605 elapsed, loss: 3.2349387e-06\n",
      "step: 69410 train: 0.12816381454467773 elapsed, loss: 3.7885816e-06\n",
      "step: 69420 train: 0.12812352180480957 elapsed, loss: 2.7907026e-06\n",
      "step: 69430 train: 0.12784910202026367 elapsed, loss: 2.3338903e-06\n",
      "step: 69440 train: 0.12934017181396484 elapsed, loss: 2.3539124e-06\n",
      "step: 69450 train: 0.13878130912780762 elapsed, loss: 2.1019919e-06\n",
      "step: 69460 train: 0.12581658363342285 elapsed, loss: 3.78069e-06\n",
      "step: 69470 train: 0.12269949913024902 elapsed, loss: 3.2344592e-06\n",
      "step: 69480 train: 0.1236412525177002 elapsed, loss: 3.3383171e-06\n",
      "step: 69490 train: 0.14064288139343262 elapsed, loss: 2.5341233e-06\n",
      "step: 69500 train: 0.14168310165405273 elapsed, loss: 2.5448326e-06\n",
      "step: 69510 train: 0.12335896492004395 elapsed, loss: 3.6787123e-06\n",
      "step: 69520 train: 0.12893271446228027 elapsed, loss: 2.7017563e-06\n",
      "step: 69530 train: 0.1286940574645996 elapsed, loss: 3.6311646e-06\n",
      "step: 69540 train: 0.12312555313110352 elapsed, loss: 5.084515e-06\n",
      "step: 69550 train: 0.12148809432983398 elapsed, loss: 3.7457482e-06\n",
      "step: 69560 train: 0.14707612991333008 elapsed, loss: 2.629118e-06\n",
      "step: 69570 train: 0.14275527000427246 elapsed, loss: 2.6640419e-06\n",
      "step: 69580 train: 0.1166691780090332 elapsed, loss: 6.253784e-06\n",
      "step: 69590 train: 0.13074994087219238 elapsed, loss: 4.5233846e-06\n",
      "step: 69600 train: 0.12529468536376953 elapsed, loss: 2.786977e-06\n",
      "step: 69610 train: 0.12546229362487793 elapsed, loss: 2.5839495e-06\n",
      "step: 69620 train: 0.1299431324005127 elapsed, loss: 2.4130509e-06\n",
      "step: 69630 train: 0.14263319969177246 elapsed, loss: 2.214681e-06\n",
      "step: 69640 train: 0.12533283233642578 elapsed, loss: 2.7147998e-06\n",
      "step: 69650 train: 0.1305234432220459 elapsed, loss: 2.342272e-06\n",
      "step: 69660 train: 0.1255033016204834 elapsed, loss: 4.6491286e-06\n",
      "step: 69670 train: 0.12717294692993164 elapsed, loss: 2.484298e-06\n",
      "step: 69680 train: 0.14484143257141113 elapsed, loss: 2.2095558e-06\n",
      "step: 69690 train: 0.1374053955078125 elapsed, loss: 2.160665e-06\n",
      "step: 69700 train: 0.12084770202636719 elapsed, loss: 5.5934875e-06\n",
      "step: 69710 train: 0.13510656356811523 elapsed, loss: 2.9248101e-06\n",
      "step: 69720 train: 0.12358617782592773 elapsed, loss: 3.588374e-06\n",
      "step: 69730 train: 0.12801861763000488 elapsed, loss: 2.4908177e-06\n",
      "step: 69740 train: 0.12363958358764648 elapsed, loss: 4.2556603e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 69750 train: 0.12633132934570312 elapsed, loss: 3.064039e-06\n",
      "step: 69760 train: 0.12626194953918457 elapsed, loss: 2.8242284e-06\n",
      "step: 69770 train: 0.12655878067016602 elapsed, loss: 4.7259723e-06\n",
      "step: 69780 train: 0.13285422325134277 elapsed, loss: 0.15734352\n",
      "step: 69790 train: 0.1267409324645996 elapsed, loss: 0.00017290715\n",
      "step: 69800 train: 0.15175366401672363 elapsed, loss: 2.9173869e-05\n",
      "step: 69810 train: 0.1401677131652832 elapsed, loss: 1.7083083e-05\n",
      "step: 69820 train: 0.13259196281433105 elapsed, loss: 1.5513691e-05\n",
      "step: 69830 train: 0.1350264549255371 elapsed, loss: 1.4902595e-05\n",
      "step: 69840 train: 0.12118816375732422 elapsed, loss: 1.8054532e-05\n",
      "step: 69850 train: 0.13157987594604492 elapsed, loss: 9.414138e-06\n",
      "step: 69860 train: 0.12819576263427734 elapsed, loss: 1.8001976e-05\n",
      "step: 69870 train: 0.14516472816467285 elapsed, loss: 5.357397e-06\n",
      "step: 69880 train: 0.12303924560546875 elapsed, loss: 8.519475e-06\n",
      "step: 69890 train: 0.1374039649963379 elapsed, loss: 5.7592315e-06\n",
      "step: 69900 train: 0.1246335506439209 elapsed, loss: 3.774638e-06\n",
      "step: 69910 train: 0.1251811981201172 elapsed, loss: 3.2414537e-06\n",
      "step: 69920 train: 0.13773441314697266 elapsed, loss: 3.7042964e-06\n",
      "step: 69930 train: 0.14962434768676758 elapsed, loss: 2.7510607e-06\n",
      "step: 69940 train: 0.13190102577209473 elapsed, loss: 2.0745156e-06\n",
      "step: 69950 train: 0.12246847152709961 elapsed, loss: 3.5199018e-06\n",
      "step: 69960 train: 0.14122438430786133 elapsed, loss: 2.622597e-06\n",
      "step: 69970 train: 0.1339104175567627 elapsed, loss: 1.8398238e-06\n",
      "step: 69980 train: 0.122589111328125 elapsed, loss: 2.6309806e-06\n",
      "step: 69990 train: 0.13748526573181152 elapsed, loss: 1.6433162e-06\n",
      "step: 70000 train: 0.13014531135559082 elapsed, loss: 2.7371507e-06\n",
      "step: 70010 train: 0.1313934326171875 elapsed, loss: 1.8919782e-06\n",
      "step: 70020 train: 0.1239006519317627 elapsed, loss: 2.7161948e-06\n",
      "step: 70030 train: 0.12324261665344238 elapsed, loss: 2.4661358e-06\n",
      "step: 70040 train: 0.13379979133605957 elapsed, loss: 1.8714899e-06\n",
      "step: 70050 train: 0.12033772468566895 elapsed, loss: 2.3585699e-06\n",
      "step: 70060 train: 0.1283872127532959 elapsed, loss: 1.9059487e-06\n",
      "step: 70070 train: 0.12967395782470703 elapsed, loss: 2.3944258e-06\n",
      "step: 70080 train: 0.130234956741333 elapsed, loss: 2.246346e-06\n",
      "step: 70090 train: 0.12080049514770508 elapsed, loss: 4.4251547e-06\n",
      "step: 70100 train: 0.12454891204833984 elapsed, loss: 3.293142e-06\n",
      "step: 70110 train: 0.12229108810424805 elapsed, loss: 3.1380812e-06\n",
      "step: 70120 train: 0.1295773983001709 elapsed, loss: 2.8661382e-06\n",
      "step: 70130 train: 0.13672471046447754 elapsed, loss: 2.2686975e-06\n",
      "step: 70140 train: 0.13573503494262695 elapsed, loss: 1.9692786e-06\n",
      "step: 70150 train: 0.13294363021850586 elapsed, loss: 1.8188704e-06\n",
      "step: 70160 train: 0.13668036460876465 elapsed, loss: 1.6312091e-06\n",
      "step: 70170 train: 0.1302623748779297 elapsed, loss: 2.522013e-06\n",
      "step: 70180 train: 0.12242603302001953 elapsed, loss: 2.3981484e-06\n",
      "step: 70190 train: 0.1305375099182129 elapsed, loss: 2.1676492e-06\n",
      "step: 70200 train: 0.13285231590270996 elapsed, loss: 2.1350525e-06\n",
      "step: 70210 train: 0.13877296447753906 elapsed, loss: 1.904084e-06\n",
      "step: 70220 train: 0.12092852592468262 elapsed, loss: 2.8312134e-06\n",
      "step: 70230 train: 0.12687158584594727 elapsed, loss: 2.7324945e-06\n",
      "step: 70240 train: 0.13242626190185547 elapsed, loss: 2.4549604e-06\n",
      "step: 70250 train: 0.12609171867370605 elapsed, loss: 3.0547255e-06\n",
      "step: 70260 train: 0.12835121154785156 elapsed, loss: 2.219334e-06\n",
      "step: 70270 train: 0.12882423400878906 elapsed, loss: 2.2812706e-06\n",
      "step: 70280 train: 0.12851643562316895 elapsed, loss: 4.202554e-06\n",
      "step: 70290 train: 0.1392061710357666 elapsed, loss: 1.9823149e-06\n",
      "step: 70300 train: 0.12686681747436523 elapsed, loss: 2.4624123e-06\n",
      "step: 70310 train: 0.1246480941772461 elapsed, loss: 3.4295754e-06\n",
      "step: 70320 train: 0.13859009742736816 elapsed, loss: 2.0507687e-06\n",
      "step: 70330 train: 0.1365489959716797 elapsed, loss: 2.428418e-06\n",
      "step: 70340 train: 0.13733839988708496 elapsed, loss: 1.9776603e-06\n",
      "step: 70350 train: 0.12610983848571777 elapsed, loss: 2.751116e-06\n",
      "step: 70360 train: 0.12376046180725098 elapsed, loss: 3.0072338e-06\n",
      "step: 70370 train: 0.12857556343078613 elapsed, loss: 2.5466966e-06\n",
      "step: 70380 train: 0.132537841796875 elapsed, loss: 5.4444663e-06\n",
      "step: 70390 train: 0.13101458549499512 elapsed, loss: 3.978125e-06\n",
      "step: 70400 train: 0.12734270095825195 elapsed, loss: 2.0554257e-06\n",
      "step: 70410 train: 0.1273505687713623 elapsed, loss: 2.4367982e-06\n",
      "step: 70420 train: 0.1362760066986084 elapsed, loss: 1.8011755e-06\n",
      "step: 70430 train: 0.13120460510253906 elapsed, loss: 3.5124501e-06\n",
      "step: 70440 train: 0.12049221992492676 elapsed, loss: 3.120385e-06\n",
      "step: 70450 train: 0.12451767921447754 elapsed, loss: 3.6773163e-06\n",
      "step: 70460 train: 0.12404870986938477 elapsed, loss: 2.9206192e-06\n",
      "step: 70470 train: 0.12820982933044434 elapsed, loss: 2.9634582e-06\n",
      "step: 70480 train: 0.1346721649169922 elapsed, loss: 2.3301616e-06\n",
      "step: 70490 train: 0.1318359375 elapsed, loss: 1.7029212e-06\n",
      "step: 70500 train: 0.12584638595581055 elapsed, loss: 4.184418e-06\n",
      "step: 70510 train: 0.1442883014678955 elapsed, loss: 2.3189887e-06\n",
      "step: 70520 train: 0.14988970756530762 elapsed, loss: 2.4102574e-06\n",
      "step: 70530 train: 0.1355576515197754 elapsed, loss: 1.0365024e-05\n",
      "step: 70540 train: 0.13280606269836426 elapsed, loss: 2.8111785e-06\n",
      "step: 70550 train: 0.14832425117492676 elapsed, loss: 1.9851109e-06\n",
      "step: 70560 train: 0.12128806114196777 elapsed, loss: 2.7608994e-06\n",
      "step: 70570 train: 0.13075971603393555 elapsed, loss: 1.8831302e-06\n",
      "step: 70580 train: 0.13932085037231445 elapsed, loss: 2.0046618e-06\n",
      "step: 70590 train: 0.11771702766418457 elapsed, loss: 3.5231812e-06\n",
      "step: 70600 train: 0.13226580619812012 elapsed, loss: 2.6421549e-06\n",
      "step: 70610 train: 0.13840699195861816 elapsed, loss: 2.0740522e-06\n",
      "step: 70620 train: 0.13631439208984375 elapsed, loss: 2.426089e-06\n",
      "step: 70630 train: 0.1273212432861328 elapsed, loss: 4.131278e-06\n",
      "step: 70640 train: 0.1234283447265625 elapsed, loss: 0.19375366\n",
      "step: 70650 train: 0.13340520858764648 elapsed, loss: 3.9283274e-05\n",
      "step: 70660 train: 0.11706304550170898 elapsed, loss: 2.7985721e-05\n",
      "step: 70670 train: 0.12651777267456055 elapsed, loss: 1.9696661e-05\n",
      "step: 70680 train: 0.12692523002624512 elapsed, loss: 1.624889e-05\n",
      "step: 70690 train: 0.12489843368530273 elapsed, loss: 1.1495062e-05\n",
      "step: 70700 train: 0.1377413272857666 elapsed, loss: 6.641673e-06\n",
      "step: 70710 train: 0.1306297779083252 elapsed, loss: 6.705911e-06\n",
      "step: 70720 train: 0.1311812400817871 elapsed, loss: 6.6480698e-06\n",
      "step: 70730 train: 0.12746596336364746 elapsed, loss: 6.6160646e-06\n",
      "step: 70740 train: 0.13319778442382812 elapsed, loss: 3.821664e-06\n",
      "step: 70750 train: 0.12921595573425293 elapsed, loss: 2.8642755e-06\n",
      "step: 70760 train: 0.12853693962097168 elapsed, loss: 3.6144488e-06\n",
      "step: 70770 train: 0.1251683235168457 elapsed, loss: 2.7506542e-06\n",
      "step: 70780 train: 0.11513781547546387 elapsed, loss: 3.125511e-06\n",
      "step: 70790 train: 0.13109207153320312 elapsed, loss: 2.4135174e-06\n",
      "step: 70800 train: 0.13100934028625488 elapsed, loss: 2.1518163e-06\n",
      "step: 70810 train: 0.1281144618988037 elapsed, loss: 2.7981482e-06\n",
      "step: 70820 train: 0.13900995254516602 elapsed, loss: 2.2165434e-06\n",
      "step: 70830 train: 0.1426074504852295 elapsed, loss: 2.0414557e-06\n",
      "step: 70840 train: 0.13615775108337402 elapsed, loss: 2.437722e-06\n",
      "step: 70850 train: 0.13959503173828125 elapsed, loss: 1.766716e-06\n",
      "step: 70860 train: 0.12291932106018066 elapsed, loss: 2.3650891e-06\n",
      "step: 70870 train: 0.14949965476989746 elapsed, loss: 2.9955806e-06\n",
      "step: 70880 train: 0.12554192543029785 elapsed, loss: 2.9778948e-06\n",
      "step: 70890 train: 0.13142943382263184 elapsed, loss: 2.2104905e-06\n",
      "step: 70900 train: 0.12283658981323242 elapsed, loss: 2.6123541e-06\n",
      "step: 70910 train: 0.12897396087646484 elapsed, loss: 2.1001283e-06\n",
      "step: 70920 train: 0.13372278213500977 elapsed, loss: 1.761593e-06\n",
      "step: 70930 train: 0.13659954071044922 elapsed, loss: 1.9888348e-06\n",
      "step: 70940 train: 0.13946318626403809 elapsed, loss: 3.4021734e-05\n",
      "step: 70950 train: 0.1371901035308838 elapsed, loss: 0.00026853007\n",
      "step: 70960 train: 0.12716197967529297 elapsed, loss: 3.577329e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 70970 train: 0.12965059280395508 elapsed, loss: 2.7327014e-05\n",
      "step: 70980 train: 0.1373579502105713 elapsed, loss: 1.9715986e-05\n",
      "step: 70990 train: 0.12235426902770996 elapsed, loss: 1.438445e-05\n",
      "step: 71000 train: 0.11826634407043457 elapsed, loss: 1.2427767e-05\n",
      "step: 71010 train: 0.1281719207763672 elapsed, loss: 6.453532e-06\n",
      "step: 71020 train: 0.13139891624450684 elapsed, loss: 5.5538994e-06\n",
      "step: 71030 train: 0.12147164344787598 elapsed, loss: 6.56297e-06\n",
      "step: 71040 train: 0.12757325172424316 elapsed, loss: 4.2700303e-06\n",
      "step: 71050 train: 0.12691760063171387 elapsed, loss: 4.203972e-06\n",
      "step: 71060 train: 0.125136137008667 elapsed, loss: 3.921656e-06\n",
      "step: 71070 train: 0.12407422065734863 elapsed, loss: 3.285691e-06\n",
      "step: 71080 train: 0.13713455200195312 elapsed, loss: 2.747393e-06\n",
      "step: 71090 train: 0.14024567604064941 elapsed, loss: 1.940873e-06\n",
      "step: 71100 train: 0.13130927085876465 elapsed, loss: 2.3860412e-06\n",
      "step: 71110 train: 0.14322757720947266 elapsed, loss: 1.6908139e-06\n",
      "step: 71120 train: 0.14428019523620605 elapsed, loss: 1.7685777e-06\n",
      "step: 71130 train: 0.12310123443603516 elapsed, loss: 1.9972172e-06\n",
      "step: 71140 train: 0.11723971366882324 elapsed, loss: 1.9175905e-06\n",
      "step: 71150 train: 0.13038253784179688 elapsed, loss: 1.8314424e-06\n",
      "step: 71160 train: 0.1419966220855713 elapsed, loss: 1.7969844e-06\n",
      "step: 71170 train: 0.13916945457458496 elapsed, loss: 1.3168885e-06\n",
      "step: 71180 train: 0.14862775802612305 elapsed, loss: 1.3564695e-06\n",
      "step: 71190 train: 0.13110756874084473 elapsed, loss: 2.3925622e-06\n",
      "step: 71200 train: 0.1306777000427246 elapsed, loss: 1.4081583e-06\n",
      "step: 71210 train: 0.13034749031066895 elapsed, loss: 4.004659e-06\n",
      "step: 71220 train: 0.13018369674682617 elapsed, loss: 2.9471616e-06\n",
      "step: 71230 train: 0.13236141204833984 elapsed, loss: 1.9706727e-06\n",
      "step: 71240 train: 0.13141298294067383 elapsed, loss: 1.8631067e-06\n",
      "step: 71250 train: 0.14117908477783203 elapsed, loss: 1.4239904e-06\n",
      "step: 71260 train: 0.11763572692871094 elapsed, loss: 2.1303968e-06\n",
      "step: 71270 train: 0.1243894100189209 elapsed, loss: 2.130389e-06\n",
      "step: 71280 train: 0.12754607200622559 elapsed, loss: 1.9608958e-06\n",
      "step: 71290 train: 0.13192033767700195 elapsed, loss: 1.6055982e-06\n",
      "step: 71300 train: 0.12633919715881348 elapsed, loss: 2.461013e-06\n",
      "step: 71310 train: 0.12576770782470703 elapsed, loss: 1.7154937e-06\n",
      "step: 71320 train: 0.14313077926635742 elapsed, loss: 1.4421514e-06\n",
      "step: 71330 train: 0.13951992988586426 elapsed, loss: 1.8426191e-06\n",
      "step: 71340 train: 0.12812328338623047 elapsed, loss: 1.5753301e-06\n",
      "step: 71350 train: 0.13214540481567383 elapsed, loss: 2.2170086e-06\n",
      "step: 71360 train: 0.13306283950805664 elapsed, loss: 1.7168906e-06\n",
      "step: 71370 train: 0.12382674217224121 elapsed, loss: 2.74367e-06\n",
      "step: 71380 train: 0.13044500350952148 elapsed, loss: 3.460903e-06\n",
      "step: 71390 train: 0.1346430778503418 elapsed, loss: 1.8924422e-06\n",
      "step: 71400 train: 0.13661766052246094 elapsed, loss: 1.7723034e-06\n",
      "step: 71410 train: 0.11694121360778809 elapsed, loss: 2.9117714e-06\n",
      "step: 71420 train: 0.13357901573181152 elapsed, loss: 1.9785916e-06\n",
      "step: 71430 train: 0.14219903945922852 elapsed, loss: 1.4849913e-06\n",
      "step: 71440 train: 0.13010478019714355 elapsed, loss: 2.2244594e-06\n",
      "step: 71450 train: 0.1301860809326172 elapsed, loss: 2.1890692e-06\n",
      "step: 71460 train: 0.13421273231506348 elapsed, loss: 1.2791703e-06\n",
      "step: 71470 train: 0.13483905792236328 elapsed, loss: 1.8486724e-06\n",
      "step: 71480 train: 0.13211631774902344 elapsed, loss: 1.7844116e-06\n",
      "step: 71490 train: 0.12407517433166504 elapsed, loss: 2.3855773e-06\n",
      "step: 71500 train: 0.12350630760192871 elapsed, loss: 2.6333078e-06\n",
      "step: 71510 train: 0.12806057929992676 elapsed, loss: 2.915498e-06\n",
      "step: 71520 train: 0.14775609970092773 elapsed, loss: 2.3958182e-06\n",
      "step: 71530 train: 0.13181281089782715 elapsed, loss: 2.172772e-06\n",
      "step: 71540 train: 0.13588523864746094 elapsed, loss: 1.7755636e-06\n",
      "step: 71550 train: 0.13491296768188477 elapsed, loss: 3.1888399e-06\n",
      "step: 71560 train: 0.12392067909240723 elapsed, loss: 2.4181745e-06\n",
      "step: 71570 train: 0.1257164478302002 elapsed, loss: 2.5331915e-06\n",
      "step: 71580 train: 0.12976503372192383 elapsed, loss: 2.5150207e-06\n",
      "step: 71590 train: 0.12316298484802246 elapsed, loss: 4.92361e-06\n",
      "step: 71600 train: 0.12670493125915527 elapsed, loss: 2.5708418e-05\n",
      "step: 71610 train: 0.14140868186950684 elapsed, loss: 4.8009265e-06\n",
      "step: 71620 train: 0.13110733032226562 elapsed, loss: 5.928034e-06\n",
      "step: 71630 train: 0.12498736381530762 elapsed, loss: 4.690587e-06\n",
      "step: 71640 train: 0.12688493728637695 elapsed, loss: 3.832365e-06\n",
      "step: 71650 train: 0.12096834182739258 elapsed, loss: 2.9816215e-06\n",
      "step: 71660 train: 0.12669944763183594 elapsed, loss: 2.4903488e-06\n",
      "step: 71670 train: 0.1254732608795166 elapsed, loss: 3.0831266e-06\n",
      "step: 71680 train: 0.12211251258850098 elapsed, loss: 3.0621798e-06\n",
      "step: 71690 train: 0.12811589241027832 elapsed, loss: 2.3823188e-06\n",
      "step: 71700 train: 0.13457345962524414 elapsed, loss: 2.3553087e-06\n",
      "step: 71710 train: 0.13406920433044434 elapsed, loss: 2.2784761e-06\n",
      "step: 71720 train: 0.13697099685668945 elapsed, loss: 1.8854594e-06\n",
      "step: 71730 train: 0.13654351234436035 elapsed, loss: 1.9199188e-06\n",
      "step: 71740 train: 0.13967347145080566 elapsed, loss: 2.1290002e-06\n",
      "step: 71750 train: 0.1334071159362793 elapsed, loss: 2.2263225e-06\n",
      "step: 71760 train: 0.11731672286987305 elapsed, loss: 2.2230633e-06\n",
      "step: 71770 train: 0.13119268417358398 elapsed, loss: 2.102457e-06\n",
      "step: 71780 train: 0.1303863525390625 elapsed, loss: 1.877078e-06\n",
      "step: 71790 train: 0.14564871788024902 elapsed, loss: 1.8500695e-06\n",
      "step: 71800 train: 0.12957525253295898 elapsed, loss: 2.2528657e-06\n",
      "step: 71810 train: 0.12030792236328125 elapsed, loss: 3.2614812e-06\n",
      "step: 71820 train: 0.1267836093902588 elapsed, loss: 1.973469e-06\n",
      "step: 71830 train: 0.1249237060546875 elapsed, loss: 1.9976837e-06\n",
      "step: 71840 train: 0.12822675704956055 elapsed, loss: 2.282667e-06\n",
      "step: 71850 train: 0.13122105598449707 elapsed, loss: 3.2447022e-06\n",
      "step: 71860 train: 0.12462329864501953 elapsed, loss: 2.718058e-06\n",
      "step: 71870 train: 0.13499760627746582 elapsed, loss: 2.400014e-06\n",
      "step: 71880 train: 0.12406086921691895 elapsed, loss: 3.4505397e-06\n",
      "step: 71890 train: 0.12103819847106934 elapsed, loss: 3.3797528e-06\n",
      "step: 71900 train: 0.13454818725585938 elapsed, loss: 2.7995463e-06\n",
      "step: 71910 train: 0.1259145736694336 elapsed, loss: 2.7804567e-06\n",
      "step: 71920 train: 0.1394639015197754 elapsed, loss: 2.6407422e-06\n",
      "step: 71930 train: 0.13546299934387207 elapsed, loss: 2.0377306e-06\n",
      "step: 71940 train: 0.13004183769226074 elapsed, loss: 2.6635603e-06\n",
      "step: 71950 train: 0.1377553939819336 elapsed, loss: 2.2835984e-06\n",
      "step: 71960 train: 0.12391972541809082 elapsed, loss: 6.803539e-06\n",
      "step: 71970 train: 0.14125728607177734 elapsed, loss: 1.9562399e-06\n",
      "step: 71980 train: 0.13706564903259277 elapsed, loss: 1.7676475e-06\n",
      "step: 71990 train: 0.1381540298461914 elapsed, loss: 2.178825e-06\n",
      "step: 72000 train: 0.13329124450683594 elapsed, loss: 2.1965093e-06\n",
      "step: 72010 train: 0.12862730026245117 elapsed, loss: 2.1834817e-06\n",
      "step: 72020 train: 0.1300976276397705 elapsed, loss: 2.437732e-06\n",
      "step: 72030 train: 0.13474178314208984 elapsed, loss: 2.598384e-06\n",
      "step: 72040 train: 0.11794304847717285 elapsed, loss: 4.1052544e-06\n",
      "step: 72050 train: 0.12822341918945312 elapsed, loss: 2.9536802e-06\n",
      "step: 72060 train: 0.12544870376586914 elapsed, loss: 2.4586866e-06\n",
      "step: 72070 train: 0.12312602996826172 elapsed, loss: 3.4239965e-06\n",
      "step: 72080 train: 0.1308913230895996 elapsed, loss: 2.9168946e-06\n",
      "step: 72090 train: 0.12381887435913086 elapsed, loss: 2.5881407e-06\n",
      "step: 72100 train: 0.1355891227722168 elapsed, loss: 1.8435504e-06\n",
      "step: 72110 train: 0.12873291969299316 elapsed, loss: 2.2617135e-06\n",
      "step: 72120 train: 0.12590456008911133 elapsed, loss: 2.404668e-06\n",
      "step: 72130 train: 0.12313151359558105 elapsed, loss: 3.322949e-06\n",
      "step: 72140 train: 0.12464118003845215 elapsed, loss: 0.00057854195\n",
      "step: 72150 train: 0.1295015811920166 elapsed, loss: 0.00011502736\n",
      "step: 72160 train: 0.1273965835571289 elapsed, loss: 2.4983385e-05\n",
      "step: 72170 train: 0.12655925750732422 elapsed, loss: 2.259933e-05\n",
      "step: 72180 train: 0.13407373428344727 elapsed, loss: 1.3006888e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 72190 train: 0.12280082702636719 elapsed, loss: 1.5081062e-05\n",
      "step: 72200 train: 0.12162089347839355 elapsed, loss: 1.0105169e-05\n",
      "step: 72210 train: 0.14031100273132324 elapsed, loss: 5.7154925e-06\n",
      "step: 72220 train: 0.12807536125183105 elapsed, loss: 7.515496e-06\n",
      "step: 72230 train: 0.13218092918395996 elapsed, loss: 4.6709883e-06\n",
      "step: 72240 train: 0.1326758861541748 elapsed, loss: 4.55601e-06\n",
      "step: 72250 train: 0.13180303573608398 elapsed, loss: 2.8917482e-06\n",
      "step: 72260 train: 0.1393904685974121 elapsed, loss: 4.3458404e-06\n",
      "step: 72270 train: 0.11934876441955566 elapsed, loss: 3.3094452e-06\n",
      "step: 72280 train: 0.1266036033630371 elapsed, loss: 2.4586852e-06\n",
      "step: 72290 train: 0.1238105297088623 elapsed, loss: 2.7096767e-06\n",
      "step: 72300 train: 0.11904764175415039 elapsed, loss: 2.6915168e-06\n",
      "step: 72310 train: 0.125579833984375 elapsed, loss: 0.00010684995\n",
      "step: 72320 train: 0.13081884384155273 elapsed, loss: 9.816025e-06\n",
      "step: 72330 train: 0.13026094436645508 elapsed, loss: 0.00010230033\n",
      "step: 72340 train: 0.13607263565063477 elapsed, loss: 2.0294254e-05\n",
      "step: 72350 train: 0.1258230209350586 elapsed, loss: 1.3934695e-05\n",
      "step: 72360 train: 0.1363537311553955 elapsed, loss: 1.2271426e-05\n",
      "step: 72370 train: 0.12685084342956543 elapsed, loss: 7.971571e-06\n",
      "step: 72380 train: 0.12715888023376465 elapsed, loss: 6.210469e-06\n",
      "step: 72390 train: 0.12077188491821289 elapsed, loss: 5.73544e-06\n",
      "step: 72400 train: 0.14200091361999512 elapsed, loss: 4.236101e-06\n",
      "step: 72410 train: 0.1456279754638672 elapsed, loss: 3.910525e-06\n",
      "step: 72420 train: 0.14756178855895996 elapsed, loss: 2.5108388e-06\n",
      "step: 72430 train: 0.1316521167755127 elapsed, loss: 2.087089e-06\n",
      "step: 72440 train: 0.1282484531402588 elapsed, loss: 2.4014075e-06\n",
      "step: 72450 train: 0.12285685539245605 elapsed, loss: 2.450768e-06\n",
      "step: 72460 train: 0.1238396167755127 elapsed, loss: 1.9241088e-06\n",
      "step: 72470 train: 0.13321471214294434 elapsed, loss: 1.8584499e-06\n",
      "step: 72480 train: 0.12128829956054688 elapsed, loss: 1.8724202e-06\n",
      "step: 72490 train: 0.13283061981201172 elapsed, loss: 1.2791702e-06\n",
      "step: 72500 train: 0.13158607482910156 elapsed, loss: 1.3811498e-06\n",
      "step: 72510 train: 0.11774849891662598 elapsed, loss: 2.2556396e-06\n",
      "step: 72520 train: 0.13245224952697754 elapsed, loss: 1.4593779e-06\n",
      "step: 72530 train: 0.13709378242492676 elapsed, loss: 1.2596105e-06\n",
      "step: 72540 train: 0.1285414695739746 elapsed, loss: 1.6521637e-06\n",
      "step: 72550 train: 0.14332318305969238 elapsed, loss: 1.2344669e-06\n",
      "step: 72560 train: 0.13169336318969727 elapsed, loss: 4.9694827e-06\n",
      "step: 72570 train: 0.12631535530090332 elapsed, loss: 2.1797555e-06\n",
      "step: 72580 train: 0.12057375907897949 elapsed, loss: 1.920384e-06\n",
      "step: 72590 train: 0.12928557395935059 elapsed, loss: 1.9296967e-06\n",
      "step: 72600 train: 0.13974571228027344 elapsed, loss: 1.79419e-06\n",
      "step: 72610 train: 0.12146949768066406 elapsed, loss: 2.8059328e-06\n",
      "step: 72620 train: 0.13634300231933594 elapsed, loss: 1.6242234e-06\n",
      "step: 72630 train: 0.1318511962890625 elapsed, loss: 1.3490192e-06\n",
      "step: 72640 train: 0.12894082069396973 elapsed, loss: 1.9795216e-06\n",
      "step: 72650 train: 0.13577771186828613 elapsed, loss: 1.5776584e-06\n",
      "step: 72660 train: 0.12601447105407715 elapsed, loss: 1.715493e-06\n",
      "step: 72670 train: 0.13053035736083984 elapsed, loss: 1.4277159e-06\n",
      "step: 72680 train: 0.13520121574401855 elapsed, loss: 1.6437815e-06\n",
      "step: 72690 train: 0.12885212898254395 elapsed, loss: 1.5767264e-06\n",
      "step: 72700 train: 0.1366417407989502 elapsed, loss: 1.4295784e-06\n",
      "step: 72710 train: 0.13037776947021484 elapsed, loss: 3.075682e-06\n",
      "step: 72720 train: 0.12430548667907715 elapsed, loss: 1.9371473e-06\n",
      "step: 72730 train: 0.12451601028442383 elapsed, loss: 2.3152638e-06\n",
      "step: 72740 train: 0.12974309921264648 elapsed, loss: 3.5506512e-06\n",
      "step: 72750 train: 0.12509989738464355 elapsed, loss: 2.413508e-06\n",
      "step: 72760 train: 0.12452054023742676 elapsed, loss: 2.3613632e-06\n",
      "step: 72770 train: 0.12782549858093262 elapsed, loss: 1.5334207e-06\n",
      "step: 72780 train: 0.13266921043395996 elapsed, loss: 2.2393351e-06\n",
      "step: 72790 train: 0.1397850513458252 elapsed, loss: 3.5203816e-06\n",
      "step: 72800 train: 0.12191128730773926 elapsed, loss: 3.5120052e-06\n",
      "step: 72810 train: 0.1375577449798584 elapsed, loss: 1.7117678e-06\n",
      "step: 72820 train: 0.12286996841430664 elapsed, loss: 2.5005959e-06\n",
      "step: 72830 train: 0.12961149215698242 elapsed, loss: 1.5762616e-06\n",
      "step: 72840 train: 0.12580037117004395 elapsed, loss: 2.7045512e-06\n",
      "step: 72850 train: 0.12356424331665039 elapsed, loss: 2.5285267e-06\n",
      "step: 72860 train: 0.14041972160339355 elapsed, loss: 1.8868561e-06\n",
      "step: 72870 train: 0.13593673706054688 elapsed, loss: 2.131328e-06\n",
      "step: 72880 train: 0.13969707489013672 elapsed, loss: 2.6295825e-06\n",
      "step: 72890 train: 0.12434554100036621 elapsed, loss: 2.514565e-06\n",
      "step: 72900 train: 0.13254117965698242 elapsed, loss: 1.8821999e-06\n",
      "step: 72910 train: 0.12254738807678223 elapsed, loss: 2.9480868e-06\n",
      "step: 72920 train: 0.1319277286529541 elapsed, loss: 1.983714e-06\n",
      "step: 72930 train: 0.12988591194152832 elapsed, loss: 2.9922985e-06\n",
      "step: 72940 train: 0.12403106689453125 elapsed, loss: 2.9560056e-06\n",
      "step: 72950 train: 0.1304950714111328 elapsed, loss: 2.3557764e-06\n",
      "step: 72960 train: 0.12359142303466797 elapsed, loss: 3.257751e-06\n",
      "step: 72970 train: 0.13054418563842773 elapsed, loss: 2.9946345e-06\n",
      "step: 72980 train: 0.1256086826324463 elapsed, loss: 2.7641568e-06\n",
      "step: 72990 train: 0.13855838775634766 elapsed, loss: 2.421881e-06\n",
      "step: 73000 train: 0.13827276229858398 elapsed, loss: 1.9362164e-06\n",
      "step: 73010 train: 0.14965128898620605 elapsed, loss: 1.5557723e-06\n",
      "step: 73020 train: 0.13517355918884277 elapsed, loss: 0.025714444\n",
      "step: 73030 train: 0.136915922164917 elapsed, loss: 0.00023346119\n",
      "step: 73040 train: 0.1316208839416504 elapsed, loss: 2.9146771e-05\n",
      "step: 73050 train: 0.12329959869384766 elapsed, loss: 2.3067994e-05\n",
      "step: 73060 train: 0.13178682327270508 elapsed, loss: 1.1190997e-05\n",
      "step: 73070 train: 0.13000273704528809 elapsed, loss: 1.3655428e-05\n",
      "step: 73080 train: 0.13293004035949707 elapsed, loss: 9.406158e-06\n",
      "step: 73090 train: 0.13144421577453613 elapsed, loss: 6.1601922e-06\n",
      "step: 73100 train: 0.12889981269836426 elapsed, loss: 5.9590047e-06\n",
      "step: 73110 train: 0.12856316566467285 elapsed, loss: 6.4576725e-06\n",
      "step: 73120 train: 0.12535524368286133 elapsed, loss: 5.112003e-06\n",
      "step: 73130 train: 0.12866973876953125 elapsed, loss: 4.5690385e-06\n",
      "step: 73140 train: 0.12598586082458496 elapsed, loss: 4.4903486e-06\n",
      "step: 73150 train: 0.12859749794006348 elapsed, loss: 2.861936e-06\n",
      "step: 73160 train: 0.1343393325805664 elapsed, loss: 2.0354025e-06\n",
      "step: 73170 train: 0.12468409538269043 elapsed, loss: 2.0852265e-06\n",
      "step: 73180 train: 0.1264951229095459 elapsed, loss: 2.386043e-06\n",
      "step: 73190 train: 0.12698602676391602 elapsed, loss: 1.8863889e-06\n",
      "step: 73200 train: 0.12965059280395508 elapsed, loss: 2.1415717e-06\n",
      "step: 73210 train: 0.1359553337097168 elapsed, loss: 2.1159426e-06\n",
      "step: 73220 train: 0.12598180770874023 elapsed, loss: 1.6684619e-06\n",
      "step: 73230 train: 0.12917113304138184 elapsed, loss: 1.4845261e-06\n",
      "step: 73240 train: 0.12667322158813477 elapsed, loss: 1.6456447e-06\n",
      "step: 73250 train: 0.14279389381408691 elapsed, loss: 1.5171227e-06\n",
      "step: 73260 train: 0.14181923866271973 elapsed, loss: 1.8575174e-06\n",
      "step: 73270 train: 0.1271371841430664 elapsed, loss: 2.1206183e-06\n",
      "step: 73280 train: 0.12057280540466309 elapsed, loss: 2.3399436e-06\n",
      "step: 73290 train: 0.13261699676513672 elapsed, loss: 1.6451787e-06\n",
      "step: 73300 train: 0.13031506538391113 elapsed, loss: 2.1504175e-06\n",
      "step: 73310 train: 0.13454747200012207 elapsed, loss: 1.627484e-06\n",
      "step: 73320 train: 0.12233757972717285 elapsed, loss: 3.0053648e-06\n",
      "step: 73330 train: 0.13671326637268066 elapsed, loss: 1.7848711e-06\n",
      "step: 73340 train: 0.13738155364990234 elapsed, loss: 1.4398232e-06\n",
      "step: 73350 train: 0.12499761581420898 elapsed, loss: 2.5955908e-06\n",
      "step: 73360 train: 0.12877774238586426 elapsed, loss: 2.0973348e-06\n",
      "step: 73370 train: 0.12544822692871094 elapsed, loss: 2.6971047e-06\n",
      "step: 73380 train: 0.12232828140258789 elapsed, loss: 6.0567954e-06\n",
      "step: 73390 train: 0.13071250915527344 elapsed, loss: 2.0945406e-06\n",
      "step: 73400 train: 0.1343214511871338 elapsed, loss: 2.769625e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 73410 train: 0.12450361251831055 elapsed, loss: 2.446577e-06\n",
      "step: 73420 train: 0.12343692779541016 elapsed, loss: 2.110373e-06\n",
      "step: 73430 train: 0.12583637237548828 elapsed, loss: 2.3380808e-06\n",
      "step: 73440 train: 0.12577557563781738 elapsed, loss: 2.7585606e-06\n",
      "step: 73450 train: 0.13692259788513184 elapsed, loss: 2.1853423e-06\n",
      "step: 73460 train: 0.1235661506652832 elapsed, loss: 3.3671881e-06\n",
      "step: 73470 train: 0.1315748691558838 elapsed, loss: 2.3408716e-06\n",
      "step: 73480 train: 0.1371145248413086 elapsed, loss: 1.9245754e-06\n",
      "step: 73490 train: 0.12824749946594238 elapsed, loss: 2.0237608e-06\n",
      "step: 73500 train: 0.12796831130981445 elapsed, loss: 2.718059e-06\n",
      "step: 73510 train: 0.12868595123291016 elapsed, loss: 2.4782441e-06\n",
      "step: 73520 train: 0.14250469207763672 elapsed, loss: 2.3152921e-05\n",
      "step: 73530 train: 0.11680936813354492 elapsed, loss: 0.023878241\n",
      "step: 73540 train: 0.1268172264099121 elapsed, loss: 8.12261e-05\n",
      "step: 73550 train: 0.12909436225891113 elapsed, loss: 3.1259806e-05\n",
      "step: 73560 train: 0.13962531089782715 elapsed, loss: 3.0718085e-05\n",
      "step: 73570 train: 0.13875508308410645 elapsed, loss: 2.2101322e-05\n",
      "step: 73580 train: 0.12998223304748535 elapsed, loss: 3.3179407e-05\n",
      "step: 73590 train: 0.12962126731872559 elapsed, loss: 1.3298053e-05\n",
      "step: 73600 train: 0.1472618579864502 elapsed, loss: 6.7468663e-06\n",
      "step: 73610 train: 0.13583731651306152 elapsed, loss: 5.9622826e-06\n",
      "step: 73620 train: 0.13019132614135742 elapsed, loss: 9.89301e-06\n",
      "step: 73630 train: 0.13303256034851074 elapsed, loss: 5.2334763e-06\n",
      "step: 73640 train: 0.12380433082580566 elapsed, loss: 4.4042054e-06\n",
      "step: 73650 train: 0.1316699981689453 elapsed, loss: 3.481723e-06\n",
      "step: 73660 train: 0.13096141815185547 elapsed, loss: 3.4379639e-06\n",
      "step: 73670 train: 0.12549233436584473 elapsed, loss: 2.2966365e-06\n",
      "step: 73680 train: 0.13228702545166016 elapsed, loss: 2.9876608e-06\n",
      "step: 73690 train: 0.13413619995117188 elapsed, loss: 1.6344677e-06\n",
      "step: 73700 train: 0.12586569786071777 elapsed, loss: 1.7695102e-06\n",
      "step: 73710 train: 0.13294529914855957 elapsed, loss: 1.943201e-06\n",
      "step: 73720 train: 0.1347484588623047 elapsed, loss: 1.7131619e-06\n",
      "step: 73730 train: 0.12431931495666504 elapsed, loss: 2.1178225e-06\n",
      "step: 73740 train: 0.13077163696289062 elapsed, loss: 1.6549582e-06\n",
      "step: 73750 train: 0.13448047637939453 elapsed, loss: 1.7811518e-06\n",
      "step: 73760 train: 0.12428402900695801 elapsed, loss: 2.0787088e-06\n",
      "step: 73770 train: 0.13971233367919922 elapsed, loss: 1.5776584e-06\n",
      "step: 73780 train: 0.11544156074523926 elapsed, loss: 2.3953571e-06\n",
      "step: 73790 train: 0.12154531478881836 elapsed, loss: 2.1946541e-06\n",
      "step: 73800 train: 0.12875938415527344 elapsed, loss: 1.6363305e-06\n",
      "step: 73810 train: 0.14289212226867676 elapsed, loss: 1.4370285e-06\n",
      "step: 73820 train: 0.12568950653076172 elapsed, loss: 2.2947743e-06\n",
      "step: 73830 train: 0.12960171699523926 elapsed, loss: 1.7960526e-06\n",
      "step: 73840 train: 0.1271822452545166 elapsed, loss: 2.2291142e-06\n",
      "step: 73850 train: 0.11745738983154297 elapsed, loss: 2.6142159e-06\n",
      "step: 73860 train: 0.12302970886230469 elapsed, loss: 6.6932803e-06\n",
      "step: 73870 train: 0.13225269317626953 elapsed, loss: 2.5480936e-06\n",
      "step: 73880 train: 0.13223671913146973 elapsed, loss: 2.4125814e-06\n",
      "step: 73890 train: 0.12764334678649902 elapsed, loss: 2.230048e-06\n",
      "step: 73900 train: 0.12834644317626953 elapsed, loss: 1.8761465e-06\n",
      "step: 73910 train: 0.1373579502105713 elapsed, loss: 1.6172394e-06\n",
      "step: 73920 train: 0.13043737411499023 elapsed, loss: 1.8877868e-06\n",
      "step: 73930 train: 0.14082622528076172 elapsed, loss: 1.3336523e-06\n",
      "step: 73940 train: 0.1341085433959961 elapsed, loss: 2.058219e-06\n",
      "step: 73950 train: 0.12307024002075195 elapsed, loss: 1.8100232e-06\n",
      "step: 73960 train: 0.13104796409606934 elapsed, loss: 2.4600843e-06\n",
      "step: 73970 train: 0.12401342391967773 elapsed, loss: 2.4782446e-06\n",
      "step: 73980 train: 0.12988805770874023 elapsed, loss: 2.4735866e-06\n",
      "step: 73990 train: 0.134415864944458 elapsed, loss: 2.122944e-06\n",
      "step: 74000 train: 0.12916946411132812 elapsed, loss: 1.0221903e-05\n",
      "step: 74010 train: 0.11614727973937988 elapsed, loss: 3.4607851e-06\n",
      "step: 74020 train: 0.13536715507507324 elapsed, loss: 1.8453355e-05\n",
      "step: 74030 train: 0.13689208030700684 elapsed, loss: 0.00019987742\n",
      "step: 74040 train: 0.12461996078491211 elapsed, loss: 4.7278503e-05\n",
      "step: 74050 train: 0.12662720680236816 elapsed, loss: 2.3296801e-05\n",
      "step: 74060 train: 0.12734007835388184 elapsed, loss: 3.6793834e-05\n",
      "step: 74070 train: 0.11571216583251953 elapsed, loss: 2.4400546e-05\n",
      "step: 74080 train: 0.12034368515014648 elapsed, loss: 1.5972411e-05\n",
      "step: 74090 train: 0.1249539852142334 elapsed, loss: 1.1572936e-05\n",
      "step: 74100 train: 0.14608097076416016 elapsed, loss: 6.8600266e-06\n",
      "step: 74110 train: 0.13535761833190918 elapsed, loss: 6.84327e-06\n",
      "step: 74120 train: 0.13370490074157715 elapsed, loss: 4.810232e-06\n",
      "step: 74130 train: 0.11937189102172852 elapsed, loss: 4.4097796e-06\n",
      "step: 74140 train: 0.14081382751464844 elapsed, loss: 3.4849825e-06\n",
      "step: 74150 train: 0.1367490291595459 elapsed, loss: 3.3936967e-06\n",
      "step: 74160 train: 0.13143587112426758 elapsed, loss: 2.9564753e-06\n",
      "step: 74170 train: 0.13239550590515137 elapsed, loss: 2.3036177e-06\n",
      "step: 74180 train: 0.13162493705749512 elapsed, loss: 1.620499e-06\n",
      "step: 74190 train: 0.12543463706970215 elapsed, loss: 1.9418032e-06\n",
      "step: 74200 train: 0.1221613883972168 elapsed, loss: 1.9329566e-06\n",
      "step: 74210 train: 0.1290440559387207 elapsed, loss: 1.6610114e-06\n",
      "step: 74220 train: 0.12573862075805664 elapsed, loss: 1.5133973e-06\n",
      "step: 74230 train: 0.1297311782836914 elapsed, loss: 2.0800912e-06\n",
      "step: 74240 train: 0.14332270622253418 elapsed, loss: 1.8575201e-06\n",
      "step: 74250 train: 0.13125061988830566 elapsed, loss: 2.5383138e-06\n",
      "step: 74260 train: 0.1349930763244629 elapsed, loss: 1.7066463e-06\n",
      "step: 74270 train: 0.12144827842712402 elapsed, loss: 2.0745174e-06\n",
      "step: 74280 train: 0.12913036346435547 elapsed, loss: 1.5087406e-06\n",
      "step: 74290 train: 0.1292259693145752 elapsed, loss: 1.8649707e-06\n",
      "step: 74300 train: 0.12199068069458008 elapsed, loss: 2.083365e-06\n",
      "step: 74310 train: 0.13401556015014648 elapsed, loss: 1.8579822e-06\n",
      "step: 74320 train: 0.13230228424072266 elapsed, loss: 1.6144455e-06\n",
      "step: 74330 train: 0.13082075119018555 elapsed, loss: 1.7504183e-06\n",
      "step: 74340 train: 0.12380099296569824 elapsed, loss: 1.8542605e-06\n",
      "step: 74350 train: 0.12940239906311035 elapsed, loss: 1.961362e-06\n",
      "step: 74360 train: 0.1263277530670166 elapsed, loss: 1.9781257e-06\n",
      "step: 74370 train: 0.13586020469665527 elapsed, loss: 1.9180557e-06\n",
      "step: 74380 train: 0.12862443923950195 elapsed, loss: 3.0579868e-06\n",
      "step: 74390 train: 0.1318223476409912 elapsed, loss: 1.6675306e-06\n",
      "step: 74400 train: 0.13357925415039062 elapsed, loss: 2.771125e-06\n",
      "step: 74410 train: 0.12990021705627441 elapsed, loss: 2.6170069e-06\n",
      "step: 74420 train: 0.13050222396850586 elapsed, loss: 2.493609e-06\n",
      "step: 74430 train: 0.12094855308532715 elapsed, loss: 2.4298158e-06\n",
      "step: 74440 train: 0.14012813568115234 elapsed, loss: 1.5436653e-06\n",
      "step: 74450 train: 0.12635207176208496 elapsed, loss: 2.0987318e-06\n",
      "step: 74460 train: 0.12071919441223145 elapsed, loss: 2.7161927e-06\n",
      "step: 74470 train: 0.13581442832946777 elapsed, loss: 2.8288853e-06\n",
      "step: 74480 train: 0.1317284107208252 elapsed, loss: 2.0949134e-05\n",
      "step: 74490 train: 0.13044452667236328 elapsed, loss: 0.00068008853\n",
      "step: 74500 train: 0.13952970504760742 elapsed, loss: 3.860438e-05\n",
      "step: 74510 train: 0.13337087631225586 elapsed, loss: 1.8285813e-05\n",
      "step: 74520 train: 0.1306154727935791 elapsed, loss: 1.5283835e-05\n",
      "step: 74530 train: 0.1338813304901123 elapsed, loss: 1.2961529e-05\n",
      "step: 74540 train: 0.14572739601135254 elapsed, loss: 7.0221076e-06\n",
      "step: 74550 train: 0.14206552505493164 elapsed, loss: 6.107089e-06\n",
      "step: 74560 train: 0.14460515975952148 elapsed, loss: 5.4444336e-06\n",
      "step: 74570 train: 0.12392473220825195 elapsed, loss: 5.4132825e-06\n",
      "step: 74580 train: 0.1365365982055664 elapsed, loss: 3.5618127e-06\n",
      "step: 74590 train: 0.13194608688354492 elapsed, loss: 3.6414522e-06\n",
      "step: 74600 train: 0.12744951248168945 elapsed, loss: 2.7366848e-06\n",
      "step: 74610 train: 0.12430071830749512 elapsed, loss: 3.1185205e-06\n",
      "step: 74620 train: 0.12459731101989746 elapsed, loss: 5.138545e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 74630 train: 0.12032580375671387 elapsed, loss: 3.3932604e-06\n",
      "step: 74640 train: 0.13216924667358398 elapsed, loss: 2.6198036e-06\n",
      "step: 74650 train: 0.1287672519683838 elapsed, loss: 2.224458e-06\n",
      "step: 74660 train: 0.1317734718322754 elapsed, loss: 1.7019884e-06\n",
      "step: 74670 train: 0.13087797164916992 elapsed, loss: 2.1029223e-06\n",
      "step: 74680 train: 0.14074993133544922 elapsed, loss: 1.5725357e-06\n",
      "step: 74690 train: 0.1431593894958496 elapsed, loss: 1.7895329e-06\n",
      "step: 74700 train: 0.12916135787963867 elapsed, loss: 1.9306276e-06\n",
      "step: 74710 train: 0.12381291389465332 elapsed, loss: 7.0685383e-06\n",
      "step: 74720 train: 0.11940479278564453 elapsed, loss: 2.9322614e-06\n",
      "step: 74730 train: 0.12837576866149902 elapsed, loss: 2.4628775e-06\n",
      "step: 74740 train: 0.12355852127075195 elapsed, loss: 2.2337729e-06\n",
      "step: 74750 train: 0.1285231113433838 elapsed, loss: 2.0670668e-06\n",
      "step: 74760 train: 0.12194323539733887 elapsed, loss: 2.212353e-06\n",
      "step: 74770 train: 0.12545323371887207 elapsed, loss: 2.0153782e-06\n",
      "step: 74780 train: 0.1295933723449707 elapsed, loss: 1.7234096e-06\n",
      "step: 74790 train: 0.1268291473388672 elapsed, loss: 1.7131657e-06\n",
      "step: 74800 train: 0.12158489227294922 elapsed, loss: 2.1057158e-06\n",
      "step: 74810 train: 0.12632226943969727 elapsed, loss: 1.4109515e-06\n",
      "step: 74820 train: 0.12909483909606934 elapsed, loss: 1.3839438e-06\n",
      "step: 74830 train: 0.15354537963867188 elapsed, loss: 1.4207307e-06\n",
      "step: 74840 train: 0.12560033798217773 elapsed, loss: 2.1262058e-06\n",
      "step: 74850 train: 0.1302356719970703 elapsed, loss: 2.3907e-06\n",
      "step: 74860 train: 0.1304941177368164 elapsed, loss: 1.4789388e-06\n",
      "step: 74870 train: 0.13164997100830078 elapsed, loss: 2.303539e-06\n",
      "step: 74880 train: 0.12328839302062988 elapsed, loss: 2.3259727e-06\n",
      "step: 74890 train: 0.1346111297607422 elapsed, loss: 2.055425e-06\n",
      "step: 74900 train: 0.1390082836151123 elapsed, loss: 2.6267867e-06\n",
      "step: 74910 train: 0.141221284866333 elapsed, loss: 2.2859267e-06\n",
      "step: 74920 train: 0.13486170768737793 elapsed, loss: 1.4146776e-06\n",
      "step: 74930 train: 0.12799620628356934 elapsed, loss: 1.7657854e-06\n",
      "step: 74940 train: 0.13141369819641113 elapsed, loss: 1.606064e-06\n",
      "step: 74950 train: 0.12520647048950195 elapsed, loss: 2.0144466e-06\n",
      "step: 74960 train: 0.12715554237365723 elapsed, loss: 2.0074626e-06\n",
      "step: 74970 train: 0.12931203842163086 elapsed, loss: 1.996285e-06\n",
      "step: 74980 train: 0.13760638236999512 elapsed, loss: 1.7909308e-06\n",
      "step: 74990 train: 0.1316814422607422 elapsed, loss: 2.2761387e-06\n",
      "step: 75000 train: 0.1359701156616211 elapsed, loss: 2.1164267e-06\n",
      "step: 75010 train: 0.13392424583435059 elapsed, loss: 1.7317919e-06\n",
      "step: 75020 train: 0.12531423568725586 elapsed, loss: 2.0554248e-06\n",
      "step: 75030 train: 0.12438416481018066 elapsed, loss: 2.4479764e-06\n",
      "step: 75040 train: 0.13411688804626465 elapsed, loss: 2.2635745e-06\n",
      "step: 75050 train: 0.13159537315368652 elapsed, loss: 2.4666024e-06\n",
      "step: 75060 train: 0.13524937629699707 elapsed, loss: 1.8062974e-06\n",
      "step: 75070 train: 0.12398862838745117 elapsed, loss: 2.6430873e-06\n",
      "step: 75080 train: 0.13039040565490723 elapsed, loss: 2.1187552e-06\n",
      "step: 75090 train: 0.12233972549438477 elapsed, loss: 2.4707936e-06\n",
      "step: 75100 train: 0.12462878227233887 elapsed, loss: 6.8431145e-06\n",
      "step: 75110 train: 0.12580585479736328 elapsed, loss: 7.738282e-05\n",
      "step: 75120 train: 0.11786818504333496 elapsed, loss: 1.0896331e-05\n",
      "step: 75130 train: 0.13344383239746094 elapsed, loss: 5.444947e-06\n",
      "step: 75140 train: 0.13091731071472168 elapsed, loss: 5.226555e-06\n",
      "step: 75150 train: 0.12187886238098145 elapsed, loss: 6.101461e-06\n",
      "step: 75160 train: 0.13561582565307617 elapsed, loss: 3.362994e-06\n",
      "step: 75170 train: 0.13117003440856934 elapsed, loss: 3.33878e-06\n",
      "step: 75180 train: 0.1400604248046875 elapsed, loss: 2.318057e-06\n",
      "step: 75190 train: 0.1419363021850586 elapsed, loss: 0.019403476\n",
      "step: 75200 train: 0.12879037857055664 elapsed, loss: 8.602264e-05\n",
      "step: 75210 train: 0.14010214805603027 elapsed, loss: 4.9113412e-05\n",
      "step: 75220 train: 0.12319731712341309 elapsed, loss: 4.4103435e-05\n",
      "step: 75230 train: 0.13890671730041504 elapsed, loss: 2.7324724e-05\n",
      "step: 75240 train: 0.12185239791870117 elapsed, loss: 1.4353199e-05\n",
      "step: 75250 train: 0.1254885196685791 elapsed, loss: 1.3665045e-05\n",
      "step: 75260 train: 0.12434077262878418 elapsed, loss: 2.386041e-05\n",
      "step: 75270 train: 0.11877202987670898 elapsed, loss: 8.811059e-06\n",
      "step: 75280 train: 0.12217092514038086 elapsed, loss: 9.201114e-06\n",
      "step: 75290 train: 0.13148856163024902 elapsed, loss: 5.1175866e-06\n",
      "step: 75300 train: 0.12093567848205566 elapsed, loss: 4.4777626e-06\n",
      "step: 75310 train: 0.12122058868408203 elapsed, loss: 4.5876745e-06\n",
      "step: 75320 train: 0.1298367977142334 elapsed, loss: 3.5753224e-06\n",
      "step: 75330 train: 0.13239526748657227 elapsed, loss: 2.4139827e-06\n",
      "step: 75340 train: 0.12125158309936523 elapsed, loss: 2.5192219e-06\n",
      "step: 75350 train: 0.13288450241088867 elapsed, loss: 1.5161897e-06\n",
      "step: 75360 train: 0.13729476928710938 elapsed, loss: 1.435166e-06\n",
      "step: 75370 train: 0.1250762939453125 elapsed, loss: 2.235169e-06\n",
      "step: 75380 train: 0.13268375396728516 elapsed, loss: 2.102919e-06\n",
      "step: 75390 train: 0.12778282165527344 elapsed, loss: 1.922705e-06\n",
      "step: 75400 train: 0.1246497631072998 elapsed, loss: 4.0758955e-06\n",
      "step: 75410 train: 0.1347031593322754 elapsed, loss: 2.2542622e-06\n",
      "step: 75420 train: 0.12734174728393555 elapsed, loss: 2.1737028e-06\n",
      "step: 75430 train: 0.1292273998260498 elapsed, loss: 0.00037654984\n",
      "step: 75440 train: 0.12464380264282227 elapsed, loss: 0.00012953818\n",
      "step: 75450 train: 0.13178014755249023 elapsed, loss: 2.5378937e-05\n",
      "step: 75460 train: 0.13224172592163086 elapsed, loss: 1.4650745e-05\n",
      "step: 75470 train: 0.12005615234375 elapsed, loss: 1.6337415e-05\n",
      "step: 75480 train: 0.12731266021728516 elapsed, loss: 2.3865432e-05\n",
      "step: 75490 train: 0.13229703903198242 elapsed, loss: 6.6481725e-06\n",
      "step: 75500 train: 0.1259174346923828 elapsed, loss: 6.2742624e-06\n",
      "step: 75510 train: 0.11876368522644043 elapsed, loss: 5.706174e-06\n",
      "step: 75520 train: 0.1298220157623291 elapsed, loss: 3.4840673e-06\n",
      "step: 75530 train: 0.11933112144470215 elapsed, loss: 5.9643417e-06\n",
      "step: 75540 train: 0.12675905227661133 elapsed, loss: 3.4640404e-06\n",
      "step: 75550 train: 0.13977575302124023 elapsed, loss: 2.1816181e-06\n",
      "step: 75560 train: 0.12650680541992188 elapsed, loss: 3.40954e-06\n",
      "step: 75570 train: 0.12763452529907227 elapsed, loss: 2.3068815e-06\n",
      "step: 75580 train: 0.1350693702697754 elapsed, loss: 1.7243406e-06\n",
      "step: 75590 train: 0.12221264839172363 elapsed, loss: 2.0405187e-06\n",
      "step: 75600 train: 0.13257098197937012 elapsed, loss: 1.5953533e-06\n",
      "step: 75610 train: 0.11555886268615723 elapsed, loss: 2.1890653e-06\n",
      "step: 75620 train: 0.12500500679016113 elapsed, loss: 1.6926763e-06\n",
      "step: 75630 train: 0.12976574897766113 elapsed, loss: 1.4114173e-06\n",
      "step: 75640 train: 0.13013458251953125 elapsed, loss: 1.4742816e-06\n",
      "step: 75650 train: 0.1240997314453125 elapsed, loss: 1.3513463e-06\n",
      "step: 75660 train: 0.12021327018737793 elapsed, loss: 1.644248e-06\n",
      "step: 75670 train: 0.1270146369934082 elapsed, loss: 2.4218316e-06\n",
      "step: 75680 train: 0.1270618438720703 elapsed, loss: 1.9226918e-06\n",
      "step: 75690 train: 0.11618351936340332 elapsed, loss: 2.04844e-06\n",
      "step: 75700 train: 0.1268157958984375 elapsed, loss: 1.5012893e-06\n",
      "step: 75710 train: 0.12422776222229004 elapsed, loss: 1.8649699e-06\n",
      "step: 75720 train: 0.1410667896270752 elapsed, loss: 1.6936074e-06\n",
      "step: 75730 train: 0.12876415252685547 elapsed, loss: 1.523176e-06\n",
      "step: 75740 train: 0.13786888122558594 elapsed, loss: 2.784641e-06\n",
      "step: 75750 train: 0.13500595092773438 elapsed, loss: 1.669858e-06\n",
      "step: 75760 train: 0.1368250846862793 elapsed, loss: 1.7527457e-06\n",
      "step: 75770 train: 0.1349198818206787 elapsed, loss: 1.3797529e-06\n",
      "step: 75780 train: 0.13376593589782715 elapsed, loss: 1.6670651e-06\n",
      "step: 75790 train: 0.12802934646606445 elapsed, loss: 3.1497216e-06\n",
      "step: 75800 train: 0.11739516258239746 elapsed, loss: 2.0563573e-06\n",
      "step: 75810 train: 0.13254261016845703 elapsed, loss: 2.4810324e-06\n",
      "step: 75820 train: 0.13039422035217285 elapsed, loss: 1.9692782e-06\n",
      "step: 75830 train: 0.13107991218566895 elapsed, loss: 1.9902284e-06\n",
      "step: 75840 train: 0.12307429313659668 elapsed, loss: 1.9329573e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 75850 train: 0.13762617111206055 elapsed, loss: 1.9194508e-06\n",
      "step: 75860 train: 0.1424882411956787 elapsed, loss: 1.4910455e-06\n",
      "step: 75870 train: 0.14431047439575195 elapsed, loss: 1.5362145e-06\n",
      "step: 75880 train: 0.13822102546691895 elapsed, loss: 1.6326065e-06\n",
      "step: 75890 train: 0.127593994140625 elapsed, loss: 2.3338898e-06\n",
      "step: 75900 train: 0.13719463348388672 elapsed, loss: 1.6437817e-06\n",
      "step: 75910 train: 0.11535143852233887 elapsed, loss: 3.1241116e-06\n",
      "step: 75920 train: 0.13086795806884766 elapsed, loss: 1.7723046e-06\n",
      "step: 75930 train: 0.13447213172912598 elapsed, loss: 1.6307436e-06\n",
      "step: 75940 train: 0.12359905242919922 elapsed, loss: 2.2905836e-06\n",
      "step: 75950 train: 0.12115263938903809 elapsed, loss: 2.9592693e-06\n",
      "step: 75960 train: 0.11685562133789062 elapsed, loss: 2.971376e-06\n",
      "step: 75970 train: 0.12619948387145996 elapsed, loss: 2.4549608e-06\n",
      "step: 75980 train: 0.13320064544677734 elapsed, loss: 2.155064e-06\n",
      "step: 75990 train: 0.11336851119995117 elapsed, loss: 3.3080455e-06\n",
      "step: 76000 train: 0.13260436058044434 elapsed, loss: 8.876683e-06\n",
      "step: 76010 train: 0.13610148429870605 elapsed, loss: 4.8603115e-06\n",
      "step: 76020 train: 0.13141107559204102 elapsed, loss: 2.9080404e-06\n",
      "step: 76030 train: 0.1261577606201172 elapsed, loss: 3.4519335e-06\n",
      "step: 76040 train: 0.14567780494689941 elapsed, loss: 2.0628759e-06\n",
      "step: 76050 train: 0.13595795631408691 elapsed, loss: 1.7504185e-06\n",
      "step: 76060 train: 0.1302652359008789 elapsed, loss: 2.13645e-06\n",
      "step: 76070 train: 0.12474822998046875 elapsed, loss: 2.451235e-06\n",
      "step: 76080 train: 0.12736725807189941 elapsed, loss: 2.7376118e-06\n",
      "step: 76090 train: 0.13535737991333008 elapsed, loss: 1.6838245e-06\n",
      "step: 76100 train: 0.11863160133361816 elapsed, loss: 2.6374998e-06\n",
      "step: 76110 train: 0.12820768356323242 elapsed, loss: 1.9776608e-06\n",
      "step: 76120 train: 0.13145923614501953 elapsed, loss: 2.375793e-06\n",
      "step: 76130 train: 0.1310408115386963 elapsed, loss: 1.9371469e-06\n",
      "step: 76140 train: 0.13632416725158691 elapsed, loss: 1.8631068e-06\n",
      "step: 76150 train: 0.13362932205200195 elapsed, loss: 2.3469283e-06\n",
      "step: 76160 train: 0.13738560676574707 elapsed, loss: 2.3944249e-06\n",
      "step: 76170 train: 0.13726139068603516 elapsed, loss: 1.5725358e-06\n",
      "step: 76180 train: 0.12164497375488281 elapsed, loss: 2.5131694e-06\n",
      "step: 76190 train: 0.12880396842956543 elapsed, loss: 2.4191004e-06\n",
      "step: 76200 train: 0.13277196884155273 elapsed, loss: 2.0759148e-06\n",
      "step: 76210 train: 0.12926125526428223 elapsed, loss: 2.367417e-06\n",
      "step: 76220 train: 0.12494993209838867 elapsed, loss: 2.1345877e-06\n",
      "step: 76230 train: 0.12378215789794922 elapsed, loss: 2.528536e-06\n",
      "step: 76240 train: 0.11506962776184082 elapsed, loss: 3.4519242e-06\n",
      "step: 76250 train: 0.12343621253967285 elapsed, loss: 2.8814977e-06\n",
      "step: 76260 train: 0.13520503044128418 elapsed, loss: 1.8579851e-06\n",
      "step: 76270 train: 0.12766337394714355 elapsed, loss: 2.1234114e-06\n",
      "step: 76280 train: 0.1278698444366455 elapsed, loss: 2.4684623e-06\n",
      "step: 76290 train: 0.12366199493408203 elapsed, loss: 2.623064e-06\n",
      "step: 76300 train: 0.1277921199798584 elapsed, loss: 1.9692782e-06\n",
      "step: 76310 train: 0.13542914390563965 elapsed, loss: 1.8170075e-06\n",
      "step: 76320 train: 0.1279294490814209 elapsed, loss: 2.5075801e-06\n",
      "step: 76330 train: 0.12435317039489746 elapsed, loss: 2.8875595e-06\n",
      "step: 76340 train: 0.1345508098602295 elapsed, loss: 1.9553074e-06\n",
      "step: 76350 train: 0.12726140022277832 elapsed, loss: 2.0828988e-06\n",
      "step: 76360 train: 0.14506959915161133 elapsed, loss: 1.5767273e-06\n",
      "step: 76370 train: 0.13750243186950684 elapsed, loss: 2.1480907e-06\n",
      "step: 76380 train: 0.141463041305542 elapsed, loss: 2.027952e-06\n",
      "step: 76390 train: 0.13851618766784668 elapsed, loss: 2.3087437e-06\n",
      "step: 76400 train: 0.14284682273864746 elapsed, loss: 2.1271364e-06\n",
      "step: 76410 train: 0.1382901668548584 elapsed, loss: 2.0405244e-06\n",
      "step: 76420 train: 0.13553571701049805 elapsed, loss: 2.8009345e-06\n",
      "step: 76430 train: 0.12694287300109863 elapsed, loss: 2.4950086e-06\n",
      "step: 76440 train: 0.1347661018371582 elapsed, loss: 2.5685815e-06\n",
      "step: 76450 train: 0.134474515914917 elapsed, loss: 3.263809e-06\n",
      "step: 76460 train: 0.12230634689331055 elapsed, loss: 6.1504097e-06\n",
      "step: 76470 train: 0.13133955001831055 elapsed, loss: 2.4028059e-06\n",
      "step: 76480 train: 0.13779187202453613 elapsed, loss: 2.4042051e-06\n",
      "step: 76490 train: 0.1274735927581787 elapsed, loss: 2.9648568e-06\n",
      "step: 76500 train: 0.13964390754699707 elapsed, loss: 1.7415709e-06\n",
      "step: 76510 train: 0.11791276931762695 elapsed, loss: 1.7091128e-05\n",
      "step: 76520 train: 0.1375102996826172 elapsed, loss: 8.363533e-06\n",
      "step: 76530 train: 0.12899184226989746 elapsed, loss: 1.0180624e-05\n",
      "step: 76540 train: 0.13768339157104492 elapsed, loss: 4.568576e-06\n",
      "step: 76550 train: 0.13006949424743652 elapsed, loss: 4.92387e-06\n",
      "step: 76560 train: 0.12326383590698242 elapsed, loss: 4.514098e-06\n",
      "step: 76570 train: 0.11867117881774902 elapsed, loss: 4.667747e-06\n",
      "step: 76580 train: 0.12596487998962402 elapsed, loss: 2.6500693e-06\n",
      "step: 76590 train: 0.12066078186035156 elapsed, loss: 3.402551e-06\n",
      "step: 76600 train: 0.12094783782958984 elapsed, loss: 2.5261215e-05\n",
      "step: 76610 train: 0.12285614013671875 elapsed, loss: 0.00013900008\n",
      "step: 76620 train: 0.12103962898254395 elapsed, loss: 0.0004583698\n",
      "step: 76630 train: 0.14496302604675293 elapsed, loss: 3.5707257e-05\n",
      "step: 76640 train: 0.12732315063476562 elapsed, loss: 2.9276423e-05\n",
      "step: 76650 train: 0.12465405464172363 elapsed, loss: 2.5972184e-05\n",
      "step: 76660 train: 0.1244208812713623 elapsed, loss: 1.2951648e-05\n",
      "step: 76670 train: 0.1328732967376709 elapsed, loss: 1.2286305e-05\n",
      "step: 76680 train: 0.12841534614562988 elapsed, loss: 1.7034035e-05\n",
      "step: 76690 train: 0.13152503967285156 elapsed, loss: 9.033251e-06\n",
      "step: 76700 train: 0.12696027755737305 elapsed, loss: 6.10711e-06\n",
      "step: 76710 train: 0.12848305702209473 elapsed, loss: 4.5806773e-06\n",
      "step: 76720 train: 0.12123465538024902 elapsed, loss: 5.5152555e-06\n",
      "step: 76730 train: 0.12197160720825195 elapsed, loss: 4.7948806e-06\n",
      "step: 76740 train: 0.13100576400756836 elapsed, loss: 2.8433192e-06\n",
      "step: 76750 train: 0.11861276626586914 elapsed, loss: 2.888489e-06\n",
      "step: 76760 train: 0.1434035301208496 elapsed, loss: 2.28313e-06\n",
      "step: 76770 train: 0.1385207176208496 elapsed, loss: 1.98837e-06\n",
      "step: 76780 train: 0.14318418502807617 elapsed, loss: 1.9906984e-06\n",
      "step: 76790 train: 0.13413405418395996 elapsed, loss: 2.1327241e-06\n",
      "step: 76800 train: 0.1317300796508789 elapsed, loss: 1.7480901e-06\n",
      "step: 76810 train: 0.1366121768951416 elapsed, loss: 1.5003588e-06\n",
      "step: 76820 train: 0.12902045249938965 elapsed, loss: 1.5408707e-06\n",
      "step: 76830 train: 0.1291520595550537 elapsed, loss: 2.8242246e-06\n",
      "step: 76840 train: 0.12944817543029785 elapsed, loss: 2.0493696e-06\n",
      "step: 76850 train: 0.13877320289611816 elapsed, loss: 1.456586e-06\n",
      "step: 76860 train: 0.13293123245239258 elapsed, loss: 1.3234067e-06\n",
      "step: 76870 train: 0.12723231315612793 elapsed, loss: 1.756472e-06\n",
      "step: 76880 train: 0.13097572326660156 elapsed, loss: 1.1399379e-06\n",
      "step: 76890 train: 0.1263577938079834 elapsed, loss: 1.8309773e-06\n",
      "step: 76900 train: 0.13675570487976074 elapsed, loss: 1.5157254e-06\n",
      "step: 76910 train: 0.11925387382507324 elapsed, loss: 1.9166587e-06\n",
      "step: 76920 train: 0.13462424278259277 elapsed, loss: 1.5608947e-06\n",
      "step: 76930 train: 0.13313746452331543 elapsed, loss: 1.3778889e-06\n",
      "step: 76940 train: 0.1270749568939209 elapsed, loss: 6.077256e-06\n",
      "step: 76950 train: 0.12384986877441406 elapsed, loss: 2.7026915e-06\n",
      "step: 76960 train: 0.13387298583984375 elapsed, loss: 2.2961617e-06\n",
      "step: 76970 train: 0.1287834644317627 elapsed, loss: 1.7969841e-06\n",
      "step: 76980 train: 0.1291334629058838 elapsed, loss: 2.183015e-06\n",
      "step: 76990 train: 0.11576294898986816 elapsed, loss: 2.1923292e-06\n",
      "step: 77000 train: 0.12661218643188477 elapsed, loss: 2.2076954e-06\n",
      "step: 77010 train: 0.1324605941772461 elapsed, loss: 1.6526292e-06\n",
      "step: 77020 train: 0.1335909366607666 elapsed, loss: 1.3750962e-06\n",
      "step: 77030 train: 0.13094258308410645 elapsed, loss: 2.1289977e-06\n",
      "step: 77040 train: 0.130934476852417 elapsed, loss: 2.084294e-06\n",
      "step: 77050 train: 0.12022995948791504 elapsed, loss: 2.1732374e-06\n",
      "step: 77060 train: 0.11904478073120117 elapsed, loss: 2.188139e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 77070 train: 0.1220712661743164 elapsed, loss: 4.149958e-06\n",
      "step: 77080 train: 0.11947107315063477 elapsed, loss: 3.3946312e-06\n",
      "step: 77090 train: 0.13407039642333984 elapsed, loss: 2.0354018e-06\n",
      "step: 77100 train: 0.14262032508850098 elapsed, loss: 1.5511155e-06\n",
      "step: 77110 train: 0.1317734718322754 elapsed, loss: 2.0004752e-06\n",
      "step: 77120 train: 0.1407155990600586 elapsed, loss: 2.1057167e-06\n",
      "step: 77130 train: 0.13442182540893555 elapsed, loss: 1.6763785e-06\n",
      "step: 77140 train: 0.13609790802001953 elapsed, loss: 1.4090894e-06\n",
      "step: 77150 train: 0.12752461433410645 elapsed, loss: 2.490351e-06\n",
      "step: 77160 train: 0.13240337371826172 elapsed, loss: 1.5986134e-06\n",
      "step: 77170 train: 0.13077783584594727 elapsed, loss: 1.9515815e-06\n",
      "step: 77180 train: 0.13153862953186035 elapsed, loss: 2.12993e-06\n",
      "step: 77190 train: 0.12267851829528809 elapsed, loss: 2.2430863e-06\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
