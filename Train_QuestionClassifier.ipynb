{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 0\n",
    "device = 'cuda:' + str(device_id)\n",
    "#device = 'cpu'\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "log_dir = os.path.join('./log/question_classifier', time.strftime('%Y%m%d_%H%M%S', time.localtime(time.time())))\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.parse import *\n",
    "from pkg.words import *\n",
    "from problems import *\n",
    "\n",
    "problems = [P1_1_1, P1_1_2, P1_1_3, P1_1_4, P1_1_5, P1_1_6, P1_1_7, P1_1_8, P1_1_9, P1_1_10, P1_1_11, P1_1_12, \n",
    "            P1_2_1, P1_2_2, P1_3_1, P1_4_1, \n",
    "            P2_1_1, P2_2_2, P2_3_1, \n",
    "            P3_1_1, P3_2_1, P3_2_2, P3_3_1, \n",
    "            P4_1_1, P4_2_1, P4_2_2, P4_3_1, \n",
    "            P5_1_1, P5_2_1, P5_3_1,\n",
    "            P6_1_1, P6_3_1, P6_4_1,\n",
    "            P7_1_1, P7_1_2, P7_3_1,\n",
    "            P8_1_1, P8_2_1, P8_3_1, \n",
    "            P9_1_1, P9_2_1, P9_2_2, P9_3_1, P9_3_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.trainer_question_classifier import Hyper as hp\n",
    "\n",
    "# dataset\n",
    "hp.add_bos = False\n",
    "hp.add_eos = False\n",
    "\n",
    "hp.batch_size = None\n",
    "hp.batch_type = 'normal'\n",
    "hp.ds_batch_size = 128\n",
    "\n",
    "# train\n",
    "hp.num_workers = 4\n",
    "\n",
    "hp.steps_log = 10\n",
    "hp.steps_eval = 50\n",
    "hp.steps_save = 10000\n",
    "\n",
    "hp.weight_decay = 0.000001\n",
    "hp.initial_lr = 0.0001\n",
    "hp.final_lr = 0.00001\n",
    "hp.lr_decay_factor = 0.99\n",
    "hp.lr_patience = 300\n",
    "hp.ema = 0.99\n",
    "hp.grad_norm_max = 10.0\n",
    "\n",
    "hp.adam_alpha = 2e-4\n",
    "hp.adam_betas = (0.5, 0.9)\n",
    "hp.adam_eps = 1e-6\n",
    "\n",
    "hp.vocab_size = 512\n",
    "hp.add_space_token = False\n",
    "\n",
    "hp.num_problems = len(problems)\n",
    "hp.problems = problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.dataset import ProblemDataset, QuestionDataset, read_questions, write_questions\n",
    "\n",
    "dir_question = 'data/question'\n",
    "_prefix = 'question_'\n",
    "_ids = [0] #, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "path_questions = [os.path.join(dir_question, _prefix + str(i) + '.txt') for i in _ids]\n",
    "metas = [read_questions(path) for path in path_questions]\n",
    "meta = []\n",
    "for m in metas:\n",
    "    meta += m\n",
    "len_train = int(len(meta) * 0.8)\n",
    "meta_train = meta[:len_train]\n",
    "meta_val = meta[len_train:]\n",
    "print(len(meta), len(meta_train), len(meta_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.vocab import Vocab, CharVocab, SPVocab\n",
    "dir_token = 'tokenization'\n",
    "filename = 'prob'\n",
    "filename += '_' + str(hp.vocab_size)\n",
    "if hp.add_space_token:\n",
    "    filename += '_'\n",
    "filename += '.model'\n",
    "path_model = os.path.join(dir_token, filename)\n",
    "print(path_model)\n",
    "vocab = SPVocab(path_model)\n",
    "print(vocab.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dsTrain = QuestionDataset(meta_train, vocab, batch_size=hp.ds_batch_size)\n",
    "dsVal = QuestionDataset(meta_val, vocab, batch_size=hp.ds_batch_size)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pkg.models.model import QuestionClassifier\n",
    "from pkg.models.config import Config\n",
    "from pkg.models.extractor import AverageExtractor, RNNExtractor\n",
    "from pkg.models.encoders_conv import ConvEncoder, HighwayEncoder\n",
    "from pkg.models.embedding import Embed, Regressor\n",
    "\n",
    "cfg = QuestionClassifier.default_config()\n",
    "cfg.text_embed.num_symbols = hp.vocab_size\n",
    "cfg.regressor.num_symbols = hp.num_problems\n",
    "model = cfg.create_object().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pkg.trainer_question_classifier import Trainer_QuestionClassifier\n",
    "\n",
    "trainer = Trainer_QuestionClassifier(model, dsTrain, dsVal, hp=hp, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
